################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 18406 steps/s (collection: 5.101s, learning 0.240s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0083
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 11.3593
                       Mean reward: 0.00
               Mean episode length: 21.93
    Episode_Reward/reaching_object: 0.0003
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0001
          Episode_Reward/joint_vel: -0.0001
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 5.34s
                      Time elapsed: 00:00:05
                               ETA: 02:58:01

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 33849 steps/s (collection: 2.761s, learning 0.143s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 11.3757
                       Mean reward: 0.01
               Mean episode length: 45.42
    Episode_Reward/reaching_object: 0.0012
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0002
          Episode_Reward/joint_vel: -0.0003
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 2.90s
                      Time elapsed: 00:00:08
                               ETA: 02:17:20

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 32990 steps/s (collection: 2.845s, learning 0.135s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 11.3927
                       Mean reward: 0.01
               Mean episode length: 69.07
    Episode_Reward/reaching_object: 0.0021
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0004
          Episode_Reward/joint_vel: -0.0006
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 2.98s
                      Time elapsed: 00:00:11
                               ETA: 02:04:35

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 33366 steps/s (collection: 2.801s, learning 0.145s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 11.4086
                       Mean reward: 0.02
               Mean episode length: 93.12
    Episode_Reward/reaching_object: 0.0032
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0006
          Episode_Reward/joint_vel: -0.0008
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 2.95s
                      Time elapsed: 00:00:14
                               ETA: 01:57:54

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 31976 steps/s (collection: 2.926s, learning 0.149s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 11.3952
                       Mean reward: 0.02
               Mean episode length: 117.84
    Episode_Reward/reaching_object: 0.0046
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0007
          Episode_Reward/joint_vel: -0.0010
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 3.07s
                      Time elapsed: 00:00:17
                               ETA: 01:54:44

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 32421 steps/s (collection: 2.884s, learning 0.148s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 11.4055
                       Mean reward: 0.03
               Mean episode length: 141.19
    Episode_Reward/reaching_object: 0.0065
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0009
          Episode_Reward/joint_vel: -0.0013
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 3.03s
                      Time elapsed: 00:00:20
                               ETA: 01:52:22

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 32014 steps/s (collection: 2.932s, learning 0.139s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 11.4173
                       Mean reward: 0.04
               Mean episode length: 165.18
    Episode_Reward/reaching_object: 0.0084
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0010
          Episode_Reward/joint_vel: -0.0015
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 3.07s
                      Time elapsed: 00:00:23
                               ETA: 01:50:50

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 31583 steps/s (collection: 2.971s, learning 0.141s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 11.4272
                       Mean reward: 0.05
               Mean episode length: 189.88
    Episode_Reward/reaching_object: 0.0109
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0012
          Episode_Reward/joint_vel: -0.0017
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 3.11s
                      Time elapsed: 00:00:26
                               ETA: 01:49:51

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 26496 steps/s (collection: 3.595s, learning 0.115s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 11.4166
                       Mean reward: 0.08
               Mean episode length: 213.51
    Episode_Reward/reaching_object: 0.0133
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0014
          Episode_Reward/joint_vel: -0.0020
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 3.71s
                      Time elapsed: 00:00:30
                               ETA: 01:51:17

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 116934 steps/s (collection: 0.739s, learning 0.102s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 11.4504
                       Mean reward: 0.08
               Mean episode length: 236.90
    Episode_Reward/reaching_object: 0.0174
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0015
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 0.84s
                      Time elapsed: 00:00:31
                               ETA: 01:42:54

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 121517 steps/s (collection: 0.713s, learning 0.096s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 11.4673
                       Mean reward: 0.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0217
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0016
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 0.81s
                      Time elapsed: 00:00:31
                               ETA: 01:35:56

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 118517 steps/s (collection: 0.724s, learning 0.105s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0082
                 Mean entropy loss: 11.4601
                       Mean reward: 0.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0280
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 0.83s
                      Time elapsed: 00:00:32
                               ETA: 01:30:11

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 121866 steps/s (collection: 0.711s, learning 0.096s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0104
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 11.4480
                       Mean reward: 0.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0310
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 0.81s
                      Time elapsed: 00:00:33
                               ETA: 01:25:16

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 119175 steps/s (collection: 0.726s, learning 0.099s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0973
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 11.4562
                       Mean reward: 0.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0470
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 0.82s
                      Time elapsed: 00:00:34
                               ETA: 01:21:05

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 117282 steps/s (collection: 0.745s, learning 0.093s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.2464
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 11.4828
                       Mean reward: 0.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0563
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 0.84s
                      Time elapsed: 00:00:35
                               ETA: 01:17:29

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 113340 steps/s (collection: 0.766s, learning 0.102s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.5743
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 11.5097
                       Mean reward: 0.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0739
     Episode_Reward/lifting_object: 0.0108
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 0.87s
                      Time elapsed: 00:00:35
                               ETA: 01:14:24

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 114828 steps/s (collection: 0.759s, learning 0.097s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.7315
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 11.5666
                       Mean reward: 0.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0954
     Episode_Reward/lifting_object: 0.0556
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 0.86s
                      Time elapsed: 00:00:36
                               ETA: 01:11:39

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 118485 steps/s (collection: 0.739s, learning 0.090s)
             Mean action noise std: 1.04
          Mean value_function loss: 0.4970
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 11.6637
                       Mean reward: 1.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1107
     Episode_Reward/lifting_object: 0.1225
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 0.83s
                      Time elapsed: 00:00:37
                               ETA: 01:09:10

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 113073 steps/s (collection: 0.776s, learning 0.094s)
             Mean action noise std: 1.05
          Mean value_function loss: 0.5777
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 11.7205
                       Mean reward: 1.77
               Mean episode length: 246.36
    Episode_Reward/reaching_object: 0.1252
     Episode_Reward/lifting_object: 0.2219
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 0.87s
                      Time elapsed: 00:00:38
                               ETA: 01:07:00

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 116638 steps/s (collection: 0.753s, learning 0.089s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.7168
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 11.8065
                       Mean reward: 2.10
               Mean episode length: 249.94
    Episode_Reward/reaching_object: 0.1412
     Episode_Reward/lifting_object: 0.2411
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 0.84s
                      Time elapsed: 00:00:39
                               ETA: 01:05:01

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 116700 steps/s (collection: 0.753s, learning 0.090s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.6945
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 11.8966
                       Mean reward: 2.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1496
     Episode_Reward/lifting_object: 0.2008
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0017
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 0.84s
                      Time elapsed: 00:00:40
                               ETA: 01:03:12

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 113186 steps/s (collection: 0.768s, learning 0.101s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.7760
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 11.9550
                       Mean reward: 1.51
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.1621
     Episode_Reward/lifting_object: 0.1740
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 0.87s
                      Time elapsed: 00:00:41
                               ETA: 01:01:36

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 115403 steps/s (collection: 0.750s, learning 0.102s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.4732
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 12.0561
                       Mean reward: 2.63
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.1719
     Episode_Reward/lifting_object: 0.2521
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 0.85s
                      Time elapsed: 00:00:41
                               ETA: 01:00:07

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 115346 steps/s (collection: 0.763s, learning 0.090s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.7901
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 12.1255
                       Mean reward: 2.65
               Mean episode length: 247.74
    Episode_Reward/reaching_object: 0.1747
     Episode_Reward/lifting_object: 0.2956
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 0.85s
                      Time elapsed: 00:00:42
                               ETA: 00:58:45

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 113875 steps/s (collection: 0.760s, learning 0.103s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.6601
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 12.2052
                       Mean reward: 2.71
               Mean episode length: 247.07
    Episode_Reward/reaching_object: 0.1752
     Episode_Reward/lifting_object: 0.4153
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 0.86s
                      Time elapsed: 00:00:43
                               ETA: 00:57:31

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 113883 steps/s (collection: 0.752s, learning 0.111s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.6064
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 12.2524
                       Mean reward: 2.78
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.1744
     Episode_Reward/lifting_object: 0.2679
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 0.86s
                      Time elapsed: 00:00:44
                               ETA: 00:56:22

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 114248 steps/s (collection: 0.753s, learning 0.107s)
             Mean action noise std: 1.13
          Mean value_function loss: 1.1756
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 12.3021
                       Mean reward: 2.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1723
     Episode_Reward/lifting_object: 0.3224
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 0.86s
                      Time elapsed: 00:00:45
                               ETA: 00:55:18

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 111344 steps/s (collection: 0.776s, learning 0.107s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.8509
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 12.3828
                       Mean reward: 2.67
               Mean episode length: 246.33
    Episode_Reward/reaching_object: 0.1718
     Episode_Reward/lifting_object: 0.2805
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0019
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 0.88s
                      Time elapsed: 00:00:46
                               ETA: 00:54:20

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 111055 steps/s (collection: 0.780s, learning 0.106s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.6561
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 12.4603
                       Mean reward: 2.31
               Mean episode length: 246.21
    Episode_Reward/reaching_object: 0.1678
     Episode_Reward/lifting_object: 0.4433
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 0.89s
                      Time elapsed: 00:00:47
                               ETA: 00:53:26

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 111929 steps/s (collection: 0.771s, learning 0.107s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.6449
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 12.5663
                       Mean reward: 4.23
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.1664
     Episode_Reward/lifting_object: 0.4662
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 0.88s
                      Time elapsed: 00:00:48
                               ETA: 00:52:35

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 111913 steps/s (collection: 0.781s, learning 0.097s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.9061
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 12.6471
                       Mean reward: 2.77
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.1667
     Episode_Reward/lifting_object: 0.4427
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0020
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 0.88s
                      Time elapsed: 00:00:48
                               ETA: 00:51:48

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 113169 steps/s (collection: 0.780s, learning 0.089s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.7453
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 12.7003
                       Mean reward: 3.10
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 0.1650
     Episode_Reward/lifting_object: 0.5030
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 0.87s
                      Time elapsed: 00:00:49
                               ETA: 00:51:03

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 106536 steps/s (collection: 0.830s, learning 0.093s)
             Mean action noise std: 1.20
          Mean value_function loss: 1.0340
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 12.7880
                       Mean reward: 3.32
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 0.1611
     Episode_Reward/lifting_object: 0.4713
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 0.92s
                      Time elapsed: 00:00:50
                               ETA: 00:50:23

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 102720 steps/s (collection: 0.862s, learning 0.095s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.9163
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 12.8435
                       Mean reward: 2.96
               Mean episode length: 244.31
    Episode_Reward/reaching_object: 0.1642
     Episode_Reward/lifting_object: 0.4835
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 0.96s
                      Time elapsed: 00:00:51
                               ETA: 00:49:48

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 115394 steps/s (collection: 0.759s, learning 0.093s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.9171
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 12.9156
                       Mean reward: 3.37
               Mean episode length: 248.88
    Episode_Reward/reaching_object: 0.1612
     Episode_Reward/lifting_object: 0.4857
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 0.85s
                      Time elapsed: 00:00:52
                               ETA: 00:49:09

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 115747 steps/s (collection: 0.758s, learning 0.092s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.7429
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 12.9824
                       Mean reward: 4.29
               Mean episode length: 245.34
    Episode_Reward/reaching_object: 0.1588
     Episode_Reward/lifting_object: 0.5638
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0022
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 0.85s
                      Time elapsed: 00:00:53
                               ETA: 00:48:32

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 114918 steps/s (collection: 0.761s, learning 0.094s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.8886
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 13.0756
                       Mean reward: 2.87
               Mean episode length: 246.16
    Episode_Reward/reaching_object: 0.1621
     Episode_Reward/lifting_object: 0.4555
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 0.86s
                      Time elapsed: 00:00:54
                               ETA: 00:47:57

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 112502 steps/s (collection: 0.788s, learning 0.086s)
             Mean action noise std: 1.26
          Mean value_function loss: 3.9169
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 13.1633
                       Mean reward: 1.91
               Mean episode length: 247.37
    Episode_Reward/reaching_object: 0.1598
     Episode_Reward/lifting_object: 0.4269
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 0.87s
                      Time elapsed: 00:00:55
                               ETA: 00:47:25

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 113698 steps/s (collection: 0.766s, learning 0.099s)
             Mean action noise std: 1.26
          Mean value_function loss: 2.8733
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 13.1762
                       Mean reward: 3.14
               Mean episode length: 247.84
    Episode_Reward/reaching_object: 0.1628
     Episode_Reward/lifting_object: 0.5423
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 0.86s
                      Time elapsed: 00:00:55
                               ETA: 00:46:54

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 114154 steps/s (collection: 0.771s, learning 0.090s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.9033
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 13.2373
                       Mean reward: 4.49
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.1561
     Episode_Reward/lifting_object: 0.4468
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 0.86s
                      Time elapsed: 00:00:56
                               ETA: 00:46:25

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 111487 steps/s (collection: 0.777s, learning 0.104s)
             Mean action noise std: 1.28
          Mean value_function loss: 2.5446
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 13.3023
                       Mean reward: 3.66
               Mean episode length: 241.64
    Episode_Reward/reaching_object: 0.1575
     Episode_Reward/lifting_object: 0.4407
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0024
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 0.88s
                      Time elapsed: 00:00:57
                               ETA: 00:45:58

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 108418 steps/s (collection: 0.785s, learning 0.122s)
             Mean action noise std: 1.29
          Mean value_function loss: 1.0693
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 13.3712
                       Mean reward: 4.38
               Mean episode length: 241.55
    Episode_Reward/reaching_object: 0.1568
     Episode_Reward/lifting_object: 0.4292
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 0.91s
                      Time elapsed: 00:00:58
                               ETA: 00:45:33

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 113122 steps/s (collection: 0.763s, learning 0.106s)
             Mean action noise std: 1.30
          Mean value_function loss: 1.1093
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 13.4128
                       Mean reward: 5.40
               Mean episode length: 241.87
    Episode_Reward/reaching_object: 0.1587
     Episode_Reward/lifting_object: 0.6175
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0025
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 0.87s
                      Time elapsed: 00:00:59
                               ETA: 00:45:08

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 110030 steps/s (collection: 0.793s, learning 0.100s)
             Mean action noise std: 1.30
          Mean value_function loss: 1.2546
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 13.4483
                       Mean reward: 3.40
               Mean episode length: 246.36
    Episode_Reward/reaching_object: 0.1573
     Episode_Reward/lifting_object: 0.6345
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 0.89s
                      Time elapsed: 00:01:00
                               ETA: 00:44:44

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 112495 steps/s (collection: 0.775s, learning 0.099s)
             Mean action noise std: 1.31
          Mean value_function loss: 1.0979
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 13.4984
                       Mean reward: 3.39
               Mean episode length: 243.59
    Episode_Reward/reaching_object: 0.1647
     Episode_Reward/lifting_object: 0.6417
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 0.87s
                      Time elapsed: 00:01:01
                               ETA: 00:44:21

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 106618 steps/s (collection: 0.832s, learning 0.090s)
             Mean action noise std: 1.32
          Mean value_function loss: 1.0561
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 13.5709
                       Mean reward: 2.74
               Mean episode length: 246.98
    Episode_Reward/reaching_object: 0.1692
     Episode_Reward/lifting_object: 0.6359
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0026
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 0.92s
                      Time elapsed: 00:01:02
                               ETA: 00:44:01

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 111594 steps/s (collection: 0.787s, learning 0.094s)
             Mean action noise std: 1.33
          Mean value_function loss: 1.2267
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 13.6269
                       Mean reward: 2.35
               Mean episode length: 242.82
    Episode_Reward/reaching_object: 0.1711
     Episode_Reward/lifting_object: 0.5711
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 0.88s
                      Time elapsed: 00:01:03
                               ETA: 00:43:40

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 112896 steps/s (collection: 0.780s, learning 0.091s)
             Mean action noise std: 1.34
          Mean value_function loss: 1.4475
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 13.6634
                       Mean reward: 2.99
               Mean episode length: 244.99
    Episode_Reward/reaching_object: 0.1731
     Episode_Reward/lifting_object: 0.5872
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 0.87s
                      Time elapsed: 00:01:03
                               ETA: 00:43:20

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 111270 steps/s (collection: 0.786s, learning 0.097s)
             Mean action noise std: 1.35
          Mean value_function loss: 1.7458
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 13.7191
                       Mean reward: 4.55
               Mean episode length: 241.76
    Episode_Reward/reaching_object: 0.1772
     Episode_Reward/lifting_object: 0.7483
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 0.88s
                      Time elapsed: 00:01:04
                               ETA: 00:43:01

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 113256 steps/s (collection: 0.774s, learning 0.094s)
             Mean action noise std: 1.36
          Mean value_function loss: 2.9621
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 13.7716
                       Mean reward: 5.40
               Mean episode length: 246.13
    Episode_Reward/reaching_object: 0.1893
     Episode_Reward/lifting_object: 0.7452
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 0.87s
                      Time elapsed: 00:01:05
                               ETA: 00:42:42

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 114514 steps/s (collection: 0.764s, learning 0.095s)
             Mean action noise std: 1.37
          Mean value_function loss: 2.3913
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 13.8398
                       Mean reward: 4.14
               Mean episode length: 239.82
    Episode_Reward/reaching_object: 0.1881
     Episode_Reward/lifting_object: 0.7562
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 0.86s
                      Time elapsed: 00:01:06
                               ETA: 00:42:23

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 114987 steps/s (collection: 0.757s, learning 0.098s)
             Mean action noise std: 1.38
          Mean value_function loss: 2.4814
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 13.8979
                       Mean reward: 5.67
               Mean episode length: 245.09
    Episode_Reward/reaching_object: 0.1951
     Episode_Reward/lifting_object: 0.6858
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 0.85s
                      Time elapsed: 00:01:07
                               ETA: 00:42:05

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 111574 steps/s (collection: 0.789s, learning 0.092s)
             Mean action noise std: 1.39
          Mean value_function loss: 7.3450
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 13.9234
                       Mean reward: 7.43
               Mean episode length: 249.29
    Episode_Reward/reaching_object: 0.1965
     Episode_Reward/lifting_object: 0.9102
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 0.88s
                      Time elapsed: 00:01:08
                               ETA: 00:41:48

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 109305 steps/s (collection: 0.807s, learning 0.092s)
             Mean action noise std: 1.39
          Mean value_function loss: 1.9902
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 13.9769
                       Mean reward: 3.71
               Mean episode length: 243.97
    Episode_Reward/reaching_object: 0.2029
     Episode_Reward/lifting_object: 0.9197
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0029
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 0.90s
                      Time elapsed: 00:01:09
                               ETA: 00:41:33

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 111797 steps/s (collection: 0.788s, learning 0.092s)
             Mean action noise std: 1.40
          Mean value_function loss: 2.4175
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.0064
                       Mean reward: 8.51
               Mean episode length: 240.18
    Episode_Reward/reaching_object: 0.2098
     Episode_Reward/lifting_object: 1.1077
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 0.88s
                      Time elapsed: 00:01:10
                               ETA: 00:41:18

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 112093 steps/s (collection: 0.786s, learning 0.091s)
             Mean action noise std: 1.40
          Mean value_function loss: 3.4229
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.0391
                       Mean reward: 5.63
               Mean episode length: 234.78
    Episode_Reward/reaching_object: 0.2064
     Episode_Reward/lifting_object: 1.0434
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0030
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 0.88s
                      Time elapsed: 00:01:10
                               ETA: 00:41:03

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 111395 steps/s (collection: 0.795s, learning 0.087s)
             Mean action noise std: 1.41
          Mean value_function loss: 2.7502
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.0763
                       Mean reward: 7.28
               Mean episode length: 244.45
    Episode_Reward/reaching_object: 0.2090
     Episode_Reward/lifting_object: 1.0484
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 0.88s
                      Time elapsed: 00:01:11
                               ETA: 00:40:48

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 110430 steps/s (collection: 0.790s, learning 0.100s)
             Mean action noise std: 1.41
          Mean value_function loss: 2.8901
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.1040
                       Mean reward: 8.25
               Mean episode length: 233.41
    Episode_Reward/reaching_object: 0.2149
     Episode_Reward/lifting_object: 1.3399
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 0.89s
                      Time elapsed: 00:01:12
                               ETA: 00:40:35

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 108573 steps/s (collection: 0.802s, learning 0.103s)
             Mean action noise std: 1.42
          Mean value_function loss: 3.7219
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 14.1316
                       Mean reward: 8.05
               Mean episode length: 237.93
    Episode_Reward/reaching_object: 0.2134
     Episode_Reward/lifting_object: 1.4220
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0031
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 0.91s
                      Time elapsed: 00:01:13
                               ETA: 00:40:22

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 110766 steps/s (collection: 0.784s, learning 0.103s)
             Mean action noise std: 1.43
          Mean value_function loss: 3.1756
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.1691
                       Mean reward: 9.53
               Mean episode length: 239.84
    Episode_Reward/reaching_object: 0.2140
     Episode_Reward/lifting_object: 1.4125
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 0.89s
                      Time elapsed: 00:01:14
                               ETA: 00:40:09

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 111986 steps/s (collection: 0.777s, learning 0.101s)
             Mean action noise std: 1.43
          Mean value_function loss: 3.2156
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.1987
                       Mean reward: 9.05
               Mean episode length: 239.62
    Episode_Reward/reaching_object: 0.2210
     Episode_Reward/lifting_object: 1.5200
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0032
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 0.88s
                      Time elapsed: 00:01:15
                               ETA: 00:39:56

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 112181 steps/s (collection: 0.781s, learning 0.096s)
             Mean action noise std: 1.43
          Mean value_function loss: 3.6065
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.2120
                       Mean reward: 9.81
               Mean episode length: 239.10
    Episode_Reward/reaching_object: 0.2230
     Episode_Reward/lifting_object: 1.4922
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 0.88s
                      Time elapsed: 00:01:16
                               ETA: 00:39:44

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 111013 steps/s (collection: 0.785s, learning 0.101s)
             Mean action noise std: 1.44
          Mean value_function loss: 4.5609
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.2282
                       Mean reward: 10.03
               Mean episode length: 242.90
    Episode_Reward/reaching_object: 0.2200
     Episode_Reward/lifting_object: 1.6752
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 0.89s
                      Time elapsed: 00:01:17
                               ETA: 00:39:32

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 109129 steps/s (collection: 0.798s, learning 0.103s)
             Mean action noise std: 1.44
          Mean value_function loss: 4.1265
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.2431
                       Mean reward: 8.79
               Mean episode length: 240.08
    Episode_Reward/reaching_object: 0.2213
     Episode_Reward/lifting_object: 1.5937
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 0.90s
                      Time elapsed: 00:01:18
                               ETA: 00:39:21

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 109862 steps/s (collection: 0.799s, learning 0.096s)
             Mean action noise std: 1.44
          Mean value_function loss: 6.4408
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.2556
                       Mean reward: 10.46
               Mean episode length: 239.72
    Episode_Reward/reaching_object: 0.2171
     Episode_Reward/lifting_object: 1.7397
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 0.89s
                      Time elapsed: 00:01:18
                               ETA: 00:39:10

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 108665 steps/s (collection: 0.808s, learning 0.097s)
             Mean action noise std: 1.45
          Mean value_function loss: 4.3586
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.2810
                       Mean reward: 7.99
               Mean episode length: 234.75
    Episode_Reward/reaching_object: 0.2193
     Episode_Reward/lifting_object: 1.6301
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0034
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 0.90s
                      Time elapsed: 00:01:19
                               ETA: 00:39:00

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 111457 steps/s (collection: 0.787s, learning 0.095s)
             Mean action noise std: 1.45
          Mean value_function loss: 5.8858
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.3074
                       Mean reward: 9.73
               Mean episode length: 240.96
    Episode_Reward/reaching_object: 0.2261
     Episode_Reward/lifting_object: 1.9513
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 0.88s
                      Time elapsed: 00:01:20
                               ETA: 00:38:49

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 111281 steps/s (collection: 0.784s, learning 0.099s)
             Mean action noise std: 1.45
          Mean value_function loss: 5.6323
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.3266
                       Mean reward: 11.75
               Mean episode length: 239.71
    Episode_Reward/reaching_object: 0.2256
     Episode_Reward/lifting_object: 2.0711
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 0.88s
                      Time elapsed: 00:01:21
                               ETA: 00:38:39

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 107234 steps/s (collection: 0.810s, learning 0.107s)
             Mean action noise std: 1.46
          Mean value_function loss: 5.1155
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 14.3449
                       Mean reward: 11.06
               Mean episode length: 244.09
    Episode_Reward/reaching_object: 0.2282
     Episode_Reward/lifting_object: 2.3164
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 0.92s
                      Time elapsed: 00:01:22
                               ETA: 00:38:30

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 107623 steps/s (collection: 0.805s, learning 0.109s)
             Mean action noise std: 1.46
          Mean value_function loss: 7.3065
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.3559
                       Mean reward: 13.27
               Mean episode length: 238.62
    Episode_Reward/reaching_object: 0.2295
     Episode_Reward/lifting_object: 1.9829
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 0.91s
                      Time elapsed: 00:01:23
                               ETA: 00:38:21

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 105171 steps/s (collection: 0.838s, learning 0.097s)
             Mean action noise std: 1.46
          Mean value_function loss: 5.8565
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.3688
                       Mean reward: 15.71
               Mean episode length: 238.30
    Episode_Reward/reaching_object: 0.2335
     Episode_Reward/lifting_object: 2.7048
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0035
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 0.93s
                      Time elapsed: 00:01:24
                               ETA: 00:38:12

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 112406 steps/s (collection: 0.784s, learning 0.091s)
             Mean action noise std: 1.47
          Mean value_function loss: 8.6127
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.3896
                       Mean reward: 14.96
               Mean episode length: 239.96
    Episode_Reward/reaching_object: 0.2329
     Episode_Reward/lifting_object: 2.7536
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 0.87s
                      Time elapsed: 00:01:25
                               ETA: 00:38:03

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 112653 steps/s (collection: 0.786s, learning 0.087s)
             Mean action noise std: 1.47
          Mean value_function loss: 7.0997
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.4043
                       Mean reward: 10.63
               Mean episode length: 243.69
    Episode_Reward/reaching_object: 0.2259
     Episode_Reward/lifting_object: 2.3999
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 0.87s
                      Time elapsed: 00:01:26
                               ETA: 00:37:53

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 110080 steps/s (collection: 0.789s, learning 0.104s)
             Mean action noise std: 1.47
          Mean value_function loss: 5.8872
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 14.4130
                       Mean reward: 13.82
               Mean episode length: 239.67
    Episode_Reward/reaching_object: 0.2314
     Episode_Reward/lifting_object: 2.3581
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 0.89s
                      Time elapsed: 00:01:26
                               ETA: 00:37:45

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 110530 steps/s (collection: 0.784s, learning 0.106s)
             Mean action noise std: 1.47
          Mean value_function loss: 7.6873
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 14.4211
                       Mean reward: 13.84
               Mean episode length: 230.30
    Episode_Reward/reaching_object: 0.2243
     Episode_Reward/lifting_object: 2.7463
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0036
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 0.89s
                      Time elapsed: 00:01:27
                               ETA: 00:37:36

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 110322 steps/s (collection: 0.795s, learning 0.096s)
             Mean action noise std: 1.47
          Mean value_function loss: 6.5851
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.4293
                       Mean reward: 15.47
               Mean episode length: 237.17
    Episode_Reward/reaching_object: 0.2286
     Episode_Reward/lifting_object: 3.2713
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 0.89s
                      Time elapsed: 00:01:28
                               ETA: 00:37:28

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 110861 steps/s (collection: 0.790s, learning 0.097s)
             Mean action noise std: 1.48
          Mean value_function loss: 6.4965
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.4353
                       Mean reward: 16.17
               Mean episode length: 236.98
    Episode_Reward/reaching_object: 0.2277
     Episode_Reward/lifting_object: 2.8038
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 0.89s
                      Time elapsed: 00:01:29
                               ETA: 00:37:20

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 111754 steps/s (collection: 0.778s, learning 0.102s)
             Mean action noise std: 1.48
          Mean value_function loss: 6.6946
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 14.4417
                       Mean reward: 19.42
               Mean episode length: 241.59
    Episode_Reward/reaching_object: 0.2341
     Episode_Reward/lifting_object: 3.1747
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 0.88s
                      Time elapsed: 00:01:30
                               ETA: 00:37:12

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 112694 steps/s (collection: 0.778s, learning 0.095s)
             Mean action noise std: 1.48
          Mean value_function loss: 11.5966
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.4435
                       Mean reward: 15.02
               Mean episode length: 230.50
    Episode_Reward/reaching_object: 0.2320
     Episode_Reward/lifting_object: 3.0871
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 0.87s
                      Time elapsed: 00:01:31
                               ETA: 00:37:03

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 110823 steps/s (collection: 0.799s, learning 0.088s)
             Mean action noise std: 1.48
          Mean value_function loss: 7.9590
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.4684
                       Mean reward: 17.28
               Mean episode length: 233.97
    Episode_Reward/reaching_object: 0.2317
     Episode_Reward/lifting_object: 3.1523
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0037
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 0.89s
                      Time elapsed: 00:01:32
                               ETA: 00:36:56

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 111208 steps/s (collection: 0.793s, learning 0.091s)
             Mean action noise std: 1.49
          Mean value_function loss: 14.2531
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.4816
                       Mean reward: 17.87
               Mean episode length: 241.38
    Episode_Reward/reaching_object: 0.2346
     Episode_Reward/lifting_object: 3.4137
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 0.88s
                      Time elapsed: 00:01:33
                               ETA: 00:36:48

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 114138 steps/s (collection: 0.768s, learning 0.093s)
             Mean action noise std: 1.49
          Mean value_function loss: 9.7666
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 14.4917
                       Mean reward: 17.84
               Mean episode length: 240.43
    Episode_Reward/reaching_object: 0.2292
     Episode_Reward/lifting_object: 3.4379
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 0.86s
                      Time elapsed: 00:01:34
                               ETA: 00:36:40

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 111490 steps/s (collection: 0.786s, learning 0.096s)
             Mean action noise std: 1.49
          Mean value_function loss: 11.9341
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.4989
                       Mean reward: 18.88
               Mean episode length: 234.02
    Episode_Reward/reaching_object: 0.2298
     Episode_Reward/lifting_object: 3.7139
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 0.88s
                      Time elapsed: 00:01:34
                               ETA: 00:36:33

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 111069 steps/s (collection: 0.799s, learning 0.086s)
             Mean action noise std: 1.49
          Mean value_function loss: 9.0771
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 14.5113
                       Mean reward: 18.12
               Mean episode length: 233.24
    Episode_Reward/reaching_object: 0.2340
     Episode_Reward/lifting_object: 3.3955
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 0.89s
                      Time elapsed: 00:01:35
                               ETA: 00:36:26

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 110343 steps/s (collection: 0.798s, learning 0.093s)
             Mean action noise std: 1.49
          Mean value_function loss: 8.8946
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.5119
                       Mean reward: 18.27
               Mean episode length: 226.33
    Episode_Reward/reaching_object: 0.2373
     Episode_Reward/lifting_object: 3.8257
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 0.89s
                      Time elapsed: 00:01:36
                               ETA: 00:36:19

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 107886 steps/s (collection: 0.814s, learning 0.098s)
             Mean action noise std: 1.49
          Mean value_function loss: 8.9193
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.5084
                       Mean reward: 24.42
               Mean episode length: 233.80
    Episode_Reward/reaching_object: 0.2372
     Episode_Reward/lifting_object: 4.4304
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 0.91s
                      Time elapsed: 00:01:37
                               ETA: 00:36:13

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 106295 steps/s (collection: 0.819s, learning 0.106s)
             Mean action noise std: 1.50
          Mean value_function loss: 7.6194
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.5183
                       Mean reward: 19.64
               Mean episode length: 241.12
    Episode_Reward/reaching_object: 0.2436
     Episode_Reward/lifting_object: 4.1323
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 0.92s
                      Time elapsed: 00:01:38
                               ETA: 00:36:07

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 107639 steps/s (collection: 0.819s, learning 0.094s)
             Mean action noise std: 1.50
          Mean value_function loss: 8.9956
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 14.5291
                       Mean reward: 21.40
               Mean episode length: 236.73
    Episode_Reward/reaching_object: 0.2416
     Episode_Reward/lifting_object: 4.1366
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 0.91s
                      Time elapsed: 00:01:39
                               ETA: 00:36:01

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 113313 steps/s (collection: 0.777s, learning 0.091s)
             Mean action noise std: 1.50
          Mean value_function loss: 8.9771
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.5317
                       Mean reward: 20.38
               Mean episode length: 228.62
    Episode_Reward/reaching_object: 0.2403
     Episode_Reward/lifting_object: 4.3806
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 0.87s
                      Time elapsed: 00:01:40
                               ETA: 00:35:55

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 111790 steps/s (collection: 0.786s, learning 0.093s)
             Mean action noise std: 1.50
          Mean value_function loss: 11.2380
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 14.5476
                       Mean reward: 19.90
               Mean episode length: 235.73
    Episode_Reward/reaching_object: 0.2577
     Episode_Reward/lifting_object: 4.1450
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 0.88s
                      Time elapsed: 00:01:41
                               ETA: 00:35:48

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 105417 steps/s (collection: 0.833s, learning 0.099s)
             Mean action noise std: 1.51
          Mean value_function loss: 13.6375
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.5593
                       Mean reward: 21.76
               Mean episode length: 238.77
    Episode_Reward/reaching_object: 0.2461
     Episode_Reward/lifting_object: 4.0631
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 0.93s
                      Time elapsed: 00:01:42
                               ETA: 00:35:43

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 109269 steps/s (collection: 0.811s, learning 0.089s)
             Mean action noise std: 1.51
          Mean value_function loss: 13.0985
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 14.5704
                       Mean reward: 22.89
               Mean episode length: 228.89
    Episode_Reward/reaching_object: 0.2605
     Episode_Reward/lifting_object: 5.2051
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 0.90s
                      Time elapsed: 00:01:43
                               ETA: 00:35:37

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 107998 steps/s (collection: 0.819s, learning 0.092s)
             Mean action noise std: 1.51
          Mean value_function loss: 11.5280
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 14.5733
                       Mean reward: 31.98
               Mean episode length: 237.91
    Episode_Reward/reaching_object: 0.2542
     Episode_Reward/lifting_object: 4.9616
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 0.91s
                      Time elapsed: 00:01:43
                               ETA: 00:35:32

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 109278 steps/s (collection: 0.792s, learning 0.108s)
             Mean action noise std: 1.51
          Mean value_function loss: 15.4339
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 14.5751
                       Mean reward: 23.26
               Mean episode length: 233.16
    Episode_Reward/reaching_object: 0.2547
     Episode_Reward/lifting_object: 4.8590
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 0.90s
                      Time elapsed: 00:01:44
                               ETA: 00:35:26

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 108700 steps/s (collection: 0.795s, learning 0.109s)
             Mean action noise std: 1.51
          Mean value_function loss: 13.4434
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 14.5820
                       Mean reward: 29.03
               Mean episode length: 234.23
    Episode_Reward/reaching_object: 0.2526
     Episode_Reward/lifting_object: 4.8222
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 0.90s
                      Time elapsed: 00:01:45
                               ETA: 00:35:21

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 104371 steps/s (collection: 0.826s, learning 0.116s)
             Mean action noise std: 1.52
          Mean value_function loss: 15.2503
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 14.5999
                       Mean reward: 33.55
               Mean episode length: 231.66
    Episode_Reward/reaching_object: 0.2547
     Episode_Reward/lifting_object: 5.4963
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0039
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 0.94s
                      Time elapsed: 00:01:46
                               ETA: 00:35:17

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 103780 steps/s (collection: 0.836s, learning 0.112s)
             Mean action noise std: 1.52
          Mean value_function loss: 12.9537
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.6182
                       Mean reward: 29.54
               Mean episode length: 222.91
    Episode_Reward/reaching_object: 0.2584
     Episode_Reward/lifting_object: 5.1353
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 0.95s
                      Time elapsed: 00:01:47
                               ETA: 00:35:12

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 108245 steps/s (collection: 0.797s, learning 0.112s)
             Mean action noise std: 1.52
          Mean value_function loss: 11.4015
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 14.6187
                       Mean reward: 27.34
               Mean episode length: 236.29
    Episode_Reward/reaching_object: 0.2601
     Episode_Reward/lifting_object: 5.2953
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 0.91s
                      Time elapsed: 00:01:48
                               ETA: 00:35:07

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 109158 steps/s (collection: 0.795s, learning 0.106s)
             Mean action noise std: 1.52
          Mean value_function loss: 13.8350
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 14.6215
                       Mean reward: 26.24
               Mean episode length: 234.85
    Episode_Reward/reaching_object: 0.2577
     Episode_Reward/lifting_object: 5.2774
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 0.90s
                      Time elapsed: 00:01:49
                               ETA: 00:35:02

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 107868 steps/s (collection: 0.794s, learning 0.117s)
             Mean action noise std: 1.52
          Mean value_function loss: 18.9565
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 14.6236
                       Mean reward: 26.85
               Mean episode length: 236.97
    Episode_Reward/reaching_object: 0.2566
     Episode_Reward/lifting_object: 5.4188
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 0.91s
                      Time elapsed: 00:01:50
                               ETA: 00:34:57

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 110924 steps/s (collection: 0.790s, learning 0.096s)
             Mean action noise std: 1.52
          Mean value_function loss: 17.7180
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 14.6238
                       Mean reward: 29.00
               Mean episode length: 236.28
    Episode_Reward/reaching_object: 0.2678
     Episode_Reward/lifting_object: 5.9007
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 0.89s
                      Time elapsed: 00:01:51
                               ETA: 00:34:52

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 107934 steps/s (collection: 0.822s, learning 0.089s)
             Mean action noise std: 1.52
          Mean value_function loss: 21.2347
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.6242
                       Mean reward: 23.44
               Mean episode length: 223.50
    Episode_Reward/reaching_object: 0.2558
     Episode_Reward/lifting_object: 4.8165
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 0.91s
                      Time elapsed: 00:01:52
                               ETA: 00:34:47

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 103236 steps/s (collection: 0.848s, learning 0.105s)
             Mean action noise std: 1.52
          Mean value_function loss: 15.3547
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 14.6270
                       Mean reward: 26.74
               Mean episode length: 231.02
    Episode_Reward/reaching_object: 0.2578
     Episode_Reward/lifting_object: 5.8597
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 0.95s
                      Time elapsed: 00:01:53
                               ETA: 00:34:44

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 110496 steps/s (collection: 0.795s, learning 0.095s)
             Mean action noise std: 1.52
          Mean value_function loss: 13.6954
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.6309
                       Mean reward: 28.30
               Mean episode length: 234.36
    Episode_Reward/reaching_object: 0.2636
     Episode_Reward/lifting_object: 5.3906
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 0.89s
                      Time elapsed: 00:01:53
                               ETA: 00:34:39

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 109661 steps/s (collection: 0.804s, learning 0.092s)
             Mean action noise std: 1.52
          Mean value_function loss: 17.4769
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.6340
                       Mean reward: 33.10
               Mean episode length: 227.79
    Episode_Reward/reaching_object: 0.2566
     Episode_Reward/lifting_object: 5.6187
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 0.90s
                      Time elapsed: 00:01:54
                               ETA: 00:34:34

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 111792 steps/s (collection: 0.788s, learning 0.091s)
             Mean action noise std: 1.52
          Mean value_function loss: 20.7452
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.6358
                       Mean reward: 25.99
               Mean episode length: 219.08
    Episode_Reward/reaching_object: 0.2526
     Episode_Reward/lifting_object: 5.7127
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 0.88s
                      Time elapsed: 00:01:55
                               ETA: 00:34:29

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 110792 steps/s (collection: 0.793s, learning 0.094s)
             Mean action noise std: 1.53
          Mean value_function loss: 19.5727
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 14.6415
                       Mean reward: 28.43
               Mean episode length: 230.33
    Episode_Reward/reaching_object: 0.2546
     Episode_Reward/lifting_object: 6.0406
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 0.89s
                      Time elapsed: 00:01:56
                               ETA: 00:34:24

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 112427 steps/s (collection: 0.778s, learning 0.096s)
             Mean action noise std: 1.53
          Mean value_function loss: 15.4237
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 14.6466
                       Mean reward: 29.43
               Mean episode length: 225.67
    Episode_Reward/reaching_object: 0.2508
     Episode_Reward/lifting_object: 5.3032
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 0.87s
                      Time elapsed: 00:01:57
                               ETA: 00:34:20

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 111741 steps/s (collection: 0.784s, learning 0.096s)
             Mean action noise std: 1.53
          Mean value_function loss: 19.9488
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.6584
                       Mean reward: 29.75
               Mean episode length: 237.91
    Episode_Reward/reaching_object: 0.2549
     Episode_Reward/lifting_object: 6.2566
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 0.88s
                      Time elapsed: 00:01:58
                               ETA: 00:34:15

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 111833 steps/s (collection: 0.790s, learning 0.089s)
             Mean action noise std: 1.53
          Mean value_function loss: 16.8587
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.6687
                       Mean reward: 27.41
               Mean episode length: 229.93
    Episode_Reward/reaching_object: 0.2606
     Episode_Reward/lifting_object: 5.7602
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 0.88s
                      Time elapsed: 00:01:59
                               ETA: 00:34:10

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 110237 steps/s (collection: 0.799s, learning 0.092s)
             Mean action noise std: 1.53
          Mean value_function loss: 17.4120
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 14.6781
                       Mean reward: 36.78
               Mean episode length: 230.64
    Episode_Reward/reaching_object: 0.2577
     Episode_Reward/lifting_object: 6.1392
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 0.89s
                      Time elapsed: 00:02:00
                               ETA: 00:34:06

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 107862 steps/s (collection: 0.815s, learning 0.097s)
             Mean action noise std: 1.53
          Mean value_function loss: 17.6869
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 14.6835
                       Mean reward: 33.46
               Mean episode length: 216.32
    Episode_Reward/reaching_object: 0.2571
     Episode_Reward/lifting_object: 6.2698
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 0.91s
                      Time elapsed: 00:02:01
                               ETA: 00:34:02

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 108988 steps/s (collection: 0.814s, learning 0.088s)
             Mean action noise std: 1.54
          Mean value_function loss: 18.3303
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 14.6863
                       Mean reward: 29.64
               Mean episode length: 222.67
    Episode_Reward/reaching_object: 0.2569
     Episode_Reward/lifting_object: 5.5785
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 0.90s
                      Time elapsed: 00:02:01
                               ETA: 00:33:58

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 111881 steps/s (collection: 0.793s, learning 0.086s)
             Mean action noise std: 1.54
          Mean value_function loss: 23.9865
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 14.6887
                       Mean reward: 28.31
               Mean episode length: 228.77
    Episode_Reward/reaching_object: 0.2561
     Episode_Reward/lifting_object: 6.1526
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0041
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 0.88s
                      Time elapsed: 00:02:02
                               ETA: 00:33:53

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 109379 steps/s (collection: 0.807s, learning 0.092s)
             Mean action noise std: 1.54
          Mean value_function loss: 17.1064
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 14.6916
                       Mean reward: 30.59
               Mean episode length: 234.50
    Episode_Reward/reaching_object: 0.2616
     Episode_Reward/lifting_object: 5.9150
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 0.90s
                      Time elapsed: 00:02:03
                               ETA: 00:33:49

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 109615 steps/s (collection: 0.805s, learning 0.092s)
             Mean action noise std: 1.54
          Mean value_function loss: 16.7168
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 14.6929
                       Mean reward: 28.16
               Mean episode length: 230.25
    Episode_Reward/reaching_object: 0.2593
     Episode_Reward/lifting_object: 5.8283
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 0.90s
                      Time elapsed: 00:02:04
                               ETA: 00:33:45

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 110157 steps/s (collection: 0.806s, learning 0.086s)
             Mean action noise std: 1.54
          Mean value_function loss: 15.9007
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.6931
                       Mean reward: 34.93
               Mean episode length: 233.27
    Episode_Reward/reaching_object: 0.2597
     Episode_Reward/lifting_object: 6.4499
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 0.89s
                      Time elapsed: 00:02:05
                               ETA: 00:33:41

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 113622 steps/s (collection: 0.777s, learning 0.088s)
             Mean action noise std: 1.54
          Mean value_function loss: 18.4970
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 14.6956
                       Mean reward: 32.08
               Mean episode length: 226.74
    Episode_Reward/reaching_object: 0.2637
     Episode_Reward/lifting_object: 6.9771
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 0.87s
                      Time elapsed: 00:02:06
                               ETA: 00:33:37

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 109159 steps/s (collection: 0.801s, learning 0.100s)
             Mean action noise std: 1.54
          Mean value_function loss: 17.0471
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 14.6979
                       Mean reward: 36.95
               Mean episode length: 221.49
    Episode_Reward/reaching_object: 0.2624
     Episode_Reward/lifting_object: 6.9188
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 0.90s
                      Time elapsed: 00:02:07
                               ETA: 00:33:33

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 109002 steps/s (collection: 0.811s, learning 0.091s)
             Mean action noise std: 1.54
          Mean value_function loss: 18.8460
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.7048
                       Mean reward: 35.14
               Mean episode length: 227.59
    Episode_Reward/reaching_object: 0.2646
     Episode_Reward/lifting_object: 6.8086
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0042
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 0.90s
                      Time elapsed: 00:02:08
                               ETA: 00:33:29

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 109926 steps/s (collection: 0.797s, learning 0.097s)
             Mean action noise std: 1.54
          Mean value_function loss: 19.8899
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 14.7146
                       Mean reward: 34.52
               Mean episode length: 233.81
    Episode_Reward/reaching_object: 0.2620
     Episode_Reward/lifting_object: 6.7214
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 0.89s
                      Time elapsed: 00:02:09
                               ETA: 00:33:26

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 110622 steps/s (collection: 0.788s, learning 0.100s)
             Mean action noise std: 1.55
          Mean value_function loss: 22.3352
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 14.7239
                       Mean reward: 34.33
               Mean episode length: 227.07
    Episode_Reward/reaching_object: 0.2691
     Episode_Reward/lifting_object: 6.2486
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 0.89s
                      Time elapsed: 00:02:10
                               ETA: 00:33:22

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 107887 steps/s (collection: 0.810s, learning 0.102s)
             Mean action noise std: 1.55
          Mean value_function loss: 17.6465
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.7401
                       Mean reward: 32.15
               Mean episode length: 233.81
    Episode_Reward/reaching_object: 0.2692
     Episode_Reward/lifting_object: 6.3680
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 0.91s
                      Time elapsed: 00:02:10
                               ETA: 00:33:18

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 109667 steps/s (collection: 0.801s, learning 0.096s)
             Mean action noise std: 1.55
          Mean value_function loss: 18.7211
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.7543
                       Mean reward: 32.79
               Mean episode length: 225.77
    Episode_Reward/reaching_object: 0.2754
     Episode_Reward/lifting_object: 6.8458
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 0.90s
                      Time elapsed: 00:02:11
                               ETA: 00:33:15

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 110169 steps/s (collection: 0.796s, learning 0.097s)
             Mean action noise std: 1.55
          Mean value_function loss: 20.6556
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 14.7574
                       Mean reward: 37.36
               Mean episode length: 225.60
    Episode_Reward/reaching_object: 0.2740
     Episode_Reward/lifting_object: 6.6897
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 0.89s
                      Time elapsed: 00:02:12
                               ETA: 00:33:11

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 109286 steps/s (collection: 0.798s, learning 0.101s)
             Mean action noise std: 1.55
          Mean value_function loss: 20.4317
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 14.7606
                       Mean reward: 33.36
               Mean episode length: 233.92
    Episode_Reward/reaching_object: 0.2754
     Episode_Reward/lifting_object: 6.1905
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 0.90s
                      Time elapsed: 00:02:13
                               ETA: 00:33:08

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 110095 steps/s (collection: 0.781s, learning 0.112s)
             Mean action noise std: 1.55
          Mean value_function loss: 21.8123
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 14.7680
                       Mean reward: 34.51
               Mean episode length: 229.54
    Episode_Reward/reaching_object: 0.2781
     Episode_Reward/lifting_object: 6.1360
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 0.89s
                      Time elapsed: 00:02:14
                               ETA: 00:33:04

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 109769 steps/s (collection: 0.787s, learning 0.108s)
             Mean action noise std: 1.56
          Mean value_function loss: 20.3487
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.7708
                       Mean reward: 39.67
               Mean episode length: 224.09
    Episode_Reward/reaching_object: 0.2784
     Episode_Reward/lifting_object: 7.2354
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 0.90s
                      Time elapsed: 00:02:15
                               ETA: 00:33:01

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 109523 steps/s (collection: 0.792s, learning 0.106s)
             Mean action noise std: 1.56
          Mean value_function loss: 23.3064
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.7749
                       Mean reward: 29.68
               Mean episode length: 222.26
    Episode_Reward/reaching_object: 0.2827
     Episode_Reward/lifting_object: 6.1531
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 0.90s
                      Time elapsed: 00:02:16
                               ETA: 00:32:57

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 107800 steps/s (collection: 0.807s, learning 0.105s)
             Mean action noise std: 1.56
          Mean value_function loss: 20.6205
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.7787
                       Mean reward: 34.94
               Mean episode length: 227.10
    Episode_Reward/reaching_object: 0.2796
     Episode_Reward/lifting_object: 6.7695
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 0.91s
                      Time elapsed: 00:02:17
                               ETA: 00:32:54

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 104666 steps/s (collection: 0.839s, learning 0.100s)
             Mean action noise std: 1.56
          Mean value_function loss: 26.7259
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.7751
                       Mean reward: 36.55
               Mean episode length: 235.58
    Episode_Reward/reaching_object: 0.2848
     Episode_Reward/lifting_object: 7.2238
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 0.94s
                      Time elapsed: 00:02:18
                               ETA: 00:32:52

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 107862 steps/s (collection: 0.812s, learning 0.100s)
             Mean action noise std: 1.56
          Mean value_function loss: 24.7357
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.7673
                       Mean reward: 39.78
               Mean episode length: 216.54
    Episode_Reward/reaching_object: 0.2809
     Episode_Reward/lifting_object: 7.5682
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 0.91s
                      Time elapsed: 00:02:19
                               ETA: 00:32:48

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 113530 steps/s (collection: 0.775s, learning 0.091s)
             Mean action noise std: 1.56
          Mean value_function loss: 26.8022
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.7681
                       Mean reward: 39.62
               Mean episode length: 216.71
    Episode_Reward/reaching_object: 0.2800
     Episode_Reward/lifting_object: 7.7456
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 0.87s
                      Time elapsed: 00:02:19
                               ETA: 00:32:45

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 108965 steps/s (collection: 0.794s, learning 0.108s)
             Mean action noise std: 1.56
          Mean value_function loss: 28.2595
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 14.7718
                       Mean reward: 42.95
               Mean episode length: 225.67
    Episode_Reward/reaching_object: 0.2810
     Episode_Reward/lifting_object: 7.8237
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 0.90s
                      Time elapsed: 00:02:20
                               ETA: 00:32:42

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 112436 steps/s (collection: 0.783s, learning 0.091s)
             Mean action noise std: 1.56
          Mean value_function loss: 33.2617
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 14.7769
                       Mean reward: 37.96
               Mean episode length: 220.84
    Episode_Reward/reaching_object: 0.2780
     Episode_Reward/lifting_object: 7.6942
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 0.87s
                      Time elapsed: 00:02:21
                               ETA: 00:32:38

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 104094 steps/s (collection: 0.843s, learning 0.102s)
             Mean action noise std: 1.56
          Mean value_function loss: 33.3536
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 14.7789
                       Mean reward: 55.62
               Mean episode length: 214.25
    Episode_Reward/reaching_object: 0.2753
     Episode_Reward/lifting_object: 8.7996
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0044
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 0.94s
                      Time elapsed: 00:02:22
                               ETA: 00:32:36

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 107849 steps/s (collection: 0.823s, learning 0.088s)
             Mean action noise std: 1.56
          Mean value_function loss: 36.4482
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.7811
                       Mean reward: 46.22
               Mean episode length: 213.29
    Episode_Reward/reaching_object: 0.2785
     Episode_Reward/lifting_object: 8.7596
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 0.91s
                      Time elapsed: 00:02:23
                               ETA: 00:32:33

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 111222 steps/s (collection: 0.785s, learning 0.099s)
             Mean action noise std: 1.56
          Mean value_function loss: 41.7489
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.7827
                       Mean reward: 60.35
               Mean episode length: 225.46
    Episode_Reward/reaching_object: 0.2772
     Episode_Reward/lifting_object: 9.4419
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 0.88s
                      Time elapsed: 00:02:24
                               ETA: 00:32:29

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 113469 steps/s (collection: 0.775s, learning 0.091s)
             Mean action noise std: 1.56
          Mean value_function loss: 37.7720
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.7832
                       Mean reward: 49.38
               Mean episode length: 224.84
    Episode_Reward/reaching_object: 0.2820
     Episode_Reward/lifting_object: 8.5734
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 0.87s
                      Time elapsed: 00:02:25
                               ETA: 00:32:26

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 108874 steps/s (collection: 0.808s, learning 0.095s)
             Mean action noise std: 1.56
          Mean value_function loss: 43.5723
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 14.7811
                       Mean reward: 44.34
               Mean episode length: 224.31
    Episode_Reward/reaching_object: 0.2784
     Episode_Reward/lifting_object: 9.0906
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 0.90s
                      Time elapsed: 00:02:26
                               ETA: 00:32:23

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 109832 steps/s (collection: 0.802s, learning 0.093s)
             Mean action noise std: 1.56
          Mean value_function loss: 52.3432
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 14.7867
                       Mean reward: 48.96
               Mean episode length: 228.93
    Episode_Reward/reaching_object: 0.2860
     Episode_Reward/lifting_object: 9.8124
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 0.90s
                      Time elapsed: 00:02:27
                               ETA: 00:32:20

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 110297 steps/s (collection: 0.797s, learning 0.095s)
             Mean action noise std: 1.56
          Mean value_function loss: 44.5739
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.7895
                       Mean reward: 54.25
               Mean episode length: 230.42
    Episode_Reward/reaching_object: 0.2811
     Episode_Reward/lifting_object: 9.4309
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 0.89s
                      Time elapsed: 00:02:27
                               ETA: 00:32:17

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 110491 steps/s (collection: 0.798s, learning 0.092s)
             Mean action noise std: 1.57
          Mean value_function loss: 50.5998
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.7943
                       Mean reward: 44.58
               Mean episode length: 222.23
    Episode_Reward/reaching_object: 0.2788
     Episode_Reward/lifting_object: 9.7351
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 0.89s
                      Time elapsed: 00:02:28
                               ETA: 00:32:14

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 109347 steps/s (collection: 0.811s, learning 0.088s)
             Mean action noise std: 1.56
          Mean value_function loss: 42.2890
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.7974
                       Mean reward: 57.32
               Mean episode length: 222.93
    Episode_Reward/reaching_object: 0.2767
     Episode_Reward/lifting_object: 9.9656
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 0.90s
                      Time elapsed: 00:02:29
                               ETA: 00:32:11

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 110662 steps/s (collection: 0.796s, learning 0.092s)
             Mean action noise std: 1.56
          Mean value_function loss: 50.0380
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.7898
                       Mean reward: 54.96
               Mean episode length: 220.54
    Episode_Reward/reaching_object: 0.2787
     Episode_Reward/lifting_object: 10.6544
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 0.89s
                      Time elapsed: 00:02:30
                               ETA: 00:32:08

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 110159 steps/s (collection: 0.798s, learning 0.095s)
             Mean action noise std: 1.57
          Mean value_function loss: 48.0132
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 14.7879
                       Mean reward: 62.06
               Mean episode length: 222.84
    Episode_Reward/reaching_object: 0.2791
     Episode_Reward/lifting_object: 11.4184
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 0.89s
                      Time elapsed: 00:02:31
                               ETA: 00:32:05

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 107521 steps/s (collection: 0.821s, learning 0.094s)
             Mean action noise std: 1.57
          Mean value_function loss: 51.7827
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 14.7903
                       Mean reward: 58.67
               Mean episode length: 213.35
    Episode_Reward/reaching_object: 0.2699
     Episode_Reward/lifting_object: 10.8693
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0045
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 0.91s
                      Time elapsed: 00:02:32
                               ETA: 00:32:03

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 108106 steps/s (collection: 0.817s, learning 0.093s)
             Mean action noise std: 1.57
          Mean value_function loss: 51.8438
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 14.7940
                       Mean reward: 57.73
               Mean episode length: 223.66
    Episode_Reward/reaching_object: 0.2821
     Episode_Reward/lifting_object: 11.2803
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 0.91s
                      Time elapsed: 00:02:33
                               ETA: 00:32:00

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 110759 steps/s (collection: 0.800s, learning 0.088s)
             Mean action noise std: 1.57
          Mean value_function loss: 61.6725
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.7982
                       Mean reward: 52.44
               Mean episode length: 229.18
    Episode_Reward/reaching_object: 0.2734
     Episode_Reward/lifting_object: 10.7619
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 0.89s
                      Time elapsed: 00:02:34
                               ETA: 00:31:57

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 111720 steps/s (collection: 0.777s, learning 0.103s)
             Mean action noise std: 1.57
          Mean value_function loss: 56.7991
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.7972
                       Mean reward: 64.30
               Mean episode length: 223.84
    Episode_Reward/reaching_object: 0.2778
     Episode_Reward/lifting_object: 12.3986
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 0.88s
                      Time elapsed: 00:02:35
                               ETA: 00:31:54

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 107735 steps/s (collection: 0.819s, learning 0.093s)
             Mean action noise std: 1.57
          Mean value_function loss: 54.9965
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.8006
                       Mean reward: 57.38
               Mean episode length: 219.51
    Episode_Reward/reaching_object: 0.2721
     Episode_Reward/lifting_object: 11.6207
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 0.91s
                      Time elapsed: 00:02:36
                               ETA: 00:31:52

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 106407 steps/s (collection: 0.818s, learning 0.106s)
             Mean action noise std: 1.57
          Mean value_function loss: 52.1994
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 14.8037
                       Mean reward: 61.63
               Mean episode length: 214.95
    Episode_Reward/reaching_object: 0.2731
     Episode_Reward/lifting_object: 11.9395
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 0.92s
                      Time elapsed: 00:02:36
                               ETA: 00:31:49

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 107085 steps/s (collection: 0.813s, learning 0.105s)
             Mean action noise std: 1.57
          Mean value_function loss: 58.7381
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 14.8066
                       Mean reward: 57.75
               Mean episode length: 213.88
    Episode_Reward/reaching_object: 0.2693
     Episode_Reward/lifting_object: 11.8123
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 0.92s
                      Time elapsed: 00:02:37
                               ETA: 00:31:47

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 111501 steps/s (collection: 0.793s, learning 0.089s)
             Mean action noise std: 1.57
          Mean value_function loss: 56.4828
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 14.8058
                       Mean reward: 77.49
               Mean episode length: 219.15
    Episode_Reward/reaching_object: 0.2723
     Episode_Reward/lifting_object: 13.0898
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 0.88s
                      Time elapsed: 00:02:38
                               ETA: 00:31:44

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 110398 steps/s (collection: 0.805s, learning 0.086s)
             Mean action noise std: 1.57
          Mean value_function loss: 61.7252
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 14.8058
                       Mean reward: 69.25
               Mean episode length: 226.13
    Episode_Reward/reaching_object: 0.2757
     Episode_Reward/lifting_object: 13.3997
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 0.89s
                      Time elapsed: 00:02:39
                               ETA: 00:31:41

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 109405 steps/s (collection: 0.807s, learning 0.091s)
             Mean action noise std: 1.57
          Mean value_function loss: 60.8928
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 14.8060
                       Mean reward: 74.20
               Mean episode length: 215.66
    Episode_Reward/reaching_object: 0.2696
     Episode_Reward/lifting_object: 13.1687
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 0.90s
                      Time elapsed: 00:02:40
                               ETA: 00:31:39

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 109723 steps/s (collection: 0.795s, learning 0.101s)
             Mean action noise std: 1.57
          Mean value_function loss: 65.1109
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 14.8061
                       Mean reward: 75.93
               Mean episode length: 222.07
    Episode_Reward/reaching_object: 0.2735
     Episode_Reward/lifting_object: 14.4195
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 0.90s
                      Time elapsed: 00:02:41
                               ETA: 00:31:36

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 112311 steps/s (collection: 0.783s, learning 0.093s)
             Mean action noise std: 1.57
          Mean value_function loss: 59.8900
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 14.8061
                       Mean reward: 70.20
               Mean episode length: 210.20
    Episode_Reward/reaching_object: 0.2727
     Episode_Reward/lifting_object: 14.6909
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 0.88s
                      Time elapsed: 00:02:42
                               ETA: 00:31:33

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 112242 steps/s (collection: 0.779s, learning 0.097s)
             Mean action noise std: 1.57
          Mean value_function loss: 70.0480
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 14.8058
                       Mean reward: 69.05
               Mean episode length: 212.93
    Episode_Reward/reaching_object: 0.2708
     Episode_Reward/lifting_object: 13.7874
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 0.88s
                      Time elapsed: 00:02:43
                               ETA: 00:31:30

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 110966 steps/s (collection: 0.795s, learning 0.090s)
             Mean action noise std: 1.57
          Mean value_function loss: 69.9189
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 14.8034
                       Mean reward: 76.64
               Mean episode length: 217.82
    Episode_Reward/reaching_object: 0.2764
     Episode_Reward/lifting_object: 13.3441
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 0.89s
                      Time elapsed: 00:02:44
                               ETA: 00:31:28

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 111064 steps/s (collection: 0.792s, learning 0.093s)
             Mean action noise std: 1.57
          Mean value_function loss: 73.5973
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 14.8019
                       Mean reward: 80.29
               Mean episode length: 221.15
    Episode_Reward/reaching_object: 0.2720
     Episode_Reward/lifting_object: 13.5352
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 0.89s
                      Time elapsed: 00:02:44
                               ETA: 00:31:25

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 110896 steps/s (collection: 0.789s, learning 0.097s)
             Mean action noise std: 1.57
          Mean value_function loss: 66.8208
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.8028
                       Mean reward: 85.67
               Mean episode length: 217.38
    Episode_Reward/reaching_object: 0.2789
     Episode_Reward/lifting_object: 14.4036
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 0.89s
                      Time elapsed: 00:02:45
                               ETA: 00:31:23

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 108320 steps/s (collection: 0.809s, learning 0.099s)
             Mean action noise std: 1.57
          Mean value_function loss: 74.9128
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.8003
                       Mean reward: 88.88
               Mean episode length: 223.74
    Episode_Reward/reaching_object: 0.2778
     Episode_Reward/lifting_object: 16.3146
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 0.91s
                      Time elapsed: 00:02:46
                               ETA: 00:31:20

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 110449 steps/s (collection: 0.788s, learning 0.102s)
             Mean action noise std: 1.57
          Mean value_function loss: 76.8155
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.7946
                       Mean reward: 68.20
               Mean episode length: 222.73
    Episode_Reward/reaching_object: 0.2761
     Episode_Reward/lifting_object: 16.0235
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 0.89s
                      Time elapsed: 00:02:47
                               ETA: 00:31:18

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 109583 steps/s (collection: 0.800s, learning 0.097s)
             Mean action noise std: 1.57
          Mean value_function loss: 79.2712
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.7914
                       Mean reward: 102.33
               Mean episode length: 224.82
    Episode_Reward/reaching_object: 0.2886
     Episode_Reward/lifting_object: 16.8693
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 0.90s
                      Time elapsed: 00:02:48
                               ETA: 00:31:15

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 109233 steps/s (collection: 0.801s, learning 0.099s)
             Mean action noise std: 1.57
          Mean value_function loss: 82.6946
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 14.7970
                       Mean reward: 73.07
               Mean episode length: 217.57
    Episode_Reward/reaching_object: 0.2807
     Episode_Reward/lifting_object: 16.0314
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 0.90s
                      Time elapsed: 00:02:49
                               ETA: 00:31:13

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 110291 steps/s (collection: 0.787s, learning 0.104s)
             Mean action noise std: 1.57
          Mean value_function loss: 85.2118
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 14.8017
                       Mean reward: 97.78
               Mean episode length: 220.43
    Episode_Reward/reaching_object: 0.2788
     Episode_Reward/lifting_object: 17.3153
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 0.89s
                      Time elapsed: 00:02:50
                               ETA: 00:31:11

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 110356 steps/s (collection: 0.788s, learning 0.103s)
             Mean action noise std: 1.57
          Mean value_function loss: 90.7462
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 14.7987
                       Mean reward: 80.15
               Mean episode length: 217.01
    Episode_Reward/reaching_object: 0.2837
     Episode_Reward/lifting_object: 17.5861
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 0.89s
                      Time elapsed: 00:02:51
                               ETA: 00:31:08

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 108958 steps/s (collection: 0.809s, learning 0.094s)
             Mean action noise std: 1.57
          Mean value_function loss: 98.7455
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 14.7934
                       Mean reward: 97.35
               Mean episode length: 222.30
    Episode_Reward/reaching_object: 0.2812
     Episode_Reward/lifting_object: 17.1453
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 0.90s
                      Time elapsed: 00:02:52
                               ETA: 00:31:06

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 110080 steps/s (collection: 0.797s, learning 0.096s)
             Mean action noise std: 1.57
          Mean value_function loss: 102.3418
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 14.7946
                       Mean reward: 67.51
               Mean episode length: 210.07
    Episode_Reward/reaching_object: 0.2815
     Episode_Reward/lifting_object: 17.8167
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 0.89s
                      Time elapsed: 00:02:53
                               ETA: 00:31:03

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 110531 steps/s (collection: 0.795s, learning 0.094s)
             Mean action noise std: 1.57
          Mean value_function loss: 109.7179
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 14.7973
                       Mean reward: 117.23
               Mean episode length: 217.74
    Episode_Reward/reaching_object: 0.2841
     Episode_Reward/lifting_object: 19.0499
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 0.89s
                      Time elapsed: 00:02:53
                               ETA: 00:31:01

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 109249 steps/s (collection: 0.814s, learning 0.086s)
             Mean action noise std: 1.57
          Mean value_function loss: 99.1004
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 14.7979
                       Mean reward: 120.96
               Mean episode length: 217.31
    Episode_Reward/reaching_object: 0.3034
     Episode_Reward/lifting_object: 22.0986
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 0.90s
                      Time elapsed: 00:02:54
                               ETA: 00:30:59

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 108350 steps/s (collection: 0.814s, learning 0.093s)
             Mean action noise std: 1.57
          Mean value_function loss: 97.4193
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.7920
                       Mean reward: 84.47
               Mean episode length: 213.45
    Episode_Reward/reaching_object: 0.3016
     Episode_Reward/lifting_object: 20.9565
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 0.91s
                      Time elapsed: 00:02:55
                               ETA: 00:30:57

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 110357 steps/s (collection: 0.795s, learning 0.096s)
             Mean action noise std: 1.57
          Mean value_function loss: 105.3609
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.7868
                       Mean reward: 122.57
               Mean episode length: 215.10
    Episode_Reward/reaching_object: 0.3019
     Episode_Reward/lifting_object: 22.5402
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 0.89s
                      Time elapsed: 00:02:56
                               ETA: 00:30:54

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 110917 steps/s (collection: 0.786s, learning 0.100s)
             Mean action noise std: 1.57
          Mean value_function loss: 105.3848
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.7831
                       Mean reward: 121.67
               Mean episode length: 215.48
    Episode_Reward/reaching_object: 0.3139
     Episode_Reward/lifting_object: 26.3755
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 0.89s
                      Time elapsed: 00:02:57
                               ETA: 00:30:52

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 108990 steps/s (collection: 0.803s, learning 0.099s)
             Mean action noise std: 1.57
          Mean value_function loss: 113.4814
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 14.7778
                       Mean reward: 96.07
               Mean episode length: 210.14
    Episode_Reward/reaching_object: 0.2970
     Episode_Reward/lifting_object: 21.3540
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 0.90s
                      Time elapsed: 00:02:58
                               ETA: 00:30:50

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 110993 steps/s (collection: 0.783s, learning 0.103s)
             Mean action noise std: 1.57
          Mean value_function loss: 126.4355
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 14.7766
                       Mean reward: 125.35
               Mean episode length: 226.36
    Episode_Reward/reaching_object: 0.2939
     Episode_Reward/lifting_object: 21.9970
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 0.89s
                      Time elapsed: 00:02:59
                               ETA: 00:30:47

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 106921 steps/s (collection: 0.819s, learning 0.100s)
             Mean action noise std: 1.57
          Mean value_function loss: 140.4428
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 14.7770
                       Mean reward: 99.78
               Mean episode length: 215.48
    Episode_Reward/reaching_object: 0.2957
     Episode_Reward/lifting_object: 24.0992
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 0.92s
                      Time elapsed: 00:03:00
                               ETA: 00:30:45

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 108093 steps/s (collection: 0.811s, learning 0.099s)
             Mean action noise std: 1.57
          Mean value_function loss: 136.6550
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.7772
                       Mean reward: 133.39
               Mean episode length: 216.36
    Episode_Reward/reaching_object: 0.2972
     Episode_Reward/lifting_object: 23.7253
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 0.91s
                      Time elapsed: 00:03:01
                               ETA: 00:30:43

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 105438 steps/s (collection: 0.834s, learning 0.098s)
             Mean action noise std: 1.57
          Mean value_function loss: 131.9275
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.7743
                       Mean reward: 125.99
               Mean episode length: 220.53
    Episode_Reward/reaching_object: 0.3102
     Episode_Reward/lifting_object: 27.9471
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 0.93s
                      Time elapsed: 00:03:02
                               ETA: 00:30:42

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 109779 steps/s (collection: 0.804s, learning 0.092s)
             Mean action noise std: 1.57
          Mean value_function loss: 151.3160
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 14.7677
                       Mean reward: 136.91
               Mean episode length: 221.71
    Episode_Reward/reaching_object: 0.3116
     Episode_Reward/lifting_object: 27.5560
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 0.90s
                      Time elapsed: 00:03:02
                               ETA: 00:30:39

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 109850 steps/s (collection: 0.794s, learning 0.101s)
             Mean action noise std: 1.57
          Mean value_function loss: 162.8547
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.7663
                       Mean reward: 122.95
               Mean episode length: 222.41
    Episode_Reward/reaching_object: 0.3101
     Episode_Reward/lifting_object: 26.8359
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 0.89s
                      Time elapsed: 00:03:03
                               ETA: 00:30:37

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 111721 steps/s (collection: 0.787s, learning 0.093s)
             Mean action noise std: 1.57
          Mean value_function loss: 150.3562
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.7611
                       Mean reward: 122.89
               Mean episode length: 217.97
    Episode_Reward/reaching_object: 0.3033
     Episode_Reward/lifting_object: 25.0735
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 0.88s
                      Time elapsed: 00:03:04
                               ETA: 00:30:35

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 113401 steps/s (collection: 0.779s, learning 0.088s)
             Mean action noise std: 1.57
          Mean value_function loss: 157.1272
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 14.7570
                       Mean reward: 150.14
               Mean episode length: 214.34
    Episode_Reward/reaching_object: 0.3028
     Episode_Reward/lifting_object: 27.4461
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0047
          Episode_Reward/joint_vel: -0.0022
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 0.87s
                      Time elapsed: 00:03:05
                               ETA: 00:30:32

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 112111 steps/s (collection: 0.786s, learning 0.091s)
             Mean action noise std: 1.57
          Mean value_function loss: 146.9632
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 14.7570
                       Mean reward: 125.39
               Mean episode length: 220.06
    Episode_Reward/reaching_object: 0.3050
     Episode_Reward/lifting_object: 27.5627
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 0.88s
                      Time elapsed: 00:03:06
                               ETA: 00:30:30

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 111214 steps/s (collection: 0.790s, learning 0.094s)
             Mean action noise std: 1.57
          Mean value_function loss: 149.5654
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 14.7576
                       Mean reward: 150.16
               Mean episode length: 222.73
    Episode_Reward/reaching_object: 0.3203
     Episode_Reward/lifting_object: 33.2920
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 0.88s
                      Time elapsed: 00:03:07
                               ETA: 00:30:28

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 110340 steps/s (collection: 0.790s, learning 0.101s)
             Mean action noise std: 1.57
          Mean value_function loss: 150.2524
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 14.7579
                       Mean reward: 220.79
               Mean episode length: 223.82
    Episode_Reward/reaching_object: 0.3297
     Episode_Reward/lifting_object: 35.8077
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 0.89s
                      Time elapsed: 00:03:08
                               ETA: 00:30:26

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 111655 steps/s (collection: 0.791s, learning 0.089s)
             Mean action noise std: 1.57
          Mean value_function loss: 164.6808
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.7584
                       Mean reward: 159.34
               Mean episode length: 217.98
    Episode_Reward/reaching_object: 0.3200
     Episode_Reward/lifting_object: 32.7537
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 0.88s
                      Time elapsed: 00:03:09
                               ETA: 00:30:24

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 111980 steps/s (collection: 0.786s, learning 0.092s)
             Mean action noise std: 1.57
          Mean value_function loss: 153.9237
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.7614
                       Mean reward: 107.87
               Mean episode length: 223.89
    Episode_Reward/reaching_object: 0.3113
     Episode_Reward/lifting_object: 28.3021
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 0.88s
                      Time elapsed: 00:03:10
                               ETA: 00:30:21

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 112238 steps/s (collection: 0.788s, learning 0.088s)
             Mean action noise std: 1.57
          Mean value_function loss: 169.1788
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.7606
                       Mean reward: 178.99
               Mean episode length: 221.00
    Episode_Reward/reaching_object: 0.3358
     Episode_Reward/lifting_object: 38.2805
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 0.88s
                      Time elapsed: 00:03:10
                               ETA: 00:30:19

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 110848 steps/s (collection: 0.788s, learning 0.099s)
             Mean action noise std: 1.57
          Mean value_function loss: 177.1857
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 14.7553
                       Mean reward: 177.41
               Mean episode length: 221.74
    Episode_Reward/reaching_object: 0.3272
     Episode_Reward/lifting_object: 36.1148
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 0.89s
                      Time elapsed: 00:03:11
                               ETA: 00:30:17

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 110041 steps/s (collection: 0.804s, learning 0.090s)
             Mean action noise std: 1.57
          Mean value_function loss: 185.0839
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 14.7540
                       Mean reward: 190.83
               Mean episode length: 227.75
    Episode_Reward/reaching_object: 0.3352
     Episode_Reward/lifting_object: 38.3248
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 0.89s
                      Time elapsed: 00:03:12
                               ETA: 00:30:15

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 113072 steps/s (collection: 0.783s, learning 0.086s)
             Mean action noise std: 1.57
          Mean value_function loss: 181.0433
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 14.7546
                       Mean reward: 175.97
               Mean episode length: 220.14
    Episode_Reward/reaching_object: 0.3153
     Episode_Reward/lifting_object: 34.5114
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 0.87s
                      Time elapsed: 00:03:13
                               ETA: 00:30:13

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 111592 steps/s (collection: 0.785s, learning 0.096s)
             Mean action noise std: 1.57
          Mean value_function loss: 156.8228
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.7559
                       Mean reward: 212.75
               Mean episode length: 228.58
    Episode_Reward/reaching_object: 0.3405
     Episode_Reward/lifting_object: 41.3119
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 0.88s
                      Time elapsed: 00:03:14
                               ETA: 00:30:11

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 113073 steps/s (collection: 0.779s, learning 0.090s)
             Mean action noise std: 1.57
          Mean value_function loss: 179.6197
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 14.7565
                       Mean reward: 244.44
               Mean episode length: 230.63
    Episode_Reward/reaching_object: 0.3392
     Episode_Reward/lifting_object: 39.1812
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 0.87s
                      Time elapsed: 00:03:15
                               ETA: 00:30:08

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 108759 steps/s (collection: 0.806s, learning 0.098s)
             Mean action noise std: 1.57
          Mean value_function loss: 172.6492
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 14.7570
                       Mean reward: 233.36
               Mean episode length: 229.53
    Episode_Reward/reaching_object: 0.3448
     Episode_Reward/lifting_object: 42.8956
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 0.90s
                      Time elapsed: 00:03:16
                               ETA: 00:30:06

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 105382 steps/s (collection: 0.844s, learning 0.089s)
             Mean action noise std: 1.57
          Mean value_function loss: 186.1396
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 14.7581
                       Mean reward: 207.61
               Mean episode length: 232.14
    Episode_Reward/reaching_object: 0.3521
     Episode_Reward/lifting_object: 43.0798
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 0.93s
                      Time elapsed: 00:03:17
                               ETA: 00:30:05

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 107919 steps/s (collection: 0.812s, learning 0.099s)
             Mean action noise std: 1.57
          Mean value_function loss: 179.4835
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 14.7581
                       Mean reward: 177.68
               Mean episode length: 229.42
    Episode_Reward/reaching_object: 0.3481
     Episode_Reward/lifting_object: 40.5665
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 0.91s
                      Time elapsed: 00:03:18
                               ETA: 00:30:03

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 107133 steps/s (collection: 0.808s, learning 0.110s)
             Mean action noise std: 1.57
          Mean value_function loss: 175.8063
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 14.7573
                       Mean reward: 236.30
               Mean episode length: 225.41
    Episode_Reward/reaching_object: 0.3519
     Episode_Reward/lifting_object: 43.8016
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 0.92s
                      Time elapsed: 00:03:18
                               ETA: 00:30:01

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 111412 steps/s (collection: 0.786s, learning 0.097s)
             Mean action noise std: 1.57
          Mean value_function loss: 191.0161
               Mean surrogate loss: 0.0097
                 Mean entropy loss: 14.7571
                       Mean reward: 274.57
               Mean episode length: 227.26
    Episode_Reward/reaching_object: 0.3447
     Episode_Reward/lifting_object: 40.4032
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 0.88s
                      Time elapsed: 00:03:19
                               ETA: 00:29:59

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 109360 steps/s (collection: 0.800s, learning 0.099s)
             Mean action noise std: 1.57
          Mean value_function loss: 209.4104
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.7564
                       Mean reward: 186.30
               Mean episode length: 225.42
    Episode_Reward/reaching_object: 0.3654
     Episode_Reward/lifting_object: 48.8975
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 0.90s
                      Time elapsed: 00:03:20
                               ETA: 00:29:57

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 110411 steps/s (collection: 0.795s, learning 0.095s)
             Mean action noise std: 1.57
          Mean value_function loss: 223.3063
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.7517
                       Mean reward: 227.10
               Mean episode length: 226.48
    Episode_Reward/reaching_object: 0.3576
     Episode_Reward/lifting_object: 48.0151
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 0.89s
                      Time elapsed: 00:03:21
                               ETA: 00:29:55

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 111780 steps/s (collection: 0.790s, learning 0.090s)
             Mean action noise std: 1.57
          Mean value_function loss: 225.5957
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 14.7510
                       Mean reward: 211.16
               Mean episode length: 232.76
    Episode_Reward/reaching_object: 0.3845
     Episode_Reward/lifting_object: 49.7603
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 0.88s
                      Time elapsed: 00:03:22
                               ETA: 00:29:53

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 111657 steps/s (collection: 0.779s, learning 0.101s)
             Mean action noise std: 1.57
          Mean value_function loss: 230.4960
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 14.7503
                       Mean reward: 269.76
               Mean episode length: 224.64
    Episode_Reward/reaching_object: 0.3659
     Episode_Reward/lifting_object: 48.0122
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 0.88s
                      Time elapsed: 00:03:23
                               ETA: 00:29:51

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 112549 steps/s (collection: 0.786s, learning 0.087s)
             Mean action noise std: 1.57
          Mean value_function loss: 227.2030
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.7524
                       Mean reward: 232.30
               Mean episode length: 227.15
    Episode_Reward/reaching_object: 0.3699
     Episode_Reward/lifting_object: 46.7258
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 0.87s
                      Time elapsed: 00:03:24
                               ETA: 00:29:49

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 109721 steps/s (collection: 0.796s, learning 0.100s)
             Mean action noise std: 1.57
          Mean value_function loss: 262.2192
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 14.7549
                       Mean reward: 231.03
               Mean episode length: 229.19
    Episode_Reward/reaching_object: 0.3706
     Episode_Reward/lifting_object: 46.6844
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 0.90s
                      Time elapsed: 00:03:25
                               ETA: 00:29:47

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 111480 steps/s (collection: 0.783s, learning 0.099s)
             Mean action noise std: 1.57
          Mean value_function loss: 272.0297
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 14.7567
                       Mean reward: 305.45
               Mean episode length: 226.65
    Episode_Reward/reaching_object: 0.3773
     Episode_Reward/lifting_object: 50.8197
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 0.88s
                      Time elapsed: 00:03:26
                               ETA: 00:29:45

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 108600 steps/s (collection: 0.807s, learning 0.099s)
             Mean action noise std: 1.57
          Mean value_function loss: 258.1939
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 14.7551
                       Mean reward: 200.16
               Mean episode length: 222.55
    Episode_Reward/reaching_object: 0.3416
     Episode_Reward/lifting_object: 41.1302
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 0.91s
                      Time elapsed: 00:03:26
                               ETA: 00:29:43

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 108822 steps/s (collection: 0.804s, learning 0.099s)
             Mean action noise std: 1.57
          Mean value_function loss: 268.1603
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.7517
                       Mean reward: 277.66
               Mean episode length: 230.16
    Episode_Reward/reaching_object: 0.3767
     Episode_Reward/lifting_object: 49.8302
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 0.90s
                      Time elapsed: 00:03:27
                               ETA: 00:29:42

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 110421 steps/s (collection: 0.801s, learning 0.090s)
             Mean action noise std: 1.57
          Mean value_function loss: 283.8178
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.7487
                       Mean reward: 214.47
               Mean episode length: 232.39
    Episode_Reward/reaching_object: 0.3820
     Episode_Reward/lifting_object: 52.1363
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 0.89s
                      Time elapsed: 00:03:28
                               ETA: 00:29:40

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 110417 steps/s (collection: 0.795s, learning 0.096s)
             Mean action noise std: 1.57
          Mean value_function loss: 270.9771
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 14.7441
                       Mean reward: 280.78
               Mean episode length: 236.88
    Episode_Reward/reaching_object: 0.3911
     Episode_Reward/lifting_object: 55.0467
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 0.89s
                      Time elapsed: 00:03:29
                               ETA: 00:29:38

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 108448 steps/s (collection: 0.814s, learning 0.092s)
             Mean action noise std: 1.57
          Mean value_function loss: 290.8529
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.7377
                       Mean reward: 262.94
               Mean episode length: 225.79
    Episode_Reward/reaching_object: 0.4026
     Episode_Reward/lifting_object: 58.6052
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 0.91s
                      Time elapsed: 00:03:30
                               ETA: 00:29:36

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 111079 steps/s (collection: 0.779s, learning 0.106s)
             Mean action noise std: 1.57
          Mean value_function loss: 296.5024
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.7379
                       Mean reward: 279.08
               Mean episode length: 228.37
    Episode_Reward/reaching_object: 0.3875
     Episode_Reward/lifting_object: 54.6790
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 0.88s
                      Time elapsed: 00:03:31
                               ETA: 00:29:34

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 110463 steps/s (collection: 0.770s, learning 0.120s)
             Mean action noise std: 1.57
          Mean value_function loss: 263.7978
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.7404
                       Mean reward: 309.53
               Mean episode length: 227.71
    Episode_Reward/reaching_object: 0.3894
     Episode_Reward/lifting_object: 56.7386
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 0.89s
                      Time elapsed: 00:03:32
                               ETA: 00:29:32

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 111422 steps/s (collection: 0.784s, learning 0.098s)
             Mean action noise std: 1.57
          Mean value_function loss: 270.2990
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.7415
                       Mean reward: 276.09
               Mean episode length: 227.19
    Episode_Reward/reaching_object: 0.3943
     Episode_Reward/lifting_object: 57.2981
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 0.88s
                      Time elapsed: 00:03:33
                               ETA: 00:29:31

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 113937 steps/s (collection: 0.774s, learning 0.089s)
             Mean action noise std: 1.57
          Mean value_function loss: 290.1008
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.7406
                       Mean reward: 295.96
               Mean episode length: 229.29
    Episode_Reward/reaching_object: 0.3924
     Episode_Reward/lifting_object: 59.2913
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 0.86s
                      Time elapsed: 00:03:34
                               ETA: 00:29:29

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 111814 steps/s (collection: 0.791s, learning 0.089s)
             Mean action noise std: 1.57
          Mean value_function loss: 336.1964
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.7397
                       Mean reward: 306.85
               Mean episode length: 221.59
    Episode_Reward/reaching_object: 0.3876
     Episode_Reward/lifting_object: 57.9861
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 0.88s
                      Time elapsed: 00:03:34
                               ETA: 00:29:27

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 113201 steps/s (collection: 0.779s, learning 0.090s)
             Mean action noise std: 1.57
          Mean value_function loss: 280.7984
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.7378
                       Mean reward: 292.29
               Mean episode length: 224.52
    Episode_Reward/reaching_object: 0.3989
     Episode_Reward/lifting_object: 62.0621
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 0.87s
                      Time elapsed: 00:03:35
                               ETA: 00:29:25

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 109471 steps/s (collection: 0.808s, learning 0.090s)
             Mean action noise std: 1.57
          Mean value_function loss: 297.6849
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.7406
                       Mean reward: 285.38
               Mean episode length: 220.95
    Episode_Reward/reaching_object: 0.4073
     Episode_Reward/lifting_object: 64.8599
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 0.90s
                      Time elapsed: 00:03:36
                               ETA: 00:29:23

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 111680 steps/s (collection: 0.785s, learning 0.096s)
             Mean action noise std: 1.57
          Mean value_function loss: 277.3482
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.7408
                       Mean reward: 253.01
               Mean episode length: 224.22
    Episode_Reward/reaching_object: 0.4017
     Episode_Reward/lifting_object: 61.1721
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 0.88s
                      Time elapsed: 00:03:37
                               ETA: 00:29:21

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 110819 steps/s (collection: 0.796s, learning 0.092s)
             Mean action noise std: 1.57
          Mean value_function loss: 294.3955
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.7381
                       Mean reward: 303.63
               Mean episode length: 219.54
    Episode_Reward/reaching_object: 0.3948
     Episode_Reward/lifting_object: 61.8459
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 0.89s
                      Time elapsed: 00:03:38
                               ETA: 00:29:19

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 113237 steps/s (collection: 0.774s, learning 0.095s)
             Mean action noise std: 1.57
          Mean value_function loss: 293.7213
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.7351
                       Mean reward: 374.87
               Mean episode length: 217.82
    Episode_Reward/reaching_object: 0.4196
     Episode_Reward/lifting_object: 70.0698
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0050
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 0.87s
                      Time elapsed: 00:03:39
                               ETA: 00:29:17

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 115700 steps/s (collection: 0.758s, learning 0.092s)
             Mean action noise std: 1.57
          Mean value_function loss: 325.5104
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 14.7323
                       Mean reward: 316.72
               Mean episode length: 236.83
    Episode_Reward/reaching_object: 0.3977
     Episode_Reward/lifting_object: 62.4355
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 0.85s
                      Time elapsed: 00:03:40
                               ETA: 00:29:15

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 110520 steps/s (collection: 0.797s, learning 0.093s)
             Mean action noise std: 1.57
          Mean value_function loss: 308.1730
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.7331
                       Mean reward: 386.46
               Mean episode length: 235.30
    Episode_Reward/reaching_object: 0.4269
     Episode_Reward/lifting_object: 69.3233
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 0.89s
                      Time elapsed: 00:03:41
                               ETA: 00:29:13

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 112411 steps/s (collection: 0.786s, learning 0.088s)
             Mean action noise std: 1.57
          Mean value_function loss: 290.9177
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.7376
                       Mean reward: 313.63
               Mean episode length: 223.66
    Episode_Reward/reaching_object: 0.4133
     Episode_Reward/lifting_object: 66.6712
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 0.87s
                      Time elapsed: 00:03:41
                               ETA: 00:29:11

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 112442 steps/s (collection: 0.784s, learning 0.091s)
             Mean action noise std: 1.57
          Mean value_function loss: 291.2779
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 14.7401
                       Mean reward: 318.20
               Mean episode length: 236.22
    Episode_Reward/reaching_object: 0.3804
     Episode_Reward/lifting_object: 59.4498
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 0.87s
                      Time elapsed: 00:03:42
                               ETA: 00:29:10

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 110977 steps/s (collection: 0.795s, learning 0.091s)
             Mean action noise std: 1.57
          Mean value_function loss: 299.1269
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 14.7406
                       Mean reward: 311.33
               Mean episode length: 221.45
    Episode_Reward/reaching_object: 0.3840
     Episode_Reward/lifting_object: 61.0037
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 0.89s
                      Time elapsed: 00:03:43
                               ETA: 00:29:08

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 112232 steps/s (collection: 0.783s, learning 0.093s)
             Mean action noise std: 1.57
          Mean value_function loss: 326.7129
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 14.7464
                       Mean reward: 346.25
               Mean episode length: 223.42
    Episode_Reward/reaching_object: 0.4101
     Episode_Reward/lifting_object: 69.7852
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 0.88s
                      Time elapsed: 00:03:44
                               ETA: 00:29:06

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 112488 steps/s (collection: 0.783s, learning 0.090s)
             Mean action noise std: 1.57
          Mean value_function loss: 319.6439
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 14.7492
                       Mean reward: 360.56
               Mean episode length: 225.55
    Episode_Reward/reaching_object: 0.4332
     Episode_Reward/lifting_object: 74.0427
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 0.87s
                      Time elapsed: 00:03:45
                               ETA: 00:29:04

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 115808 steps/s (collection: 0.760s, learning 0.089s)
             Mean action noise std: 1.57
          Mean value_function loss: 302.7437
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.7497
                       Mean reward: 393.96
               Mean episode length: 230.18
    Episode_Reward/reaching_object: 0.4178
     Episode_Reward/lifting_object: 70.4457
      Episode_Reward/object_height: 0.0258
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 0.85s
                      Time elapsed: 00:03:46
                               ETA: 00:29:02

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 108989 steps/s (collection: 0.792s, learning 0.110s)
             Mean action noise std: 1.57
          Mean value_function loss: 317.5372
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.7494
                       Mean reward: 382.89
               Mean episode length: 222.27
    Episode_Reward/reaching_object: 0.4422
     Episode_Reward/lifting_object: 77.8572
      Episode_Reward/object_height: 0.0287
        Episode_Reward/action_rate: -0.0051
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 0.90s
                      Time elapsed: 00:03:47
                               ETA: 00:29:01

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 110472 steps/s (collection: 0.783s, learning 0.107s)
             Mean action noise std: 1.57
          Mean value_function loss: 317.2700
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.7504
                       Mean reward: 444.30
               Mean episode length: 237.28
    Episode_Reward/reaching_object: 0.4544
     Episode_Reward/lifting_object: 80.7552
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 0.89s
                      Time elapsed: 00:03:48
                               ETA: 00:28:59

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 111249 steps/s (collection: 0.785s, learning 0.099s)
             Mean action noise std: 1.57
          Mean value_function loss: 302.2647
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 14.7532
                       Mean reward: 392.01
               Mean episode length: 233.54
    Episode_Reward/reaching_object: 0.4546
     Episode_Reward/lifting_object: 83.1129
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 0.88s
                      Time elapsed: 00:03:48
                               ETA: 00:28:57

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 108434 steps/s (collection: 0.800s, learning 0.107s)
             Mean action noise std: 1.57
          Mean value_function loss: 303.5056
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 14.7525
                       Mean reward: 342.61
               Mean episode length: 229.92
    Episode_Reward/reaching_object: 0.4179
     Episode_Reward/lifting_object: 71.7889
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 0.91s
                      Time elapsed: 00:03:49
                               ETA: 00:28:56

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 112604 steps/s (collection: 0.784s, learning 0.089s)
             Mean action noise std: 1.57
          Mean value_function loss: 313.5422
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 14.7535
                       Mean reward: 389.06
               Mean episode length: 232.43
    Episode_Reward/reaching_object: 0.4496
     Episode_Reward/lifting_object: 79.7793
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 0.87s
                      Time elapsed: 00:03:50
                               ETA: 00:28:54

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 108812 steps/s (collection: 0.786s, learning 0.118s)
             Mean action noise std: 1.57
          Mean value_function loss: 319.6695
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.7550
                       Mean reward: 440.09
               Mean episode length: 232.03
    Episode_Reward/reaching_object: 0.4627
     Episode_Reward/lifting_object: 83.6278
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 0.90s
                      Time elapsed: 00:03:51
                               ETA: 00:28:52

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 108758 steps/s (collection: 0.811s, learning 0.093s)
             Mean action noise std: 1.57
          Mean value_function loss: 322.7791
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.7550
                       Mean reward: 439.33
               Mean episode length: 235.45
    Episode_Reward/reaching_object: 0.4772
     Episode_Reward/lifting_object: 87.8727
      Episode_Reward/object_height: 0.0331
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 0.90s
                      Time elapsed: 00:03:52
                               ETA: 00:28:51

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 111987 steps/s (collection: 0.775s, learning 0.103s)
             Mean action noise std: 1.57
          Mean value_function loss: 316.3826
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.7548
                       Mean reward: 421.42
               Mean episode length: 229.65
    Episode_Reward/reaching_object: 0.4513
     Episode_Reward/lifting_object: 79.9664
      Episode_Reward/object_height: 0.0300
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 0.88s
                      Time elapsed: 00:03:53
                               ETA: 00:28:49

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 112894 steps/s (collection: 0.783s, learning 0.088s)
             Mean action noise std: 1.57
          Mean value_function loss: 355.4836
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.7545
                       Mean reward: 404.34
               Mean episode length: 232.70
    Episode_Reward/reaching_object: 0.4473
     Episode_Reward/lifting_object: 78.1397
      Episode_Reward/object_height: 0.0296
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 0.87s
                      Time elapsed: 00:03:54
                               ETA: 00:28:47

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 113688 steps/s (collection: 0.776s, learning 0.089s)
             Mean action noise std: 1.57
          Mean value_function loss: 331.0204
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.7539
                       Mean reward: 427.41
               Mean episode length: 231.73
    Episode_Reward/reaching_object: 0.4495
     Episode_Reward/lifting_object: 78.0978
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 0.86s
                      Time elapsed: 00:03:55
                               ETA: 00:28:45

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 111843 steps/s (collection: 0.792s, learning 0.087s)
             Mean action noise std: 1.58
          Mean value_function loss: 317.8975
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 14.7551
                       Mean reward: 329.64
               Mean episode length: 230.95
    Episode_Reward/reaching_object: 0.4441
     Episode_Reward/lifting_object: 78.0210
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 0.88s
                      Time elapsed: 00:03:56
                               ETA: 00:28:44

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 112203 steps/s (collection: 0.785s, learning 0.092s)
             Mean action noise std: 1.58
          Mean value_function loss: 351.4189
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.7560
                       Mean reward: 372.12
               Mean episode length: 228.00
    Episode_Reward/reaching_object: 0.4280
     Episode_Reward/lifting_object: 71.9359
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 0.88s
                      Time elapsed: 00:03:56
                               ETA: 00:28:42

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 108212 steps/s (collection: 0.811s, learning 0.097s)
             Mean action noise std: 1.58
          Mean value_function loss: 342.4165
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 14.7538
                       Mean reward: 381.96
               Mean episode length: 227.15
    Episode_Reward/reaching_object: 0.4352
     Episode_Reward/lifting_object: 75.7433
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 0.91s
                      Time elapsed: 00:03:57
                               ETA: 00:28:40

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 113639 steps/s (collection: 0.771s, learning 0.094s)
             Mean action noise std: 1.58
          Mean value_function loss: 363.7938
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.7542
                       Mean reward: 350.05
               Mean episode length: 224.89
    Episode_Reward/reaching_object: 0.4390
     Episode_Reward/lifting_object: 77.1293
      Episode_Reward/object_height: 0.0287
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 0.87s
                      Time elapsed: 00:03:58
                               ETA: 00:28:38

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 113178 steps/s (collection: 0.777s, learning 0.091s)
             Mean action noise std: 1.58
          Mean value_function loss: 360.5548
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.7576
                       Mean reward: 449.09
               Mean episode length: 228.32
    Episode_Reward/reaching_object: 0.4509
     Episode_Reward/lifting_object: 81.7463
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 0.87s
                      Time elapsed: 00:03:59
                               ETA: 00:28:37

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 108722 steps/s (collection: 0.799s, learning 0.105s)
             Mean action noise std: 1.58
          Mean value_function loss: 342.4997
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 14.7578
                       Mean reward: 382.54
               Mean episode length: 233.87
    Episode_Reward/reaching_object: 0.4557
     Episode_Reward/lifting_object: 80.6822
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 0.90s
                      Time elapsed: 00:04:00
                               ETA: 00:28:35

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 112422 steps/s (collection: 0.779s, learning 0.095s)
             Mean action noise std: 1.58
          Mean value_function loss: 357.8634
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 14.7598
                       Mean reward: 381.54
               Mean episode length: 240.44
    Episode_Reward/reaching_object: 0.4451
     Episode_Reward/lifting_object: 77.1348
      Episode_Reward/object_height: 0.0288
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 0.87s
                      Time elapsed: 00:04:01
                               ETA: 00:28:34

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 113497 steps/s (collection: 0.773s, learning 0.094s)
             Mean action noise std: 1.58
          Mean value_function loss: 329.9248
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.7614
                       Mean reward: 367.05
               Mean episode length: 223.75
    Episode_Reward/reaching_object: 0.4673
     Episode_Reward/lifting_object: 84.7487
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 0.87s
                      Time elapsed: 00:04:02
                               ETA: 00:28:32

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 112963 steps/s (collection: 0.779s, learning 0.091s)
             Mean action noise std: 1.58
          Mean value_function loss: 350.5365
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.7636
                       Mean reward: 478.54
               Mean episode length: 235.87
    Episode_Reward/reaching_object: 0.4730
     Episode_Reward/lifting_object: 86.6693
      Episode_Reward/object_height: 0.0325
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 0.87s
                      Time elapsed: 00:04:03
                               ETA: 00:28:30

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 111535 steps/s (collection: 0.780s, learning 0.102s)
             Mean action noise std: 1.58
          Mean value_function loss: 347.9380
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 14.7642
                       Mean reward: 405.88
               Mean episode length: 235.98
    Episode_Reward/reaching_object: 0.4656
     Episode_Reward/lifting_object: 85.6757
      Episode_Reward/object_height: 0.0325
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 0.88s
                      Time elapsed: 00:04:03
                               ETA: 00:28:28

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 112524 steps/s (collection: 0.783s, learning 0.091s)
             Mean action noise std: 1.58
          Mean value_function loss: 334.1027
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.7630
                       Mean reward: 445.98
               Mean episode length: 229.63
    Episode_Reward/reaching_object: 0.4739
     Episode_Reward/lifting_object: 89.4297
      Episode_Reward/object_height: 0.0337
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 0.87s
                      Time elapsed: 00:04:04
                               ETA: 00:28:27

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 116708 steps/s (collection: 0.756s, learning 0.087s)
             Mean action noise std: 1.58
          Mean value_function loss: 347.8527
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 14.7630
                       Mean reward: 345.48
               Mean episode length: 226.71
    Episode_Reward/reaching_object: 0.4491
     Episode_Reward/lifting_object: 79.0610
      Episode_Reward/object_height: 0.0301
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 0.84s
                      Time elapsed: 00:04:05
                               ETA: 00:28:25

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 112616 steps/s (collection: 0.781s, learning 0.092s)
             Mean action noise std: 1.58
          Mean value_function loss: 345.8436
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 14.7647
                       Mean reward: 460.82
               Mean episode length: 228.05
    Episode_Reward/reaching_object: 0.4696
     Episode_Reward/lifting_object: 89.2072
      Episode_Reward/object_height: 0.0342
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 0.87s
                      Time elapsed: 00:04:06
                               ETA: 00:28:23

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 111991 steps/s (collection: 0.780s, learning 0.098s)
             Mean action noise std: 1.58
          Mean value_function loss: 312.3791
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.7655
                       Mean reward: 410.67
               Mean episode length: 224.91
    Episode_Reward/reaching_object: 0.4730
     Episode_Reward/lifting_object: 90.2847
      Episode_Reward/object_height: 0.0348
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 0.88s
                      Time elapsed: 00:04:07
                               ETA: 00:28:21

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 113411 steps/s (collection: 0.773s, learning 0.094s)
             Mean action noise std: 1.58
          Mean value_function loss: 358.4987
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.7633
                       Mean reward: 446.03
               Mean episode length: 233.75
    Episode_Reward/reaching_object: 0.4551
     Episode_Reward/lifting_object: 85.2819
      Episode_Reward/object_height: 0.0331
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 0.87s
                      Time elapsed: 00:04:08
                               ETA: 00:28:20

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 110816 steps/s (collection: 0.794s, learning 0.093s)
             Mean action noise std: 1.58
          Mean value_function loss: 333.3506
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.7605
                       Mean reward: 507.64
               Mean episode length: 234.53
    Episode_Reward/reaching_object: 0.4924
     Episode_Reward/lifting_object: 94.1659
      Episode_Reward/object_height: 0.0367
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 0.89s
                      Time elapsed: 00:04:09
                               ETA: 00:28:18

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 112736 steps/s (collection: 0.781s, learning 0.091s)
             Mean action noise std: 1.58
          Mean value_function loss: 389.9760
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 14.7623
                       Mean reward: 418.89
               Mean episode length: 236.79
    Episode_Reward/reaching_object: 0.4401
     Episode_Reward/lifting_object: 79.5809
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 0.87s
                      Time elapsed: 00:04:10
                               ETA: 00:28:17

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 110670 steps/s (collection: 0.793s, learning 0.095s)
             Mean action noise std: 1.58
          Mean value_function loss: 370.6437
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.7628
                       Mean reward: 420.74
               Mean episode length: 244.59
    Episode_Reward/reaching_object: 0.4862
     Episode_Reward/lifting_object: 91.9298
      Episode_Reward/object_height: 0.0360
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 0.89s
                      Time elapsed: 00:04:10
                               ETA: 00:28:15

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 114193 steps/s (collection: 0.775s, learning 0.086s)
             Mean action noise std: 1.58
          Mean value_function loss: 371.5757
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.7613
                       Mean reward: 445.33
               Mean episode length: 225.43
    Episode_Reward/reaching_object: 0.4717
     Episode_Reward/lifting_object: 90.9240
      Episode_Reward/object_height: 0.0357
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 0.86s
                      Time elapsed: 00:04:11
                               ETA: 00:28:13

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 104005 steps/s (collection: 0.857s, learning 0.088s)
             Mean action noise std: 1.58
          Mean value_function loss: 434.0579
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.7574
                       Mean reward: 451.60
               Mean episode length: 235.42
    Episode_Reward/reaching_object: 0.4624
     Episode_Reward/lifting_object: 86.1182
      Episode_Reward/object_height: 0.0337
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 0.95s
                      Time elapsed: 00:04:12
                               ETA: 00:28:12

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 109485 steps/s (collection: 0.801s, learning 0.097s)
             Mean action noise std: 1.58
          Mean value_function loss: 399.6697
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 14.7579
                       Mean reward: 488.20
               Mean episode length: 233.34
    Episode_Reward/reaching_object: 0.4707
     Episode_Reward/lifting_object: 89.8202
      Episode_Reward/object_height: 0.0352
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 0.90s
                      Time elapsed: 00:04:13
                               ETA: 00:28:11

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 108770 steps/s (collection: 0.791s, learning 0.112s)
             Mean action noise std: 1.58
          Mean value_function loss: 378.6229
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.7616
                       Mean reward: 432.53
               Mean episode length: 231.13
    Episode_Reward/reaching_object: 0.4549
     Episode_Reward/lifting_object: 84.6650
      Episode_Reward/object_height: 0.0336
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 0.90s
                      Time elapsed: 00:04:14
                               ETA: 00:28:09

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 107881 steps/s (collection: 0.810s, learning 0.101s)
             Mean action noise std: 1.58
          Mean value_function loss: 395.3207
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.7630
                       Mean reward: 481.34
               Mean episode length: 235.21
    Episode_Reward/reaching_object: 0.4806
     Episode_Reward/lifting_object: 88.9413
      Episode_Reward/object_height: 0.0349
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 0.91s
                      Time elapsed: 00:04:15
                               ETA: 00:28:08

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 111017 steps/s (collection: 0.782s, learning 0.104s)
             Mean action noise std: 1.58
          Mean value_function loss: 391.5955
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.7619
                       Mean reward: 467.00
               Mean episode length: 233.95
    Episode_Reward/reaching_object: 0.4764
     Episode_Reward/lifting_object: 91.3696
      Episode_Reward/object_height: 0.0352
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 0.89s
                      Time elapsed: 00:04:16
                               ETA: 00:28:06

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 110954 steps/s (collection: 0.786s, learning 0.100s)
             Mean action noise std: 1.58
          Mean value_function loss: 384.5465
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.7641
                       Mean reward: 538.57
               Mean episode length: 230.22
    Episode_Reward/reaching_object: 0.5059
     Episode_Reward/lifting_object: 99.0352
      Episode_Reward/object_height: 0.0386
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 0.89s
                      Time elapsed: 00:04:17
                               ETA: 00:28:05

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 113402 steps/s (collection: 0.761s, learning 0.106s)
             Mean action noise std: 1.58
          Mean value_function loss: 360.0459
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.7710
                       Mean reward: 456.42
               Mean episode length: 219.71
    Episode_Reward/reaching_object: 0.4695
     Episode_Reward/lifting_object: 89.4216
      Episode_Reward/object_height: 0.0350
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 0.87s
                      Time elapsed: 00:04:18
                               ETA: 00:28:03

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 111730 steps/s (collection: 0.789s, learning 0.091s)
             Mean action noise std: 1.59
          Mean value_function loss: 349.9439
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.7799
                       Mean reward: 457.73
               Mean episode length: 231.94
    Episode_Reward/reaching_object: 0.4876
     Episode_Reward/lifting_object: 92.0553
      Episode_Reward/object_height: 0.0362
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 0.88s
                      Time elapsed: 00:04:19
                               ETA: 00:28:02

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 112410 steps/s (collection: 0.779s, learning 0.096s)
             Mean action noise std: 1.59
          Mean value_function loss: 336.4734
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 14.7869
                       Mean reward: 544.31
               Mean episode length: 243.97
    Episode_Reward/reaching_object: 0.5015
     Episode_Reward/lifting_object: 98.8425
      Episode_Reward/object_height: 0.0391
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 0.87s
                      Time elapsed: 00:04:19
                               ETA: 00:28:00

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 110613 steps/s (collection: 0.798s, learning 0.091s)
             Mean action noise std: 1.59
          Mean value_function loss: 381.9132
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 14.7904
                       Mean reward: 576.05
               Mean episode length: 245.26
    Episode_Reward/reaching_object: 0.5566
     Episode_Reward/lifting_object: 113.2722
      Episode_Reward/object_height: 0.0449
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 0.89s
                      Time elapsed: 00:04:20
                               ETA: 00:27:58

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 112503 steps/s (collection: 0.781s, learning 0.093s)
             Mean action noise std: 1.59
          Mean value_function loss: 396.4653
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 14.7924
                       Mean reward: 486.15
               Mean episode length: 234.33
    Episode_Reward/reaching_object: 0.4971
     Episode_Reward/lifting_object: 98.5656
      Episode_Reward/object_height: 0.0394
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 0.87s
                      Time elapsed: 00:04:21
                               ETA: 00:27:57

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 108196 steps/s (collection: 0.811s, learning 0.098s)
             Mean action noise std: 1.59
          Mean value_function loss: 479.4845
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.7916
                       Mean reward: 464.81
               Mean episode length: 237.27
    Episode_Reward/reaching_object: 0.4788
     Episode_Reward/lifting_object: 90.6537
      Episode_Reward/object_height: 0.0364
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 0.91s
                      Time elapsed: 00:04:22
                               ETA: 00:27:56

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 107554 steps/s (collection: 0.813s, learning 0.101s)
             Mean action noise std: 1.59
          Mean value_function loss: 408.4186
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.7943
                       Mean reward: 493.90
               Mean episode length: 236.68
    Episode_Reward/reaching_object: 0.5026
     Episode_Reward/lifting_object: 98.5292
      Episode_Reward/object_height: 0.0395
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 0.91s
                      Time elapsed: 00:04:23
                               ETA: 00:27:54

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 110782 steps/s (collection: 0.791s, learning 0.096s)
             Mean action noise std: 1.59
          Mean value_function loss: 462.8266
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 14.7965
                       Mean reward: 482.34
               Mean episode length: 235.66
    Episode_Reward/reaching_object: 0.4883
     Episode_Reward/lifting_object: 96.1351
      Episode_Reward/object_height: 0.0386
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 0.89s
                      Time elapsed: 00:04:24
                               ETA: 00:27:53

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 113253 steps/s (collection: 0.768s, learning 0.100s)
             Mean action noise std: 1.59
          Mean value_function loss: 404.9763
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.8008
                       Mean reward: 526.24
               Mean episode length: 238.07
    Episode_Reward/reaching_object: 0.4993
     Episode_Reward/lifting_object: 96.9413
      Episode_Reward/object_height: 0.0393
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 0.87s
                      Time elapsed: 00:04:25
                               ETA: 00:27:51

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 108387 steps/s (collection: 0.816s, learning 0.091s)
             Mean action noise std: 1.59
          Mean value_function loss: 441.2927
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.8044
                       Mean reward: 513.92
               Mean episode length: 234.89
    Episode_Reward/reaching_object: 0.5088
     Episode_Reward/lifting_object: 101.2771
      Episode_Reward/object_height: 0.0410
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 0.91s
                      Time elapsed: 00:04:26
                               ETA: 00:27:50

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 111126 steps/s (collection: 0.797s, learning 0.088s)
             Mean action noise std: 1.59
          Mean value_function loss: 405.7363
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.8058
                       Mean reward: 497.79
               Mean episode length: 232.47
    Episode_Reward/reaching_object: 0.5118
     Episode_Reward/lifting_object: 101.4549
      Episode_Reward/object_height: 0.0407
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 0.88s
                      Time elapsed: 00:04:27
                               ETA: 00:27:48

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 114336 steps/s (collection: 0.770s, learning 0.089s)
             Mean action noise std: 1.59
          Mean value_function loss: 453.7618
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 14.8077
                       Mean reward: 494.29
               Mean episode length: 237.15
    Episode_Reward/reaching_object: 0.4860
     Episode_Reward/lifting_object: 92.2576
      Episode_Reward/object_height: 0.0365
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 0.86s
                      Time elapsed: 00:04:27
                               ETA: 00:27:47

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 112297 steps/s (collection: 0.783s, learning 0.093s)
             Mean action noise std: 1.59
          Mean value_function loss: 438.9105
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 14.8060
                       Mean reward: 481.16
               Mean episode length: 232.65
    Episode_Reward/reaching_object: 0.4939
     Episode_Reward/lifting_object: 94.8894
      Episode_Reward/object_height: 0.0374
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 0.88s
                      Time elapsed: 00:04:28
                               ETA: 00:27:45

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 109605 steps/s (collection: 0.805s, learning 0.092s)
             Mean action noise std: 1.59
          Mean value_function loss: 409.7434
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 14.8070
                       Mean reward: 486.07
               Mean episode length: 239.47
    Episode_Reward/reaching_object: 0.4849
     Episode_Reward/lifting_object: 92.4870
      Episode_Reward/object_height: 0.0365
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 0.90s
                      Time elapsed: 00:04:29
                               ETA: 00:27:44

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 112864 steps/s (collection: 0.774s, learning 0.097s)
             Mean action noise std: 1.59
          Mean value_function loss: 375.3462
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 14.8073
                       Mean reward: 514.75
               Mean episode length: 241.98
    Episode_Reward/reaching_object: 0.5204
     Episode_Reward/lifting_object: 103.3731
      Episode_Reward/object_height: 0.0410
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 0.87s
                      Time elapsed: 00:04:30
                               ETA: 00:27:42

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 108864 steps/s (collection: 0.790s, learning 0.113s)
             Mean action noise std: 1.59
          Mean value_function loss: 433.2728
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 14.8044
                       Mean reward: 467.60
               Mean episode length: 235.85
    Episode_Reward/reaching_object: 0.5067
     Episode_Reward/lifting_object: 99.0798
      Episode_Reward/object_height: 0.0401
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 0.90s
                      Time elapsed: 00:04:31
                               ETA: 00:27:41

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 115319 steps/s (collection: 0.761s, learning 0.091s)
             Mean action noise std: 1.59
          Mean value_function loss: 407.6142
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 14.8037
                       Mean reward: 594.63
               Mean episode length: 235.49
    Episode_Reward/reaching_object: 0.5314
     Episode_Reward/lifting_object: 107.6942
      Episode_Reward/object_height: 0.0432
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 0.85s
                      Time elapsed: 00:04:32
                               ETA: 00:27:39

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 113161 steps/s (collection: 0.779s, learning 0.090s)
             Mean action noise std: 1.59
          Mean value_function loss: 418.5592
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.8040
                       Mean reward: 547.55
               Mean episode length: 238.03
    Episode_Reward/reaching_object: 0.5174
     Episode_Reward/lifting_object: 104.3490
      Episode_Reward/object_height: 0.0419
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 0.87s
                      Time elapsed: 00:04:33
                               ETA: 00:27:38

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 114993 steps/s (collection: 0.768s, learning 0.087s)
             Mean action noise std: 1.59
          Mean value_function loss: 447.2331
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 14.8057
                       Mean reward: 517.10
               Mean episode length: 236.36
    Episode_Reward/reaching_object: 0.5312
     Episode_Reward/lifting_object: 105.0775
      Episode_Reward/object_height: 0.0423
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 0.85s
                      Time elapsed: 00:04:33
                               ETA: 00:27:36

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 112870 steps/s (collection: 0.780s, learning 0.091s)
             Mean action noise std: 1.59
          Mean value_function loss: 399.0692
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.8078
                       Mean reward: 541.93
               Mean episode length: 232.94
    Episode_Reward/reaching_object: 0.5188
     Episode_Reward/lifting_object: 104.2580
      Episode_Reward/object_height: 0.0425
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 0.87s
                      Time elapsed: 00:04:34
                               ETA: 00:27:34

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 112346 steps/s (collection: 0.788s, learning 0.087s)
             Mean action noise std: 1.59
          Mean value_function loss: 407.3292
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.8108
                       Mean reward: 520.84
               Mean episode length: 226.13
    Episode_Reward/reaching_object: 0.5373
     Episode_Reward/lifting_object: 110.0400
      Episode_Reward/object_height: 0.0446
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 0.88s
                      Time elapsed: 00:04:35
                               ETA: 00:27:33

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 113856 steps/s (collection: 0.775s, learning 0.089s)
             Mean action noise std: 1.59
          Mean value_function loss: 386.7266
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.8102
                       Mean reward: 587.51
               Mean episode length: 246.36
    Episode_Reward/reaching_object: 0.5377
     Episode_Reward/lifting_object: 108.9208
      Episode_Reward/object_height: 0.0447
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 0.86s
                      Time elapsed: 00:04:36
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 111613 steps/s (collection: 0.769s, learning 0.111s)
             Mean action noise std: 1.59
          Mean value_function loss: 369.5589
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.8085
                       Mean reward: 529.38
               Mean episode length: 235.58
    Episode_Reward/reaching_object: 0.5350
     Episode_Reward/lifting_object: 108.2500
      Episode_Reward/object_height: 0.0445
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 0.88s
                      Time elapsed: 00:04:37
                               ETA: 00:27:30

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 112742 steps/s (collection: 0.761s, learning 0.111s)
             Mean action noise std: 1.59
          Mean value_function loss: 361.8529
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 14.8068
                       Mean reward: 548.70
               Mean episode length: 233.36
    Episode_Reward/reaching_object: 0.5429
     Episode_Reward/lifting_object: 111.1291
      Episode_Reward/object_height: 0.0458
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 0.87s
                      Time elapsed: 00:04:38
                               ETA: 00:27:28

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 113619 steps/s (collection: 0.770s, learning 0.096s)
             Mean action noise std: 1.59
          Mean value_function loss: 340.3779
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.8070
                       Mean reward: 481.46
               Mean episode length: 239.41
    Episode_Reward/reaching_object: 0.5448
     Episode_Reward/lifting_object: 111.2289
      Episode_Reward/object_height: 0.0461
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 0.87s
                      Time elapsed: 00:04:39
                               ETA: 00:27:27

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 113706 steps/s (collection: 0.772s, learning 0.092s)
             Mean action noise std: 1.60
          Mean value_function loss: 387.6659
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.8074
                       Mean reward: 501.27
               Mean episode length: 243.29
    Episode_Reward/reaching_object: 0.5374
     Episode_Reward/lifting_object: 108.3457
      Episode_Reward/object_height: 0.0447
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 0.86s
                      Time elapsed: 00:04:40
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 111561 steps/s (collection: 0.788s, learning 0.094s)
             Mean action noise std: 1.60
          Mean value_function loss: 420.7591
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 14.8057
                       Mean reward: 594.07
               Mean episode length: 239.81
    Episode_Reward/reaching_object: 0.5375
     Episode_Reward/lifting_object: 108.2632
      Episode_Reward/object_height: 0.0448
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 0.88s
                      Time elapsed: 00:04:40
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 110407 steps/s (collection: 0.793s, learning 0.098s)
             Mean action noise std: 1.60
          Mean value_function loss: 381.9023
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 14.8057
                       Mean reward: 530.75
               Mean episode length: 230.41
    Episode_Reward/reaching_object: 0.5431
     Episode_Reward/lifting_object: 111.2563
      Episode_Reward/object_height: 0.0459
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 0.89s
                      Time elapsed: 00:04:41
                               ETA: 00:27:23

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 114064 steps/s (collection: 0.770s, learning 0.092s)
             Mean action noise std: 1.60
          Mean value_function loss: 397.5814
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.8071
                       Mean reward: 550.66
               Mean episode length: 232.35
    Episode_Reward/reaching_object: 0.5386
     Episode_Reward/lifting_object: 111.4602
      Episode_Reward/object_height: 0.0462
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 0.86s
                      Time elapsed: 00:04:42
                               ETA: 00:27:21

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 111626 steps/s (collection: 0.788s, learning 0.093s)
             Mean action noise std: 1.60
          Mean value_function loss: 367.1511
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 14.8080
                       Mean reward: 522.67
               Mean episode length: 229.40
    Episode_Reward/reaching_object: 0.5374
     Episode_Reward/lifting_object: 108.9621
      Episode_Reward/object_height: 0.0452
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 0.88s
                      Time elapsed: 00:04:43
                               ETA: 00:27:20

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 112005 steps/s (collection: 0.785s, learning 0.092s)
             Mean action noise std: 1.60
          Mean value_function loss: 405.6614
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 14.8081
                       Mean reward: 564.76
               Mean episode length: 237.92
    Episode_Reward/reaching_object: 0.5614
     Episode_Reward/lifting_object: 115.3474
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 0.88s
                      Time elapsed: 00:04:44
                               ETA: 00:27:18

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 111459 steps/s (collection: 0.783s, learning 0.099s)
             Mean action noise std: 1.60
          Mean value_function loss: 330.0621
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 14.8062
                       Mean reward: 588.63
               Mean episode length: 242.47
    Episode_Reward/reaching_object: 0.5332
     Episode_Reward/lifting_object: 109.4370
      Episode_Reward/object_height: 0.0451
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 0.88s
                      Time elapsed: 00:04:45
                               ETA: 00:27:17

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 112051 steps/s (collection: 0.781s, learning 0.096s)
             Mean action noise std: 1.60
          Mean value_function loss: 322.7564
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.8073
                       Mean reward: 592.67
               Mean episode length: 235.63
    Episode_Reward/reaching_object: 0.5447
     Episode_Reward/lifting_object: 113.6440
      Episode_Reward/object_height: 0.0467
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 0.88s
                      Time elapsed: 00:04:46
                               ETA: 00:27:15

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 113874 steps/s (collection: 0.772s, learning 0.092s)
             Mean action noise std: 1.60
          Mean value_function loss: 280.7679
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 14.8070
                       Mean reward: 581.21
               Mean episode length: 232.36
    Episode_Reward/reaching_object: 0.5202
     Episode_Reward/lifting_object: 105.4665
      Episode_Reward/object_height: 0.0431
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 0.86s
                      Time elapsed: 00:04:47
                               ETA: 00:27:14

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 113037 steps/s (collection: 0.782s, learning 0.088s)
             Mean action noise std: 1.60
          Mean value_function loss: 299.1513
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.8086
                       Mean reward: 591.09
               Mean episode length: 236.79
    Episode_Reward/reaching_object: 0.5402
     Episode_Reward/lifting_object: 112.0036
      Episode_Reward/object_height: 0.0460
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 0.87s
                      Time elapsed: 00:04:47
                               ETA: 00:27:12

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 112890 steps/s (collection: 0.780s, learning 0.091s)
             Mean action noise std: 1.60
          Mean value_function loss: 300.5046
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.8120
                       Mean reward: 584.54
               Mean episode length: 233.42
    Episode_Reward/reaching_object: 0.5476
     Episode_Reward/lifting_object: 113.2558
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 0.87s
                      Time elapsed: 00:04:48
                               ETA: 00:27:11

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 110234 steps/s (collection: 0.794s, learning 0.098s)
             Mean action noise std: 1.60
          Mean value_function loss: 272.8484
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.8150
                       Mean reward: 595.08
               Mean episode length: 238.91
    Episode_Reward/reaching_object: 0.5633
     Episode_Reward/lifting_object: 117.8364
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 0.89s
                      Time elapsed: 00:04:49
                               ETA: 00:27:09

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 109552 steps/s (collection: 0.793s, learning 0.104s)
             Mean action noise std: 1.60
          Mean value_function loss: 338.9623
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.8165
                       Mean reward: 612.69
               Mean episode length: 236.66
    Episode_Reward/reaching_object: 0.5588
     Episode_Reward/lifting_object: 116.5910
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 0.90s
                      Time elapsed: 00:04:50
                               ETA: 00:27:08

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 112482 steps/s (collection: 0.772s, learning 0.102s)
             Mean action noise std: 1.60
          Mean value_function loss: 314.9369
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 14.8168
                       Mean reward: 640.87
               Mean episode length: 238.18
    Episode_Reward/reaching_object: 0.5690
     Episode_Reward/lifting_object: 119.3835
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 0.87s
                      Time elapsed: 00:04:51
                               ETA: 00:27:07

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 113451 steps/s (collection: 0.770s, learning 0.096s)
             Mean action noise std: 1.60
          Mean value_function loss: 339.9931
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 14.8178
                       Mean reward: 605.23
               Mean episode length: 238.65
    Episode_Reward/reaching_object: 0.5633
     Episode_Reward/lifting_object: 117.5075
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 0.87s
                      Time elapsed: 00:04:52
                               ETA: 00:27:05

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 114590 steps/s (collection: 0.765s, learning 0.093s)
             Mean action noise std: 1.60
          Mean value_function loss: 349.2060
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 14.8180
                       Mean reward: 466.11
               Mean episode length: 235.88
    Episode_Reward/reaching_object: 0.5316
     Episode_Reward/lifting_object: 110.3987
      Episode_Reward/object_height: 0.0462
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 0.86s
                      Time elapsed: 00:04:53
                               ETA: 00:27:04

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 111492 steps/s (collection: 0.783s, learning 0.099s)
             Mean action noise std: 1.60
          Mean value_function loss: 312.0403
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.8209
                       Mean reward: 581.39
               Mean episode length: 229.81
    Episode_Reward/reaching_object: 0.5365
     Episode_Reward/lifting_object: 111.7030
      Episode_Reward/object_height: 0.0466
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 0.88s
                      Time elapsed: 00:04:54
                               ETA: 00:27:02

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 113987 steps/s (collection: 0.774s, learning 0.088s)
             Mean action noise std: 1.60
          Mean value_function loss: 357.9438
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 14.8255
                       Mean reward: 633.85
               Mean episode length: 243.13
    Episode_Reward/reaching_object: 0.5591
     Episode_Reward/lifting_object: 117.1328
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 0.86s
                      Time elapsed: 00:04:54
                               ETA: 00:27:01

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 110920 steps/s (collection: 0.794s, learning 0.092s)
             Mean action noise std: 1.60
          Mean value_function loss: 321.0953
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 14.8319
                       Mean reward: 570.91
               Mean episode length: 237.38
    Episode_Reward/reaching_object: 0.5634
     Episode_Reward/lifting_object: 116.8087
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 0.89s
                      Time elapsed: 00:04:55
                               ETA: 00:27:00

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 114415 steps/s (collection: 0.771s, learning 0.089s)
             Mean action noise std: 1.60
          Mean value_function loss: 299.5490
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 14.8361
                       Mean reward: 565.44
               Mean episode length: 238.81
    Episode_Reward/reaching_object: 0.5384
     Episode_Reward/lifting_object: 111.0577
      Episode_Reward/object_height: 0.0463
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 0.86s
                      Time elapsed: 00:04:56
                               ETA: 00:26:58

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 114992 steps/s (collection: 0.767s, learning 0.088s)
             Mean action noise std: 1.60
          Mean value_function loss: 298.4708
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 14.8336
                       Mean reward: 589.88
               Mean episode length: 238.95
    Episode_Reward/reaching_object: 0.5549
     Episode_Reward/lifting_object: 114.6764
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 0.85s
                      Time elapsed: 00:04:57
                               ETA: 00:26:57

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 115127 steps/s (collection: 0.766s, learning 0.088s)
             Mean action noise std: 1.60
          Mean value_function loss: 267.7448
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 14.8299
                       Mean reward: 658.25
               Mean episode length: 241.43
    Episode_Reward/reaching_object: 0.5794
     Episode_Reward/lifting_object: 122.7724
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 0.85s
                      Time elapsed: 00:04:58
                               ETA: 00:26:55

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 113083 steps/s (collection: 0.778s, learning 0.092s)
             Mean action noise std: 1.60
          Mean value_function loss: 259.5835
               Mean surrogate loss: 0.0132
                 Mean entropy loss: 14.8296
                       Mean reward: 685.49
               Mean episode length: 245.50
    Episode_Reward/reaching_object: 0.6014
     Episode_Reward/lifting_object: 127.3715
      Episode_Reward/object_height: 0.0527
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 0.87s
                      Time elapsed: 00:04:59
                               ETA: 00:26:54

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 111624 steps/s (collection: 0.785s, learning 0.096s)
             Mean action noise std: 1.60
          Mean value_function loss: 282.2340
               Mean surrogate loss: 0.0116
                 Mean entropy loss: 14.8299
                       Mean reward: 611.29
               Mean episode length: 240.29
    Episode_Reward/reaching_object: 0.5853
     Episode_Reward/lifting_object: 125.1042
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 0.88s
                      Time elapsed: 00:05:00
                               ETA: 00:26:52

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 112620 steps/s (collection: 0.779s, learning 0.094s)
             Mean action noise std: 1.60
          Mean value_function loss: 304.1948
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 14.8308
                       Mean reward: 650.05
               Mean episode length: 245.03
    Episode_Reward/reaching_object: 0.5890
     Episode_Reward/lifting_object: 125.1602
      Episode_Reward/object_height: 0.0518
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 0.87s
                      Time elapsed: 00:05:01
                               ETA: 00:26:51

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 110595 steps/s (collection: 0.789s, learning 0.099s)
             Mean action noise std: 1.60
          Mean value_function loss: 318.3325
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 14.8342
                       Mean reward: 560.46
               Mean episode length: 229.50
    Episode_Reward/reaching_object: 0.5767
     Episode_Reward/lifting_object: 122.7414
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 0.89s
                      Time elapsed: 00:05:01
                               ETA: 00:26:50

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 111044 steps/s (collection: 0.783s, learning 0.103s)
             Mean action noise std: 1.61
          Mean value_function loss: 288.6094
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 14.8372
                       Mean reward: 554.15
               Mean episode length: 223.42
    Episode_Reward/reaching_object: 0.5677
     Episode_Reward/lifting_object: 120.4039
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 0.89s
                      Time elapsed: 00:05:02
                               ETA: 00:26:48

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 112038 steps/s (collection: 0.779s, learning 0.098s)
             Mean action noise std: 1.61
          Mean value_function loss: 277.0544
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 14.8393
                       Mean reward: 680.08
               Mean episode length: 233.96
    Episode_Reward/reaching_object: 0.5762
     Episode_Reward/lifting_object: 123.2097
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 0.88s
                      Time elapsed: 00:05:03
                               ETA: 00:26:47

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 111875 steps/s (collection: 0.787s, learning 0.092s)
             Mean action noise std: 1.61
          Mean value_function loss: 294.7583
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.8410
                       Mean reward: 594.55
               Mean episode length: 232.36
    Episode_Reward/reaching_object: 0.5867
     Episode_Reward/lifting_object: 126.6022
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 0.88s
                      Time elapsed: 00:05:04
                               ETA: 00:26:45

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 110227 steps/s (collection: 0.773s, learning 0.119s)
             Mean action noise std: 1.61
          Mean value_function loss: 284.6636
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.8438
                       Mean reward: 584.25
               Mean episode length: 236.11
    Episode_Reward/reaching_object: 0.5578
     Episode_Reward/lifting_object: 116.6487
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 0.89s
                      Time elapsed: 00:05:05
                               ETA: 00:26:44

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 114782 steps/s (collection: 0.760s, learning 0.096s)
             Mean action noise std: 1.61
          Mean value_function loss: 230.2754
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 14.8424
                       Mean reward: 676.23
               Mean episode length: 241.74
    Episode_Reward/reaching_object: 0.5882
     Episode_Reward/lifting_object: 123.7211
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 0.86s
                      Time elapsed: 00:05:06
                               ETA: 00:26:43

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 112861 steps/s (collection: 0.773s, learning 0.099s)
             Mean action noise std: 1.61
          Mean value_function loss: 278.9817
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.8382
                       Mean reward: 611.01
               Mean episode length: 241.51
    Episode_Reward/reaching_object: 0.5847
     Episode_Reward/lifting_object: 121.7221
      Episode_Reward/object_height: 0.0508
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 0.87s
                      Time elapsed: 00:05:07
                               ETA: 00:26:41

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 111885 steps/s (collection: 0.787s, learning 0.092s)
             Mean action noise std: 1.61
          Mean value_function loss: 273.2678
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 14.8322
                       Mean reward: 626.07
               Mean episode length: 245.96
    Episode_Reward/reaching_object: 0.6044
     Episode_Reward/lifting_object: 128.2419
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 0.88s
                      Time elapsed: 00:05:08
                               ETA: 00:26:40

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 114315 steps/s (collection: 0.771s, learning 0.089s)
             Mean action noise std: 1.61
          Mean value_function loss: 309.3140
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.8300
                       Mean reward: 565.40
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 0.5677
     Episode_Reward/lifting_object: 118.4508
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 0.86s
                      Time elapsed: 00:05:08
                               ETA: 00:26:39

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 111003 steps/s (collection: 0.792s, learning 0.094s)
             Mean action noise std: 1.61
          Mean value_function loss: 296.7240
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 14.8299
                       Mean reward: 609.28
               Mean episode length: 242.62
    Episode_Reward/reaching_object: 0.5280
     Episode_Reward/lifting_object: 106.3474
      Episode_Reward/object_height: 0.0457
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 0.89s
                      Time elapsed: 00:05:09
                               ETA: 00:26:37

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 113005 steps/s (collection: 0.782s, learning 0.088s)
             Mean action noise std: 1.61
          Mean value_function loss: 282.5878
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.8300
                       Mean reward: 560.47
               Mean episode length: 240.05
    Episode_Reward/reaching_object: 0.5294
     Episode_Reward/lifting_object: 106.8685
      Episode_Reward/object_height: 0.0456
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 0.87s
                      Time elapsed: 00:05:10
                               ETA: 00:26:36

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 113943 steps/s (collection: 0.775s, learning 0.088s)
             Mean action noise std: 1.61
          Mean value_function loss: 307.0796
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.8331
                       Mean reward: 547.26
               Mean episode length: 238.92
    Episode_Reward/reaching_object: 0.5643
     Episode_Reward/lifting_object: 116.9612
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 0.86s
                      Time elapsed: 00:05:11
                               ETA: 00:26:34

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 111923 steps/s (collection: 0.786s, learning 0.092s)
             Mean action noise std: 1.61
          Mean value_function loss: 308.9310
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 14.8364
                       Mean reward: 590.33
               Mean episode length: 241.11
    Episode_Reward/reaching_object: 0.5721
     Episode_Reward/lifting_object: 119.5770
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 0.88s
                      Time elapsed: 00:05:12
                               ETA: 00:26:33

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 112888 steps/s (collection: 0.776s, learning 0.095s)
             Mean action noise std: 1.61
          Mean value_function loss: 314.2195
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 14.8379
                       Mean reward: 649.45
               Mean episode length: 243.48
    Episode_Reward/reaching_object: 0.5788
     Episode_Reward/lifting_object: 122.1127
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 0.87s
                      Time elapsed: 00:05:13
                               ETA: 00:26:32

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 114274 steps/s (collection: 0.771s, learning 0.090s)
             Mean action noise std: 1.61
          Mean value_function loss: 280.0423
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.8416
                       Mean reward: 609.73
               Mean episode length: 234.01
    Episode_Reward/reaching_object: 0.5803
     Episode_Reward/lifting_object: 122.0914
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 0.86s
                      Time elapsed: 00:05:14
                               ETA: 00:26:30

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 113674 steps/s (collection: 0.777s, learning 0.088s)
             Mean action noise std: 1.61
          Mean value_function loss: 285.7539
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.8440
                       Mean reward: 649.75
               Mean episode length: 239.01
    Episode_Reward/reaching_object: 0.5865
     Episode_Reward/lifting_object: 124.5565
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 0.86s
                      Time elapsed: 00:05:15
                               ETA: 00:26:29

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 111015 steps/s (collection: 0.798s, learning 0.088s)
             Mean action noise std: 1.61
          Mean value_function loss: 275.5829
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 14.8468
                       Mean reward: 600.96
               Mean episode length: 239.16
    Episode_Reward/reaching_object: 0.5806
     Episode_Reward/lifting_object: 122.3899
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 0.89s
                      Time elapsed: 00:05:15
                               ETA: 00:26:28

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 110217 steps/s (collection: 0.803s, learning 0.089s)
             Mean action noise std: 1.61
          Mean value_function loss: 263.7938
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 14.8486
                       Mean reward: 627.79
               Mean episode length: 236.67
    Episode_Reward/reaching_object: 0.5945
     Episode_Reward/lifting_object: 127.2746
      Episode_Reward/object_height: 0.0538
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 0.89s
                      Time elapsed: 00:05:16
                               ETA: 00:26:26

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 45750 steps/s (collection: 2.048s, learning 0.101s)
             Mean action noise std: 1.61
          Mean value_function loss: 281.3838
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 14.8526
                       Mean reward: 673.57
               Mean episode length: 234.96
    Episode_Reward/reaching_object: 0.5884
     Episode_Reward/lifting_object: 126.2186
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 2.15s
                      Time elapsed: 00:05:18
                               ETA: 00:26:31

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 32947 steps/s (collection: 2.863s, learning 0.120s)
             Mean action noise std: 1.61
          Mean value_function loss: 277.4536
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 14.8541
                       Mean reward: 711.16
               Mean episode length: 241.80
    Episode_Reward/reaching_object: 0.6095
     Episode_Reward/lifting_object: 131.9335
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 2.98s
                      Time elapsed: 00:05:21
                               ETA: 00:26:41

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 31201 steps/s (collection: 3.018s, learning 0.133s)
             Mean action noise std: 1.61
          Mean value_function loss: 256.5362
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 14.8564
                       Mean reward: 649.70
               Mean episode length: 231.99
    Episode_Reward/reaching_object: 0.6131
     Episode_Reward/lifting_object: 132.9878
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 3.15s
                      Time elapsed: 00:05:25
                               ETA: 00:26:50

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 31981 steps/s (collection: 2.947s, learning 0.127s)
             Mean action noise std: 1.61
          Mean value_function loss: 275.1423
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.8587
                       Mean reward: 659.76
               Mean episode length: 237.87
    Episode_Reward/reaching_object: 0.5843
     Episode_Reward/lifting_object: 124.2679
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 3.07s
                      Time elapsed: 00:05:28
                               ETA: 00:27:00

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 30851 steps/s (collection: 3.054s, learning 0.132s)
             Mean action noise std: 1.61
          Mean value_function loss: 244.2304
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 14.8598
                       Mean reward: 539.31
               Mean episode length: 237.33
    Episode_Reward/reaching_object: 0.5728
     Episode_Reward/lifting_object: 122.0582
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 3.19s
                      Time elapsed: 00:05:31
                               ETA: 00:27:10

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 32389 steps/s (collection: 2.919s, learning 0.116s)
             Mean action noise std: 1.61
          Mean value_function loss: 266.2609
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 14.8617
                       Mean reward: 615.72
               Mean episode length: 240.98
    Episode_Reward/reaching_object: 0.5475
     Episode_Reward/lifting_object: 112.7789
      Episode_Reward/object_height: 0.0482
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 3.04s
                      Time elapsed: 00:05:34
                               ETA: 00:27:19

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 30699 steps/s (collection: 3.067s, learning 0.135s)
             Mean action noise std: 1.62
          Mean value_function loss: 286.7107
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 14.8651
                       Mean reward: 612.61
               Mean episode length: 241.29
    Episode_Reward/reaching_object: 0.5746
     Episode_Reward/lifting_object: 120.6556
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 3.20s
                      Time elapsed: 00:05:37
                               ETA: 00:27:29

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 30684 steps/s (collection: 3.085s, learning 0.119s)
             Mean action noise std: 1.62
          Mean value_function loss: 259.6460
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 14.8676
                       Mean reward: 540.08
               Mean episode length: 242.01
    Episode_Reward/reaching_object: 0.5328
     Episode_Reward/lifting_object: 107.3601
      Episode_Reward/object_height: 0.0459
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 3.20s
                      Time elapsed: 00:05:40
                               ETA: 00:27:38

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 22694 steps/s (collection: 4.199s, learning 0.133s)
             Mean action noise std: 1.62
          Mean value_function loss: 242.7936
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.8662
                       Mean reward: 688.34
               Mean episode length: 241.26
    Episode_Reward/reaching_object: 0.6079
     Episode_Reward/lifting_object: 131.4025
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 4.33s
                      Time elapsed: 00:05:45
                               ETA: 00:27:54

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 98285 steps/s (collection: 0.893s, learning 0.107s)
             Mean action noise std: 1.62
          Mean value_function loss: 283.3659
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.8691
                       Mean reward: 555.58
               Mean episode length: 236.75
    Episode_Reward/reaching_object: 0.5506
     Episode_Reward/lifting_object: 114.6562
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 1.00s
                      Time elapsed: 00:05:46
                               ETA: 00:27:53

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 110894 steps/s (collection: 0.797s, learning 0.090s)
             Mean action noise std: 1.62
          Mean value_function loss: 292.7786
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 14.8712
                       Mean reward: 618.56
               Mean episode length: 237.87
    Episode_Reward/reaching_object: 0.5840
     Episode_Reward/lifting_object: 124.3567
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 0.89s
                      Time elapsed: 00:05:47
                               ETA: 00:27:51

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 111976 steps/s (collection: 0.785s, learning 0.093s)
             Mean action noise std: 1.62
          Mean value_function loss: 274.9927
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.8707
                       Mean reward: 653.55
               Mean episode length: 242.47
    Episode_Reward/reaching_object: 0.5903
     Episode_Reward/lifting_object: 125.1293
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 0.88s
                      Time elapsed: 00:05:47
                               ETA: 00:27:49

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 112967 steps/s (collection: 0.776s, learning 0.094s)
             Mean action noise std: 1.62
          Mean value_function loss: 249.1322
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 14.8693
                       Mean reward: 667.01
               Mean episode length: 238.34
    Episode_Reward/reaching_object: 0.5799
     Episode_Reward/lifting_object: 123.7531
      Episode_Reward/object_height: 0.0527
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 0.87s
                      Time elapsed: 00:05:48
                               ETA: 00:27:48

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 115203 steps/s (collection: 0.764s, learning 0.089s)
             Mean action noise std: 1.62
          Mean value_function loss: 219.0885
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 14.8687
                       Mean reward: 646.99
               Mean episode length: 245.28
    Episode_Reward/reaching_object: 0.5918
     Episode_Reward/lifting_object: 125.3016
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 0.85s
                      Time elapsed: 00:05:49
                               ETA: 00:27:46

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 114057 steps/s (collection: 0.758s, learning 0.104s)
             Mean action noise std: 1.62
          Mean value_function loss: 249.4985
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.8697
                       Mean reward: 693.50
               Mean episode length: 245.55
    Episode_Reward/reaching_object: 0.6126
     Episode_Reward/lifting_object: 130.3617
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 0.86s
                      Time elapsed: 00:05:50
                               ETA: 00:27:44

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 112039 steps/s (collection: 0.783s, learning 0.094s)
             Mean action noise std: 1.62
          Mean value_function loss: 246.3106
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 14.8686
                       Mean reward: 650.08
               Mean episode length: 239.88
    Episode_Reward/reaching_object: 0.5952
     Episode_Reward/lifting_object: 127.6786
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 0.88s
                      Time elapsed: 00:05:51
                               ETA: 00:27:43

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 110897 steps/s (collection: 0.796s, learning 0.091s)
             Mean action noise std: 1.62
          Mean value_function loss: 250.8693
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 14.8695
                       Mean reward: 667.82
               Mean episode length: 239.01
    Episode_Reward/reaching_object: 0.6134
     Episode_Reward/lifting_object: 131.9605
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 0.89s
                      Time elapsed: 00:05:52
                               ETA: 00:27:41

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 115753 steps/s (collection: 0.763s, learning 0.086s)
             Mean action noise std: 1.62
          Mean value_function loss: 218.6694
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 14.8696
                       Mean reward: 657.11
               Mean episode length: 243.33
    Episode_Reward/reaching_object: 0.5965
     Episode_Reward/lifting_object: 127.8567
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 0.85s
                      Time elapsed: 00:05:53
                               ETA: 00:27:39

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 112305 steps/s (collection: 0.783s, learning 0.092s)
             Mean action noise std: 1.62
          Mean value_function loss: 214.0391
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 14.8714
                       Mean reward: 617.32
               Mean episode length: 243.99
    Episode_Reward/reaching_object: 0.6183
     Episode_Reward/lifting_object: 132.6996
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 0.88s
                      Time elapsed: 00:05:53
                               ETA: 00:27:38

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 114741 steps/s (collection: 0.770s, learning 0.087s)
             Mean action noise std: 1.62
          Mean value_function loss: 214.8721
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 14.8761
                       Mean reward: 689.23
               Mean episode length: 244.85
    Episode_Reward/reaching_object: 0.6214
     Episode_Reward/lifting_object: 133.2037
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 0.86s
                      Time elapsed: 00:05:54
                               ETA: 00:27:36

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 110351 steps/s (collection: 0.789s, learning 0.102s)
             Mean action noise std: 1.62
          Mean value_function loss: 258.7419
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 14.8853
                       Mean reward: 684.98
               Mean episode length: 242.84
    Episode_Reward/reaching_object: 0.6242
     Episode_Reward/lifting_object: 134.5048
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 0.89s
                      Time elapsed: 00:05:55
                               ETA: 00:27:34

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 111807 steps/s (collection: 0.775s, learning 0.104s)
             Mean action noise std: 1.62
          Mean value_function loss: 243.8467
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 14.8928
                       Mean reward: 632.63
               Mean episode length: 237.47
    Episode_Reward/reaching_object: 0.6086
     Episode_Reward/lifting_object: 129.9338
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 0.88s
                      Time elapsed: 00:05:56
                               ETA: 00:27:33

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 111677 steps/s (collection: 0.773s, learning 0.107s)
             Mean action noise std: 1.62
          Mean value_function loss: 268.3957
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 14.8963
                       Mean reward: 699.93
               Mean episode length: 241.04
    Episode_Reward/reaching_object: 0.6205
     Episode_Reward/lifting_object: 134.4306
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 0.88s
                      Time elapsed: 00:05:57
                               ETA: 00:27:31

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 113621 steps/s (collection: 0.758s, learning 0.107s)
             Mean action noise std: 1.62
          Mean value_function loss: 216.3857
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 14.8985
                       Mean reward: 684.49
               Mean episode length: 240.86
    Episode_Reward/reaching_object: 0.6352
     Episode_Reward/lifting_object: 138.6484
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 0.87s
                      Time elapsed: 00:05:58
                               ETA: 00:27:30

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 113287 steps/s (collection: 0.766s, learning 0.102s)
             Mean action noise std: 1.63
          Mean value_function loss: 284.1601
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.9010
                       Mean reward: 723.15
               Mean episode length: 246.22
    Episode_Reward/reaching_object: 0.6416
     Episode_Reward/lifting_object: 139.4457
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 0.87s
                      Time elapsed: 00:05:59
                               ETA: 00:27:28

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 110682 steps/s (collection: 0.785s, learning 0.104s)
             Mean action noise std: 1.63
          Mean value_function loss: 243.1014
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 14.9044
                       Mean reward: 726.66
               Mean episode length: 244.97
    Episode_Reward/reaching_object: 0.6253
     Episode_Reward/lifting_object: 136.3882
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 0.89s
                      Time elapsed: 00:06:00
                               ETA: 00:27:26

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 111857 steps/s (collection: 0.771s, learning 0.108s)
             Mean action noise std: 1.63
          Mean value_function loss: 286.8326
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 14.9076
                       Mean reward: 657.37
               Mean episode length: 234.28
    Episode_Reward/reaching_object: 0.6304
     Episode_Reward/lifting_object: 138.4126
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 0.88s
                      Time elapsed: 00:06:00
                               ETA: 00:27:25

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 113528 steps/s (collection: 0.776s, learning 0.090s)
             Mean action noise std: 1.63
          Mean value_function loss: 246.0326
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 14.9114
                       Mean reward: 649.12
               Mean episode length: 231.34
    Episode_Reward/reaching_object: 0.6068
     Episode_Reward/lifting_object: 133.1100
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 0.87s
                      Time elapsed: 00:06:01
                               ETA: 00:27:23

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 113574 steps/s (collection: 0.779s, learning 0.087s)
             Mean action noise std: 1.63
          Mean value_function loss: 227.5033
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 14.9161
                       Mean reward: 678.76
               Mean episode length: 241.32
    Episode_Reward/reaching_object: 0.6077
     Episode_Reward/lifting_object: 132.0326
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 0.87s
                      Time elapsed: 00:06:02
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 115869 steps/s (collection: 0.756s, learning 0.093s)
             Mean action noise std: 1.63
          Mean value_function loss: 195.1141
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.9200
                       Mean reward: 650.43
               Mean episode length: 241.32
    Episode_Reward/reaching_object: 0.6077
     Episode_Reward/lifting_object: 132.7256
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 0.85s
                      Time elapsed: 00:06:03
                               ETA: 00:27:20

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 116421 steps/s (collection: 0.746s, learning 0.098s)
             Mean action noise std: 1.63
          Mean value_function loss: 202.2413
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.9252
                       Mean reward: 606.48
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 0.5843
     Episode_Reward/lifting_object: 125.8383
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 0.84s
                      Time elapsed: 00:06:04
                               ETA: 00:27:18

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 114555 steps/s (collection: 0.762s, learning 0.096s)
             Mean action noise std: 1.63
          Mean value_function loss: 202.5404
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 14.9287
                       Mean reward: 592.94
               Mean episode length: 243.60
    Episode_Reward/reaching_object: 0.5528
     Episode_Reward/lifting_object: 117.1113
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 0.86s
                      Time elapsed: 00:06:05
                               ETA: 00:27:17

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 113325 steps/s (collection: 0.782s, learning 0.085s)
             Mean action noise std: 1.63
          Mean value_function loss: 218.5464
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 14.9279
                       Mean reward: 584.44
               Mean episode length: 242.93
    Episode_Reward/reaching_object: 0.5763
     Episode_Reward/lifting_object: 122.1463
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 0.87s
                      Time elapsed: 00:06:06
                               ETA: 00:27:15

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 110032 steps/s (collection: 0.785s, learning 0.109s)
             Mean action noise std: 1.63
          Mean value_function loss: 219.9811
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 14.9299
                       Mean reward: 659.61
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.5991
     Episode_Reward/lifting_object: 129.0530
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 0.89s
                      Time elapsed: 00:06:07
                               ETA: 00:27:14

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 114082 steps/s (collection: 0.774s, learning 0.088s)
             Mean action noise std: 1.63
          Mean value_function loss: 218.8505
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.9330
                       Mean reward: 670.28
               Mean episode length: 240.80
    Episode_Reward/reaching_object: 0.5752
     Episode_Reward/lifting_object: 122.8376
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 0.86s
                      Time elapsed: 00:06:07
                               ETA: 00:27:12

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 111641 steps/s (collection: 0.795s, learning 0.086s)
             Mean action noise std: 1.63
          Mean value_function loss: 221.1498
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 14.9333
                       Mean reward: 582.06
               Mean episode length: 243.28
    Episode_Reward/reaching_object: 0.5370
     Episode_Reward/lifting_object: 112.4984
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 0.88s
                      Time elapsed: 00:06:08
                               ETA: 00:27:10

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 117345 steps/s (collection: 0.744s, learning 0.094s)
             Mean action noise std: 1.63
          Mean value_function loss: 222.2572
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 14.9323
                       Mean reward: 719.77
               Mean episode length: 247.41
    Episode_Reward/reaching_object: 0.5476
     Episode_Reward/lifting_object: 113.2276
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 0.84s
                      Time elapsed: 00:06:09
                               ETA: 00:27:09

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 116771 steps/s (collection: 0.749s, learning 0.093s)
             Mean action noise std: 1.63
          Mean value_function loss: 223.5776
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 14.9329
                       Mean reward: 589.25
               Mean episode length: 238.99
    Episode_Reward/reaching_object: 0.6009
     Episode_Reward/lifting_object: 128.8799
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 0.84s
                      Time elapsed: 00:06:10
                               ETA: 00:27:07

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 113450 steps/s (collection: 0.775s, learning 0.092s)
             Mean action noise std: 1.63
          Mean value_function loss: 197.2256
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 14.9326
                       Mean reward: 728.63
               Mean episode length: 240.98
    Episode_Reward/reaching_object: 0.6273
     Episode_Reward/lifting_object: 137.0233
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 0.87s
                      Time elapsed: 00:06:11
                               ETA: 00:27:05

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 114510 steps/s (collection: 0.769s, learning 0.090s)
             Mean action noise std: 1.63
          Mean value_function loss: 200.2509
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 14.9339
                       Mean reward: 712.50
               Mean episode length: 242.93
    Episode_Reward/reaching_object: 0.6242
     Episode_Reward/lifting_object: 135.2041
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 0.86s
                      Time elapsed: 00:06:12
                               ETA: 00:27:04

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 113851 steps/s (collection: 0.772s, learning 0.092s)
             Mean action noise std: 1.63
          Mean value_function loss: 195.2353
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 14.9358
                       Mean reward: 665.46
               Mean episode length: 247.18
    Episode_Reward/reaching_object: 0.6529
     Episode_Reward/lifting_object: 141.0251
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 0.86s
                      Time elapsed: 00:06:13
                               ETA: 00:27:02

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 112180 steps/s (collection: 0.776s, learning 0.101s)
             Mean action noise std: 1.64
          Mean value_function loss: 207.1520
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 14.9366
                       Mean reward: 721.96
               Mean episode length: 249.03
    Episode_Reward/reaching_object: 0.6295
     Episode_Reward/lifting_object: 137.2440
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 0.88s
                      Time elapsed: 00:06:13
                               ETA: 00:27:01

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 116233 steps/s (collection: 0.760s, learning 0.086s)
             Mean action noise std: 1.64
          Mean value_function loss: 183.0883
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 14.9370
                       Mean reward: 748.71
               Mean episode length: 240.88
    Episode_Reward/reaching_object: 0.6499
     Episode_Reward/lifting_object: 142.0620
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 0.85s
                      Time elapsed: 00:06:14
                               ETA: 00:26:59

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 114376 steps/s (collection: 0.764s, learning 0.096s)
             Mean action noise std: 1.64
          Mean value_function loss: 190.8369
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 14.9374
                       Mean reward: 717.14
               Mean episode length: 243.81
    Episode_Reward/reaching_object: 0.6715
     Episode_Reward/lifting_object: 148.5294
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 0.86s
                      Time elapsed: 00:06:15
                               ETA: 00:26:57

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 108793 steps/s (collection: 0.791s, learning 0.113s)
             Mean action noise std: 1.64
          Mean value_function loss: 206.3987
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 14.9378
                       Mean reward: 740.01
               Mean episode length: 236.47
    Episode_Reward/reaching_object: 0.6634
     Episode_Reward/lifting_object: 146.9593
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 0.90s
                      Time elapsed: 00:06:16
                               ETA: 00:26:56

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 109254 steps/s (collection: 0.784s, learning 0.116s)
             Mean action noise std: 1.64
          Mean value_function loss: 194.5249
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.9380
                       Mean reward: 764.15
               Mean episode length: 239.27
    Episode_Reward/reaching_object: 0.6488
     Episode_Reward/lifting_object: 142.7466
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 0.90s
                      Time elapsed: 00:06:17
                               ETA: 00:26:55

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 107986 steps/s (collection: 0.806s, learning 0.105s)
             Mean action noise std: 1.64
          Mean value_function loss: 207.3954
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.9386
                       Mean reward: 706.66
               Mean episode length: 241.77
    Episode_Reward/reaching_object: 0.6523
     Episode_Reward/lifting_object: 143.7672
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 0.91s
                      Time elapsed: 00:06:18
                               ETA: 00:26:53

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 110866 steps/s (collection: 0.775s, learning 0.112s)
             Mean action noise std: 1.64
          Mean value_function loss: 221.4420
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 14.9390
                       Mean reward: 721.55
               Mean episode length: 239.92
    Episode_Reward/reaching_object: 0.6505
     Episode_Reward/lifting_object: 142.4840
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 0.89s
                      Time elapsed: 00:06:19
                               ETA: 00:26:52

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 105484 steps/s (collection: 0.823s, learning 0.109s)
             Mean action noise std: 1.64
          Mean value_function loss: 183.6744
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 14.9388
                       Mean reward: 694.66
               Mean episode length: 246.28
    Episode_Reward/reaching_object: 0.6578
     Episode_Reward/lifting_object: 144.3653
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 0.93s
                      Time elapsed: 00:06:20
                               ETA: 00:26:51

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 109228 steps/s (collection: 0.791s, learning 0.109s)
             Mean action noise std: 1.64
          Mean value_function loss: 187.4612
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 14.9393
                       Mean reward: 762.18
               Mean episode length: 244.86
    Episode_Reward/reaching_object: 0.6540
     Episode_Reward/lifting_object: 144.8059
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 0.90s
                      Time elapsed: 00:06:21
                               ETA: 00:26:49

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 113400 steps/s (collection: 0.769s, learning 0.098s)
             Mean action noise std: 1.64
          Mean value_function loss: 193.0803
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.9373
                       Mean reward: 724.61
               Mean episode length: 243.21
    Episode_Reward/reaching_object: 0.6439
     Episode_Reward/lifting_object: 141.8138
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 0.87s
                      Time elapsed: 00:06:21
                               ETA: 00:26:48

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 107489 steps/s (collection: 0.798s, learning 0.117s)
             Mean action noise std: 1.64
          Mean value_function loss: 194.3159
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 14.9364
                       Mean reward: 679.83
               Mean episode length: 240.48
    Episode_Reward/reaching_object: 0.6564
     Episode_Reward/lifting_object: 144.5593
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 0.91s
                      Time elapsed: 00:06:22
                               ETA: 00:26:46

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 111896 steps/s (collection: 0.783s, learning 0.096s)
             Mean action noise std: 1.64
          Mean value_function loss: 204.0211
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 14.9383
                       Mean reward: 688.98
               Mean episode length: 242.56
    Episode_Reward/reaching_object: 0.6433
     Episode_Reward/lifting_object: 139.5896
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 0.88s
                      Time elapsed: 00:06:23
                               ETA: 00:26:45

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 109066 steps/s (collection: 0.791s, learning 0.111s)
             Mean action noise std: 1.64
          Mean value_function loss: 191.6494
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 14.9385
                       Mean reward: 690.33
               Mean episode length: 232.99
    Episode_Reward/reaching_object: 0.6359
     Episode_Reward/lifting_object: 139.8152
      Episode_Reward/object_height: 0.0592
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 0.90s
                      Time elapsed: 00:06:24
                               ETA: 00:26:43

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 112879 steps/s (collection: 0.759s, learning 0.112s)
             Mean action noise std: 1.64
          Mean value_function loss: 190.6525
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 14.9388
                       Mean reward: 718.78
               Mean episode length: 243.51
    Episode_Reward/reaching_object: 0.6561
     Episode_Reward/lifting_object: 144.2668
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 0.87s
                      Time elapsed: 00:06:25
                               ETA: 00:26:42

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 109851 steps/s (collection: 0.778s, learning 0.117s)
             Mean action noise std: 1.64
          Mean value_function loss: 177.2048
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 14.9384
                       Mean reward: 698.47
               Mean episode length: 239.21
    Episode_Reward/reaching_object: 0.6492
     Episode_Reward/lifting_object: 142.6987
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 0.89s
                      Time elapsed: 00:06:26
                               ETA: 00:26:41

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 109816 steps/s (collection: 0.781s, learning 0.114s)
             Mean action noise std: 1.64
          Mean value_function loss: 176.0112
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.9376
                       Mean reward: 711.43
               Mean episode length: 235.98
    Episode_Reward/reaching_object: 0.6591
     Episode_Reward/lifting_object: 145.1567
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 0.90s
                      Time elapsed: 00:06:27
                               ETA: 00:26:39

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 114633 steps/s (collection: 0.767s, learning 0.091s)
             Mean action noise std: 1.64
          Mean value_function loss: 179.6965
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 14.9343
                       Mean reward: 757.96
               Mean episode length: 243.56
    Episode_Reward/reaching_object: 0.6739
     Episode_Reward/lifting_object: 149.0562
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 0.86s
                      Time elapsed: 00:06:28
                               ETA: 00:26:38

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 112471 steps/s (collection: 0.759s, learning 0.115s)
             Mean action noise std: 1.64
          Mean value_function loss: 207.5955
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 14.9379
                       Mean reward: 736.87
               Mean episode length: 245.89
    Episode_Reward/reaching_object: 0.6646
     Episode_Reward/lifting_object: 147.9530
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 0.87s
                      Time elapsed: 00:06:28
                               ETA: 00:26:36

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 113382 steps/s (collection: 0.763s, learning 0.104s)
             Mean action noise std: 1.64
          Mean value_function loss: 180.1225
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 14.9438
                       Mean reward: 741.91
               Mean episode length: 244.72
    Episode_Reward/reaching_object: 0.6435
     Episode_Reward/lifting_object: 144.6472
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 0.87s
                      Time elapsed: 00:06:29
                               ETA: 00:26:35

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 111308 steps/s (collection: 0.777s, learning 0.106s)
             Mean action noise std: 1.64
          Mean value_function loss: 200.6846
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 14.9455
                       Mean reward: 745.06
               Mean episode length: 249.48
    Episode_Reward/reaching_object: 0.6549
     Episode_Reward/lifting_object: 145.1158
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 0.88s
                      Time elapsed: 00:06:30
                               ETA: 00:26:33

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 109644 steps/s (collection: 0.786s, learning 0.111s)
             Mean action noise std: 1.64
          Mean value_function loss: 194.3016
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.9463
                       Mean reward: 682.36
               Mean episode length: 246.27
    Episode_Reward/reaching_object: 0.6364
     Episode_Reward/lifting_object: 139.8419
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 0.90s
                      Time elapsed: 00:06:31
                               ETA: 00:26:32

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 112085 steps/s (collection: 0.761s, learning 0.116s)
             Mean action noise std: 1.64
          Mean value_function loss: 201.6051
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 14.9463
                       Mean reward: 720.60
               Mean episode length: 240.64
    Episode_Reward/reaching_object: 0.6323
     Episode_Reward/lifting_object: 139.4276
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 0.88s
                      Time elapsed: 00:06:32
                               ETA: 00:26:30

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 113090 steps/s (collection: 0.757s, learning 0.112s)
             Mean action noise std: 1.64
          Mean value_function loss: 193.4607
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 14.9473
                       Mean reward: 769.02
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.6696
     Episode_Reward/lifting_object: 148.8773
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 0.87s
                      Time elapsed: 00:06:33
                               ETA: 00:26:29

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 111114 steps/s (collection: 0.771s, learning 0.114s)
             Mean action noise std: 1.64
          Mean value_function loss: 199.0758
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 14.9507
                       Mean reward: 746.43
               Mean episode length: 242.28
    Episode_Reward/reaching_object: 0.6640
     Episode_Reward/lifting_object: 146.5930
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 0.88s
                      Time elapsed: 00:06:34
                               ETA: 00:26:27

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 111824 steps/s (collection: 0.771s, learning 0.108s)
             Mean action noise std: 1.64
          Mean value_function loss: 207.8422
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 14.9550
                       Mean reward: 747.54
               Mean episode length: 242.90
    Episode_Reward/reaching_object: 0.6595
     Episode_Reward/lifting_object: 145.2470
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 0.88s
                      Time elapsed: 00:06:35
                               ETA: 00:26:26

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 113008 steps/s (collection: 0.765s, learning 0.105s)
             Mean action noise std: 1.64
          Mean value_function loss: 189.1322
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.9555
                       Mean reward: 765.13
               Mean episode length: 244.61
    Episode_Reward/reaching_object: 0.6650
     Episode_Reward/lifting_object: 146.9944
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 0.87s
                      Time elapsed: 00:06:36
                               ETA: 00:26:25

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 111224 steps/s (collection: 0.773s, learning 0.111s)
             Mean action noise std: 1.64
          Mean value_function loss: 205.1526
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.9564
                       Mean reward: 717.95
               Mean episode length: 243.31
    Episode_Reward/reaching_object: 0.6638
     Episode_Reward/lifting_object: 146.2701
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 0.88s
                      Time elapsed: 00:06:36
                               ETA: 00:26:23

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 108915 steps/s (collection: 0.791s, learning 0.112s)
             Mean action noise std: 1.64
          Mean value_function loss: 186.4872
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 14.9567
                       Mean reward: 708.58
               Mean episode length: 246.06
    Episode_Reward/reaching_object: 0.6522
     Episode_Reward/lifting_object: 142.6007
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 0.90s
                      Time elapsed: 00:06:37
                               ETA: 00:26:22

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 111543 steps/s (collection: 0.772s, learning 0.109s)
             Mean action noise std: 1.64
          Mean value_function loss: 156.2220
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 14.9575
                       Mean reward: 734.06
               Mean episode length: 244.12
    Episode_Reward/reaching_object: 0.6592
     Episode_Reward/lifting_object: 144.2835
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 0.88s
                      Time elapsed: 00:06:38
                               ETA: 00:26:20

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 113459 steps/s (collection: 0.763s, learning 0.104s)
             Mean action noise std: 1.64
          Mean value_function loss: 171.3569
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 14.9627
                       Mean reward: 710.94
               Mean episode length: 242.15
    Episode_Reward/reaching_object: 0.6661
     Episode_Reward/lifting_object: 145.4717
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 0.87s
                      Time elapsed: 00:06:39
                               ETA: 00:26:19

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 110462 steps/s (collection: 0.781s, learning 0.109s)
             Mean action noise std: 1.65
          Mean value_function loss: 171.1777
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 14.9687
                       Mean reward: 759.73
               Mean episode length: 246.09
    Episode_Reward/reaching_object: 0.6647
     Episode_Reward/lifting_object: 146.6110
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 0.89s
                      Time elapsed: 00:06:40
                               ETA: 00:26:18

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 116930 steps/s (collection: 0.751s, learning 0.090s)
             Mean action noise std: 1.65
          Mean value_function loss: 173.2956
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.9729
                       Mean reward: 756.26
               Mean episode length: 244.55
    Episode_Reward/reaching_object: 0.6763
     Episode_Reward/lifting_object: 150.9071
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 0.84s
                      Time elapsed: 00:06:41
                               ETA: 00:26:16

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 112875 steps/s (collection: 0.760s, learning 0.111s)
             Mean action noise std: 1.65
          Mean value_function loss: 196.0524
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 14.9804
                       Mean reward: 814.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.6950
     Episode_Reward/lifting_object: 155.0933
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 0.87s
                      Time elapsed: 00:06:42
                               ETA: 00:26:15

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 114155 steps/s (collection: 0.749s, learning 0.112s)
             Mean action noise std: 1.65
          Mean value_function loss: 147.9938
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 14.9857
                       Mean reward: 729.14
               Mean episode length: 241.66
    Episode_Reward/reaching_object: 0.6901
     Episode_Reward/lifting_object: 153.8434
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 0.86s
                      Time elapsed: 00:06:43
                               ETA: 00:26:13

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 111359 steps/s (collection: 0.774s, learning 0.108s)
             Mean action noise std: 1.65
          Mean value_function loss: 165.4847
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 14.9908
                       Mean reward: 754.75
               Mean episode length: 244.73
    Episode_Reward/reaching_object: 0.6753
     Episode_Reward/lifting_object: 148.5932
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 0.88s
                      Time elapsed: 00:06:43
                               ETA: 00:26:12

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 110267 steps/s (collection: 0.783s, learning 0.108s)
             Mean action noise std: 1.65
          Mean value_function loss: 178.6152
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 14.9938
                       Mean reward: 772.47
               Mean episode length: 247.88
    Episode_Reward/reaching_object: 0.6921
     Episode_Reward/lifting_object: 154.0268
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 0.89s
                      Time elapsed: 00:06:44
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 112821 steps/s (collection: 0.781s, learning 0.090s)
             Mean action noise std: 1.65
          Mean value_function loss: 200.9735
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 14.9956
                       Mean reward: 733.13
               Mean episode length: 243.50
    Episode_Reward/reaching_object: 0.6749
     Episode_Reward/lifting_object: 150.4030
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 0.87s
                      Time elapsed: 00:06:45
                               ETA: 00:26:09

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 113598 steps/s (collection: 0.767s, learning 0.098s)
             Mean action noise std: 1.65
          Mean value_function loss: 216.1112
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 14.9987
                       Mean reward: 729.49
               Mean episode length: 245.86
    Episode_Reward/reaching_object: 0.6691
     Episode_Reward/lifting_object: 149.4203
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 0.87s
                      Time elapsed: 00:06:46
                               ETA: 00:26:07

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 114642 steps/s (collection: 0.764s, learning 0.094s)
             Mean action noise std: 1.65
          Mean value_function loss: 186.1288
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 14.9996
                       Mean reward: 776.85
               Mean episode length: 244.53
    Episode_Reward/reaching_object: 0.6857
     Episode_Reward/lifting_object: 152.8592
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 0.86s
                      Time elapsed: 00:06:47
                               ETA: 00:26:06

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 110854 steps/s (collection: 0.762s, learning 0.124s)
             Mean action noise std: 1.65
          Mean value_function loss: 182.2955
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 14.9993
                       Mean reward: 761.20
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.6785
     Episode_Reward/lifting_object: 151.0587
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 0.89s
                      Time elapsed: 00:06:48
                               ETA: 00:26:05

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 113111 steps/s (collection: 0.753s, learning 0.116s)
             Mean action noise std: 1.65
          Mean value_function loss: 187.6634
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 14.9980
                       Mean reward: 761.31
               Mean episode length: 245.46
    Episode_Reward/reaching_object: 0.6863
     Episode_Reward/lifting_object: 152.6713
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 0.87s
                      Time elapsed: 00:06:49
                               ETA: 00:26:03

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 111409 steps/s (collection: 0.779s, learning 0.104s)
             Mean action noise std: 1.65
          Mean value_function loss: 197.8574
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 14.9998
                       Mean reward: 709.19
               Mean episode length: 242.02
    Episode_Reward/reaching_object: 0.6601
     Episode_Reward/lifting_object: 145.3816
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 0.88s
                      Time elapsed: 00:06:50
                               ETA: 00:26:02

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 110752 steps/s (collection: 0.781s, learning 0.107s)
             Mean action noise std: 1.65
          Mean value_function loss: 169.1841
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.0013
                       Mean reward: 807.98
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.6776
     Episode_Reward/lifting_object: 150.0887
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 0.89s
                      Time elapsed: 00:06:50
                               ETA: 00:26:00

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 111731 steps/s (collection: 0.777s, learning 0.103s)
             Mean action noise std: 1.65
          Mean value_function loss: 183.2790
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.0011
                       Mean reward: 739.57
               Mean episode length: 247.33
    Episode_Reward/reaching_object: 0.6732
     Episode_Reward/lifting_object: 148.1382
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 0.88s
                      Time elapsed: 00:06:51
                               ETA: 00:25:59

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 111428 steps/s (collection: 0.783s, learning 0.100s)
             Mean action noise std: 1.65
          Mean value_function loss: 179.2972
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.0011
                       Mean reward: 769.42
               Mean episode length: 243.90
    Episode_Reward/reaching_object: 0.6637
     Episode_Reward/lifting_object: 146.2058
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 0.88s
                      Time elapsed: 00:06:52
                               ETA: 00:25:58

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 112502 steps/s (collection: 0.780s, learning 0.094s)
             Mean action noise std: 1.65
          Mean value_function loss: 153.6891
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.0092
                       Mean reward: 703.43
               Mean episode length: 247.09
    Episode_Reward/reaching_object: 0.6647
     Episode_Reward/lifting_object: 147.3908
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 0.87s
                      Time elapsed: 00:06:53
                               ETA: 00:25:56

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 111237 steps/s (collection: 0.765s, learning 0.119s)
             Mean action noise std: 1.65
          Mean value_function loss: 153.4742
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.0122
                       Mean reward: 792.98
               Mean episode length: 246.68
    Episode_Reward/reaching_object: 0.6751
     Episode_Reward/lifting_object: 150.1547
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 0.88s
                      Time elapsed: 00:06:54
                               ETA: 00:25:55

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 109402 steps/s (collection: 0.783s, learning 0.115s)
             Mean action noise std: 1.65
          Mean value_function loss: 148.8970
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.0156
                       Mean reward: 791.57
               Mean episode length: 247.18
    Episode_Reward/reaching_object: 0.6862
     Episode_Reward/lifting_object: 153.0948
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 0.90s
                      Time elapsed: 00:06:55
                               ETA: 00:25:54

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 108956 steps/s (collection: 0.786s, learning 0.116s)
             Mean action noise std: 1.66
          Mean value_function loss: 157.3009
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.0178
                       Mean reward: 767.51
               Mean episode length: 243.24
    Episode_Reward/reaching_object: 0.6776
     Episode_Reward/lifting_object: 150.9472
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 0.90s
                      Time elapsed: 00:06:56
                               ETA: 00:25:52

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 109925 steps/s (collection: 0.786s, learning 0.109s)
             Mean action noise std: 1.66
          Mean value_function loss: 158.7803
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.0205
                       Mean reward: 769.38
               Mean episode length: 247.23
    Episode_Reward/reaching_object: 0.6896
     Episode_Reward/lifting_object: 153.9535
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 0.89s
                      Time elapsed: 00:06:57
                               ETA: 00:25:51

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 112077 steps/s (collection: 0.764s, learning 0.114s)
             Mean action noise std: 1.66
          Mean value_function loss: 146.1264
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.0221
                       Mean reward: 762.37
               Mean episode length: 241.54
    Episode_Reward/reaching_object: 0.6876
     Episode_Reward/lifting_object: 152.2862
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 0.88s
                      Time elapsed: 00:06:57
                               ETA: 00:25:50

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 108888 steps/s (collection: 0.791s, learning 0.112s)
             Mean action noise std: 1.66
          Mean value_function loss: 153.0716
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.0251
                       Mean reward: 775.92
               Mean episode length: 245.07
    Episode_Reward/reaching_object: 0.6842
     Episode_Reward/lifting_object: 152.9689
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 0.90s
                      Time elapsed: 00:06:58
                               ETA: 00:25:48

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 112819 steps/s (collection: 0.767s, learning 0.105s)
             Mean action noise std: 1.66
          Mean value_function loss: 133.5341
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 15.0297
                       Mean reward: 762.77
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.6973
     Episode_Reward/lifting_object: 155.2199
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 0.87s
                      Time elapsed: 00:06:59
                               ETA: 00:25:47

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 111823 steps/s (collection: 0.772s, learning 0.107s)
             Mean action noise std: 1.66
          Mean value_function loss: 140.2329
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 15.0352
                       Mean reward: 814.67
               Mean episode length: 243.31
    Episode_Reward/reaching_object: 0.7030
     Episode_Reward/lifting_object: 157.4310
      Episode_Reward/object_height: 0.0659
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 0.88s
                      Time elapsed: 00:07:00
                               ETA: 00:25:45

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 111474 steps/s (collection: 0.767s, learning 0.115s)
             Mean action noise std: 1.66
          Mean value_function loss: 152.9558
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.0384
                       Mean reward: 782.84
               Mean episode length: 246.50
    Episode_Reward/reaching_object: 0.6712
     Episode_Reward/lifting_object: 149.4100
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 0.88s
                      Time elapsed: 00:07:01
                               ETA: 00:25:44

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 110784 steps/s (collection: 0.770s, learning 0.117s)
             Mean action noise std: 1.66
          Mean value_function loss: 136.5528
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.0374
                       Mean reward: 801.76
               Mean episode length: 245.89
    Episode_Reward/reaching_object: 0.7058
     Episode_Reward/lifting_object: 157.5833
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 0.89s
                      Time elapsed: 00:07:02
                               ETA: 00:25:43

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 110550 steps/s (collection: 0.767s, learning 0.122s)
             Mean action noise std: 1.66
          Mean value_function loss: 128.8903
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.0392
                       Mean reward: 789.80
               Mean episode length: 244.27
    Episode_Reward/reaching_object: 0.7065
     Episode_Reward/lifting_object: 157.3506
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 0.89s
                      Time elapsed: 00:07:03
                               ETA: 00:25:41

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 113279 steps/s (collection: 0.758s, learning 0.110s)
             Mean action noise std: 1.66
          Mean value_function loss: 128.9861
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.0429
                       Mean reward: 805.52
               Mean episode length: 247.11
    Episode_Reward/reaching_object: 0.7155
     Episode_Reward/lifting_object: 159.5493
      Episode_Reward/object_height: 0.0673
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 0.87s
                      Time elapsed: 00:07:04
                               ETA: 00:25:40

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 112139 steps/s (collection: 0.772s, learning 0.105s)
             Mean action noise std: 1.66
          Mean value_function loss: 133.5610
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.0489
                       Mean reward: 777.45
               Mean episode length: 244.46
    Episode_Reward/reaching_object: 0.6874
     Episode_Reward/lifting_object: 153.1845
      Episode_Reward/object_height: 0.0649
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 0.88s
                      Time elapsed: 00:07:05
                               ETA: 00:25:39

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 111730 steps/s (collection: 0.778s, learning 0.102s)
             Mean action noise std: 1.66
          Mean value_function loss: 134.6021
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 15.0518
                       Mean reward: 798.79
               Mean episode length: 249.08
    Episode_Reward/reaching_object: 0.6995
     Episode_Reward/lifting_object: 155.6907
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 0.88s
                      Time elapsed: 00:07:05
                               ETA: 00:25:37

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 112705 steps/s (collection: 0.772s, learning 0.101s)
             Mean action noise std: 1.66
          Mean value_function loss: 113.2880
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.0515
                       Mean reward: 777.60
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.6862
     Episode_Reward/lifting_object: 153.1073
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 0.87s
                      Time elapsed: 00:07:06
                               ETA: 00:25:36

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 110186 steps/s (collection: 0.778s, learning 0.115s)
             Mean action noise std: 1.66
          Mean value_function loss: 126.7772
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 15.0512
                       Mean reward: 814.22
               Mean episode length: 247.24
    Episode_Reward/reaching_object: 0.7154
     Episode_Reward/lifting_object: 160.8834
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 0.89s
                      Time elapsed: 00:07:07
                               ETA: 00:25:35

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 114333 steps/s (collection: 0.755s, learning 0.105s)
             Mean action noise std: 1.66
          Mean value_function loss: 112.9770
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.0520
                       Mean reward: 789.79
               Mean episode length: 244.76
    Episode_Reward/reaching_object: 0.6926
     Episode_Reward/lifting_object: 153.8989
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 0.86s
                      Time elapsed: 00:07:08
                               ETA: 00:25:33

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 112210 steps/s (collection: 0.761s, learning 0.115s)
             Mean action noise std: 1.66
          Mean value_function loss: 138.8290
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.0558
                       Mean reward: 801.04
               Mean episode length: 247.12
    Episode_Reward/reaching_object: 0.7106
     Episode_Reward/lifting_object: 159.6238
      Episode_Reward/object_height: 0.0679
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 0.88s
                      Time elapsed: 00:07:09
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 111799 steps/s (collection: 0.764s, learning 0.116s)
             Mean action noise std: 1.67
          Mean value_function loss: 153.2241
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.0653
                       Mean reward: 793.04
               Mean episode length: 244.14
    Episode_Reward/reaching_object: 0.7029
     Episode_Reward/lifting_object: 156.7920
      Episode_Reward/object_height: 0.0668
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 0.88s
                      Time elapsed: 00:07:10
                               ETA: 00:25:31

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 111734 steps/s (collection: 0.766s, learning 0.114s)
             Mean action noise std: 1.67
          Mean value_function loss: 130.9461
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 15.0689
                       Mean reward: 796.94
               Mean episode length: 246.05
    Episode_Reward/reaching_object: 0.7027
     Episode_Reward/lifting_object: 156.1524
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 0.88s
                      Time elapsed: 00:07:11
                               ETA: 00:25:29

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 110364 steps/s (collection: 0.779s, learning 0.112s)
             Mean action noise std: 1.67
          Mean value_function loss: 144.9695
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.0671
                       Mean reward: 817.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7001
     Episode_Reward/lifting_object: 157.6478
      Episode_Reward/object_height: 0.0671
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 0.89s
                      Time elapsed: 00:07:12
                               ETA: 00:25:28

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 113785 steps/s (collection: 0.771s, learning 0.093s)
             Mean action noise std: 1.67
          Mean value_function loss: 121.0176
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.0644
                       Mean reward: 800.91
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7093
     Episode_Reward/lifting_object: 158.9057
      Episode_Reward/object_height: 0.0675
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 0.86s
                      Time elapsed: 00:07:12
                               ETA: 00:25:27

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 110026 steps/s (collection: 0.776s, learning 0.118s)
             Mean action noise std: 1.67
          Mean value_function loss: 125.8903
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 15.0647
                       Mean reward: 738.06
               Mean episode length: 240.66
    Episode_Reward/reaching_object: 0.6825
     Episode_Reward/lifting_object: 149.6917
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 0.89s
                      Time elapsed: 00:07:13
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 111350 steps/s (collection: 0.775s, learning 0.108s)
             Mean action noise std: 1.67
          Mean value_function loss: 136.7918
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.0673
                       Mean reward: 817.95
               Mean episode length: 247.79
    Episode_Reward/reaching_object: 0.7246
     Episode_Reward/lifting_object: 162.7722
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 0.88s
                      Time elapsed: 00:07:14
                               ETA: 00:25:24

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 111088 steps/s (collection: 0.781s, learning 0.104s)
             Mean action noise std: 1.67
          Mean value_function loss: 144.6750
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.0665
                       Mean reward: 764.41
               Mean episode length: 246.01
    Episode_Reward/reaching_object: 0.7073
     Episode_Reward/lifting_object: 157.5868
      Episode_Reward/object_height: 0.0669
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 0.88s
                      Time elapsed: 00:07:15
                               ETA: 00:25:23

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 110852 steps/s (collection: 0.770s, learning 0.117s)
             Mean action noise std: 1.67
          Mean value_function loss: 132.5494
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 15.0685
                       Mean reward: 751.65
               Mean episode length: 246.06
    Episode_Reward/reaching_object: 0.7034
     Episode_Reward/lifting_object: 156.2858
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 0.89s
                      Time elapsed: 00:07:16
                               ETA: 00:25:21

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 111502 steps/s (collection: 0.769s, learning 0.113s)
             Mean action noise std: 1.67
          Mean value_function loss: 139.6777
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.0715
                       Mean reward: 705.25
               Mean episode length: 237.51
    Episode_Reward/reaching_object: 0.6904
     Episode_Reward/lifting_object: 153.0963
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 0.88s
                      Time elapsed: 00:07:17
                               ETA: 00:25:20

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 109348 steps/s (collection: 0.780s, learning 0.119s)
             Mean action noise std: 1.67
          Mean value_function loss: 145.8961
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.0706
                       Mean reward: 749.81
               Mean episode length: 246.88
    Episode_Reward/reaching_object: 0.6926
     Episode_Reward/lifting_object: 154.4792
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 0.90s
                      Time elapsed: 00:07:18
                               ETA: 00:25:19

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 112968 steps/s (collection: 0.780s, learning 0.090s)
             Mean action noise std: 1.67
          Mean value_function loss: 132.6788
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.0678
                       Mean reward: 795.08
               Mean episode length: 245.92
    Episode_Reward/reaching_object: 0.7122
     Episode_Reward/lifting_object: 158.9721
      Episode_Reward/object_height: 0.0672
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 0.87s
                      Time elapsed: 00:07:19
                               ETA: 00:25:17

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 110439 steps/s (collection: 0.792s, learning 0.099s)
             Mean action noise std: 1.67
          Mean value_function loss: 149.2161
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 15.0676
                       Mean reward: 796.13
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.6998
     Episode_Reward/lifting_object: 156.7936
      Episode_Reward/object_height: 0.0666
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 0.89s
                      Time elapsed: 00:07:20
                               ETA: 00:25:16

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 105831 steps/s (collection: 0.811s, learning 0.118s)
             Mean action noise std: 1.67
          Mean value_function loss: 130.4310
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.0742
                       Mean reward: 796.55
               Mean episode length: 242.22
    Episode_Reward/reaching_object: 0.7115
     Episode_Reward/lifting_object: 160.3030
      Episode_Reward/object_height: 0.0679
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 0.93s
                      Time elapsed: 00:07:20
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 111846 steps/s (collection: 0.770s, learning 0.109s)
             Mean action noise std: 1.67
          Mean value_function loss: 133.7345
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 15.0831
                       Mean reward: 746.07
               Mean episode length: 241.96
    Episode_Reward/reaching_object: 0.7070
     Episode_Reward/lifting_object: 157.6962
      Episode_Reward/object_height: 0.0668
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 0.88s
                      Time elapsed: 00:07:21
                               ETA: 00:25:14

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 109664 steps/s (collection: 0.782s, learning 0.114s)
             Mean action noise std: 1.67
          Mean value_function loss: 131.9875
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.0877
                       Mean reward: 748.56
               Mean episode length: 246.63
    Episode_Reward/reaching_object: 0.7075
     Episode_Reward/lifting_object: 155.6362
      Episode_Reward/object_height: 0.0659
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 0.90s
                      Time elapsed: 00:07:22
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 113547 steps/s (collection: 0.779s, learning 0.087s)
             Mean action noise std: 1.67
          Mean value_function loss: 137.9848
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.0933
                       Mean reward: 779.28
               Mean episode length: 243.91
    Episode_Reward/reaching_object: 0.6852
     Episode_Reward/lifting_object: 150.6283
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 0.87s
                      Time elapsed: 00:07:23
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 108522 steps/s (collection: 0.790s, learning 0.115s)
             Mean action noise std: 1.68
          Mean value_function loss: 130.6046
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.0987
                       Mean reward: 808.59
               Mean episode length: 246.08
    Episode_Reward/reaching_object: 0.7064
     Episode_Reward/lifting_object: 157.2741
      Episode_Reward/object_height: 0.0668
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 0.91s
                      Time elapsed: 00:07:24
                               ETA: 00:25:10

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 114439 steps/s (collection: 0.754s, learning 0.105s)
             Mean action noise std: 1.68
          Mean value_function loss: 128.8503
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 15.0995
                       Mean reward: 758.50
               Mean episode length: 241.17
    Episode_Reward/reaching_object: 0.6913
     Episode_Reward/lifting_object: 153.6431
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 0.86s
                      Time elapsed: 00:07:25
                               ETA: 00:25:08

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 115251 steps/s (collection: 0.757s, learning 0.096s)
             Mean action noise std: 1.68
          Mean value_function loss: 121.6709
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.1016
                       Mean reward: 787.67
               Mean episode length: 246.19
    Episode_Reward/reaching_object: 0.6956
     Episode_Reward/lifting_object: 155.2319
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 0.85s
                      Time elapsed: 00:07:26
                               ETA: 00:25:07

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 113122 steps/s (collection: 0.759s, learning 0.110s)
             Mean action noise std: 1.68
          Mean value_function loss: 126.5751
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.1075
                       Mean reward: 802.71
               Mean episode length: 241.77
    Episode_Reward/reaching_object: 0.6947
     Episode_Reward/lifting_object: 156.1203
      Episode_Reward/object_height: 0.0671
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 0.87s
                      Time elapsed: 00:07:27
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 110741 steps/s (collection: 0.775s, learning 0.113s)
             Mean action noise std: 1.68
          Mean value_function loss: 105.2152
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.1097
                       Mean reward: 788.83
               Mean episode length: 245.83
    Episode_Reward/reaching_object: 0.7041
     Episode_Reward/lifting_object: 157.7977
      Episode_Reward/object_height: 0.0680
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 0.89s
                      Time elapsed: 00:07:27
                               ETA: 00:25:04

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 106399 steps/s (collection: 0.810s, learning 0.114s)
             Mean action noise std: 1.68
          Mean value_function loss: 103.3492
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 15.1113
                       Mean reward: 802.39
               Mean episode length: 244.91
    Episode_Reward/reaching_object: 0.6999
     Episode_Reward/lifting_object: 157.0894
      Episode_Reward/object_height: 0.0677
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 0.92s
                      Time elapsed: 00:07:28
                               ETA: 00:25:03

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 108043 steps/s (collection: 0.793s, learning 0.117s)
             Mean action noise std: 1.68
          Mean value_function loss: 127.0219
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.1125
                       Mean reward: 818.79
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.7080
     Episode_Reward/lifting_object: 159.6073
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 0.91s
                      Time elapsed: 00:07:29
                               ETA: 00:25:02

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 113260 steps/s (collection: 0.753s, learning 0.115s)
             Mean action noise std: 1.68
          Mean value_function loss: 114.2877
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.1145
                       Mean reward: 795.65
               Mean episode length: 244.78
    Episode_Reward/reaching_object: 0.6967
     Episode_Reward/lifting_object: 156.9005
      Episode_Reward/object_height: 0.0679
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 0.87s
                      Time elapsed: 00:07:30
                               ETA: 00:25:01

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 109047 steps/s (collection: 0.783s, learning 0.118s)
             Mean action noise std: 1.68
          Mean value_function loss: 104.9881
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 15.1195
                       Mean reward: 760.79
               Mean episode length: 245.50
    Episode_Reward/reaching_object: 0.6819
     Episode_Reward/lifting_object: 152.7375
      Episode_Reward/object_height: 0.0660
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 0.90s
                      Time elapsed: 00:07:31
                               ETA: 00:25:00

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 116431 steps/s (collection: 0.753s, learning 0.091s)
             Mean action noise std: 1.68
          Mean value_function loss: 99.4037
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 15.1226
                       Mean reward: 742.65
               Mean episode length: 239.85
    Episode_Reward/reaching_object: 0.6863
     Episode_Reward/lifting_object: 154.3298
      Episode_Reward/object_height: 0.0667
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 0.84s
                      Time elapsed: 00:07:32
                               ETA: 00:24:58

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 110291 steps/s (collection: 0.781s, learning 0.111s)
             Mean action noise std: 1.68
          Mean value_function loss: 97.4783
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 15.1239
                       Mean reward: 797.02
               Mean episode length: 246.58
    Episode_Reward/reaching_object: 0.6986
     Episode_Reward/lifting_object: 156.7468
      Episode_Reward/object_height: 0.0676
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 0.89s
                      Time elapsed: 00:07:33
                               ETA: 00:24:57

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 107711 steps/s (collection: 0.785s, learning 0.127s)
             Mean action noise std: 1.68
          Mean value_function loss: 94.2093
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 15.1262
                       Mean reward: 774.33
               Mean episode length: 242.48
    Episode_Reward/reaching_object: 0.7056
     Episode_Reward/lifting_object: 157.9014
      Episode_Reward/object_height: 0.0678
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 0.91s
                      Time elapsed: 00:07:34
                               ETA: 00:24:56

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 108283 steps/s (collection: 0.793s, learning 0.115s)
             Mean action noise std: 1.68
          Mean value_function loss: 99.8274
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.1308
                       Mean reward: 837.00
               Mean episode length: 248.90
    Episode_Reward/reaching_object: 0.7122
     Episode_Reward/lifting_object: 161.1891
      Episode_Reward/object_height: 0.0691
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 0.91s
                      Time elapsed: 00:07:35
                               ETA: 00:24:55

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 109316 steps/s (collection: 0.787s, learning 0.112s)
             Mean action noise std: 1.69
          Mean value_function loss: 104.9986
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.1369
                       Mean reward: 809.89
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.7220
     Episode_Reward/lifting_object: 163.2818
      Episode_Reward/object_height: 0.0701
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 0.90s
                      Time elapsed: 00:07:36
                               ETA: 00:24:53

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 110012 steps/s (collection: 0.779s, learning 0.115s)
             Mean action noise std: 1.69
          Mean value_function loss: 96.5903
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.1381
                       Mean reward: 771.65
               Mean episode length: 243.15
    Episode_Reward/reaching_object: 0.7117
     Episode_Reward/lifting_object: 160.4314
      Episode_Reward/object_height: 0.0688
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 0.89s
                      Time elapsed: 00:07:36
                               ETA: 00:24:52

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 108438 steps/s (collection: 0.785s, learning 0.122s)
             Mean action noise std: 1.69
          Mean value_function loss: 96.5238
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.1349
                       Mean reward: 789.09
               Mean episode length: 244.93
    Episode_Reward/reaching_object: 0.7029
     Episode_Reward/lifting_object: 157.9265
      Episode_Reward/object_height: 0.0676
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 0.91s
                      Time elapsed: 00:07:37
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 111991 steps/s (collection: 0.767s, learning 0.111s)
             Mean action noise std: 1.69
          Mean value_function loss: 96.5322
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 15.1350
                       Mean reward: 824.15
               Mean episode length: 247.23
    Episode_Reward/reaching_object: 0.7023
     Episode_Reward/lifting_object: 158.2673
      Episode_Reward/object_height: 0.0678
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 0.88s
                      Time elapsed: 00:07:38
                               ETA: 00:24:50

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 109130 steps/s (collection: 0.788s, learning 0.113s)
             Mean action noise std: 1.69
          Mean value_function loss: 84.3042
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.1402
                       Mean reward: 785.63
               Mean episode length: 241.90
    Episode_Reward/reaching_object: 0.7180
     Episode_Reward/lifting_object: 162.5160
      Episode_Reward/object_height: 0.0696
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 0.90s
                      Time elapsed: 00:07:39
                               ETA: 00:24:48

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 109376 steps/s (collection: 0.790s, learning 0.108s)
             Mean action noise std: 1.69
          Mean value_function loss: 93.8211
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.1439
                       Mean reward: 799.36
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7131
     Episode_Reward/lifting_object: 159.9032
      Episode_Reward/object_height: 0.0682
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 0.90s
                      Time elapsed: 00:07:40
                               ETA: 00:24:47

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 108806 steps/s (collection: 0.780s, learning 0.124s)
             Mean action noise std: 1.69
          Mean value_function loss: 90.4060
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.1463
                       Mean reward: 792.39
               Mean episode length: 244.13
    Episode_Reward/reaching_object: 0.7272
     Episode_Reward/lifting_object: 165.0075
      Episode_Reward/object_height: 0.0704
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 0.90s
                      Time elapsed: 00:07:41
                               ETA: 00:24:46

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 112497 steps/s (collection: 0.757s, learning 0.117s)
             Mean action noise std: 1.69
          Mean value_function loss: 98.4145
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.1538
                       Mean reward: 824.53
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.7041
     Episode_Reward/lifting_object: 158.3024
      Episode_Reward/object_height: 0.0676
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 0.87s
                      Time elapsed: 00:07:42
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 109600 steps/s (collection: 0.783s, learning 0.114s)
             Mean action noise std: 1.69
          Mean value_function loss: 94.2490
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.1593
                       Mean reward: 816.95
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.6973
     Episode_Reward/lifting_object: 157.2858
      Episode_Reward/object_height: 0.0669
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 0.90s
                      Time elapsed: 00:07:43
                               ETA: 00:24:43

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 108226 steps/s (collection: 0.797s, learning 0.111s)
             Mean action noise std: 1.69
          Mean value_function loss: 90.2121
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.1623
                       Mean reward: 822.94
               Mean episode length: 246.75
    Episode_Reward/reaching_object: 0.7275
     Episode_Reward/lifting_object: 164.3048
      Episode_Reward/object_height: 0.0697
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 0.91s
                      Time elapsed: 00:07:44
                               ETA: 00:24:42

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 104851 steps/s (collection: 0.823s, learning 0.114s)
             Mean action noise std: 1.69
          Mean value_function loss: 102.6258
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.1664
                       Mean reward: 762.78
               Mean episode length: 243.16
    Episode_Reward/reaching_object: 0.6995
     Episode_Reward/lifting_object: 158.3667
      Episode_Reward/object_height: 0.0672
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 0.94s
                      Time elapsed: 00:07:45
                               ETA: 00:24:41

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 112340 steps/s (collection: 0.767s, learning 0.108s)
             Mean action noise std: 1.69
          Mean value_function loss: 92.2898
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.1724
                       Mean reward: 845.81
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.7271
     Episode_Reward/lifting_object: 164.0851
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 0.88s
                      Time elapsed: 00:07:45
                               ETA: 00:24:40

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 110696 steps/s (collection: 0.784s, learning 0.104s)
             Mean action noise std: 1.69
          Mean value_function loss: 93.8897
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 15.1725
                       Mean reward: 846.73
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 0.7160
     Episode_Reward/lifting_object: 161.9789
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 0.89s
                      Time elapsed: 00:07:46
                               ETA: 00:24:39

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 113018 steps/s (collection: 0.778s, learning 0.092s)
             Mean action noise std: 1.70
          Mean value_function loss: 82.4492
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.1727
                       Mean reward: 864.82
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.7186
     Episode_Reward/lifting_object: 163.0263
      Episode_Reward/object_height: 0.0687
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 0.87s
                      Time elapsed: 00:07:47
                               ETA: 00:24:37

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 113310 steps/s (collection: 0.774s, learning 0.094s)
             Mean action noise std: 1.70
          Mean value_function loss: 92.3315
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 15.1781
                       Mean reward: 851.27
               Mean episode length: 246.55
    Episode_Reward/reaching_object: 0.7233
     Episode_Reward/lifting_object: 164.5395
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 0.87s
                      Time elapsed: 00:07:48
                               ETA: 00:24:36

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 114091 steps/s (collection: 0.753s, learning 0.109s)
             Mean action noise std: 1.70
          Mean value_function loss: 85.3641
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 15.1871
                       Mean reward: 794.76
               Mean episode length: 239.46
    Episode_Reward/reaching_object: 0.7070
     Episode_Reward/lifting_object: 159.8830
      Episode_Reward/object_height: 0.0671
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 0.86s
                      Time elapsed: 00:07:49
                               ETA: 00:24:35

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 109865 steps/s (collection: 0.780s, learning 0.115s)
             Mean action noise std: 1.70
          Mean value_function loss: 88.0273
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.1934
                       Mean reward: 825.16
               Mean episode length: 246.37
    Episode_Reward/reaching_object: 0.7210
     Episode_Reward/lifting_object: 164.2198
      Episode_Reward/object_height: 0.0689
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 0.89s
                      Time elapsed: 00:07:50
                               ETA: 00:24:34

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 109298 steps/s (collection: 0.786s, learning 0.114s)
             Mean action noise std: 1.70
          Mean value_function loss: 106.7177
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.2021
                       Mean reward: 787.80
               Mean episode length: 240.43
    Episode_Reward/reaching_object: 0.7213
     Episode_Reward/lifting_object: 163.1569
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 0.90s
                      Time elapsed: 00:07:51
                               ETA: 00:24:32

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 110775 steps/s (collection: 0.776s, learning 0.111s)
             Mean action noise std: 1.70
          Mean value_function loss: 85.7122
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 15.2048
                       Mean reward: 804.90
               Mean episode length: 240.48
    Episode_Reward/reaching_object: 0.7066
     Episode_Reward/lifting_object: 160.1170
      Episode_Reward/object_height: 0.0671
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0024
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 0.89s
                      Time elapsed: 00:07:52
                               ETA: 00:24:31

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 110112 steps/s (collection: 0.788s, learning 0.105s)
             Mean action noise std: 1.70
          Mean value_function loss: 88.4443
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 15.2147
                       Mean reward: 812.06
               Mean episode length: 246.98
    Episode_Reward/reaching_object: 0.7158
     Episode_Reward/lifting_object: 161.3081
      Episode_Reward/object_height: 0.0678
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 0.89s
                      Time elapsed: 00:07:52
                               ETA: 00:24:30

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 112110 steps/s (collection: 0.787s, learning 0.090s)
             Mean action noise std: 1.71
          Mean value_function loss: 111.4373
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 15.2201
                       Mean reward: 796.60
               Mean episode length: 244.50
    Episode_Reward/reaching_object: 0.7296
     Episode_Reward/lifting_object: 165.4419
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 0.88s
                      Time elapsed: 00:07:53
                               ETA: 00:24:29

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 113909 steps/s (collection: 0.775s, learning 0.088s)
             Mean action noise std: 1.71
          Mean value_function loss: 98.8234
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.2205
                       Mean reward: 839.57
               Mean episode length: 244.75
    Episode_Reward/reaching_object: 0.7125
     Episode_Reward/lifting_object: 161.2703
      Episode_Reward/object_height: 0.0677
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 0.86s
                      Time elapsed: 00:07:54
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 112812 steps/s (collection: 0.769s, learning 0.102s)
             Mean action noise std: 1.71
          Mean value_function loss: 88.3987
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.2224
                       Mean reward: 831.39
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7287
     Episode_Reward/lifting_object: 164.2626
      Episode_Reward/object_height: 0.0687
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 0.87s
                      Time elapsed: 00:07:55
                               ETA: 00:24:26

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 113855 steps/s (collection: 0.748s, learning 0.115s)
             Mean action noise std: 1.71
          Mean value_function loss: 108.9892
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.2320
                       Mean reward: 809.14
               Mean episode length: 244.62
    Episode_Reward/reaching_object: 0.7127
     Episode_Reward/lifting_object: 161.0952
      Episode_Reward/object_height: 0.0674
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 0.86s
                      Time elapsed: 00:07:56
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 109050 steps/s (collection: 0.787s, learning 0.115s)
             Mean action noise std: 1.71
          Mean value_function loss: 108.1687
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 15.2417
                       Mean reward: 809.23
               Mean episode length: 243.79
    Episode_Reward/reaching_object: 0.7241
     Episode_Reward/lifting_object: 165.2314
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 0.90s
                      Time elapsed: 00:07:57
                               ETA: 00:24:24

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 104430 steps/s (collection: 0.818s, learning 0.123s)
             Mean action noise std: 1.71
          Mean value_function loss: 77.4925
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.2436
                       Mean reward: 845.47
               Mean episode length: 249.21
    Episode_Reward/reaching_object: 0.7236
     Episode_Reward/lifting_object: 165.5281
      Episode_Reward/object_height: 0.0696
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 0.94s
                      Time elapsed: 00:07:58
                               ETA: 00:24:23

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 107118 steps/s (collection: 0.800s, learning 0.118s)
             Mean action noise std: 1.71
          Mean value_function loss: 87.8659
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.2466
                       Mean reward: 827.37
               Mean episode length: 247.11
    Episode_Reward/reaching_object: 0.7147
     Episode_Reward/lifting_object: 161.9604
      Episode_Reward/object_height: 0.0680
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 0.92s
                      Time elapsed: 00:07:59
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 108794 steps/s (collection: 0.786s, learning 0.117s)
             Mean action noise std: 1.71
          Mean value_function loss: 83.9998
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.2522
                       Mean reward: 821.68
               Mean episode length: 245.98
    Episode_Reward/reaching_object: 0.7189
     Episode_Reward/lifting_object: 162.6709
      Episode_Reward/object_height: 0.0682
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 0.90s
                      Time elapsed: 00:08:00
                               ETA: 00:24:20

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 105376 steps/s (collection: 0.806s, learning 0.127s)
             Mean action noise std: 1.72
          Mean value_function loss: 78.5679
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.2582
                       Mean reward: 836.13
               Mean episode length: 249.15
    Episode_Reward/reaching_object: 0.7251
     Episode_Reward/lifting_object: 165.5744
      Episode_Reward/object_height: 0.0696
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 0.93s
                      Time elapsed: 00:08:01
                               ETA: 00:24:19

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 106610 steps/s (collection: 0.825s, learning 0.097s)
             Mean action noise std: 1.72
          Mean value_function loss: 78.3245
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.2714
                       Mean reward: 787.01
               Mean episode length: 245.75
    Episode_Reward/reaching_object: 0.7180
     Episode_Reward/lifting_object: 161.9474
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 0.92s
                      Time elapsed: 00:08:01
                               ETA: 00:24:18

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 111162 steps/s (collection: 0.779s, learning 0.106s)
             Mean action noise std: 1.72
          Mean value_function loss: 75.8058
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 15.2901
                       Mean reward: 780.09
               Mean episode length: 243.10
    Episode_Reward/reaching_object: 0.7097
     Episode_Reward/lifting_object: 161.5838
      Episode_Reward/object_height: 0.0684
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 0.88s
                      Time elapsed: 00:08:02
                               ETA: 00:24:17

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 110404 steps/s (collection: 0.767s, learning 0.123s)
             Mean action noise std: 1.73
          Mean value_function loss: 87.3288
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 15.3092
                       Mean reward: 807.15
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7261
     Episode_Reward/lifting_object: 166.0134
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 0.89s
                      Time elapsed: 00:08:03
                               ETA: 00:24:16

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 111088 steps/s (collection: 0.776s, learning 0.109s)
             Mean action noise std: 1.73
          Mean value_function loss: 63.2370
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.3231
                       Mean reward: 813.15
               Mean episode length: 244.01
    Episode_Reward/reaching_object: 0.7014
     Episode_Reward/lifting_object: 159.0003
      Episode_Reward/object_height: 0.0671
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 0.88s
                      Time elapsed: 00:08:04
                               ETA: 00:24:14

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 111001 steps/s (collection: 0.765s, learning 0.121s)
             Mean action noise std: 1.73
          Mean value_function loss: 79.5444
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.3277
                       Mean reward: 832.63
               Mean episode length: 245.91
    Episode_Reward/reaching_object: 0.7276
     Episode_Reward/lifting_object: 164.1375
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 0.89s
                      Time elapsed: 00:08:05
                               ETA: 00:24:13

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 110313 steps/s (collection: 0.770s, learning 0.122s)
             Mean action noise std: 1.73
          Mean value_function loss: 69.7852
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 15.3365
                       Mean reward: 816.58
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 0.7301
     Episode_Reward/lifting_object: 165.4363
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 0.89s
                      Time elapsed: 00:08:06
                               ETA: 00:24:12

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 113671 steps/s (collection: 0.750s, learning 0.115s)
             Mean action noise std: 1.73
          Mean value_function loss: 79.4405
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.3482
                       Mean reward: 829.03
               Mean episode length: 244.35
    Episode_Reward/reaching_object: 0.7172
     Episode_Reward/lifting_object: 162.0696
      Episode_Reward/object_height: 0.0681
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 0.86s
                      Time elapsed: 00:08:07
                               ETA: 00:24:11

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 112221 steps/s (collection: 0.769s, learning 0.107s)
             Mean action noise std: 1.73
          Mean value_function loss: 81.2036
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.3546
                       Mean reward: 840.64
               Mean episode length: 244.12
    Episode_Reward/reaching_object: 0.7356
     Episode_Reward/lifting_object: 167.2613
      Episode_Reward/object_height: 0.0702
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 0.88s
                      Time elapsed: 00:08:08
                               ETA: 00:24:09

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 111847 steps/s (collection: 0.769s, learning 0.110s)
             Mean action noise std: 1.74
          Mean value_function loss: 74.1994
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 15.3624
                       Mean reward: 827.50
               Mean episode length: 244.99
    Episode_Reward/reaching_object: 0.7494
     Episode_Reward/lifting_object: 170.5700
      Episode_Reward/object_height: 0.0717
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 0.88s
                      Time elapsed: 00:08:09
                               ETA: 00:24:08

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 112762 steps/s (collection: 0.756s, learning 0.116s)
             Mean action noise std: 1.74
          Mean value_function loss: 74.7880
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.3730
                       Mean reward: 833.53
               Mean episode length: 245.87
    Episode_Reward/reaching_object: 0.7279
     Episode_Reward/lifting_object: 165.1367
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 0.87s
                      Time elapsed: 00:08:09
                               ETA: 00:24:07

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 109787 steps/s (collection: 0.789s, learning 0.107s)
             Mean action noise std: 1.74
          Mean value_function loss: 81.9212
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.3847
                       Mean reward: 851.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7506
     Episode_Reward/lifting_object: 169.9789
      Episode_Reward/object_height: 0.0716
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 0.90s
                      Time elapsed: 00:08:10
                               ETA: 00:24:06

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 112280 steps/s (collection: 0.771s, learning 0.104s)
             Mean action noise std: 1.74
          Mean value_function loss: 97.8961
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.3937
                       Mean reward: 785.72
               Mean episode length: 242.22
    Episode_Reward/reaching_object: 0.7175
     Episode_Reward/lifting_object: 162.5645
      Episode_Reward/object_height: 0.0684
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 0.88s
                      Time elapsed: 00:08:11
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 109527 steps/s (collection: 0.786s, learning 0.111s)
             Mean action noise std: 1.74
          Mean value_function loss: 93.5353
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.4055
                       Mean reward: 835.90
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7329
     Episode_Reward/lifting_object: 166.6228
      Episode_Reward/object_height: 0.0703
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 0.90s
                      Time elapsed: 00:08:12
                               ETA: 00:24:03

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 111211 steps/s (collection: 0.768s, learning 0.116s)
             Mean action noise std: 1.75
          Mean value_function loss: 75.0520
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.4171
                       Mean reward: 845.71
               Mean episode length: 246.50
    Episode_Reward/reaching_object: 0.7114
     Episode_Reward/lifting_object: 161.5212
      Episode_Reward/object_height: 0.0680
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 0.88s
                      Time elapsed: 00:08:13
                               ETA: 00:24:02

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 108460 steps/s (collection: 0.784s, learning 0.122s)
             Mean action noise std: 1.75
          Mean value_function loss: 71.1585
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.4328
                       Mean reward: 814.75
               Mean episode length: 246.73
    Episode_Reward/reaching_object: 0.7299
     Episode_Reward/lifting_object: 165.5545
      Episode_Reward/object_height: 0.0697
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 0.91s
                      Time elapsed: 00:08:14
                               ETA: 00:24:01

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 111616 steps/s (collection: 0.789s, learning 0.092s)
             Mean action noise std: 1.75
          Mean value_function loss: 83.1097
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.4366
                       Mean reward: 832.07
               Mean episode length: 245.29
    Episode_Reward/reaching_object: 0.7296
     Episode_Reward/lifting_object: 167.0132
      Episode_Reward/object_height: 0.0704
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 0.88s
                      Time elapsed: 00:08:15
                               ETA: 00:24:00

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 111295 steps/s (collection: 0.784s, learning 0.099s)
             Mean action noise std: 1.75
          Mean value_function loss: 63.6633
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.4442
                       Mean reward: 782.64
               Mean episode length: 244.96
    Episode_Reward/reaching_object: 0.7296
     Episode_Reward/lifting_object: 165.2268
      Episode_Reward/object_height: 0.0699
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 0.88s
                      Time elapsed: 00:08:16
                               ETA: 00:23:59

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 109767 steps/s (collection: 0.777s, learning 0.118s)
             Mean action noise std: 1.76
          Mean value_function loss: 72.1439
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 15.4494
                       Mean reward: 828.84
               Mean episode length: 246.49
    Episode_Reward/reaching_object: 0.7330
     Episode_Reward/lifting_object: 166.3192
      Episode_Reward/object_height: 0.0701
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 0.90s
                      Time elapsed: 00:08:17
                               ETA: 00:23:57

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 109739 steps/s (collection: 0.789s, learning 0.107s)
             Mean action noise std: 1.76
          Mean value_function loss: 70.9038
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.4618
                       Mean reward: 821.96
               Mean episode length: 247.31
    Episode_Reward/reaching_object: 0.7281
     Episode_Reward/lifting_object: 165.2269
      Episode_Reward/object_height: 0.0696
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 0.90s
                      Time elapsed: 00:08:17
                               ETA: 00:23:56

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 109594 steps/s (collection: 0.780s, learning 0.117s)
             Mean action noise std: 1.76
          Mean value_function loss: 59.5147
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.4733
                       Mean reward: 840.50
               Mean episode length: 247.19
    Episode_Reward/reaching_object: 0.7134
     Episode_Reward/lifting_object: 162.2036
      Episode_Reward/object_height: 0.0682
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 0.90s
                      Time elapsed: 00:08:18
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 111033 steps/s (collection: 0.779s, learning 0.106s)
             Mean action noise std: 1.76
          Mean value_function loss: 64.2251
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 15.4799
                       Mean reward: 848.61
               Mean episode length: 245.12
    Episode_Reward/reaching_object: 0.7321
     Episode_Reward/lifting_object: 167.6410
      Episode_Reward/object_height: 0.0705
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 0.89s
                      Time elapsed: 00:08:19
                               ETA: 00:23:54

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 113708 steps/s (collection: 0.773s, learning 0.091s)
             Mean action noise std: 1.76
          Mean value_function loss: 67.4264
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.4824
                       Mean reward: 820.77
               Mean episode length: 247.58
    Episode_Reward/reaching_object: 0.7394
     Episode_Reward/lifting_object: 167.5591
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 0.86s
                      Time elapsed: 00:08:20
                               ETA: 00:23:53

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 111690 steps/s (collection: 0.762s, learning 0.119s)
             Mean action noise std: 1.76
          Mean value_function loss: 71.2729
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.4913
                       Mean reward: 844.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7383
     Episode_Reward/lifting_object: 167.8852
      Episode_Reward/object_height: 0.0702
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 0.88s
                      Time elapsed: 00:08:21
                               ETA: 00:23:51

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 113750 steps/s (collection: 0.759s, learning 0.105s)
             Mean action noise std: 1.77
          Mean value_function loss: 77.3096
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.5001
                       Mean reward: 857.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7264
     Episode_Reward/lifting_object: 164.1998
      Episode_Reward/object_height: 0.0684
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 0.86s
                      Time elapsed: 00:08:22
                               ETA: 00:23:50

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 112891 steps/s (collection: 0.758s, learning 0.113s)
             Mean action noise std: 1.77
          Mean value_function loss: 65.3804
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 15.5071
                       Mean reward: 843.62
               Mean episode length: 246.71
    Episode_Reward/reaching_object: 0.7423
     Episode_Reward/lifting_object: 167.9931
      Episode_Reward/object_height: 0.0696
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 0.87s
                      Time elapsed: 00:08:23
                               ETA: 00:23:49

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 112295 steps/s (collection: 0.769s, learning 0.106s)
             Mean action noise std: 1.77
          Mean value_function loss: 84.2803
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.5174
                       Mean reward: 836.53
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.7398
     Episode_Reward/lifting_object: 165.9196
      Episode_Reward/object_height: 0.0689
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 0.88s
                      Time elapsed: 00:08:24
                               ETA: 00:23:48

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 112687 steps/s (collection: 0.769s, learning 0.104s)
             Mean action noise std: 1.77
          Mean value_function loss: 77.4792
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 15.5289
                       Mean reward: 848.69
               Mean episode length: 245.98
    Episode_Reward/reaching_object: 0.7330
     Episode_Reward/lifting_object: 166.5510
      Episode_Reward/object_height: 0.0691
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 0.87s
                      Time elapsed: 00:08:24
                               ETA: 00:23:46

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 108659 steps/s (collection: 0.794s, learning 0.111s)
             Mean action noise std: 1.77
          Mean value_function loss: 73.9768
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 15.5386
                       Mean reward: 834.59
               Mean episode length: 247.05
    Episode_Reward/reaching_object: 0.7358
     Episode_Reward/lifting_object: 167.6091
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 0.90s
                      Time elapsed: 00:08:25
                               ETA: 00:23:45

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 110479 steps/s (collection: 0.777s, learning 0.113s)
             Mean action noise std: 1.78
          Mean value_function loss: 78.7712
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.5434
                       Mean reward: 829.46
               Mean episode length: 244.66
    Episode_Reward/reaching_object: 0.7352
     Episode_Reward/lifting_object: 166.5815
      Episode_Reward/object_height: 0.0691
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 0.89s
                      Time elapsed: 00:08:26
                               ETA: 00:23:44

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 113428 steps/s (collection: 0.780s, learning 0.086s)
             Mean action noise std: 1.78
          Mean value_function loss: 74.5619
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.5459
                       Mean reward: 836.73
               Mean episode length: 246.89
    Episode_Reward/reaching_object: 0.7319
     Episode_Reward/lifting_object: 167.2677
      Episode_Reward/object_height: 0.0697
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 0.87s
                      Time elapsed: 00:08:27
                               ETA: 00:23:43

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 112540 steps/s (collection: 0.773s, learning 0.101s)
             Mean action noise std: 1.78
          Mean value_function loss: 56.3192
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 15.5524
                       Mean reward: 830.73
               Mean episode length: 244.25
    Episode_Reward/reaching_object: 0.7292
     Episode_Reward/lifting_object: 166.4692
      Episode_Reward/object_height: 0.0692
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 0.87s
                      Time elapsed: 00:08:28
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 109207 steps/s (collection: 0.784s, learning 0.117s)
             Mean action noise std: 1.78
          Mean value_function loss: 59.4270
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 15.5574
                       Mean reward: 851.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7319
     Episode_Reward/lifting_object: 166.3883
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 0.90s
                      Time elapsed: 00:08:29
                               ETA: 00:23:40

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 110796 steps/s (collection: 0.766s, learning 0.122s)
             Mean action noise std: 1.78
          Mean value_function loss: 54.9321
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.5601
                       Mean reward: 843.76
               Mean episode length: 249.33
    Episode_Reward/reaching_object: 0.7392
     Episode_Reward/lifting_object: 166.7895
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 0.89s
                      Time elapsed: 00:08:30
                               ETA: 00:23:39

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 113660 steps/s (collection: 0.756s, learning 0.109s)
             Mean action noise std: 1.78
          Mean value_function loss: 66.2969
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.5662
                       Mean reward: 846.34
               Mean episode length: 249.50
    Episode_Reward/reaching_object: 0.7337
     Episode_Reward/lifting_object: 166.9576
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 0.86s
                      Time elapsed: 00:08:31
                               ETA: 00:23:38

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 109406 steps/s (collection: 0.783s, learning 0.116s)
             Mean action noise std: 1.79
          Mean value_function loss: 66.0279
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 15.5805
                       Mean reward: 834.53
               Mean episode length: 242.91
    Episode_Reward/reaching_object: 0.7260
     Episode_Reward/lifting_object: 165.6198
      Episode_Reward/object_height: 0.0685
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 0.90s
                      Time elapsed: 00:08:32
                               ETA: 00:23:37

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 110638 steps/s (collection: 0.772s, learning 0.116s)
             Mean action noise std: 1.79
          Mean value_function loss: 66.9027
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.5908
                       Mean reward: 817.58
               Mean episode length: 243.01
    Episode_Reward/reaching_object: 0.7194
     Episode_Reward/lifting_object: 162.8083
      Episode_Reward/object_height: 0.0673
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 0.89s
                      Time elapsed: 00:08:32
                               ETA: 00:23:36

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 108610 steps/s (collection: 0.793s, learning 0.113s)
             Mean action noise std: 1.79
          Mean value_function loss: 67.6096
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.5959
                       Mean reward: 863.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7388
     Episode_Reward/lifting_object: 168.2812
      Episode_Reward/object_height: 0.0696
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 0.91s
                      Time elapsed: 00:08:33
                               ETA: 00:23:35

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 110636 steps/s (collection: 0.780s, learning 0.108s)
             Mean action noise std: 1.79
          Mean value_function loss: 56.1277
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 15.6011
                       Mean reward: 818.45
               Mean episode length: 241.17
    Episode_Reward/reaching_object: 0.7218
     Episode_Reward/lifting_object: 162.9489
      Episode_Reward/object_height: 0.0673
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 0.89s
                      Time elapsed: 00:08:34
                               ETA: 00:23:33

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 111778 steps/s (collection: 0.776s, learning 0.104s)
             Mean action noise std: 1.79
          Mean value_function loss: 65.0248
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 15.6164
                       Mean reward: 860.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7445
     Episode_Reward/lifting_object: 168.6646
      Episode_Reward/object_height: 0.0697
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 0.88s
                      Time elapsed: 00:08:35
                               ETA: 00:23:32

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 109909 steps/s (collection: 0.767s, learning 0.127s)
             Mean action noise std: 1.80
          Mean value_function loss: 56.9430
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 15.6369
                       Mean reward: 839.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7401
     Episode_Reward/lifting_object: 167.7119
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 0.89s
                      Time elapsed: 00:08:36
                               ETA: 00:23:31

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 113392 steps/s (collection: 0.761s, learning 0.106s)
             Mean action noise std: 1.80
          Mean value_function loss: 67.0465
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 15.6544
                       Mean reward: 832.60
               Mean episode length: 243.94
    Episode_Reward/reaching_object: 0.7414
     Episode_Reward/lifting_object: 168.5203
      Episode_Reward/object_height: 0.0699
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 0.87s
                      Time elapsed: 00:08:37
                               ETA: 00:23:30

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 111470 steps/s (collection: 0.770s, learning 0.112s)
             Mean action noise std: 1.80
          Mean value_function loss: 61.9927
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 15.6661
                       Mean reward: 844.88
               Mean episode length: 246.25
    Episode_Reward/reaching_object: 0.7459
     Episode_Reward/lifting_object: 169.3623
      Episode_Reward/object_height: 0.0703
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 0.88s
                      Time elapsed: 00:08:38
                               ETA: 00:23:29

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 112538 steps/s (collection: 0.769s, learning 0.104s)
             Mean action noise std: 1.81
          Mean value_function loss: 76.3466
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.6777
                       Mean reward: 817.88
               Mean episode length: 247.60
    Episode_Reward/reaching_object: 0.7315
     Episode_Reward/lifting_object: 165.7618
      Episode_Reward/object_height: 0.0688
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 0.87s
                      Time elapsed: 00:08:39
                               ETA: 00:23:27

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 110393 steps/s (collection: 0.781s, learning 0.110s)
             Mean action noise std: 1.81
          Mean value_function loss: 67.5820
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.6926
                       Mean reward: 808.99
               Mean episode length: 241.75
    Episode_Reward/reaching_object: 0.7379
     Episode_Reward/lifting_object: 167.2526
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 0.89s
                      Time elapsed: 00:08:39
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 112027 steps/s (collection: 0.769s, learning 0.108s)
             Mean action noise std: 1.81
          Mean value_function loss: 50.5636
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 15.7034
                       Mean reward: 817.40
               Mean episode length: 243.96
    Episode_Reward/reaching_object: 0.7299
     Episode_Reward/lifting_object: 166.0854
      Episode_Reward/object_height: 0.0692
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 0.88s
                      Time elapsed: 00:08:40
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 112608 steps/s (collection: 0.768s, learning 0.105s)
             Mean action noise std: 1.81
          Mean value_function loss: 68.9064
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 15.7095
                       Mean reward: 831.66
               Mean episode length: 245.22
    Episode_Reward/reaching_object: 0.7413
     Episode_Reward/lifting_object: 167.9961
      Episode_Reward/object_height: 0.0701
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 0.87s
                      Time elapsed: 00:08:41
                               ETA: 00:23:24

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 113527 steps/s (collection: 0.764s, learning 0.101s)
             Mean action noise std: 1.81
          Mean value_function loss: 62.1503
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.7100
                       Mean reward: 861.69
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7371
     Episode_Reward/lifting_object: 166.9623
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 0.87s
                      Time elapsed: 00:08:42
                               ETA: 00:23:23

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 110968 steps/s (collection: 0.775s, learning 0.111s)
             Mean action noise std: 1.81
          Mean value_function loss: 61.3270
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 15.7158
                       Mean reward: 851.12
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 170.6148
      Episode_Reward/object_height: 0.0708
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 0.89s
                      Time elapsed: 00:08:43
                               ETA: 00:23:22

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 111068 steps/s (collection: 0.793s, learning 0.093s)
             Mean action noise std: 1.82
          Mean value_function loss: 83.9599
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.7368
                       Mean reward: 844.88
               Mean episode length: 246.39
    Episode_Reward/reaching_object: 0.7247
     Episode_Reward/lifting_object: 164.5675
      Episode_Reward/object_height: 0.0680
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 0.89s
                      Time elapsed: 00:08:44
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 111092 steps/s (collection: 0.788s, learning 0.097s)
             Mean action noise std: 1.82
          Mean value_function loss: 67.5565
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 15.7569
                       Mean reward: 799.45
               Mean episode length: 245.92
    Episode_Reward/reaching_object: 0.7223
     Episode_Reward/lifting_object: 161.6543
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 0.88s
                      Time elapsed: 00:08:45
                               ETA: 00:23:19

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 111385 steps/s (collection: 0.792s, learning 0.091s)
             Mean action noise std: 1.82
          Mean value_function loss: 64.3470
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 15.7704
                       Mean reward: 844.35
               Mean episode length: 245.20
    Episode_Reward/reaching_object: 0.7341
     Episode_Reward/lifting_object: 165.7940
      Episode_Reward/object_height: 0.0679
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 0.88s
                      Time elapsed: 00:08:46
                               ETA: 00:23:18

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 113673 steps/s (collection: 0.777s, learning 0.088s)
             Mean action noise std: 1.83
          Mean value_function loss: 68.2614
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 15.8048
                       Mean reward: 829.31
               Mean episode length: 242.80
    Episode_Reward/reaching_object: 0.7295
     Episode_Reward/lifting_object: 166.2219
      Episode_Reward/object_height: 0.0685
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 0.86s
                      Time elapsed: 00:08:46
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 111609 steps/s (collection: 0.784s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 80.9310
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 15.8201
                       Mean reward: 863.73
               Mean episode length: 246.26
    Episode_Reward/reaching_object: 0.7226
     Episode_Reward/lifting_object: 165.4881
      Episode_Reward/object_height: 0.0681
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 0.88s
                      Time elapsed: 00:08:47
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 111983 steps/s (collection: 0.781s, learning 0.097s)
             Mean action noise std: 1.83
          Mean value_function loss: 67.7894
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.8221
                       Mean reward: 833.19
               Mean episode length: 242.28
    Episode_Reward/reaching_object: 0.7222
     Episode_Reward/lifting_object: 164.6581
      Episode_Reward/object_height: 0.0674
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 0.88s
                      Time elapsed: 00:08:48
                               ETA: 00:23:14

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 110880 steps/s (collection: 0.782s, learning 0.105s)
             Mean action noise std: 1.84
          Mean value_function loss: 84.8852
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.8302
                       Mean reward: 811.40
               Mean episode length: 246.23
    Episode_Reward/reaching_object: 0.7287
     Episode_Reward/lifting_object: 166.5041
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 0.89s
                      Time elapsed: 00:08:49
                               ETA: 00:23:13

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 111791 steps/s (collection: 0.780s, learning 0.100s)
             Mean action noise std: 1.84
          Mean value_function loss: 72.9901
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 15.8426
                       Mean reward: 806.70
               Mean episode length: 241.06
    Episode_Reward/reaching_object: 0.7286
     Episode_Reward/lifting_object: 166.5512
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 0.88s
                      Time elapsed: 00:08:50
                               ETA: 00:23:12

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 112952 steps/s (collection: 0.771s, learning 0.099s)
             Mean action noise std: 1.84
          Mean value_function loss: 66.8623
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 15.8476
                       Mean reward: 829.11
               Mean episode length: 246.92
    Episode_Reward/reaching_object: 0.7262
     Episode_Reward/lifting_object: 165.7914
      Episode_Reward/object_height: 0.0680
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.6250
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 0.87s
                      Time elapsed: 00:08:51
                               ETA: 00:23:11

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 112638 steps/s (collection: 0.774s, learning 0.099s)
             Mean action noise std: 1.84
          Mean value_function loss: 57.5450
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 15.8559
                       Mean reward: 800.82
               Mean episode length: 245.73
    Episode_Reward/reaching_object: 0.7172
     Episode_Reward/lifting_object: 164.1959
      Episode_Reward/object_height: 0.0674
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 0.87s
                      Time elapsed: 00:08:52
                               ETA: 00:23:10

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 114365 steps/s (collection: 0.763s, learning 0.096s)
             Mean action noise std: 1.84
          Mean value_function loss: 61.2194
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.8610
                       Mean reward: 850.38
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7119
     Episode_Reward/lifting_object: 163.8787
      Episode_Reward/object_height: 0.0676
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 0.86s
                      Time elapsed: 00:08:53
                               ETA: 00:23:08

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 110790 steps/s (collection: 0.782s, learning 0.105s)
             Mean action noise std: 1.85
          Mean value_function loss: 58.1528
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 15.8705
                       Mean reward: 819.65
               Mean episode length: 245.84
    Episode_Reward/reaching_object: 0.7099
     Episode_Reward/lifting_object: 163.9162
      Episode_Reward/object_height: 0.0674
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 0.89s
                      Time elapsed: 00:08:54
                               ETA: 00:23:07

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 113022 steps/s (collection: 0.775s, learning 0.095s)
             Mean action noise std: 1.85
          Mean value_function loss: 50.2168
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 15.8896
                       Mean reward: 848.83
               Mean episode length: 244.55
    Episode_Reward/reaching_object: 0.7250
     Episode_Reward/lifting_object: 167.8434
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 0.87s
                      Time elapsed: 00:08:54
                               ETA: 00:23:06

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 112406 steps/s (collection: 0.778s, learning 0.096s)
             Mean action noise std: 1.85
          Mean value_function loss: 55.4935
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 15.9050
                       Mean reward: 823.86
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.6996
     Episode_Reward/lifting_object: 161.5698
      Episode_Reward/object_height: 0.0669
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 0.87s
                      Time elapsed: 00:08:55
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 114936 steps/s (collection: 0.765s, learning 0.090s)
             Mean action noise std: 1.86
          Mean value_function loss: 58.3720
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 15.9196
                       Mean reward: 833.02
               Mean episode length: 246.49
    Episode_Reward/reaching_object: 0.7237
     Episode_Reward/lifting_object: 168.2376
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 0.86s
                      Time elapsed: 00:08:56
                               ETA: 00:23:04

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 113515 steps/s (collection: 0.770s, learning 0.096s)
             Mean action noise std: 1.86
          Mean value_function loss: 50.4785
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 15.9449
                       Mean reward: 806.34
               Mean episode length: 246.22
    Episode_Reward/reaching_object: 0.7067
     Episode_Reward/lifting_object: 160.8891
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 0.87s
                      Time elapsed: 00:08:57
                               ETA: 00:23:03

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 111238 steps/s (collection: 0.778s, learning 0.106s)
             Mean action noise std: 1.86
          Mean value_function loss: 51.3572
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 15.9549
                       Mean reward: 850.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7221
     Episode_Reward/lifting_object: 165.1727
      Episode_Reward/object_height: 0.0680
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 0.88s
                      Time elapsed: 00:08:58
                               ETA: 00:23:01

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 112497 steps/s (collection: 0.777s, learning 0.097s)
             Mean action noise std: 1.87
          Mean value_function loss: 49.5509
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 15.9669
                       Mean reward: 848.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7498
     Episode_Reward/lifting_object: 170.6960
      Episode_Reward/object_height: 0.0705
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 0.87s
                      Time elapsed: 00:08:59
                               ETA: 00:23:00

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 113340 steps/s (collection: 0.771s, learning 0.097s)
             Mean action noise std: 1.87
          Mean value_function loss: 55.8895
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 15.9788
                       Mean reward: 851.50
               Mean episode length: 245.11
    Episode_Reward/reaching_object: 0.7448
     Episode_Reward/lifting_object: 169.7336
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 0.87s
                      Time elapsed: 00:09:00
                               ETA: 00:22:59

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 110650 steps/s (collection: 0.786s, learning 0.103s)
             Mean action noise std: 1.87
          Mean value_function loss: 56.2427
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 15.9858
                       Mean reward: 819.24
               Mean episode length: 246.58
    Episode_Reward/reaching_object: 0.7359
     Episode_Reward/lifting_object: 166.9315
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 0.89s
                      Time elapsed: 00:09:00
                               ETA: 00:22:58

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 112676 steps/s (collection: 0.758s, learning 0.115s)
             Mean action noise std: 1.87
          Mean value_function loss: 62.6810
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 15.9896
                       Mean reward: 848.38
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7421
     Episode_Reward/lifting_object: 169.6474
      Episode_Reward/object_height: 0.0701
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 0.87s
                      Time elapsed: 00:09:01
                               ETA: 00:22:57

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 110842 steps/s (collection: 0.784s, learning 0.103s)
             Mean action noise std: 1.87
          Mean value_function loss: 49.0436
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 15.9948
                       Mean reward: 839.31
               Mean episode length: 246.85
    Episode_Reward/reaching_object: 0.7369
     Episode_Reward/lifting_object: 168.4876
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 0.89s
                      Time elapsed: 00:09:02
                               ETA: 00:22:56

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 112889 steps/s (collection: 0.770s, learning 0.101s)
             Mean action noise std: 1.88
          Mean value_function loss: 57.1390
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.0045
                       Mean reward: 838.06
               Mean episode length: 247.91
    Episode_Reward/reaching_object: 0.7410
     Episode_Reward/lifting_object: 168.3479
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 0.87s
                      Time elapsed: 00:09:03
                               ETA: 00:22:54

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 111604 steps/s (collection: 0.771s, learning 0.110s)
             Mean action noise std: 1.88
          Mean value_function loss: 44.6823
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.0225
                       Mean reward: 829.45
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7216
     Episode_Reward/lifting_object: 163.4540
      Episode_Reward/object_height: 0.0669
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 0.88s
                      Time elapsed: 00:09:04
                               ETA: 00:22:53

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 114537 steps/s (collection: 0.753s, learning 0.105s)
             Mean action noise std: 1.88
          Mean value_function loss: 47.1990
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 16.0367
                       Mean reward: 811.64
               Mean episode length: 246.68
    Episode_Reward/reaching_object: 0.7309
     Episode_Reward/lifting_object: 164.8401
      Episode_Reward/object_height: 0.0676
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 0.86s
                      Time elapsed: 00:09:05
                               ETA: 00:22:52

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 113477 steps/s (collection: 0.768s, learning 0.098s)
             Mean action noise std: 1.88
          Mean value_function loss: 52.3800
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.0384
                       Mean reward: 858.93
               Mean episode length: 247.95
    Episode_Reward/reaching_object: 0.7436
     Episode_Reward/lifting_object: 168.2932
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 0.87s
                      Time elapsed: 00:09:06
                               ETA: 00:22:51

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 107379 steps/s (collection: 0.808s, learning 0.107s)
             Mean action noise std: 1.89
          Mean value_function loss: 61.1876
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.0446
                       Mean reward: 841.15
               Mean episode length: 246.59
    Episode_Reward/reaching_object: 0.7409
     Episode_Reward/lifting_object: 167.7339
      Episode_Reward/object_height: 0.0686
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 0.92s
                      Time elapsed: 00:09:07
                               ETA: 00:22:50

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 114072 steps/s (collection: 0.772s, learning 0.090s)
             Mean action noise std: 1.89
          Mean value_function loss: 54.6691
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 16.0566
                       Mean reward: 850.85
               Mean episode length: 247.90
    Episode_Reward/reaching_object: 0.7433
     Episode_Reward/lifting_object: 167.3797
      Episode_Reward/object_height: 0.0686
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 0.86s
                      Time elapsed: 00:09:07
                               ETA: 00:22:49

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 114183 steps/s (collection: 0.767s, learning 0.094s)
             Mean action noise std: 1.89
          Mean value_function loss: 43.9626
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.0646
                       Mean reward: 859.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7469
     Episode_Reward/lifting_object: 168.9153
      Episode_Reward/object_height: 0.0691
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 0.86s
                      Time elapsed: 00:09:08
                               ETA: 00:22:47

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 115346 steps/s (collection: 0.765s, learning 0.087s)
             Mean action noise std: 1.89
          Mean value_function loss: 40.2863
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 16.0727
                       Mean reward: 861.18
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7411
     Episode_Reward/lifting_object: 168.0941
      Episode_Reward/object_height: 0.0691
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 0.85s
                      Time elapsed: 00:09:09
                               ETA: 00:22:46

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 113936 steps/s (collection: 0.769s, learning 0.094s)
             Mean action noise std: 1.89
          Mean value_function loss: 42.1481
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.0776
                       Mean reward: 853.08
               Mean episode length: 249.31
    Episode_Reward/reaching_object: 0.7494
     Episode_Reward/lifting_object: 170.6054
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 0.86s
                      Time elapsed: 00:09:10
                               ETA: 00:22:45

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 114474 steps/s (collection: 0.767s, learning 0.091s)
             Mean action noise std: 1.89
          Mean value_function loss: 45.2484
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.0841
                       Mean reward: 862.15
               Mean episode length: 249.46
    Episode_Reward/reaching_object: 0.7517
     Episode_Reward/lifting_object: 169.5999
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 0.86s
                      Time elapsed: 00:09:11
                               ETA: 00:22:44

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 116511 steps/s (collection: 0.753s, learning 0.091s)
             Mean action noise std: 1.90
          Mean value_function loss: 39.6099
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.0894
                       Mean reward: 880.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7517
     Episode_Reward/lifting_object: 169.9926
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 0.84s
                      Time elapsed: 00:09:12
                               ETA: 00:22:42

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 110665 steps/s (collection: 0.779s, learning 0.109s)
             Mean action noise std: 1.90
          Mean value_function loss: 42.5726
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.0902
                       Mean reward: 859.46
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 170.6324
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 0.89s
                      Time elapsed: 00:09:13
                               ETA: 00:22:41

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 113539 steps/s (collection: 0.767s, learning 0.099s)
             Mean action noise std: 1.90
          Mean value_function loss: 41.9551
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.0950
                       Mean reward: 861.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7538
     Episode_Reward/lifting_object: 169.5508
      Episode_Reward/object_height: 0.0689
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 0.87s
                      Time elapsed: 00:09:14
                               ETA: 00:22:40

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 113016 steps/s (collection: 0.780s, learning 0.090s)
             Mean action noise std: 1.90
          Mean value_function loss: 42.2598
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.1000
                       Mean reward: 871.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7378
     Episode_Reward/lifting_object: 167.7543
      Episode_Reward/object_height: 0.0684
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 0.87s
                      Time elapsed: 00:09:14
                               ETA: 00:22:39

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 113140 steps/s (collection: 0.774s, learning 0.095s)
             Mean action noise std: 1.90
          Mean value_function loss: 43.7998
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 16.1089
                       Mean reward: 875.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7494
     Episode_Reward/lifting_object: 171.3575
      Episode_Reward/object_height: 0.0696
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 0.87s
                      Time elapsed: 00:09:15
                               ETA: 00:22:38

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 111678 steps/s (collection: 0.780s, learning 0.100s)
             Mean action noise std: 1.90
          Mean value_function loss: 45.8941
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.1220
                       Mean reward: 845.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7486
     Episode_Reward/lifting_object: 168.1855
      Episode_Reward/object_height: 0.0679
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 0.88s
                      Time elapsed: 00:09:16
                               ETA: 00:22:37

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 112746 steps/s (collection: 0.769s, learning 0.103s)
             Mean action noise std: 1.91
          Mean value_function loss: 39.1643
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.1291
                       Mean reward: 853.39
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 172.6513
      Episode_Reward/object_height: 0.0692
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 0.87s
                      Time elapsed: 00:09:17
                               ETA: 00:22:36

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 112996 steps/s (collection: 0.773s, learning 0.097s)
             Mean action noise std: 1.91
          Mean value_function loss: 29.9568
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 16.1365
                       Mean reward: 855.45
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 171.9472
      Episode_Reward/object_height: 0.0689
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 0.87s
                      Time elapsed: 00:09:18
                               ETA: 00:22:34

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 113212 steps/s (collection: 0.750s, learning 0.118s)
             Mean action noise std: 1.91
          Mean value_function loss: 42.3831
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.1408
                       Mean reward: 844.46
               Mean episode length: 246.45
    Episode_Reward/reaching_object: 0.7505
     Episode_Reward/lifting_object: 169.2833
      Episode_Reward/object_height: 0.0677
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 0.87s
                      Time elapsed: 00:09:19
                               ETA: 00:22:33

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 111026 steps/s (collection: 0.778s, learning 0.107s)
             Mean action noise std: 1.91
          Mean value_function loss: 47.2336
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.1425
                       Mean reward: 853.43
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 173.0483
      Episode_Reward/object_height: 0.0692
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 0.89s
                      Time elapsed: 00:09:20
                               ETA: 00:22:32

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 118024 steps/s (collection: 0.744s, learning 0.089s)
             Mean action noise std: 1.91
          Mean value_function loss: 41.1079
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 16.1533
                       Mean reward: 868.42
               Mean episode length: 249.55
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 171.2643
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 0.83s
                      Time elapsed: 00:09:20
                               ETA: 00:22:31

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 113026 steps/s (collection: 0.779s, learning 0.091s)
             Mean action noise std: 1.92
          Mean value_function loss: 31.8030
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.1665
                       Mean reward: 852.76
               Mean episode length: 247.61
    Episode_Reward/reaching_object: 0.7401
     Episode_Reward/lifting_object: 167.6280
      Episode_Reward/object_height: 0.0667
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 0.87s
                      Time elapsed: 00:09:21
                               ETA: 00:22:30

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 114921 steps/s (collection: 0.765s, learning 0.090s)
             Mean action noise std: 1.92
          Mean value_function loss: 48.7050
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.1792
                       Mean reward: 868.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 171.4250
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 0.86s
                      Time elapsed: 00:09:22
                               ETA: 00:22:28

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 112348 steps/s (collection: 0.770s, learning 0.105s)
             Mean action noise std: 1.92
          Mean value_function loss: 50.4483
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.1915
                       Mean reward: 830.75
               Mean episode length: 249.51
    Episode_Reward/reaching_object: 0.7478
     Episode_Reward/lifting_object: 169.0269
      Episode_Reward/object_height: 0.0674
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 0.87s
                      Time elapsed: 00:09:23
                               ETA: 00:22:27

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 112415 steps/s (collection: 0.782s, learning 0.093s)
             Mean action noise std: 1.93
          Mean value_function loss: 38.8356
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.2159
                       Mean reward: 829.82
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 171.0661
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 0.87s
                      Time elapsed: 00:09:24
                               ETA: 00:22:26

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 105623 steps/s (collection: 0.743s, learning 0.188s)
             Mean action noise std: 1.93
          Mean value_function loss: 44.4501
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 16.2361
                       Mean reward: 857.90
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7561
     Episode_Reward/lifting_object: 171.2409
      Episode_Reward/object_height: 0.0684
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 0.93s
                      Time elapsed: 00:09:25
                               ETA: 00:22:25

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 88491 steps/s (collection: 0.926s, learning 0.185s)
             Mean action noise std: 1.93
          Mean value_function loss: 35.4508
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.2451
                       Mean reward: 860.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7564
     Episode_Reward/lifting_object: 171.5433
      Episode_Reward/object_height: 0.0686
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 1.11s
                      Time elapsed: 00:09:26
                               ETA: 00:22:25

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 96006 steps/s (collection: 0.842s, learning 0.182s)
             Mean action noise std: 1.94
          Mean value_function loss: 53.1393
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 16.2655
                       Mean reward: 848.13
               Mean episode length: 248.98
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 169.9737
      Episode_Reward/object_height: 0.0682
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 1.02s
                      Time elapsed: 00:09:27
                               ETA: 00:22:24

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 80327 steps/s (collection: 1.036s, learning 0.188s)
             Mean action noise std: 1.95
          Mean value_function loss: 51.0668
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.2987
                       Mean reward: 866.76
               Mean episode length: 248.83
    Episode_Reward/reaching_object: 0.7403
     Episode_Reward/lifting_object: 168.6525
      Episode_Reward/object_height: 0.0680
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 1.22s
                      Time elapsed: 00:09:28
                               ETA: 00:22:23

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 85252 steps/s (collection: 1.005s, learning 0.149s)
             Mean action noise std: 1.95
          Mean value_function loss: 42.9484
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 16.3161
                       Mean reward: 856.24
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7477
     Episode_Reward/lifting_object: 170.0984
      Episode_Reward/object_height: 0.0686
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 1.15s
                      Time elapsed: 00:09:29
                               ETA: 00:22:23

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 96668 steps/s (collection: 0.868s, learning 0.149s)
             Mean action noise std: 1.96
          Mean value_function loss: 34.8356
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.3288
                       Mean reward: 874.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 171.5452
      Episode_Reward/object_height: 0.0692
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 1.02s
                      Time elapsed: 00:09:30
                               ETA: 00:22:22

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 98902 steps/s (collection: 0.845s, learning 0.149s)
             Mean action noise std: 1.96
          Mean value_function loss: 30.9331
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.3481
                       Mean reward: 857.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 171.7802
      Episode_Reward/object_height: 0.0691
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 0.99s
                      Time elapsed: 00:09:31
                               ETA: 00:22:21

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 98004 steps/s (collection: 0.841s, learning 0.162s)
             Mean action noise std: 1.96
          Mean value_function loss: 37.9871
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 16.3635
                       Mean reward: 818.36
               Mean episode length: 243.52
    Episode_Reward/reaching_object: 0.7410
     Episode_Reward/lifting_object: 167.1617
      Episode_Reward/object_height: 0.0673
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 1.00s
                      Time elapsed: 00:09:32
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 94472 steps/s (collection: 0.929s, learning 0.111s)
             Mean action noise std: 1.96
          Mean value_function loss: 57.0345
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.3727
                       Mean reward: 833.86
               Mean episode length: 246.40
    Episode_Reward/reaching_object: 0.7430
     Episode_Reward/lifting_object: 169.4869
      Episode_Reward/object_height: 0.0684
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 1.04s
                      Time elapsed: 00:09:33
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 103318 steps/s (collection: 0.831s, learning 0.120s)
             Mean action noise std: 1.97
          Mean value_function loss: 56.6925
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.3822
                       Mean reward: 833.52
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 0.7401
     Episode_Reward/lifting_object: 169.1036
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 0.95s
                      Time elapsed: 00:09:34
                               ETA: 00:22:19

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 108519 steps/s (collection: 0.809s, learning 0.097s)
             Mean action noise std: 1.97
          Mean value_function loss: 35.6343
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 16.3952
                       Mean reward: 860.49
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7451
     Episode_Reward/lifting_object: 170.3513
      Episode_Reward/object_height: 0.0686
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 0.91s
                      Time elapsed: 00:09:35
                               ETA: 00:22:18

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 110771 steps/s (collection: 0.793s, learning 0.094s)
             Mean action noise std: 1.97
          Mean value_function loss: 35.4383
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.4043
                       Mean reward: 860.68
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.7489
     Episode_Reward/lifting_object: 170.4432
      Episode_Reward/object_height: 0.0688
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 0.89s
                      Time elapsed: 00:09:36
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 105710 steps/s (collection: 0.804s, learning 0.126s)
             Mean action noise std: 1.97
          Mean value_function loss: 35.1148
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.4107
                       Mean reward: 877.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: 173.7778
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 0.93s
                      Time elapsed: 00:09:37
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 102877 steps/s (collection: 0.827s, learning 0.128s)
             Mean action noise std: 1.98
          Mean value_function loss: 31.3446
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.4143
                       Mean reward: 856.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 172.0274
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 0.96s
                      Time elapsed: 00:09:38
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 104360 steps/s (collection: 0.816s, learning 0.126s)
             Mean action noise std: 1.98
          Mean value_function loss: 27.6164
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.4290
                       Mean reward: 852.47
               Mean episode length: 246.37
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 170.7587
      Episode_Reward/object_height: 0.0681
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 0.94s
                      Time elapsed: 00:09:39
                               ETA: 00:22:14

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 101993 steps/s (collection: 0.810s, learning 0.154s)
             Mean action noise std: 1.98
          Mean value_function loss: 34.4040
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.4488
                       Mean reward: 879.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 172.4114
      Episode_Reward/object_height: 0.0687
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 0.96s
                      Time elapsed: 00:09:40
                               ETA: 00:22:13

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 109920 steps/s (collection: 0.789s, learning 0.105s)
             Mean action noise std: 1.99
          Mean value_function loss: 45.4980
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 16.4710
                       Mean reward: 851.39
               Mean episode length: 246.72
    Episode_Reward/reaching_object: 0.7485
     Episode_Reward/lifting_object: 170.6182
      Episode_Reward/object_height: 0.0684
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 0.89s
                      Time elapsed: 00:09:41
                               ETA: 00:22:12

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 98562 steps/s (collection: 0.837s, learning 0.161s)
             Mean action noise std: 1.99
          Mean value_function loss: 30.0418
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.4874
                       Mean reward: 850.61
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.7480
     Episode_Reward/lifting_object: 169.6808
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 1.00s
                      Time elapsed: 00:09:42
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 100239 steps/s (collection: 0.833s, learning 0.148s)
             Mean action noise std: 2.00
          Mean value_function loss: 35.1111
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.4969
                       Mean reward: 848.33
               Mean episode length: 248.99
    Episode_Reward/reaching_object: 0.7552
     Episode_Reward/lifting_object: 171.4692
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 0.98s
                      Time elapsed: 00:09:43
                               ETA: 00:22:10

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 98646 steps/s (collection: 0.824s, learning 0.172s)
             Mean action noise std: 2.00
          Mean value_function loss: 43.2712
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.5080
                       Mean reward: 855.19
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7493
     Episode_Reward/lifting_object: 170.4074
      Episode_Reward/object_height: 0.0686
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 1.00s
                      Time elapsed: 00:09:44
                               ETA: 00:22:09

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 98297 steps/s (collection: 0.827s, learning 0.173s)
             Mean action noise std: 2.00
          Mean value_function loss: 45.1659
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 16.5217
                       Mean reward: 860.30
               Mean episode length: 249.09
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 171.6805
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 1.00s
                      Time elapsed: 00:09:45
                               ETA: 00:22:08

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 98563 steps/s (collection: 0.863s, learning 0.134s)
             Mean action noise std: 2.00
          Mean value_function loss: 43.7352
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 16.5278
                       Mean reward: 863.90
               Mean episode length: 247.11
    Episode_Reward/reaching_object: 0.7571
     Episode_Reward/lifting_object: 172.4854
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 1.00s
                      Time elapsed: 00:09:46
                               ETA: 00:22:07

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 95783 steps/s (collection: 0.864s, learning 0.163s)
             Mean action noise std: 2.01
          Mean value_function loss: 36.6293
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 16.5337
                       Mean reward: 851.33
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 0.7571
     Episode_Reward/lifting_object: 172.4548
      Episode_Reward/object_height: 0.0696
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 1.03s
                      Time elapsed: 00:09:47
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 100790 steps/s (collection: 0.813s, learning 0.162s)
             Mean action noise std: 2.01
          Mean value_function loss: 38.1496
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 16.5460
                       Mean reward: 861.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7514
     Episode_Reward/lifting_object: 171.2436
      Episode_Reward/object_height: 0.0692
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 0.98s
                      Time elapsed: 00:09:48
                               ETA: 00:22:05

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 100216 steps/s (collection: 0.806s, learning 0.175s)
             Mean action noise std: 2.01
          Mean value_function loss: 46.9911
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.5622
                       Mean reward: 846.62
               Mean episode length: 249.32
    Episode_Reward/reaching_object: 0.7502
     Episode_Reward/lifting_object: 171.4259
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 0.98s
                      Time elapsed: 00:09:49
                               ETA: 00:22:05

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 90224 steps/s (collection: 0.892s, learning 0.198s)
             Mean action noise std: 2.02
          Mean value_function loss: 46.9484
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 16.5777
                       Mean reward: 832.24
               Mean episode length: 243.04
    Episode_Reward/reaching_object: 0.7407
     Episode_Reward/lifting_object: 169.5247
      Episode_Reward/object_height: 0.0689
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 1.09s
                      Time elapsed: 00:09:50
                               ETA: 00:22:04

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 101134 steps/s (collection: 0.853s, learning 0.119s)
             Mean action noise std: 2.02
          Mean value_function loss: 38.8360
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 16.5992
                       Mean reward: 863.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7504
     Episode_Reward/lifting_object: 171.6678
      Episode_Reward/object_height: 0.0697
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 0.97s
                      Time elapsed: 00:09:51
                               ETA: 00:22:03

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 95030 steps/s (collection: 0.843s, learning 0.191s)
             Mean action noise std: 2.03
          Mean value_function loss: 34.5923
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.6201
                       Mean reward: 847.28
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 170.9656
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 1.03s
                      Time elapsed: 00:09:52
                               ETA: 00:22:02

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 98301 steps/s (collection: 0.816s, learning 0.184s)
             Mean action noise std: 2.03
          Mean value_function loss: 35.8264
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.6346
                       Mean reward: 860.32
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7455
     Episode_Reward/lifting_object: 169.0718
      Episode_Reward/object_height: 0.0686
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 1.00s
                      Time elapsed: 00:09:53
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 103231 steps/s (collection: 0.794s, learning 0.158s)
             Mean action noise std: 2.04
          Mean value_function loss: 42.2051
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 16.6549
                       Mean reward: 862.38
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.7464
     Episode_Reward/lifting_object: 169.6434
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 0.95s
                      Time elapsed: 00:09:54
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 97147 steps/s (collection: 0.817s, learning 0.194s)
             Mean action noise std: 2.04
          Mean value_function loss: 46.6821
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 16.6807
                       Mean reward: 874.03
               Mean episode length: 249.16
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 173.5658
      Episode_Reward/object_height: 0.0708
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 1.01s
                      Time elapsed: 00:09:55
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 90863 steps/s (collection: 0.876s, learning 0.206s)
             Mean action noise std: 2.05
          Mean value_function loss: 64.4593
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 16.6982
                       Mean reward: 848.62
               Mean episode length: 245.72
    Episode_Reward/reaching_object: 0.7460
     Episode_Reward/lifting_object: 171.4150
      Episode_Reward/object_height: 0.0699
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 1.08s
                      Time elapsed: 00:09:56
                               ETA: 00:21:59

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 88523 steps/s (collection: 0.924s, learning 0.186s)
             Mean action noise std: 2.05
          Mean value_function loss: 56.4599
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.7046
                       Mean reward: 847.10
               Mean episode length: 246.64
    Episode_Reward/reaching_object: 0.7447
     Episode_Reward/lifting_object: 169.4722
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 1.11s
                      Time elapsed: 00:09:57
                               ETA: 00:21:58

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 91958 steps/s (collection: 0.881s, learning 0.188s)
             Mean action noise std: 2.05
          Mean value_function loss: 56.2044
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.7095
                       Mean reward: 859.64
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 172.4001
      Episode_Reward/object_height: 0.0702
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 1.07s
                      Time elapsed: 00:09:58
                               ETA: 00:21:58

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 89448 steps/s (collection: 0.921s, learning 0.178s)
             Mean action noise std: 2.06
          Mean value_function loss: 46.9279
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.7217
                       Mean reward: 859.53
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.7485
     Episode_Reward/lifting_object: 170.8835
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 1.10s
                      Time elapsed: 00:09:59
                               ETA: 00:21:57

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 96912 steps/s (collection: 0.862s, learning 0.152s)
             Mean action noise std: 2.06
          Mean value_function loss: 48.6919
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 16.7371
                       Mean reward: 854.54
               Mean episode length: 246.66
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 171.6921
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 1.01s
                      Time elapsed: 00:10:00
                               ETA: 00:21:56

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 97361 steps/s (collection: 0.838s, learning 0.172s)
             Mean action noise std: 2.06
          Mean value_function loss: 53.8265
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 16.7464
                       Mean reward: 838.25
               Mean episode length: 246.46
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 171.8969
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 1.01s
                      Time elapsed: 00:10:01
                               ETA: 00:21:55

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 103864 steps/s (collection: 0.804s, learning 0.142s)
             Mean action noise std: 2.06
          Mean value_function loss: 38.2407
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 16.7596
                       Mean reward: 815.16
               Mean episode length: 246.95
    Episode_Reward/reaching_object: 0.7408
     Episode_Reward/lifting_object: 167.3804
      Episode_Reward/object_height: 0.0673
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 0.95s
                      Time elapsed: 00:10:02
                               ETA: 00:21:54

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 91831 steps/s (collection: 0.892s, learning 0.178s)
             Mean action noise std: 2.07
          Mean value_function loss: 54.0427
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.7655
                       Mean reward: 863.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 169.9676
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 1.07s
                      Time elapsed: 00:10:03
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 96011 steps/s (collection: 0.851s, learning 0.173s)
             Mean action noise std: 2.07
          Mean value_function loss: 36.3385
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 16.7777
                       Mean reward: 836.73
               Mean episode length: 247.50
    Episode_Reward/reaching_object: 0.7502
     Episode_Reward/lifting_object: 167.6541
      Episode_Reward/object_height: 0.0673
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 1.02s
                      Time elapsed: 00:10:04
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 101037 steps/s (collection: 0.823s, learning 0.150s)
             Mean action noise std: 2.08
          Mean value_function loss: 35.6427
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 16.8011
                       Mean reward: 819.31
               Mean episode length: 244.88
    Episode_Reward/reaching_object: 0.7339
     Episode_Reward/lifting_object: 166.0148
      Episode_Reward/object_height: 0.0667
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 0.97s
                      Time elapsed: 00:10:05
                               ETA: 00:21:52

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 100923 steps/s (collection: 0.848s, learning 0.126s)
             Mean action noise std: 2.08
          Mean value_function loss: 35.0761
               Mean surrogate loss: 0.0126
                 Mean entropy loss: 16.8275
                       Mean reward: 858.45
               Mean episode length: 248.93
    Episode_Reward/reaching_object: 0.7485
     Episode_Reward/lifting_object: 168.1385
      Episode_Reward/object_height: 0.0680
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 0.97s
                      Time elapsed: 00:10:06
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 99342 steps/s (collection: 0.843s, learning 0.146s)
             Mean action noise std: 2.08
          Mean value_function loss: 41.5039
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 16.8300
                       Mean reward: 852.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7418
     Episode_Reward/lifting_object: 167.8172
      Episode_Reward/object_height: 0.0682
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 0.99s
                      Time elapsed: 00:10:07
                               ETA: 00:21:50

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 100097 steps/s (collection: 0.853s, learning 0.129s)
             Mean action noise std: 2.09
          Mean value_function loss: 32.6661
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 16.8339
                       Mean reward: 837.99
               Mean episode length: 247.39
    Episode_Reward/reaching_object: 0.7501
     Episode_Reward/lifting_object: 169.8589
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 0.98s
                      Time elapsed: 00:10:08
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 103992 steps/s (collection: 0.858s, learning 0.088s)
             Mean action noise std: 2.09
          Mean value_function loss: 34.5381
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 16.8423
                       Mean reward: 872.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7552
     Episode_Reward/lifting_object: 170.8922
      Episode_Reward/object_height: 0.0698
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 0.95s
                      Time elapsed: 00:10:09
                               ETA: 00:21:48

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 106962 steps/s (collection: 0.803s, learning 0.116s)
             Mean action noise std: 2.09
          Mean value_function loss: 40.6199
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 16.8534
                       Mean reward: 851.35
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 171.5476
      Episode_Reward/object_height: 0.0705
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 0.92s
                      Time elapsed: 00:10:10
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 98228 steps/s (collection: 0.854s, learning 0.147s)
             Mean action noise std: 2.10
          Mean value_function loss: 28.0075
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 16.8692
                       Mean reward: 878.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7570
     Episode_Reward/lifting_object: 171.9041
      Episode_Reward/object_height: 0.0706
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 1.00s
                      Time elapsed: 00:10:11
                               ETA: 00:21:46

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 92785 steps/s (collection: 0.893s, learning 0.167s)
             Mean action noise std: 2.10
          Mean value_function loss: 30.2560
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 16.8854
                       Mean reward: 865.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7502
     Episode_Reward/lifting_object: 170.2202
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 1.06s
                      Time elapsed: 00:10:12
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 94795 steps/s (collection: 0.915s, learning 0.122s)
             Mean action noise std: 2.10
          Mean value_function loss: 41.3323
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 16.9041
                       Mean reward: 851.84
               Mean episode length: 247.85
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 170.8045
      Episode_Reward/object_height: 0.0706
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 1.04s
                      Time elapsed: 00:10:13
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 95411 steps/s (collection: 0.896s, learning 0.135s)
             Mean action noise std: 2.11
          Mean value_function loss: 28.3217
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 16.9218
                       Mean reward: 860.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 172.5473
      Episode_Reward/object_height: 0.0712
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 1.03s
                      Time elapsed: 00:10:14
                               ETA: 00:21:44

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 93263 steps/s (collection: 0.934s, learning 0.120s)
             Mean action noise std: 2.11
          Mean value_function loss: 34.4143
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 16.9378
                       Mean reward: 854.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7564
     Episode_Reward/lifting_object: 171.3363
      Episode_Reward/object_height: 0.0709
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 1.05s
                      Time elapsed: 00:10:15
                               ETA: 00:21:43

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 95561 steps/s (collection: 0.912s, learning 0.117s)
             Mean action noise std: 2.11
          Mean value_function loss: 37.5557
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 16.9425
                       Mean reward: 870.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7463
     Episode_Reward/lifting_object: 171.6514
      Episode_Reward/object_height: 0.0709
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 1.03s
                      Time elapsed: 00:10:16
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 97484 steps/s (collection: 0.900s, learning 0.109s)
             Mean action noise std: 2.12
          Mean value_function loss: 31.0493
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.9516
                       Mean reward: 835.64
               Mean episode length: 247.16
    Episode_Reward/reaching_object: 0.7429
     Episode_Reward/lifting_object: 168.5430
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 1.01s
                      Time elapsed: 00:10:17
                               ETA: 00:21:41

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 93185 steps/s (collection: 0.905s, learning 0.150s)
             Mean action noise std: 2.12
          Mean value_function loss: 44.1848
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 16.9608
                       Mean reward: 840.73
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.7449
     Episode_Reward/lifting_object: 168.7795
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 1.05s
                      Time elapsed: 00:10:18
                               ETA: 00:21:41

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 92803 steps/s (collection: 0.889s, learning 0.171s)
             Mean action noise std: 2.12
          Mean value_function loss: 48.1280
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 16.9687
                       Mean reward: 858.01
               Mean episode length: 246.12
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 172.2090
      Episode_Reward/object_height: 0.0703
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 1.06s
                      Time elapsed: 00:10:19
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 96700 steps/s (collection: 0.885s, learning 0.131s)
             Mean action noise std: 2.13
          Mean value_function loss: 28.1804
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 16.9989
                       Mean reward: 876.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 172.4865
      Episode_Reward/object_height: 0.0704
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 1.02s
                      Time elapsed: 00:10:20
                               ETA: 00:21:39

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 97925 steps/s (collection: 0.892s, learning 0.112s)
             Mean action noise std: 2.14
          Mean value_function loss: 35.1941
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.0285
                       Mean reward: 877.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 173.1490
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 1.00s
                      Time elapsed: 00:10:21
                               ETA: 00:21:38

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 94132 steps/s (collection: 0.910s, learning 0.134s)
             Mean action noise std: 2.14
          Mean value_function loss: 39.4925
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.0503
                       Mean reward: 864.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7509
     Episode_Reward/lifting_object: 171.8757
      Episode_Reward/object_height: 0.0704
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 1.04s
                      Time elapsed: 00:10:23
                               ETA: 00:21:37

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 92269 steps/s (collection: 0.865s, learning 0.200s)
             Mean action noise std: 2.15
          Mean value_function loss: 25.6815
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 17.0748
                       Mean reward: 863.34
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.7533
     Episode_Reward/lifting_object: 172.3500
      Episode_Reward/object_height: 0.0705
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 1.07s
                      Time elapsed: 00:10:24
                               ETA: 00:21:37

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 93515 steps/s (collection: 0.885s, learning 0.167s)
             Mean action noise std: 2.16
          Mean value_function loss: 30.8755
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.1003
                       Mean reward: 879.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 173.5329
      Episode_Reward/object_height: 0.0709
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 1.05s
                      Time elapsed: 00:10:25
                               ETA: 00:21:36

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 96304 steps/s (collection: 0.855s, learning 0.166s)
             Mean action noise std: 2.16
          Mean value_function loss: 30.5124
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.1145
                       Mean reward: 866.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 172.7506
      Episode_Reward/object_height: 0.0705
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 1.02s
                      Time elapsed: 00:10:26
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 97197 steps/s (collection: 0.843s, learning 0.169s)
             Mean action noise std: 2.17
          Mean value_function loss: 31.5488
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 17.1290
                       Mean reward: 862.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 171.6266
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 1.01s
                      Time elapsed: 00:10:27
                               ETA: 00:21:34

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 98061 steps/s (collection: 0.831s, learning 0.171s)
             Mean action noise std: 2.17
          Mean value_function loss: 38.3235
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.1523
                       Mean reward: 869.36
               Mean episode length: 249.54
    Episode_Reward/reaching_object: 0.7509
     Episode_Reward/lifting_object: 171.0499
      Episode_Reward/object_height: 0.0698
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 1.00s
                      Time elapsed: 00:10:28
                               ETA: 00:21:33

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 96201 steps/s (collection: 0.846s, learning 0.176s)
             Mean action noise std: 2.17
          Mean value_function loss: 35.4333
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 17.1610
                       Mean reward: 849.85
               Mean episode length: 249.25
    Episode_Reward/reaching_object: 0.7436
     Episode_Reward/lifting_object: 169.4362
      Episode_Reward/object_height: 0.0691
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 1.02s
                      Time elapsed: 00:10:29
                               ETA: 00:21:32

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 97912 steps/s (collection: 0.852s, learning 0.152s)
             Mean action noise std: 2.17
          Mean value_function loss: 35.1470
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.1702
                       Mean reward: 854.01
               Mean episode length: 245.49
    Episode_Reward/reaching_object: 0.7517
     Episode_Reward/lifting_object: 170.7400
      Episode_Reward/object_height: 0.0696
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 1.00s
                      Time elapsed: 00:10:30
                               ETA: 00:21:32

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 91935 steps/s (collection: 0.885s, learning 0.184s)
             Mean action noise std: 2.18
          Mean value_function loss: 35.3970
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.1873
                       Mean reward: 869.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 172.7053
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 1.07s
                      Time elapsed: 00:10:31
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 90262 steps/s (collection: 0.931s, learning 0.158s)
             Mean action noise std: 2.19
          Mean value_function loss: 42.8627
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 17.2154
                       Mean reward: 883.27
               Mean episode length: 249.94
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 172.3205
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 1.09s
                      Time elapsed: 00:10:32
                               ETA: 00:21:30

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 97533 steps/s (collection: 0.859s, learning 0.149s)
             Mean action noise std: 2.20
          Mean value_function loss: 40.7461
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 17.2453
                       Mean reward: 844.41
               Mean episode length: 244.76
    Episode_Reward/reaching_object: 0.7575
     Episode_Reward/lifting_object: 172.5810
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 1.01s
                      Time elapsed: 00:10:33
                               ETA: 00:21:29

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 102862 steps/s (collection: 0.845s, learning 0.111s)
             Mean action noise std: 2.20
          Mean value_function loss: 39.1145
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 17.2665
                       Mean reward: 866.50
               Mean episode length: 248.82
    Episode_Reward/reaching_object: 0.7495
     Episode_Reward/lifting_object: 170.5322
      Episode_Reward/object_height: 0.0684
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 0.96s
                      Time elapsed: 00:10:34
                               ETA: 00:21:28

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 99972 steps/s (collection: 0.808s, learning 0.176s)
             Mean action noise std: 2.21
          Mean value_function loss: 33.5005
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.2933
                       Mean reward: 857.69
               Mean episode length: 246.38
    Episode_Reward/reaching_object: 0.7464
     Episode_Reward/lifting_object: 169.2941
      Episode_Reward/object_height: 0.0677
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 0.98s
                      Time elapsed: 00:10:35
                               ETA: 00:21:27

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 102569 steps/s (collection: 0.838s, learning 0.120s)
             Mean action noise std: 2.21
          Mean value_function loss: 32.2794
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 17.3147
                       Mean reward: 871.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 171.9742
      Episode_Reward/object_height: 0.0684
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 0.96s
                      Time elapsed: 00:10:36
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 104701 steps/s (collection: 0.822s, learning 0.117s)
             Mean action noise std: 2.22
          Mean value_function loss: 36.4973
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.3201
                       Mean reward: 882.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 173.3831
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 0.94s
                      Time elapsed: 00:10:37
                               ETA: 00:21:25

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 98003 steps/s (collection: 0.814s, learning 0.189s)
             Mean action noise std: 2.22
          Mean value_function loss: 41.6994
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 17.3360
                       Mean reward: 875.16
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 172.8183
      Episode_Reward/object_height: 0.0692
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 1.00s
                      Time elapsed: 00:10:38
                               ETA: 00:21:25

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 104415 steps/s (collection: 0.817s, learning 0.125s)
             Mean action noise std: 2.22
          Mean value_function loss: 43.5775
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.3442
                       Mean reward: 858.11
               Mean episode length: 246.12
    Episode_Reward/reaching_object: 0.7503
     Episode_Reward/lifting_object: 170.5130
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 0.94s
                      Time elapsed: 00:10:39
                               ETA: 00:21:24

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 103442 steps/s (collection: 0.801s, learning 0.149s)
             Mean action noise std: 2.23
          Mean value_function loss: 36.0518
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 17.3584
                       Mean reward: 849.53
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.7512
     Episode_Reward/lifting_object: 170.6599
      Episode_Reward/object_height: 0.0685
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 0.95s
                      Time elapsed: 00:10:40
                               ETA: 00:21:23

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 55922 steps/s (collection: 1.607s, learning 0.151s)
             Mean action noise std: 2.23
          Mean value_function loss: 45.1292
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 17.3759
                       Mean reward: 874.44
               Mean episode length: 249.53
    Episode_Reward/reaching_object: 0.7518
     Episode_Reward/lifting_object: 172.1459
      Episode_Reward/object_height: 0.0692
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 1.76s
                      Time elapsed: 00:10:41
                               ETA: 00:21:23

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 30168 steps/s (collection: 3.088s, learning 0.171s)
             Mean action noise std: 2.24
          Mean value_function loss: 51.4306
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.3954
                       Mean reward: 863.51
               Mean episode length: 246.32
    Episode_Reward/reaching_object: 0.7417
     Episode_Reward/lifting_object: 170.5238
      Episode_Reward/object_height: 0.0687
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 3.26s
                      Time elapsed: 00:10:45
                               ETA: 00:21:27

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 28885 steps/s (collection: 3.256s, learning 0.147s)
             Mean action noise std: 2.24
          Mean value_function loss: 63.7033
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 17.4150
                       Mean reward: 856.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7360
     Episode_Reward/lifting_object: 169.4422
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 3.40s
                      Time elapsed: 00:10:48
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 29470 steps/s (collection: 3.193s, learning 0.143s)
             Mean action noise std: 2.25
          Mean value_function loss: 46.0428
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.4390
                       Mean reward: 842.55
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.7459
     Episode_Reward/lifting_object: 170.0441
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 3.34s
                      Time elapsed: 00:10:51
                               ETA: 00:21:34

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 31096 steps/s (collection: 3.037s, learning 0.124s)
             Mean action noise std: 2.26
          Mean value_function loss: 42.4197
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.4635
                       Mean reward: 872.72
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 171.4173
      Episode_Reward/object_height: 0.0686
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 3.16s
                      Time elapsed: 00:10:54
                               ETA: 00:21:38

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 31725 steps/s (collection: 2.966s, learning 0.133s)
             Mean action noise std: 2.26
          Mean value_function loss: 52.5880
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 17.4819
                       Mean reward: 856.51
               Mean episode length: 249.77
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 171.8519
      Episode_Reward/object_height: 0.0686
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 3.10s
                      Time elapsed: 00:10:58
                               ETA: 00:21:41

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 29958 steps/s (collection: 3.140s, learning 0.142s)
             Mean action noise std: 2.27
          Mean value_function loss: 44.2249
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.5017
                       Mean reward: 846.60
               Mean episode length: 245.67
    Episode_Reward/reaching_object: 0.7455
     Episode_Reward/lifting_object: 169.3915
      Episode_Reward/object_height: 0.0673
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 3.28s
                      Time elapsed: 00:11:01
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 30083 steps/s (collection: 3.136s, learning 0.132s)
             Mean action noise std: 2.27
          Mean value_function loss: 43.3975
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 17.5140
                       Mean reward: 860.77
               Mean episode length: 249.40
    Episode_Reward/reaching_object: 0.7456
     Episode_Reward/lifting_object: 167.5340
      Episode_Reward/object_height: 0.0669
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 3.27s
                      Time elapsed: 00:11:04
                               ETA: 00:21:48

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 27635 steps/s (collection: 3.397s, learning 0.160s)
             Mean action noise std: 2.28
          Mean value_function loss: 36.1582
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 17.5323
                       Mean reward: 862.10
               Mean episode length: 247.54
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 171.1708
      Episode_Reward/object_height: 0.0688
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 3.56s
                      Time elapsed: 00:11:08
                               ETA: 00:21:52

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 28571 steps/s (collection: 3.292s, learning 0.148s)
             Mean action noise std: 2.28
          Mean value_function loss: 42.6185
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 17.5517
                       Mean reward: 858.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7348
     Episode_Reward/lifting_object: 167.9904
      Episode_Reward/object_height: 0.0676
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 3.44s
                      Time elapsed: 00:11:11
                               ETA: 00:21:56

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 94404 steps/s (collection: 0.838s, learning 0.204s)
             Mean action noise std: 2.29
          Mean value_function loss: 40.2512
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 17.5643
                       Mean reward: 859.81
               Mean episode length: 245.54
    Episode_Reward/reaching_object: 0.7326
     Episode_Reward/lifting_object: 168.0575
      Episode_Reward/object_height: 0.0681
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 1.04s
                      Time elapsed: 00:11:12
                               ETA: 00:21:55

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 87330 steps/s (collection: 0.951s, learning 0.175s)
             Mean action noise std: 2.29
          Mean value_function loss: 45.7519
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 17.5792
                       Mean reward: 833.59
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7392
     Episode_Reward/lifting_object: 168.7451
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 1.13s
                      Time elapsed: 00:11:13
                               ETA: 00:21:54

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 105077 steps/s (collection: 0.832s, learning 0.104s)
             Mean action noise std: 2.30
          Mean value_function loss: 44.2289
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 17.5959
                       Mean reward: 847.41
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.7413
     Episode_Reward/lifting_object: 169.4930
      Episode_Reward/object_height: 0.0687
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 0.94s
                      Time elapsed: 00:11:14
                               ETA: 00:21:53

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 100626 steps/s (collection: 0.870s, learning 0.107s)
             Mean action noise std: 2.30
          Mean value_function loss: 44.7699
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.6167
                       Mean reward: 846.05
               Mean episode length: 247.17
    Episode_Reward/reaching_object: 0.7343
     Episode_Reward/lifting_object: 168.2192
      Episode_Reward/object_height: 0.0685
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 0.98s
                      Time elapsed: 00:11:15
                               ETA: 00:21:52

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 102203 steps/s (collection: 0.844s, learning 0.118s)
             Mean action noise std: 2.31
          Mean value_function loss: 38.9343
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 17.6416
                       Mean reward: 834.86
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.7440
     Episode_Reward/lifting_object: 170.3460
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 0.96s
                      Time elapsed: 00:11:16
                               ETA: 00:21:51

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 104385 steps/s (collection: 0.838s, learning 0.104s)
             Mean action noise std: 2.31
          Mean value_function loss: 29.1638
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 17.6467
                       Mean reward: 863.30
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7413
     Episode_Reward/lifting_object: 168.4987
      Episode_Reward/object_height: 0.0684
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 0.94s
                      Time elapsed: 00:11:17
                               ETA: 00:21:50

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 103442 steps/s (collection: 0.850s, learning 0.100s)
             Mean action noise std: 2.31
          Mean value_function loss: 45.8532
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.6549
                       Mean reward: 863.34
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7392
     Episode_Reward/lifting_object: 168.2236
      Episode_Reward/object_height: 0.0684
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 0.95s
                      Time elapsed: 00:11:18
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 98999 steps/s (collection: 0.875s, learning 0.118s)
             Mean action noise std: 2.32
          Mean value_function loss: 50.3379
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.6702
                       Mean reward: 854.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7526
     Episode_Reward/lifting_object: 172.2337
      Episode_Reward/object_height: 0.0699
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 0.99s
                      Time elapsed: 00:11:19
                               ETA: 00:21:48

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 103018 steps/s (collection: 0.856s, learning 0.098s)
             Mean action noise std: 2.32
          Mean value_function loss: 34.2203
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.6951
                       Mean reward: 867.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 172.3844
      Episode_Reward/object_height: 0.0700
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 0.95s
                      Time elapsed: 00:11:20
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 94061 steps/s (collection: 0.899s, learning 0.146s)
             Mean action noise std: 2.33
          Mean value_function loss: 35.2019
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.7115
                       Mean reward: 845.64
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 171.6407
      Episode_Reward/object_height: 0.0699
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 1.05s
                      Time elapsed: 00:11:21
                               ETA: 00:21:46

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 86491 steps/s (collection: 0.927s, learning 0.210s)
             Mean action noise std: 2.33
          Mean value_function loss: 32.2496
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.7289
                       Mean reward: 845.45
               Mean episode length: 249.18
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 171.6566
      Episode_Reward/object_height: 0.0701
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 1.14s
                      Time elapsed: 00:11:22
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 100794 steps/s (collection: 0.848s, learning 0.127s)
             Mean action noise std: 2.34
          Mean value_function loss: 39.9386
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 17.7417
                       Mean reward: 815.39
               Mean episode length: 245.31
    Episode_Reward/reaching_object: 0.7444
     Episode_Reward/lifting_object: 167.6437
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 0.98s
                      Time elapsed: 00:11:23
                               ETA: 00:21:44

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 100064 steps/s (collection: 0.850s, learning 0.133s)
             Mean action noise std: 2.34
          Mean value_function loss: 42.6523
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 17.7499
                       Mean reward: 811.97
               Mean episode length: 247.85
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 169.8730
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 0.98s
                      Time elapsed: 00:11:24
                               ETA: 00:21:43

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 97879 steps/s (collection: 0.871s, learning 0.134s)
             Mean action noise std: 2.35
          Mean value_function loss: 32.7901
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 17.7668
                       Mean reward: 874.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 171.5766
      Episode_Reward/object_height: 0.0697
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 1.00s
                      Time elapsed: 00:11:25
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 101636 steps/s (collection: 0.859s, learning 0.108s)
             Mean action noise std: 2.36
          Mean value_function loss: 37.5128
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 17.7889
                       Mean reward: 851.09
               Mean episode length: 246.46
    Episode_Reward/reaching_object: 0.7545
     Episode_Reward/lifting_object: 170.1808
      Episode_Reward/object_height: 0.0692
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 0.97s
                      Time elapsed: 00:11:26
                               ETA: 00:21:41

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 100842 steps/s (collection: 0.863s, learning 0.112s)
             Mean action noise std: 2.36
          Mean value_function loss: 29.5397
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 17.8169
                       Mean reward: 867.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7473
     Episode_Reward/lifting_object: 170.1759
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 0.97s
                      Time elapsed: 00:11:27
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 102861 steps/s (collection: 0.852s, learning 0.103s)
             Mean action noise std: 2.37
          Mean value_function loss: 39.6241
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.8295
                       Mean reward: 843.15
               Mean episode length: 246.82
    Episode_Reward/reaching_object: 0.7488
     Episode_Reward/lifting_object: 170.0696
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 0.96s
                      Time elapsed: 00:11:28
                               ETA: 00:21:39

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 101509 steps/s (collection: 0.850s, learning 0.118s)
             Mean action noise std: 2.37
          Mean value_function loss: 33.2448
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 17.8466
                       Mean reward: 861.51
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.7514
     Episode_Reward/lifting_object: 170.3590
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 0.97s
                      Time elapsed: 00:11:29
                               ETA: 00:21:38

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 96222 steps/s (collection: 0.854s, learning 0.167s)
             Mean action noise std: 2.38
          Mean value_function loss: 35.8863
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 17.8721
                       Mean reward: 865.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 172.8138
      Episode_Reward/object_height: 0.0702
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 1.02s
                      Time elapsed: 00:11:30
                               ETA: 00:21:37

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 97727 steps/s (collection: 0.869s, learning 0.137s)
             Mean action noise std: 2.38
          Mean value_function loss: 33.8938
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 17.8860
                       Mean reward: 877.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 173.8024
      Episode_Reward/object_height: 0.0707
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 1.01s
                      Time elapsed: 00:11:31
                               ETA: 00:21:36

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 102312 steps/s (collection: 0.833s, learning 0.128s)
             Mean action noise std: 2.38
          Mean value_function loss: 45.1392
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 17.8932
                       Mean reward: 862.16
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7517
     Episode_Reward/lifting_object: 170.2823
      Episode_Reward/object_height: 0.0692
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 0.96s
                      Time elapsed: 00:11:32
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 101665 steps/s (collection: 0.856s, learning 0.111s)
             Mean action noise std: 2.39
          Mean value_function loss: 45.4058
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 17.9011
                       Mean reward: 839.15
               Mean episode length: 247.21
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 171.2755
      Episode_Reward/object_height: 0.0697
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 0.97s
                      Time elapsed: 00:11:33
                               ETA: 00:21:34

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 95780 steps/s (collection: 0.842s, learning 0.185s)
             Mean action noise std: 2.40
          Mean value_function loss: 35.3100
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 17.9189
                       Mean reward: 862.18
               Mean episode length: 247.41
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 173.2629
      Episode_Reward/object_height: 0.0706
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 1.03s
                      Time elapsed: 00:11:34
                               ETA: 00:21:33

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 93271 steps/s (collection: 0.850s, learning 0.204s)
             Mean action noise std: 2.40
          Mean value_function loss: 41.2789
               Mean surrogate loss: 0.0114
                 Mean entropy loss: 17.9513
                       Mean reward: 846.14
               Mean episode length: 243.11
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 169.9411
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 1.05s
                      Time elapsed: 00:11:35
                               ETA: 00:21:32

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 85703 steps/s (collection: 0.995s, learning 0.152s)
             Mean action noise std: 2.40
          Mean value_function loss: 42.8983
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 17.9543
                       Mean reward: 876.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 172.6097
      Episode_Reward/object_height: 0.0703
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 1.15s
                      Time elapsed: 00:11:36
                               ETA: 00:21:32

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 86494 steps/s (collection: 0.998s, learning 0.139s)
             Mean action noise std: 2.40
          Mean value_function loss: 41.9315
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 17.9598
                       Mean reward: 862.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 171.3549
      Episode_Reward/object_height: 0.0697
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 1.14s
                      Time elapsed: 00:11:37
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 104525 steps/s (collection: 0.823s, learning 0.117s)
             Mean action noise std: 2.41
          Mean value_function loss: 41.4918
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 17.9765
                       Mean reward: 839.40
               Mean episode length: 243.10
    Episode_Reward/reaching_object: 0.7502
     Episode_Reward/lifting_object: 168.1110
      Episode_Reward/object_height: 0.0682
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 0.94s
                      Time elapsed: 00:11:38
                               ETA: 00:21:30

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 105924 steps/s (collection: 0.811s, learning 0.117s)
             Mean action noise std: 2.42
          Mean value_function loss: 60.6934
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.0065
                       Mean reward: 831.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 170.8948
      Episode_Reward/object_height: 0.0698
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 0.93s
                      Time elapsed: 00:11:39
                               ETA: 00:21:29

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 98451 steps/s (collection: 0.843s, learning 0.156s)
             Mean action noise std: 2.43
          Mean value_function loss: 47.0461
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.0372
                       Mean reward: 867.58
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7524
     Episode_Reward/lifting_object: 169.4259
      Episode_Reward/object_height: 0.0692
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 1.00s
                      Time elapsed: 00:11:40
                               ETA: 00:21:28

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 100840 steps/s (collection: 0.864s, learning 0.111s)
             Mean action noise std: 2.43
          Mean value_function loss: 44.4427
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.0558
                       Mean reward: 857.70
               Mean episode length: 246.47
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 170.0768
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 0.97s
                      Time elapsed: 00:11:41
                               ETA: 00:21:27

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 92920 steps/s (collection: 0.874s, learning 0.184s)
             Mean action noise std: 2.44
          Mean value_function loss: 47.2839
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 18.0646
                       Mean reward: 853.30
               Mean episode length: 249.88
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 172.9224
      Episode_Reward/object_height: 0.0705
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 1.06s
                      Time elapsed: 00:11:42
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 92371 steps/s (collection: 0.889s, learning 0.176s)
             Mean action noise std: 2.44
          Mean value_function loss: 35.1925
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 18.0876
                       Mean reward: 853.09
               Mean episode length: 247.87
    Episode_Reward/reaching_object: 0.7669
     Episode_Reward/lifting_object: 172.7324
      Episode_Reward/object_height: 0.0703
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 1.06s
                      Time elapsed: 00:11:43
                               ETA: 00:21:25

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 98808 steps/s (collection: 0.886s, learning 0.109s)
             Mean action noise std: 2.44
          Mean value_function loss: 40.6529
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.1064
                       Mean reward: 853.09
               Mean episode length: 247.50
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 170.8481
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 0.99s
                      Time elapsed: 00:11:44
                               ETA: 00:21:24

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 104942 steps/s (collection: 0.822s, learning 0.115s)
             Mean action noise std: 2.45
          Mean value_function loss: 44.6285
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 18.1158
                       Mean reward: 857.33
               Mean episode length: 249.95
    Episode_Reward/reaching_object: 0.7515
     Episode_Reward/lifting_object: 170.0726
      Episode_Reward/object_height: 0.0689
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 0.94s
                      Time elapsed: 00:11:45
                               ETA: 00:21:23

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 98964 steps/s (collection: 0.844s, learning 0.149s)
             Mean action noise std: 2.46
          Mean value_function loss: 51.8365
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.1391
                       Mean reward: 862.21
               Mean episode length: 247.76
    Episode_Reward/reaching_object: 0.7455
     Episode_Reward/lifting_object: 168.4198
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 0.99s
                      Time elapsed: 00:11:46
                               ETA: 00:21:22

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 109862 steps/s (collection: 0.800s, learning 0.095s)
             Mean action noise std: 2.46
          Mean value_function loss: 33.6080
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.1621
                       Mean reward: 829.33
               Mean episode length: 244.60
    Episode_Reward/reaching_object: 0.7454
     Episode_Reward/lifting_object: 169.6694
      Episode_Reward/object_height: 0.0685
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 0.89s
                      Time elapsed: 00:11:47
                               ETA: 00:21:21

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 104486 steps/s (collection: 0.851s, learning 0.090s)
             Mean action noise std: 2.47
          Mean value_function loss: 42.1376
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.1894
                       Mean reward: 863.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7495
     Episode_Reward/lifting_object: 169.0580
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 0.94s
                      Time elapsed: 00:11:48
                               ETA: 00:21:20

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 99835 steps/s (collection: 0.868s, learning 0.117s)
             Mean action noise std: 2.48
          Mean value_function loss: 40.0284
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 18.2120
                       Mean reward: 881.65
               Mean episode length: 249.94
    Episode_Reward/reaching_object: 0.7375
     Episode_Reward/lifting_object: 167.8784
      Episode_Reward/object_height: 0.0677
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 0.98s
                      Time elapsed: 00:11:49
                               ETA: 00:21:18

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 86221 steps/s (collection: 1.009s, learning 0.131s)
             Mean action noise std: 2.48
          Mean value_function loss: 34.0449
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.2224
                       Mean reward: 849.91
               Mean episode length: 245.40
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 170.2798
      Episode_Reward/object_height: 0.0684
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 1.14s
                      Time elapsed: 00:11:50
                               ETA: 00:21:18

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 96974 steps/s (collection: 0.868s, learning 0.146s)
             Mean action noise std: 2.48
          Mean value_function loss: 39.7874
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.2287
                       Mean reward: 876.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 172.9042
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 1.01s
                      Time elapsed: 00:11:51
                               ETA: 00:21:17

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 100217 steps/s (collection: 0.866s, learning 0.115s)
             Mean action noise std: 2.49
          Mean value_function loss: 40.0501
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.2450
                       Mean reward: 878.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 172.0258
      Episode_Reward/object_height: 0.0691
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 0.98s
                      Time elapsed: 00:11:52
                               ETA: 00:21:16

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 93944 steps/s (collection: 0.894s, learning 0.153s)
             Mean action noise std: 2.49
          Mean value_function loss: 35.4527
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.2610
                       Mean reward: 857.83
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7411
     Episode_Reward/lifting_object: 168.7358
      Episode_Reward/object_height: 0.0677
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 1.05s
                      Time elapsed: 00:11:53
                               ETA: 00:21:15

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 100923 steps/s (collection: 0.835s, learning 0.139s)
             Mean action noise std: 2.50
          Mean value_function loss: 39.5978
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.2741
                       Mean reward: 864.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 172.3834
      Episode_Reward/object_height: 0.0692
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 0.97s
                      Time elapsed: 00:11:54
                               ETA: 00:21:14

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 85261 steps/s (collection: 0.966s, learning 0.187s)
             Mean action noise std: 2.50
          Mean value_function loss: 23.3324
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 18.2966
                       Mean reward: 849.69
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 0.7608
     Episode_Reward/lifting_object: 171.7586
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 1.15s
                      Time elapsed: 00:11:55
                               ETA: 00:21:13

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 98269 steps/s (collection: 0.888s, learning 0.112s)
             Mean action noise std: 2.51
          Mean value_function loss: 38.4057
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 18.3148
                       Mean reward: 852.85
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.7547
     Episode_Reward/lifting_object: 170.7130
      Episode_Reward/object_height: 0.0682
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 1.00s
                      Time elapsed: 00:11:56
                               ETA: 00:21:12

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 101668 steps/s (collection: 0.855s, learning 0.112s)
             Mean action noise std: 2.51
          Mean value_function loss: 28.6141
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.3254
                       Mean reward: 869.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 171.7478
      Episode_Reward/object_height: 0.0687
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 0.97s
                      Time elapsed: 00:11:57
                               ETA: 00:21:11

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 109695 steps/s (collection: 0.787s, learning 0.110s)
             Mean action noise std: 2.52
          Mean value_function loss: 35.5282
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.3456
                       Mean reward: 879.64
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 0.7678
     Episode_Reward/lifting_object: 172.9916
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 0.90s
                      Time elapsed: 00:11:58
                               ETA: 00:21:10

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 102988 steps/s (collection: 0.835s, learning 0.119s)
             Mean action noise std: 2.53
          Mean value_function loss: 31.2676
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.3653
                       Mean reward: 866.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 173.5238
      Episode_Reward/object_height: 0.0697
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 0.95s
                      Time elapsed: 00:11:59
                               ETA: 00:21:09

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 105601 steps/s (collection: 0.795s, learning 0.136s)
             Mean action noise std: 2.53
          Mean value_function loss: 33.9264
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.3906
                       Mean reward: 869.31
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 171.6642
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 0.93s
                      Time elapsed: 00:12:00
                               ETA: 00:21:08

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 103145 steps/s (collection: 0.815s, learning 0.138s)
             Mean action noise std: 2.54
          Mean value_function loss: 36.4637
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.4106
                       Mean reward: 884.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7772
     Episode_Reward/lifting_object: 174.5778
      Episode_Reward/object_height: 0.0703
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 0.95s
                      Time elapsed: 00:12:01
                               ETA: 00:21:07

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 98705 steps/s (collection: 0.850s, learning 0.146s)
             Mean action noise std: 2.54
          Mean value_function loss: 40.9026
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 18.4221
                       Mean reward: 875.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 172.8020
      Episode_Reward/object_height: 0.0696
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 1.00s
                      Time elapsed: 00:12:02
                               ETA: 00:21:06

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 99578 steps/s (collection: 0.821s, learning 0.166s)
             Mean action noise std: 2.54
          Mean value_function loss: 33.0171
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.4300
                       Mean reward: 858.35
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 172.2927
      Episode_Reward/object_height: 0.0694
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 0.99s
                      Time elapsed: 00:12:03
                               ETA: 00:21:05

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 97346 steps/s (collection: 0.867s, learning 0.143s)
             Mean action noise std: 2.55
          Mean value_function loss: 33.7310
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.4399
                       Mean reward: 873.60
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7738
     Episode_Reward/lifting_object: 174.3155
      Episode_Reward/object_height: 0.0701
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 1.01s
                      Time elapsed: 00:12:04
                               ETA: 00:21:04

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 98221 steps/s (collection: 0.822s, learning 0.179s)
             Mean action noise std: 2.55
          Mean value_function loss: 35.4999
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 18.4545
                       Mean reward: 861.41
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 171.2558
      Episode_Reward/object_height: 0.0688
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 1.00s
                      Time elapsed: 00:12:05
                               ETA: 00:21:03

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 105871 steps/s (collection: 0.818s, learning 0.111s)
             Mean action noise std: 2.55
          Mean value_function loss: 44.1908
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 18.4631
                       Mean reward: 875.92
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 172.2356
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 0.93s
                      Time elapsed: 00:12:06
                               ETA: 00:21:02

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 101183 steps/s (collection: 0.828s, learning 0.144s)
             Mean action noise std: 2.55
          Mean value_function loss: 32.1327
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 18.4650
                       Mean reward: 853.66
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 171.9617
      Episode_Reward/object_height: 0.0687
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 0.97s
                      Time elapsed: 00:12:07
                               ETA: 00:21:01

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 89798 steps/s (collection: 0.919s, learning 0.176s)
             Mean action noise std: 2.56
          Mean value_function loss: 42.5217
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.4702
                       Mean reward: 880.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 170.9359
      Episode_Reward/object_height: 0.0679
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 1.09s
                      Time elapsed: 00:12:08
                               ETA: 00:21:00

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 99531 steps/s (collection: 0.852s, learning 0.136s)
             Mean action noise std: 2.57
          Mean value_function loss: 33.0739
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 18.4872
                       Mean reward: 877.81
               Mean episode length: 249.87
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 174.5967
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 0.99s
                      Time elapsed: 00:12:09
                               ETA: 00:20:59

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 98421 steps/s (collection: 0.863s, learning 0.136s)
             Mean action noise std: 2.57
          Mean value_function loss: 40.6838
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 18.5076
                       Mean reward: 858.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 171.5555
      Episode_Reward/object_height: 0.0681
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 1.00s
                      Time elapsed: 00:12:10
                               ETA: 00:20:58

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 99799 steps/s (collection: 0.815s, learning 0.170s)
             Mean action noise std: 2.58
          Mean value_function loss: 35.5567
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.5198
                       Mean reward: 853.69
               Mean episode length: 248.88
    Episode_Reward/reaching_object: 0.7554
     Episode_Reward/lifting_object: 169.6740
      Episode_Reward/object_height: 0.0676
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 0.99s
                      Time elapsed: 00:12:11
                               ETA: 00:20:57

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 100239 steps/s (collection: 0.823s, learning 0.158s)
             Mean action noise std: 2.58
          Mean value_function loss: 27.3377
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 18.5330
                       Mean reward: 864.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 171.8621
      Episode_Reward/object_height: 0.0686
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 0.98s
                      Time elapsed: 00:12:12
                               ETA: 00:20:56

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 102334 steps/s (collection: 0.823s, learning 0.137s)
             Mean action noise std: 2.58
          Mean value_function loss: 38.7980
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.5371
                       Mean reward: 872.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 172.2330
      Episode_Reward/object_height: 0.0684
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 0.96s
                      Time elapsed: 00:12:13
                               ETA: 00:20:55

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 102569 steps/s (collection: 0.795s, learning 0.163s)
             Mean action noise std: 2.58
          Mean value_function loss: 27.0735
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.5460
                       Mean reward: 848.86
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 170.1146
      Episode_Reward/object_height: 0.0678
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 0.96s
                      Time elapsed: 00:12:14
                               ETA: 00:20:54

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 101466 steps/s (collection: 0.817s, learning 0.152s)
             Mean action noise std: 2.59
          Mean value_function loss: 26.0087
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 18.5563
                       Mean reward: 874.33
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 169.8999
      Episode_Reward/object_height: 0.0677
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 0.97s
                      Time elapsed: 00:12:15
                               ETA: 00:20:53

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 103329 steps/s (collection: 0.825s, learning 0.126s)
             Mean action noise std: 2.59
          Mean value_function loss: 28.9883
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 18.5589
                       Mean reward: 889.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 173.6236
      Episode_Reward/object_height: 0.0695
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 0.95s
                      Time elapsed: 00:12:16
                               ETA: 00:20:52

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 103385 steps/s (collection: 0.824s, learning 0.127s)
             Mean action noise std: 2.59
          Mean value_function loss: 27.0724
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 18.5635
                       Mean reward: 870.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 172.2635
      Episode_Reward/object_height: 0.0689
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 0.95s
                      Time elapsed: 00:12:17
                               ETA: 00:20:51

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 100686 steps/s (collection: 0.823s, learning 0.154s)
             Mean action noise std: 2.59
          Mean value_function loss: 29.3501
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.5742
                       Mean reward: 862.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 172.0449
      Episode_Reward/object_height: 0.0687
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 0.98s
                      Time elapsed: 00:12:18
                               ETA: 00:20:49

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 101351 steps/s (collection: 0.811s, learning 0.159s)
             Mean action noise std: 2.60
          Mean value_function loss: 35.1612
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.5887
                       Mean reward: 879.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7700
     Episode_Reward/lifting_object: 173.4100
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 0.97s
                      Time elapsed: 00:12:19
                               ETA: 00:20:48

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 101181 steps/s (collection: 0.840s, learning 0.132s)
             Mean action noise std: 2.60
          Mean value_function loss: 41.5139
               Mean surrogate loss: 0.0135
                 Mean entropy loss: 18.5991
                       Mean reward: 864.54
               Mean episode length: 246.30
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 174.0095
      Episode_Reward/object_height: 0.0693
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 0.97s
                      Time elapsed: 00:12:20
                               ETA: 00:20:47

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 103286 steps/s (collection: 0.837s, learning 0.115s)
             Mean action noise std: 2.60
          Mean value_function loss: 44.4678
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 18.6006
                       Mean reward: 869.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 172.1521
      Episode_Reward/object_height: 0.0684
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 0.95s
                      Time elapsed: 00:12:21
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 103327 steps/s (collection: 0.806s, learning 0.146s)
             Mean action noise std: 2.60
          Mean value_function loss: 55.3513
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 18.6065
                       Mean reward: 851.75
               Mean episode length: 247.70
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 171.4895
      Episode_Reward/object_height: 0.0681
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 0.95s
                      Time elapsed: 00:12:22
                               ETA: 00:20:45

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 99057 steps/s (collection: 0.816s, learning 0.177s)
             Mean action noise std: 2.61
          Mean value_function loss: 70.3436
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.6223
                       Mean reward: 874.96
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.7783
     Episode_Reward/lifting_object: 174.1814
      Episode_Reward/object_height: 0.0690
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 0.99s
                      Time elapsed: 00:12:23
                               ETA: 00:20:44

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 98456 steps/s (collection: 0.834s, learning 0.165s)
             Mean action noise std: 2.62
          Mean value_function loss: 67.6772
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 18.6431
                       Mean reward: 872.49
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 173.0591
      Episode_Reward/object_height: 0.0685
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 1.00s
                      Time elapsed: 00:12:24
                               ETA: 00:20:43

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 105861 steps/s (collection: 0.788s, learning 0.140s)
             Mean action noise std: 2.62
          Mean value_function loss: 61.1302
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 18.6518
                       Mean reward: 856.88
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 169.4786
      Episode_Reward/object_height: 0.0668
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 0.93s
                      Time elapsed: 00:12:25
                               ETA: 00:20:42

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 97643 steps/s (collection: 0.841s, learning 0.166s)
             Mean action noise std: 2.62
          Mean value_function loss: 53.1658
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.6605
                       Mean reward: 854.56
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 170.0042
      Episode_Reward/object_height: 0.0668
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 1.01s
                      Time elapsed: 00:12:26
                               ETA: 00:20:41

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 101246 steps/s (collection: 0.828s, learning 0.143s)
             Mean action noise std: 2.62
          Mean value_function loss: 50.3523
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 18.6680
                       Mean reward: 836.24
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 170.9766
      Episode_Reward/object_height: 0.0670
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 0.97s
                      Time elapsed: 00:12:27
                               ETA: 00:20:40

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 103978 steps/s (collection: 0.796s, learning 0.150s)
             Mean action noise std: 2.62
          Mean value_function loss: 49.6772
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 18.6708
                       Mean reward: 853.87
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 171.6849
      Episode_Reward/object_height: 0.0674
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 0.95s
                      Time elapsed: 00:12:27
                               ETA: 00:20:39

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 107754 steps/s (collection: 0.806s, learning 0.106s)
             Mean action noise std: 2.63
          Mean value_function loss: 50.0133
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 18.6736
                       Mean reward: 858.17
               Mean episode length: 247.91
    Episode_Reward/reaching_object: 0.7525
     Episode_Reward/lifting_object: 169.5172
      Episode_Reward/object_height: 0.0666
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 0.91s
                      Time elapsed: 00:12:28
                               ETA: 00:20:38

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 110293 steps/s (collection: 0.779s, learning 0.113s)
             Mean action noise std: 2.63
          Mean value_function loss: 55.3583
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 18.6807
                       Mean reward: 843.93
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.7477
     Episode_Reward/lifting_object: 167.7859
      Episode_Reward/object_height: 0.0658
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 0.89s
                      Time elapsed: 00:12:29
                               ETA: 00:20:37

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 101458 steps/s (collection: 0.805s, learning 0.164s)
             Mean action noise std: 2.63
          Mean value_function loss: 48.2427
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.6922
                       Mean reward: 812.70
               Mean episode length: 247.70
    Episode_Reward/reaching_object: 0.7251
     Episode_Reward/lifting_object: 164.7064
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 0.97s
                      Time elapsed: 00:12:30
                               ETA: 00:20:36

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 106692 steps/s (collection: 0.782s, learning 0.139s)
             Mean action noise std: 2.64
          Mean value_function loss: 51.1006
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.7005
                       Mean reward: 833.74
               Mean episode length: 246.83
    Episode_Reward/reaching_object: 0.7212
     Episode_Reward/lifting_object: 162.5089
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 0.92s
                      Time elapsed: 00:12:31
                               ETA: 00:20:35

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 101261 steps/s (collection: 0.844s, learning 0.127s)
             Mean action noise std: 2.65
          Mean value_function loss: 38.9847
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.7196
                       Mean reward: 846.64
               Mean episode length: 247.87
    Episode_Reward/reaching_object: 0.7379
     Episode_Reward/lifting_object: 165.8782
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 0.97s
                      Time elapsed: 00:12:32
                               ETA: 00:20:34

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 103554 steps/s (collection: 0.822s, learning 0.128s)
             Mean action noise std: 2.65
          Mean value_function loss: 37.9328
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 18.7435
                       Mean reward: 859.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7496
     Episode_Reward/lifting_object: 168.1470
      Episode_Reward/object_height: 0.0665
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 0.95s
                      Time elapsed: 00:12:33
                               ETA: 00:20:33

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 102482 steps/s (collection: 0.828s, learning 0.132s)
             Mean action noise std: 2.65
          Mean value_function loss: 35.9091
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 18.7476
                       Mean reward: 867.42
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 171.8668
      Episode_Reward/object_height: 0.0681
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 0.96s
                      Time elapsed: 00:12:34
                               ETA: 00:20:32

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 111032 steps/s (collection: 0.770s, learning 0.115s)
             Mean action noise std: 2.65
          Mean value_function loss: 31.2092
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.7569
                       Mean reward: 870.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 171.1184
      Episode_Reward/object_height: 0.0677
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 0.89s
                      Time elapsed: 00:12:35
                               ETA: 00:20:30

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 104261 steps/s (collection: 0.808s, learning 0.135s)
             Mean action noise std: 2.66
          Mean value_function loss: 36.5691
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.7627
                       Mean reward: 852.49
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 171.6020
      Episode_Reward/object_height: 0.0680
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 0.94s
                      Time elapsed: 00:12:36
                               ETA: 00:20:29

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 108788 steps/s (collection: 0.793s, learning 0.111s)
             Mean action noise std: 2.66
          Mean value_function loss: 36.6591
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.7725
                       Mean reward: 845.56
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 0.7638
     Episode_Reward/lifting_object: 171.3758
      Episode_Reward/object_height: 0.0679
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 0.90s
                      Time elapsed: 00:12:37
                               ETA: 00:20:28

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 98662 steps/s (collection: 0.892s, learning 0.105s)
             Mean action noise std: 2.67
          Mean value_function loss: 35.5380
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 18.7887
                       Mean reward: 852.55
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 172.6353
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 1.00s
                      Time elapsed: 00:12:38
                               ETA: 00:20:27

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 101471 steps/s (collection: 0.788s, learning 0.181s)
             Mean action noise std: 2.67
          Mean value_function loss: 33.4109
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 18.8034
                       Mean reward: 862.09
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.7614
     Episode_Reward/lifting_object: 172.2131
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 0.97s
                      Time elapsed: 00:12:39
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 101011 steps/s (collection: 0.858s, learning 0.115s)
             Mean action noise std: 2.67
          Mean value_function loss: 37.2715
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 18.8060
                       Mean reward: 874.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 171.9810
      Episode_Reward/object_height: 0.0682
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 0.97s
                      Time elapsed: 00:12:40
                               ETA: 00:20:25

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 87953 steps/s (collection: 0.936s, learning 0.182s)
             Mean action noise std: 2.68
          Mean value_function loss: 45.8188
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 18.8145
                       Mean reward: 872.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7663
     Episode_Reward/lifting_object: 172.6352
      Episode_Reward/object_height: 0.0682
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 1.12s
                      Time elapsed: 00:12:41
                               ETA: 00:20:24

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 98307 steps/s (collection: 0.873s, learning 0.127s)
             Mean action noise std: 2.68
          Mean value_function loss: 34.4869
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 18.8355
                       Mean reward: 844.14
               Mean episode length: 243.78
    Episode_Reward/reaching_object: 0.7549
     Episode_Reward/lifting_object: 169.7038
      Episode_Reward/object_height: 0.0669
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 1.00s
                      Time elapsed: 00:12:42
                               ETA: 00:20:23

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 97887 steps/s (collection: 0.826s, learning 0.178s)
             Mean action noise std: 2.69
          Mean value_function loss: 51.2162
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 18.8571
                       Mean reward: 869.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 173.2856
      Episode_Reward/object_height: 0.0684
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 1.00s
                      Time elapsed: 00:12:43
                               ETA: 00:20:22

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 99672 steps/s (collection: 0.852s, learning 0.134s)
             Mean action noise std: 2.69
          Mean value_function loss: 39.5558
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 18.8730
                       Mean reward: 855.40
               Mean episode length: 246.37
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 171.6799
      Episode_Reward/object_height: 0.0676
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 0.99s
                      Time elapsed: 00:12:44
                               ETA: 00:20:21

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 98968 steps/s (collection: 0.871s, learning 0.122s)
             Mean action noise std: 2.70
          Mean value_function loss: 39.4285
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 18.8797
                       Mean reward: 862.18
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 171.5151
      Episode_Reward/object_height: 0.0674
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 0.99s
                      Time elapsed: 00:12:45
                               ETA: 00:20:20

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 97314 steps/s (collection: 0.862s, learning 0.148s)
             Mean action noise std: 2.70
          Mean value_function loss: 41.0751
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.8911
                       Mean reward: 861.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 173.1096
      Episode_Reward/object_height: 0.0680
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 1.01s
                      Time elapsed: 00:12:46
                               ETA: 00:20:19

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 99237 steps/s (collection: 0.820s, learning 0.171s)
             Mean action noise std: 2.70
          Mean value_function loss: 39.5236
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 18.9041
                       Mean reward: 874.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7706
     Episode_Reward/lifting_object: 173.3647
      Episode_Reward/object_height: 0.0682
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 0.99s
                      Time elapsed: 00:12:47
                               ETA: 00:20:18

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 101907 steps/s (collection: 0.852s, learning 0.113s)
             Mean action noise std: 2.71
          Mean value_function loss: 37.7228
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 18.9222
                       Mean reward: 841.02
               Mean episode length: 246.86
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 172.3008
      Episode_Reward/object_height: 0.0676
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 0.96s
                      Time elapsed: 00:12:48
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 100929 steps/s (collection: 0.845s, learning 0.129s)
             Mean action noise std: 2.72
          Mean value_function loss: 34.5570
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 18.9442
                       Mean reward: 885.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 172.4259
      Episode_Reward/object_height: 0.0676
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 0.97s
                      Time elapsed: 00:12:49
                               ETA: 00:20:16

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 109784 steps/s (collection: 0.787s, learning 0.108s)
             Mean action noise std: 2.73
          Mean value_function loss: 39.2418
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 18.9657
                       Mean reward: 859.53
               Mean episode length: 249.34
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 172.6783
      Episode_Reward/object_height: 0.0676
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 0.90s
                      Time elapsed: 00:12:50
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 113083 steps/s (collection: 0.755s, learning 0.114s)
             Mean action noise std: 2.73
          Mean value_function loss: 44.7049
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 18.9884
                       Mean reward: 839.97
               Mean episode length: 247.68
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 171.8075
      Episode_Reward/object_height: 0.0673
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 0.87s
                      Time elapsed: 00:12:51
                               ETA: 00:20:14

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 101713 steps/s (collection: 0.772s, learning 0.195s)
             Mean action noise std: 2.74
          Mean value_function loss: 30.6406
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.0075
                       Mean reward: 845.33
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 170.5846
      Episode_Reward/object_height: 0.0667
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 0.97s
                      Time elapsed: 00:12:51
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 106896 steps/s (collection: 0.765s, learning 0.154s)
             Mean action noise std: 2.75
          Mean value_function loss: 33.7839
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 19.0354
                       Mean reward: 855.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 170.8928
      Episode_Reward/object_height: 0.0669
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 0.92s
                      Time elapsed: 00:12:52
                               ETA: 00:20:12

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 109722 steps/s (collection: 0.781s, learning 0.115s)
             Mean action noise std: 2.75
          Mean value_function loss: 32.1602
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 19.0525
                       Mean reward: 867.69
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 171.4747
      Episode_Reward/object_height: 0.0670
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 0.90s
                      Time elapsed: 00:12:53
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 97313 steps/s (collection: 0.863s, learning 0.147s)
             Mean action noise std: 2.76
          Mean value_function loss: 33.1028
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.0662
                       Mean reward: 846.66
               Mean episode length: 247.15
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 169.8315
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 1.01s
                      Time elapsed: 00:12:54
                               ETA: 00:20:10

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 99050 steps/s (collection: 0.809s, learning 0.184s)
             Mean action noise std: 2.77
          Mean value_function loss: 20.8907
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.0820
                       Mean reward: 872.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7748
     Episode_Reward/lifting_object: 174.6365
      Episode_Reward/object_height: 0.0683
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 0.99s
                      Time elapsed: 00:12:55
                               ETA: 00:20:09

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 81379 steps/s (collection: 1.024s, learning 0.184s)
             Mean action noise std: 2.77
          Mean value_function loss: 34.0096
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.1013
                       Mean reward: 876.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7678
     Episode_Reward/lifting_object: 173.2441
      Episode_Reward/object_height: 0.0677
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 1.21s
                      Time elapsed: 00:12:57
                               ETA: 00:20:08

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 103253 steps/s (collection: 0.766s, learning 0.186s)
             Mean action noise std: 2.78
          Mean value_function loss: 41.7841
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.1147
                       Mean reward: 881.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 172.0947
      Episode_Reward/object_height: 0.0669
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 0.95s
                      Time elapsed: 00:12:57
                               ETA: 00:20:07

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 106467 steps/s (collection: 0.771s, learning 0.152s)
             Mean action noise std: 2.78
          Mean value_function loss: 42.5445
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 19.1332
                       Mean reward: 817.28
               Mean episode length: 241.14
    Episode_Reward/reaching_object: 0.7571
     Episode_Reward/lifting_object: 169.4391
      Episode_Reward/object_height: 0.0659
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 0.92s
                      Time elapsed: 00:12:58
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 109182 steps/s (collection: 0.765s, learning 0.136s)
             Mean action noise std: 2.79
          Mean value_function loss: 32.3682
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.1517
                       Mean reward: 853.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 171.4857
      Episode_Reward/object_height: 0.0668
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 0.90s
                      Time elapsed: 00:12:59
                               ETA: 00:20:05

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 105040 steps/s (collection: 0.784s, learning 0.152s)
             Mean action noise std: 2.80
          Mean value_function loss: 34.2972
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.1676
                       Mean reward: 871.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 173.6047
      Episode_Reward/object_height: 0.0677
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 0.94s
                      Time elapsed: 00:13:00
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 102045 steps/s (collection: 0.809s, learning 0.154s)
             Mean action noise std: 2.80
          Mean value_function loss: 36.6367
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.1851
                       Mean reward: 857.95
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.7555
     Episode_Reward/lifting_object: 170.0719
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 0.96s
                      Time elapsed: 00:13:01
                               ETA: 00:20:03

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 101423 steps/s (collection: 0.817s, learning 0.153s)
             Mean action noise std: 2.81
          Mean value_function loss: 34.3784
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 19.1934
                       Mean reward: 852.73
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 172.3005
      Episode_Reward/object_height: 0.0668
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 0.97s
                      Time elapsed: 00:13:02
                               ETA: 00:20:02

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 100119 steps/s (collection: 0.861s, learning 0.121s)
             Mean action noise std: 2.81
          Mean value_function loss: 24.8317
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.2081
                       Mean reward: 884.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 172.8853
      Episode_Reward/object_height: 0.0672
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 0.98s
                      Time elapsed: 00:13:03
                               ETA: 00:20:01

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 88719 steps/s (collection: 0.921s, learning 0.188s)
             Mean action noise std: 2.81
          Mean value_function loss: 29.5237
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.2187
                       Mean reward: 867.72
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 171.7573
      Episode_Reward/object_height: 0.0669
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 1.11s
                      Time elapsed: 00:13:04
                               ETA: 00:20:00

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 83670 steps/s (collection: 1.020s, learning 0.155s)
             Mean action noise std: 2.82
          Mean value_function loss: 26.3655
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.2292
                       Mean reward: 869.62
               Mean episode length: 249.64
    Episode_Reward/reaching_object: 0.7618
     Episode_Reward/lifting_object: 172.8695
      Episode_Reward/object_height: 0.0676
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 1.17s
                      Time elapsed: 00:13:05
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 96003 steps/s (collection: 0.869s, learning 0.155s)
             Mean action noise std: 2.82
          Mean value_function loss: 29.5866
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.2414
                       Mean reward: 872.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 172.3502
      Episode_Reward/object_height: 0.0676
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 1.02s
                      Time elapsed: 00:13:06
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 105566 steps/s (collection: 0.784s, learning 0.147s)
             Mean action noise std: 2.83
          Mean value_function loss: 27.5671
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 19.2607
                       Mean reward: 855.77
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 171.5622
      Episode_Reward/object_height: 0.0672
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 0.93s
                      Time elapsed: 00:13:07
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 103490 steps/s (collection: 0.795s, learning 0.155s)
             Mean action noise std: 2.83
          Mean value_function loss: 29.1473
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 19.2810
                       Mean reward: 866.82
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 173.2082
      Episode_Reward/object_height: 0.0679
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 0.95s
                      Time elapsed: 00:13:08
                               ETA: 00:19:56

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 103592 steps/s (collection: 0.811s, learning 0.138s)
             Mean action noise std: 2.84
          Mean value_function loss: 32.7720
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 19.2878
                       Mean reward: 867.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7567
     Episode_Reward/lifting_object: 171.7742
      Episode_Reward/object_height: 0.0675
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 0.95s
                      Time elapsed: 00:13:09
                               ETA: 00:19:55

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 109978 steps/s (collection: 0.776s, learning 0.118s)
             Mean action noise std: 2.85
          Mean value_function loss: 29.4067
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.3050
                       Mean reward: 865.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 172.6053
      Episode_Reward/object_height: 0.0678
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 0.89s
                      Time elapsed: 00:13:10
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 102573 steps/s (collection: 0.830s, learning 0.129s)
             Mean action noise std: 2.85
          Mean value_function loss: 28.2892
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 19.3248
                       Mean reward: 844.26
               Mean episode length: 246.33
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 173.0866
      Episode_Reward/object_height: 0.0676
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 0.96s
                      Time elapsed: 00:13:11
                               ETA: 00:19:53

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 101341 steps/s (collection: 0.825s, learning 0.145s)
             Mean action noise std: 2.85
          Mean value_function loss: 35.5714
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 19.3359
                       Mean reward: 857.08
               Mean episode length: 246.73
    Episode_Reward/reaching_object: 0.7754
     Episode_Reward/lifting_object: 173.6128
      Episode_Reward/object_height: 0.0677
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 0.97s
                      Time elapsed: 00:13:12
                               ETA: 00:19:52

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 101096 steps/s (collection: 0.829s, learning 0.143s)
             Mean action noise std: 2.86
          Mean value_function loss: 27.7531
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.3516
                       Mean reward: 865.17
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 172.3075
      Episode_Reward/object_height: 0.0673
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 0.97s
                      Time elapsed: 00:13:13
                               ETA: 00:19:51

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 107167 steps/s (collection: 0.803s, learning 0.114s)
             Mean action noise std: 2.86
          Mean value_function loss: 22.3141
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 19.3621
                       Mean reward: 880.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7743
     Episode_Reward/lifting_object: 175.1115
      Episode_Reward/object_height: 0.0685
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 0.92s
                      Time elapsed: 00:13:14
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 108511 steps/s (collection: 0.808s, learning 0.098s)
             Mean action noise std: 2.87
          Mean value_function loss: 45.2909
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 19.3796
                       Mean reward: 883.49
               Mean episode length: 249.87
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 173.2263
      Episode_Reward/object_height: 0.0677
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 0.91s
                      Time elapsed: 00:13:15
                               ETA: 00:19:49

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 104879 steps/s (collection: 0.799s, learning 0.139s)
             Mean action noise std: 2.87
          Mean value_function loss: 41.3063
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.3984
                       Mean reward: 868.06
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 172.2300
      Episode_Reward/object_height: 0.0671
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 0.94s
                      Time elapsed: 00:13:16
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 103988 steps/s (collection: 0.822s, learning 0.123s)
             Mean action noise std: 2.88
          Mean value_function loss: 28.2771
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.4069
                       Mean reward: 868.28
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 171.6553
      Episode_Reward/object_height: 0.0669
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 0.95s
                      Time elapsed: 00:13:17
                               ETA: 00:19:46

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 102532 steps/s (collection: 0.804s, learning 0.155s)
             Mean action noise std: 2.88
          Mean value_function loss: 31.1087
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 19.4145
                       Mean reward: 854.39
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 171.7868
      Episode_Reward/object_height: 0.0669
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 0.96s
                      Time elapsed: 00:13:18
                               ETA: 00:19:45

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 104938 steps/s (collection: 0.801s, learning 0.136s)
             Mean action noise std: 2.88
          Mean value_function loss: 32.9569
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 19.4193
                       Mean reward: 853.27
               Mean episode length: 244.94
    Episode_Reward/reaching_object: 0.7664
     Episode_Reward/lifting_object: 171.9540
      Episode_Reward/object_height: 0.0667
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 0.94s
                      Time elapsed: 00:13:19
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 104036 steps/s (collection: 0.782s, learning 0.163s)
             Mean action noise std: 2.89
          Mean value_function loss: 38.4613
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.4262
                       Mean reward: 858.65
               Mean episode length: 247.22
    Episode_Reward/reaching_object: 0.7590
     Episode_Reward/lifting_object: 170.7167
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 0.94s
                      Time elapsed: 00:13:20
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 104977 steps/s (collection: 0.797s, learning 0.140s)
             Mean action noise std: 2.89
          Mean value_function loss: 40.1627
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 19.4411
                       Mean reward: 861.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 172.2000
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 0.94s
                      Time elapsed: 00:13:21
                               ETA: 00:19:42

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 104302 steps/s (collection: 0.814s, learning 0.129s)
             Mean action noise std: 2.90
          Mean value_function loss: 44.4644
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.4601
                       Mean reward: 870.43
               Mean episode length: 249.89
    Episode_Reward/reaching_object: 0.7684
     Episode_Reward/lifting_object: 172.5082
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 0.94s
                      Time elapsed: 00:13:21
                               ETA: 00:19:41

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 100221 steps/s (collection: 0.840s, learning 0.141s)
             Mean action noise std: 2.90
          Mean value_function loss: 45.9629
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.4748
                       Mean reward: 871.96
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.7720
     Episode_Reward/lifting_object: 173.6287
      Episode_Reward/object_height: 0.0669
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.4167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 0.98s
                      Time elapsed: 00:13:22
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 96307 steps/s (collection: 0.827s, learning 0.194s)
             Mean action noise std: 2.91
          Mean value_function loss: 36.2647
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.4891
                       Mean reward: 868.56
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 172.4551
      Episode_Reward/object_height: 0.0666
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 1.02s
                      Time elapsed: 00:13:23
                               ETA: 00:19:39

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 84818 steps/s (collection: 1.011s, learning 0.148s)
             Mean action noise std: 2.92
          Mean value_function loss: 39.3439
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 19.5062
                       Mean reward: 864.37
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 171.6855
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 1.16s
                      Time elapsed: 00:13:25
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 93897 steps/s (collection: 0.896s, learning 0.151s)
             Mean action noise std: 2.92
          Mean value_function loss: 32.7153
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 19.5288
                       Mean reward: 877.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 169.2850
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 1.05s
                      Time elapsed: 00:13:26
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 93606 steps/s (collection: 0.926s, learning 0.124s)
             Mean action noise std: 2.93
          Mean value_function loss: 31.1588
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 19.5476
                       Mean reward: 866.40
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 171.6998
      Episode_Reward/object_height: 0.0658
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 1.05s
                      Time elapsed: 00:13:27
                               ETA: 00:19:37

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 102608 steps/s (collection: 0.844s, learning 0.115s)
             Mean action noise std: 2.93
          Mean value_function loss: 31.3730
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 19.5684
                       Mean reward: 870.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7664
     Episode_Reward/lifting_object: 173.9039
      Episode_Reward/object_height: 0.0667
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 0.96s
                      Time elapsed: 00:13:28
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 96576 steps/s (collection: 0.864s, learning 0.154s)
             Mean action noise std: 2.94
          Mean value_function loss: 28.4560
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.5815
                       Mean reward: 871.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7478
     Episode_Reward/lifting_object: 170.7548
      Episode_Reward/object_height: 0.0658
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 1.02s
                      Time elapsed: 00:13:29
                               ETA: 00:19:35

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 105115 steps/s (collection: 0.825s, learning 0.110s)
             Mean action noise std: 2.94
          Mean value_function loss: 32.1124
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 19.5950
                       Mean reward: 872.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7529
     Episode_Reward/lifting_object: 171.7499
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 0.94s
                      Time elapsed: 00:13:30
                               ETA: 00:19:34

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 96201 steps/s (collection: 0.864s, learning 0.158s)
             Mean action noise std: 2.95
          Mean value_function loss: 33.6365
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.6103
                       Mean reward: 861.76
               Mean episode length: 249.94
    Episode_Reward/reaching_object: 0.7472
     Episode_Reward/lifting_object: 168.2402
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 1.02s
                      Time elapsed: 00:13:31
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 109032 steps/s (collection: 0.815s, learning 0.087s)
             Mean action noise std: 2.95
          Mean value_function loss: 30.3406
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 19.6187
                       Mean reward: 880.32
               Mean episode length: 249.25
    Episode_Reward/reaching_object: 0.7537
     Episode_Reward/lifting_object: 170.5755
      Episode_Reward/object_height: 0.0656
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 0.90s
                      Time elapsed: 00:13:32
                               ETA: 00:19:32

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 114062 steps/s (collection: 0.756s, learning 0.106s)
             Mean action noise std: 2.95
          Mean value_function loss: 29.1643
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 19.6239
                       Mean reward: 852.70
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 172.6128
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 0.86s
                      Time elapsed: 00:13:32
                               ETA: 00:19:30

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 97720 steps/s (collection: 0.904s, learning 0.102s)
             Mean action noise std: 2.96
          Mean value_function loss: 35.1501
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.6324
                       Mean reward: 871.80
               Mean episode length: 248.64
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 172.0832
      Episode_Reward/object_height: 0.0658
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 1.01s
                      Time elapsed: 00:13:33
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 96527 steps/s (collection: 0.788s, learning 0.231s)
             Mean action noise std: 2.96
          Mean value_function loss: 34.6687
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 19.6459
                       Mean reward: 872.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7762
     Episode_Reward/lifting_object: 173.7918
      Episode_Reward/object_height: 0.0664
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 1.02s
                      Time elapsed: 00:13:34
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 84627 steps/s (collection: 0.983s, learning 0.178s)
             Mean action noise std: 2.97
          Mean value_function loss: 39.0010
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 19.6572
                       Mean reward: 865.94
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.7711
     Episode_Reward/lifting_object: 172.4697
      Episode_Reward/object_height: 0.0659
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 1.16s
                      Time elapsed: 00:13:36
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 72010 steps/s (collection: 1.109s, learning 0.256s)
             Mean action noise std: 2.97
          Mean value_function loss: 27.7520
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.6600
                       Mean reward: 869.98
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 170.0930
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 1.37s
                      Time elapsed: 00:13:37
                               ETA: 00:19:27

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 70452 steps/s (collection: 1.207s, learning 0.188s)
             Mean action noise std: 2.97
          Mean value_function loss: 34.3052
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 19.6696
                       Mean reward: 873.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7786
     Episode_Reward/lifting_object: 173.1163
      Episode_Reward/object_height: 0.0661
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 1.40s
                      Time elapsed: 00:13:38
                               ETA: 00:19:27

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 78270 steps/s (collection: 1.039s, learning 0.217s)
             Mean action noise std: 2.98
          Mean value_function loss: 35.5926
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 19.6899
                       Mean reward: 857.41
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 169.7523
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 1.26s
                      Time elapsed: 00:13:40
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 66360 steps/s (collection: 1.264s, learning 0.218s)
             Mean action noise std: 2.99
          Mean value_function loss: 37.6995
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 19.7099
                       Mean reward: 858.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7799
     Episode_Reward/lifting_object: 173.6188
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 1.48s
                      Time elapsed: 00:13:41
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 78428 steps/s (collection: 1.064s, learning 0.189s)
             Mean action noise std: 3.00
          Mean value_function loss: 28.8335
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 19.7288
                       Mean reward: 862.33
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: 172.7896
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 1.25s
                      Time elapsed: 00:13:42
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 84367 steps/s (collection: 0.973s, learning 0.192s)
             Mean action noise std: 3.00
          Mean value_function loss: 30.8885
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.7417
                       Mean reward: 872.17
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7788
     Episode_Reward/lifting_object: 174.1818
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 1.17s
                      Time elapsed: 00:13:44
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 87934 steps/s (collection: 0.928s, learning 0.190s)
             Mean action noise std: 3.01
          Mean value_function loss: 27.1851
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 19.7523
                       Mean reward: 867.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7782
     Episode_Reward/lifting_object: 173.3454
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 1.12s
                      Time elapsed: 00:13:45
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 102474 steps/s (collection: 0.816s, learning 0.144s)
             Mean action noise std: 3.01
          Mean value_function loss: 37.3915
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 19.7633
                       Mean reward: 856.13
               Mean episode length: 247.57
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 172.9119
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 0.96s
                      Time elapsed: 00:13:46
                               ETA: 00:19:23

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 105864 steps/s (collection: 0.829s, learning 0.100s)
             Mean action noise std: 3.01
          Mean value_function loss: 34.6275
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 19.7742
                       Mean reward: 857.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7693
     Episode_Reward/lifting_object: 171.9689
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 0.93s
                      Time elapsed: 00:13:47
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 104882 steps/s (collection: 0.813s, learning 0.124s)
             Mean action noise std: 3.02
          Mean value_function loss: 38.4863
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 19.7824
                       Mean reward: 860.32
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.7806
     Episode_Reward/lifting_object: 173.3006
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 0.94s
                      Time elapsed: 00:13:47
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 96503 steps/s (collection: 0.822s, learning 0.197s)
             Mean action noise std: 3.02
          Mean value_function loss: 38.5167
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.7949
                       Mean reward: 878.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: 172.1584
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 1.02s
                      Time elapsed: 00:13:49
                               ETA: 00:19:20

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 94046 steps/s (collection: 0.875s, learning 0.170s)
             Mean action noise std: 3.03
          Mean value_function loss: 36.2229
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 19.8116
                       Mean reward: 863.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 172.6937
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 1.05s
                      Time elapsed: 00:13:50
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 90895 steps/s (collection: 0.901s, learning 0.181s)
             Mean action noise std: 3.03
          Mean value_function loss: 34.9517
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 19.8210
                       Mean reward: 880.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: 173.7282
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 1.08s
                      Time elapsed: 00:13:51
                               ETA: 00:19:18

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 93270 steps/s (collection: 0.913s, learning 0.141s)
             Mean action noise std: 3.04
          Mean value_function loss: 29.5561
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 19.8384
                       Mean reward: 849.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 170.4372
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 1.05s
                      Time elapsed: 00:13:52
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 98760 steps/s (collection: 0.884s, learning 0.111s)
             Mean action noise std: 3.04
          Mean value_function loss: 42.5754
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.8538
                       Mean reward: 879.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7797
     Episode_Reward/lifting_object: 174.6738
      Episode_Reward/object_height: 0.0659
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 1.00s
                      Time elapsed: 00:13:53
                               ETA: 00:19:16

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 101687 steps/s (collection: 0.842s, learning 0.125s)
             Mean action noise std: 3.05
          Mean value_function loss: 35.4246
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 19.8663
                       Mean reward: 872.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 171.6209
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 0.97s
                      Time elapsed: 00:13:54
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 94638 steps/s (collection: 0.876s, learning 0.163s)
             Mean action noise std: 3.05
          Mean value_function loss: 44.9472
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 19.8808
                       Mean reward: 864.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 172.0632
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 1.04s
                      Time elapsed: 00:13:55
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 100614 steps/s (collection: 0.871s, learning 0.107s)
             Mean action noise std: 3.06
          Mean value_function loss: 39.5949
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 19.8927
                       Mean reward: 875.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 170.8676
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 0.98s
                      Time elapsed: 00:13:56
                               ETA: 00:19:13

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 101012 steps/s (collection: 0.849s, learning 0.124s)
             Mean action noise std: 3.06
          Mean value_function loss: 38.6595
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 19.8997
                       Mean reward: 875.55
               Mean episode length: 249.61
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 170.7648
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 0.97s
                      Time elapsed: 00:13:57
                               ETA: 00:19:12

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 93797 steps/s (collection: 0.895s, learning 0.153s)
             Mean action noise std: 3.06
          Mean value_function loss: 28.6517
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 19.9091
                       Mean reward: 878.45
               Mean episode length: 248.62
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 171.8102
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 1.05s
                      Time elapsed: 00:13:58
                               ETA: 00:19:11

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 106915 steps/s (collection: 0.800s, learning 0.120s)
             Mean action noise std: 3.07
          Mean value_function loss: 35.5004
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 19.9272
                       Mean reward: 861.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 171.4094
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 0.92s
                      Time elapsed: 00:13:59
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 100333 steps/s (collection: 0.853s, learning 0.127s)
             Mean action noise std: 3.08
          Mean value_function loss: 28.0114
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 19.9435
                       Mean reward: 883.90
               Mean episode length: 249.34
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 172.6067
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 0.98s
                      Time elapsed: 00:14:00
                               ETA: 00:19:09

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 102628 steps/s (collection: 0.817s, learning 0.141s)
             Mean action noise std: 3.08
          Mean value_function loss: 44.3500
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 19.9584
                       Mean reward: 868.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 172.7640
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 0.96s
                      Time elapsed: 00:14:01
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 106522 steps/s (collection: 0.807s, learning 0.116s)
             Mean action noise std: 3.09
          Mean value_function loss: 48.0612
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 19.9722
                       Mean reward: 871.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7716
     Episode_Reward/lifting_object: 172.4124
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 0.92s
                      Time elapsed: 00:14:01
                               ETA: 00:19:07

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 105116 steps/s (collection: 0.820s, learning 0.115s)
             Mean action noise std: 3.10
          Mean value_function loss: 33.7693
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 19.9907
                       Mean reward: 869.75
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 171.3945
      Episode_Reward/object_height: 0.0647
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 0.94s
                      Time elapsed: 00:14:02
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 103267 steps/s (collection: 0.815s, learning 0.137s)
             Mean action noise std: 3.10
          Mean value_function loss: 36.6805
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 20.0009
                       Mean reward: 875.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 173.1304
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 0.95s
                      Time elapsed: 00:14:03
                               ETA: 00:19:05

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 102158 steps/s (collection: 0.869s, learning 0.094s)
             Mean action noise std: 3.11
          Mean value_function loss: 40.5680
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.0204
                       Mean reward: 871.53
               Mean episode length: 248.87
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 172.0680
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 0.96s
                      Time elapsed: 00:14:04
                               ETA: 00:19:03

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 99724 steps/s (collection: 0.826s, learning 0.160s)
             Mean action noise std: 3.12
          Mean value_function loss: 41.8024
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 20.0424
                       Mean reward: 870.67
               Mean episode length: 248.85
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 172.9919
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 0.99s
                      Time elapsed: 00:14:05
                               ETA: 00:19:02

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 101892 steps/s (collection: 0.841s, learning 0.124s)
             Mean action noise std: 3.12
          Mean value_function loss: 34.8318
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 20.0612
                       Mean reward: 865.10
               Mean episode length: 249.11
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 172.3248
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.1667
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 0.96s
                      Time elapsed: 00:14:06
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 87618 steps/s (collection: 0.965s, learning 0.157s)
             Mean action noise std: 3.13
          Mean value_function loss: 34.9174
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 20.0773
                       Mean reward: 853.44
               Mean episode length: 249.08
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 172.1242
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 1.12s
                      Time elapsed: 00:14:07
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 86712 steps/s (collection: 0.936s, learning 0.198s)
             Mean action noise std: 3.14
          Mean value_function loss: 38.2757
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 20.0968
                       Mean reward: 876.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 172.7869
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 1.13s
                      Time elapsed: 00:14:09
                               ETA: 00:19:00

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 90929 steps/s (collection: 0.886s, learning 0.195s)
             Mean action noise std: 3.14
          Mean value_function loss: 35.3445
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 20.1084
                       Mean reward: 866.77
               Mean episode length: 248.98
    Episode_Reward/reaching_object: 0.7715
     Episode_Reward/lifting_object: 172.9064
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 1.08s
                      Time elapsed: 00:14:10
                               ETA: 00:18:59

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 91109 steps/s (collection: 0.900s, learning 0.179s)
             Mean action noise std: 3.15
          Mean value_function loss: 42.5255
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 20.1283
                       Mean reward: 847.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 171.1071
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 1.08s
                      Time elapsed: 00:14:11
                               ETA: 00:18:58

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 94282 steps/s (collection: 0.915s, learning 0.128s)
             Mean action noise std: 3.16
          Mean value_function loss: 40.9345
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.1451
                       Mean reward: 850.10
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 172.5930
      Episode_Reward/object_height: 0.0656
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 1.04s
                      Time elapsed: 00:14:12
                               ETA: 00:18:57

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 92051 steps/s (collection: 0.885s, learning 0.183s)
             Mean action noise std: 3.16
          Mean value_function loss: 46.9980
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 20.1608
                       Mean reward: 874.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 171.8721
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 1.07s
                      Time elapsed: 00:14:13
                               ETA: 00:18:56

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 100647 steps/s (collection: 0.871s, learning 0.106s)
             Mean action noise std: 3.17
          Mean value_function loss: 35.5605
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.1746
                       Mean reward: 863.38
               Mean episode length: 248.64
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 170.6983
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 0.98s
                      Time elapsed: 00:14:14
                               ETA: 00:18:55

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 85693 steps/s (collection: 0.954s, learning 0.193s)
             Mean action noise std: 3.17
          Mean value_function loss: 34.2107
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 20.1878
                       Mean reward: 863.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 171.6884
      Episode_Reward/object_height: 0.0659
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 1.15s
                      Time elapsed: 00:14:15
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 86186 steps/s (collection: 0.972s, learning 0.168s)
             Mean action noise std: 3.18
          Mean value_function loss: 37.3785
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.1938
                       Mean reward: 847.83
               Mean episode length: 247.58
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 170.1382
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 1.14s
                      Time elapsed: 00:14:16
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 80905 steps/s (collection: 1.082s, learning 0.133s)
             Mean action noise std: 3.18
          Mean value_function loss: 29.4924
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.2068
                       Mean reward: 871.65
               Mean episode length: 249.29
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 170.9991
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 1.22s
                      Time elapsed: 00:14:17
                               ETA: 00:18:53

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 88833 steps/s (collection: 0.914s, learning 0.193s)
             Mean action noise std: 3.19
          Mean value_function loss: 27.7539
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 20.2222
                       Mean reward: 870.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 171.1395
      Episode_Reward/object_height: 0.0660
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 1.11s
                      Time elapsed: 00:14:18
                               ETA: 00:18:52

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 80122 steps/s (collection: 1.033s, learning 0.194s)
             Mean action noise std: 3.19
          Mean value_function loss: 36.7345
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 20.2288
                       Mean reward: 874.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 169.4981
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 1.23s
                      Time elapsed: 00:14:20
                               ETA: 00:18:51

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 84320 steps/s (collection: 0.963s, learning 0.203s)
             Mean action noise std: 3.19
          Mean value_function loss: 30.8796
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 20.2400
                       Mean reward: 857.57
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 172.3234
      Episode_Reward/object_height: 0.0663
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 1.17s
                      Time elapsed: 00:14:21
                               ETA: 00:18:51

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 79164 steps/s (collection: 1.032s, learning 0.210s)
             Mean action noise std: 3.20
          Mean value_function loss: 42.9033
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 20.2519
                       Mean reward: 881.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7702
     Episode_Reward/lifting_object: 173.7995
      Episode_Reward/object_height: 0.0669
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 1.24s
                      Time elapsed: 00:14:22
                               ETA: 00:18:50

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 71031 steps/s (collection: 1.104s, learning 0.280s)
             Mean action noise std: 3.20
          Mean value_function loss: 35.0114
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.2616
                       Mean reward: 863.09
               Mean episode length: 246.22
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 172.1364
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 1.38s
                      Time elapsed: 00:14:23
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 74932 steps/s (collection: 1.174s, learning 0.138s)
             Mean action noise std: 3.21
          Mean value_function loss: 43.8365
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 20.2840
                       Mean reward: 867.30
               Mean episode length: 248.93
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 170.4935
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 1.31s
                      Time elapsed: 00:14:25
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 76762 steps/s (collection: 1.153s, learning 0.128s)
             Mean action noise std: 3.22
          Mean value_function loss: 43.9710
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 20.3031
                       Mean reward: 863.96
               Mean episode length: 247.85
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 171.9246
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 1.28s
                      Time elapsed: 00:14:26
                               ETA: 00:18:48

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 93529 steps/s (collection: 0.879s, learning 0.172s)
             Mean action noise std: 3.23
          Mean value_function loss: 54.5491
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.3206
                       Mean reward: 875.41
               Mean episode length: 249.63
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 171.2525
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 1.05s
                      Time elapsed: 00:14:27
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 83152 steps/s (collection: 1.035s, learning 0.147s)
             Mean action noise std: 3.24
          Mean value_function loss: 62.4253
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.3449
                       Mean reward: 848.80
               Mean episode length: 246.32
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 172.7121
      Episode_Reward/object_height: 0.0659
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 1.18s
                      Time elapsed: 00:14:28
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 94859 steps/s (collection: 0.836s, learning 0.201s)
             Mean action noise std: 3.24
          Mean value_function loss: 47.2762
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.3637
                       Mean reward: 875.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 171.4357
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 1.04s
                      Time elapsed: 00:14:29
                               ETA: 00:18:46

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 82261 steps/s (collection: 1.056s, learning 0.139s)
             Mean action noise std: 3.25
          Mean value_function loss: 52.9174
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.3784
                       Mean reward: 862.30
               Mean episode length: 247.04
    Episode_Reward/reaching_object: 0.7669
     Episode_Reward/lifting_object: 172.5557
      Episode_Reward/object_height: 0.0656
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 1.20s
                      Time elapsed: 00:14:30
                               ETA: 00:18:45

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 93604 steps/s (collection: 0.901s, learning 0.149s)
             Mean action noise std: 3.25
          Mean value_function loss: 49.0334
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 20.3927
                       Mean reward: 843.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7531
     Episode_Reward/lifting_object: 169.4653
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 1.05s
                      Time elapsed: 00:14:32
                               ETA: 00:18:44

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 83267 steps/s (collection: 1.080s, learning 0.101s)
             Mean action noise std: 3.26
          Mean value_function loss: 34.8839
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.4038
                       Mean reward: 874.81
               Mean episode length: 248.95
    Episode_Reward/reaching_object: 0.7542
     Episode_Reward/lifting_object: 171.2871
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 1.18s
                      Time elapsed: 00:14:33
                               ETA: 00:18:43

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 101002 steps/s (collection: 0.847s, learning 0.126s)
             Mean action noise std: 3.27
          Mean value_function loss: 47.4028
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.4206
                       Mean reward: 884.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 171.9075
      Episode_Reward/object_height: 0.0655
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 0.97s
                      Time elapsed: 00:14:34
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 98721 steps/s (collection: 0.863s, learning 0.133s)
             Mean action noise std: 3.27
          Mean value_function loss: 42.0393
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 20.4374
                       Mean reward: 879.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7716
     Episode_Reward/lifting_object: 173.1113
      Episode_Reward/object_height: 0.0658
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 1.00s
                      Time elapsed: 00:14:35
                               ETA: 00:18:41

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 113162 steps/s (collection: 0.771s, learning 0.098s)
             Mean action noise std: 3.28
          Mean value_function loss: 48.6225
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 20.4506
                       Mean reward: 871.85
               Mean episode length: 248.71
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 169.7166
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 0.87s
                      Time elapsed: 00:14:36
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 101525 steps/s (collection: 0.869s, learning 0.099s)
             Mean action noise std: 3.28
          Mean value_function loss: 62.2244
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.4584
                       Mean reward: 858.52
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7596
     Episode_Reward/lifting_object: 171.2103
      Episode_Reward/object_height: 0.0648
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 0.97s
                      Time elapsed: 00:14:36
                               ETA: 00:18:39

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 104932 steps/s (collection: 0.818s, learning 0.119s)
             Mean action noise std: 3.29
          Mean value_function loss: 39.6591
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 20.4682
                       Mean reward: 828.77
               Mean episode length: 245.05
    Episode_Reward/reaching_object: 0.7467
     Episode_Reward/lifting_object: 167.1411
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 0.94s
                      Time elapsed: 00:14:37
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 113407 steps/s (collection: 0.750s, learning 0.117s)
             Mean action noise std: 3.29
          Mean value_function loss: 44.3017
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.4816
                       Mean reward: 853.73
               Mean episode length: 245.42
    Episode_Reward/reaching_object: 0.7527
     Episode_Reward/lifting_object: 169.0113
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 0.87s
                      Time elapsed: 00:14:38
                               ETA: 00:18:37

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 98386 steps/s (collection: 0.857s, learning 0.142s)
             Mean action noise std: 3.29
          Mean value_function loss: 55.7406
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.4945
                       Mean reward: 851.96
               Mean episode length: 247.87
    Episode_Reward/reaching_object: 0.7583
     Episode_Reward/lifting_object: 169.6813
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 1.00s
                      Time elapsed: 00:14:39
                               ETA: 00:18:36

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 108795 steps/s (collection: 0.783s, learning 0.121s)
             Mean action noise std: 3.30
          Mean value_function loss: 35.7847
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 20.4992
                       Mean reward: 857.78
               Mean episode length: 248.86
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 170.9582
      Episode_Reward/object_height: 0.0644
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.2917
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 0.90s
                      Time elapsed: 00:14:40
                               ETA: 00:18:35

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 109511 steps/s (collection: 0.776s, learning 0.122s)
             Mean action noise std: 3.30
          Mean value_function loss: 44.6163
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 20.5066
                       Mean reward: 864.46
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 172.3011
      Episode_Reward/object_height: 0.0651
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 0.90s
                      Time elapsed: 00:14:41
                               ETA: 00:18:33

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 109393 steps/s (collection: 0.767s, learning 0.132s)
             Mean action noise std: 3.31
          Mean value_function loss: 40.6717
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 20.5198
                       Mean reward: 854.99
               Mean episode length: 247.44
    Episode_Reward/reaching_object: 0.7506
     Episode_Reward/lifting_object: 169.9205
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 0.90s
                      Time elapsed: 00:14:42
                               ETA: 00:18:32

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 114275 steps/s (collection: 0.768s, learning 0.093s)
             Mean action noise std: 3.31
          Mean value_function loss: 30.9594
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 20.5374
                       Mean reward: 833.78
               Mean episode length: 246.17
    Episode_Reward/reaching_object: 0.7440
     Episode_Reward/lifting_object: 169.7892
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 0.86s
                      Time elapsed: 00:14:43
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 113664 steps/s (collection: 0.776s, learning 0.089s)
             Mean action noise std: 3.32
          Mean value_function loss: 45.6706
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.5509
                       Mean reward: 847.20
               Mean episode length: 248.91
    Episode_Reward/reaching_object: 0.7361
     Episode_Reward/lifting_object: 168.3326
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 0.86s
                      Time elapsed: 00:14:44
                               ETA: 00:18:30

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 112517 steps/s (collection: 0.781s, learning 0.092s)
             Mean action noise std: 3.33
          Mean value_function loss: 48.7191
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.5706
                       Mean reward: 850.87
               Mean episode length: 247.01
    Episode_Reward/reaching_object: 0.7431
     Episode_Reward/lifting_object: 170.7292
      Episode_Reward/object_height: 0.0646
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 0.87s
                      Time elapsed: 00:14:45
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 113532 steps/s (collection: 0.777s, learning 0.088s)
             Mean action noise std: 3.34
          Mean value_function loss: 36.0893
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.5911
                       Mean reward: 858.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7349
     Episode_Reward/lifting_object: 168.9868
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 0.87s
                      Time elapsed: 00:14:45
                               ETA: 00:18:28

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 113604 steps/s (collection: 0.774s, learning 0.091s)
             Mean action noise std: 3.35
          Mean value_function loss: 47.1411
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 20.6136
                       Mean reward: 854.04
               Mean episode length: 247.03
    Episode_Reward/reaching_object: 0.7501
     Episode_Reward/lifting_object: 171.6047
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 0.87s
                      Time elapsed: 00:14:46
                               ETA: 00:18:27

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 105011 steps/s (collection: 0.787s, learning 0.149s)
             Mean action noise std: 3.35
          Mean value_function loss: 39.9708
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 20.6352
                       Mean reward: 852.71
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 0.7413
     Episode_Reward/lifting_object: 168.9109
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 0.94s
                      Time elapsed: 00:14:47
                               ETA: 00:18:25

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 113442 steps/s (collection: 0.764s, learning 0.103s)
             Mean action noise std: 3.36
          Mean value_function loss: 29.4843
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.6518
                       Mean reward: 868.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 173.1211
      Episode_Reward/object_height: 0.0654
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 0.87s
                      Time elapsed: 00:14:48
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 108160 steps/s (collection: 0.752s, learning 0.157s)
             Mean action noise std: 3.37
          Mean value_function loss: 39.1326
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.6694
                       Mean reward: 855.87
               Mean episode length: 245.03
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 172.8635
      Episode_Reward/object_height: 0.0653
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 0.91s
                      Time elapsed: 00:14:49
                               ETA: 00:18:23

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 106929 steps/s (collection: 0.783s, learning 0.137s)
             Mean action noise std: 3.37
          Mean value_function loss: 30.7790
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 20.6842
                       Mean reward: 880.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7630
     Episode_Reward/lifting_object: 171.8673
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 0.92s
                      Time elapsed: 00:14:50
                               ETA: 00:18:22

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 116301 steps/s (collection: 0.751s, learning 0.094s)
             Mean action noise std: 3.38
          Mean value_function loss: 39.2795
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 20.7065
                       Mean reward: 865.31
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 174.8384
      Episode_Reward/object_height: 0.0662
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 0.85s
                      Time elapsed: 00:14:51
                               ETA: 00:18:21

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 109317 steps/s (collection: 0.778s, learning 0.122s)
             Mean action noise std: 3.40
          Mean value_function loss: 31.4567
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 20.7346
                       Mean reward: 874.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7716
     Episode_Reward/lifting_object: 173.6580
      Episode_Reward/object_height: 0.0657
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 0.90s
                      Time elapsed: 00:14:52
                               ETA: 00:18:20

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 108761 steps/s (collection: 0.795s, learning 0.109s)
             Mean action noise std: 3.41
          Mean value_function loss: 40.2887
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 20.7565
                       Mean reward: 858.32
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 170.0844
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 0.90s
                      Time elapsed: 00:14:53
                               ETA: 00:18:19

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 104540 steps/s (collection: 0.819s, learning 0.121s)
             Mean action noise std: 3.43
          Mean value_function loss: 55.4713
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 20.7962
                       Mean reward: 835.14
               Mean episode length: 241.29
    Episode_Reward/reaching_object: 0.7577
     Episode_Reward/lifting_object: 171.3539
      Episode_Reward/object_height: 0.0645
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 0.94s
                      Time elapsed: 00:14:54
                               ETA: 00:18:18

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 106863 steps/s (collection: 0.785s, learning 0.135s)
             Mean action noise std: 3.43
          Mean value_function loss: 46.4138
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 20.8272
                       Mean reward: 840.79
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 173.2452
      Episode_Reward/object_height: 0.0650
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 0.92s
                      Time elapsed: 00:14:54
                               ETA: 00:18:17

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 107096 steps/s (collection: 0.825s, learning 0.093s)
             Mean action noise std: 3.43
          Mean value_function loss: 44.5788
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 20.8297
                       Mean reward: 876.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 173.7297
      Episode_Reward/object_height: 0.0652
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 0.92s
                      Time elapsed: 00:14:55
                               ETA: 00:18:15

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 114385 steps/s (collection: 0.756s, learning 0.104s)
             Mean action noise std: 3.44
          Mean value_function loss: 54.0258
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 20.8400
                       Mean reward: 868.82
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 171.1749
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 0.86s
                      Time elapsed: 00:14:56
                               ETA: 00:18:14

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 116845 steps/s (collection: 0.748s, learning 0.094s)
             Mean action noise std: 3.44
          Mean value_function loss: 38.7831
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 20.8513
                       Mean reward: 863.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 171.8399
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 0.84s
                      Time elapsed: 00:14:57
                               ETA: 00:18:13

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 117616 steps/s (collection: 0.744s, learning 0.092s)
             Mean action noise std: 3.45
          Mean value_function loss: 50.0810
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 20.8670
                       Mean reward: 883.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 171.8538
      Episode_Reward/object_height: 0.0643
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 0.84s
                      Time elapsed: 00:14:58
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 108045 steps/s (collection: 0.813s, learning 0.097s)
             Mean action noise std: 3.46
          Mean value_function loss: 30.8008
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 20.8836
                       Mean reward: 869.25
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 171.3979
      Episode_Reward/object_height: 0.0641
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 0.91s
                      Time elapsed: 00:14:59
                               ETA: 00:18:11

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 116355 steps/s (collection: 0.749s, learning 0.096s)
             Mean action noise std: 3.46
          Mean value_function loss: 58.5306
               Mean surrogate loss: 0.0204
                 Mean entropy loss: 20.8976
                       Mean reward: 862.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 170.8923
      Episode_Reward/object_height: 0.0639
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 0.84s
                      Time elapsed: 00:15:00
                               ETA: 00:18:10

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 112339 steps/s (collection: 0.783s, learning 0.092s)
             Mean action noise std: 3.46
          Mean value_function loss: 62.0220
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 20.8997
                       Mean reward: 855.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 171.7764
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 0.88s
                      Time elapsed: 00:15:01
                               ETA: 00:18:09

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 113339 steps/s (collection: 0.769s, learning 0.098s)
             Mean action noise std: 3.47
          Mean value_function loss: 52.1904
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 20.9068
                       Mean reward: 858.08
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.7366
     Episode_Reward/lifting_object: 167.6486
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 0.87s
                      Time elapsed: 00:15:01
                               ETA: 00:18:07

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 114191 steps/s (collection: 0.762s, learning 0.099s)
             Mean action noise std: 3.47
          Mean value_function loss: 70.7474
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 20.9173
                       Mean reward: 876.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7546
     Episode_Reward/lifting_object: 170.3527
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 0.86s
                      Time elapsed: 00:15:02
                               ETA: 00:18:06

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 114786 steps/s (collection: 0.769s, learning 0.088s)
             Mean action noise std: 3.48
          Mean value_function loss: 64.2172
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 20.9315
                       Mean reward: 860.66
               Mean episode length: 247.85
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 171.3868
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 0.86s
                      Time elapsed: 00:15:03
                               ETA: 00:18:05

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 114713 steps/s (collection: 0.747s, learning 0.110s)
             Mean action noise std: 3.49
          Mean value_function loss: 67.1223
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 20.9558
                       Mean reward: 829.98
               Mean episode length: 249.04
    Episode_Reward/reaching_object: 0.7448
     Episode_Reward/lifting_object: 168.0469
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 0.86s
                      Time elapsed: 00:15:04
                               ETA: 00:18:04

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 112106 steps/s (collection: 0.769s, learning 0.108s)
             Mean action noise std: 3.49
          Mean value_function loss: 61.4227
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 20.9625
                       Mean reward: 863.03
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 0.7475
     Episode_Reward/lifting_object: 168.9753
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 0.88s
                      Time elapsed: 00:15:05
                               ETA: 00:18:03

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 113154 steps/s (collection: 0.756s, learning 0.112s)
             Mean action noise std: 3.49
          Mean value_function loss: 57.3769
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 20.9708
                       Mean reward: 852.35
               Mean episode length: 247.07
    Episode_Reward/reaching_object: 0.7577
     Episode_Reward/lifting_object: 169.1089
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 0.87s
                      Time elapsed: 00:15:06
                               ETA: 00:18:02

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 111272 steps/s (collection: 0.789s, learning 0.095s)
             Mean action noise std: 3.50
          Mean value_function loss: 45.9593
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 20.9793
                       Mean reward: 849.04
               Mean episode length: 248.82
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 171.5653
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 0.88s
                      Time elapsed: 00:15:07
                               ETA: 00:18:00

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 116976 steps/s (collection: 0.745s, learning 0.095s)
             Mean action noise std: 3.50
          Mean value_function loss: 52.3205
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 20.9920
                       Mean reward: 847.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 170.3818
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 0.84s
                      Time elapsed: 00:15:07
                               ETA: 00:17:59

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 112417 steps/s (collection: 0.770s, learning 0.105s)
             Mean action noise std: 3.51
          Mean value_function loss: 45.6376
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 21.0015
                       Mean reward: 845.14
               Mean episode length: 247.17
    Episode_Reward/reaching_object: 0.7404
     Episode_Reward/lifting_object: 166.9560
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 0.87s
                      Time elapsed: 00:15:08
                               ETA: 00:17:58

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 106455 steps/s (collection: 0.785s, learning 0.139s)
             Mean action noise std: 3.52
          Mean value_function loss: 52.2032
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.0172
                       Mean reward: 829.02
               Mean episode length: 247.17
    Episode_Reward/reaching_object: 0.7343
     Episode_Reward/lifting_object: 167.5485
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 0.92s
                      Time elapsed: 00:15:09
                               ETA: 00:17:57

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 112170 steps/s (collection: 0.763s, learning 0.114s)
             Mean action noise std: 3.53
          Mean value_function loss: 62.4306
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.0405
                       Mean reward: 853.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7374
     Episode_Reward/lifting_object: 167.0126
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 0.88s
                      Time elapsed: 00:15:10
                               ETA: 00:17:56

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 109709 steps/s (collection: 0.744s, learning 0.152s)
             Mean action noise std: 3.53
          Mean value_function loss: 54.0327
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 21.0604
                       Mean reward: 803.59
               Mean episode length: 244.06
    Episode_Reward/reaching_object: 0.7300
     Episode_Reward/lifting_object: 166.7215
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 0.90s
                      Time elapsed: 00:15:11
                               ETA: 00:17:55

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 111650 steps/s (collection: 0.769s, learning 0.112s)
             Mean action noise std: 3.54
          Mean value_function loss: 48.0385
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 21.0694
                       Mean reward: 838.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7248
     Episode_Reward/lifting_object: 166.1515
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 0.88s
                      Time elapsed: 00:15:12
                               ETA: 00:17:54

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 108688 steps/s (collection: 0.798s, learning 0.106s)
             Mean action noise std: 3.54
          Mean value_function loss: 46.7363
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 21.0729
                       Mean reward: 852.57
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.7370
     Episode_Reward/lifting_object: 166.7629
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 0.90s
                      Time elapsed: 00:15:13
                               ETA: 00:17:53

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 109966 steps/s (collection: 0.795s, learning 0.099s)
             Mean action noise std: 3.54
          Mean value_function loss: 56.5265
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 21.0799
                       Mean reward: 853.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7530
     Episode_Reward/lifting_object: 169.9530
      Episode_Reward/object_height: 0.0636
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 0.89s
                      Time elapsed: 00:15:14
                               ETA: 00:17:52

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 110840 steps/s (collection: 0.763s, learning 0.124s)
             Mean action noise std: 3.55
          Mean value_function loss: 58.3293
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 21.0907
                       Mean reward: 849.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7461
     Episode_Reward/lifting_object: 169.0546
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 0.89s
                      Time elapsed: 00:15:15
                               ETA: 00:17:50

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 112685 steps/s (collection: 0.781s, learning 0.092s)
             Mean action noise std: 3.55
          Mean value_function loss: 61.1917
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 21.0946
                       Mean reward: 859.18
               Mean episode length: 247.43
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 171.3982
      Episode_Reward/object_height: 0.0638
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 0.87s
                      Time elapsed: 00:15:15
                               ETA: 00:17:49

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 114315 steps/s (collection: 0.767s, learning 0.093s)
             Mean action noise std: 3.55
          Mean value_function loss: 54.9246
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 21.0986
                       Mean reward: 853.43
               Mean episode length: 249.39
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 170.6073
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 0.86s
                      Time elapsed: 00:15:16
                               ETA: 00:17:48

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 108761 steps/s (collection: 0.802s, learning 0.102s)
             Mean action noise std: 3.56
          Mean value_function loss: 62.0926
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 21.1038
                       Mean reward: 833.23
               Mean episode length: 246.11
    Episode_Reward/reaching_object: 0.7456
     Episode_Reward/lifting_object: 167.1645
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 0.90s
                      Time elapsed: 00:15:17
                               ETA: 00:17:47

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 114994 steps/s (collection: 0.762s, learning 0.093s)
             Mean action noise std: 3.56
          Mean value_function loss: 38.4343
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 21.1135
                       Mean reward: 847.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7328
     Episode_Reward/lifting_object: 165.1226
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 0.85s
                      Time elapsed: 00:15:18
                               ETA: 00:17:46

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 114185 steps/s (collection: 0.735s, learning 0.126s)
             Mean action noise std: 3.56
          Mean value_function loss: 43.3651
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 21.1207
                       Mean reward: 824.55
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7516
     Episode_Reward/lifting_object: 167.3251
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 0.86s
                      Time elapsed: 00:15:19
                               ETA: 00:17:45

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 113599 steps/s (collection: 0.758s, learning 0.107s)
             Mean action noise std: 3.57
          Mean value_function loss: 40.1178
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 21.1281
                       Mean reward: 823.27
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.7529
     Episode_Reward/lifting_object: 168.3355
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 0.87s
                      Time elapsed: 00:15:20
                               ETA: 00:17:44

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 116489 steps/s (collection: 0.743s, learning 0.101s)
             Mean action noise std: 3.57
          Mean value_function loss: 42.8967
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.1354
                       Mean reward: 857.55
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 169.2193
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 0.84s
                      Time elapsed: 00:15:21
                               ETA: 00:17:42

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 111103 steps/s (collection: 0.779s, learning 0.106s)
             Mean action noise std: 3.58
          Mean value_function loss: 33.7815
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 21.1526
                       Mean reward: 869.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 172.2594
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 0.88s
                      Time elapsed: 00:15:22
                               ETA: 00:17:41

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 112288 steps/s (collection: 0.788s, learning 0.087s)
             Mean action noise std: 3.59
          Mean value_function loss: 38.5017
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.1781
                       Mean reward: 875.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 172.2441
      Episode_Reward/object_height: 0.0640
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 0.88s
                      Time elapsed: 00:15:22
                               ETA: 00:17:40

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 115024 steps/s (collection: 0.756s, learning 0.099s)
             Mean action noise std: 3.59
          Mean value_function loss: 45.3419
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 21.1950
                       Mean reward: 878.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7684
     Episode_Reward/lifting_object: 172.4465
      Episode_Reward/object_height: 0.0642
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 0.85s
                      Time elapsed: 00:15:23
                               ETA: 00:17:39

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 107907 steps/s (collection: 0.766s, learning 0.145s)
             Mean action noise std: 3.60
          Mean value_function loss: 23.6137
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 21.2021
                       Mean reward: 850.45
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 171.7834
      Episode_Reward/object_height: 0.0637
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 0.91s
                      Time elapsed: 00:15:24
                               ETA: 00:17:38

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 107274 steps/s (collection: 0.776s, learning 0.140s)
             Mean action noise std: 3.60
          Mean value_function loss: 38.3513
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 21.2144
                       Mean reward: 849.27
               Mean episode length: 249.54
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 171.5424
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 0.92s
                      Time elapsed: 00:15:25
                               ETA: 00:17:37

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 109351 steps/s (collection: 0.754s, learning 0.145s)
             Mean action noise std: 3.61
          Mean value_function loss: 38.0373
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 21.2216
                       Mean reward: 848.82
               Mean episode length: 248.66
    Episode_Reward/reaching_object: 0.7678
     Episode_Reward/lifting_object: 170.4939
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 0.90s
                      Time elapsed: 00:15:26
                               ETA: 00:17:36

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 105939 steps/s (collection: 0.787s, learning 0.140s)
             Mean action noise std: 3.61
          Mean value_function loss: 35.8922
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.2252
                       Mean reward: 849.50
               Mean episode length: 249.44
    Episode_Reward/reaching_object: 0.7565
     Episode_Reward/lifting_object: 168.3376
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 0.93s
                      Time elapsed: 00:15:27
                               ETA: 00:17:35

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 113284 steps/s (collection: 0.759s, learning 0.109s)
             Mean action noise std: 3.61
          Mean value_function loss: 36.7790
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 21.2314
                       Mean reward: 863.40
               Mean episode length: 247.85
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 172.2952
      Episode_Reward/object_height: 0.0635
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 0.87s
                      Time elapsed: 00:15:28
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 114783 steps/s (collection: 0.754s, learning 0.103s)
             Mean action noise std: 3.62
          Mean value_function loss: 31.5992
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 21.2409
                       Mean reward: 866.92
               Mean episode length: 249.48
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 171.3043
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 0.86s
                      Time elapsed: 00:15:29
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 111114 steps/s (collection: 0.777s, learning 0.107s)
             Mean action noise std: 3.62
          Mean value_function loss: 29.9779
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 21.2477
                       Mean reward: 858.55
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.7715
     Episode_Reward/lifting_object: 172.6571
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 0.88s
                      Time elapsed: 00:15:30
                               ETA: 00:17:31

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 111997 steps/s (collection: 0.767s, learning 0.111s)
             Mean action noise std: 3.63
          Mean value_function loss: 27.7375
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.2601
                       Mean reward: 851.47
               Mean episode length: 248.95
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 172.2129
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 0.88s
                      Time elapsed: 00:15:30
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 110786 steps/s (collection: 0.774s, learning 0.114s)
             Mean action noise std: 3.63
          Mean value_function loss: 32.5760
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.2750
                       Mean reward: 867.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7755
     Episode_Reward/lifting_object: 172.7526
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 0.89s
                      Time elapsed: 00:15:31
                               ETA: 00:17:29

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 114608 steps/s (collection: 0.756s, learning 0.102s)
             Mean action noise std: 3.64
          Mean value_function loss: 44.9190
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.2875
                       Mean reward: 872.66
               Mean episode length: 249.83
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 173.2538
      Episode_Reward/object_height: 0.0632
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 0.86s
                      Time elapsed: 00:15:32
                               ETA: 00:17:28

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 103718 steps/s (collection: 0.769s, learning 0.179s)
             Mean action noise std: 3.65
          Mean value_function loss: 38.6921
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 21.2992
                       Mean reward: 828.70
               Mean episode length: 244.58
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 169.6771
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 0.95s
                      Time elapsed: 00:15:33
                               ETA: 00:17:27

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 111334 steps/s (collection: 0.755s, learning 0.128s)
             Mean action noise std: 3.65
          Mean value_function loss: 52.6087
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.3069
                       Mean reward: 863.30
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 170.6870
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 0.88s
                      Time elapsed: 00:15:34
                               ETA: 00:17:26

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 118128 steps/s (collection: 0.745s, learning 0.088s)
             Mean action noise std: 3.66
          Mean value_function loss: 46.4338
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 21.3163
                       Mean reward: 881.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 173.5949
      Episode_Reward/object_height: 0.0629
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 0.83s
                      Time elapsed: 00:15:35
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 112876 steps/s (collection: 0.777s, learning 0.094s)
             Mean action noise std: 3.66
          Mean value_function loss: 31.6699
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 21.3319
                       Mean reward: 876.79
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: 172.0400
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 0.87s
                      Time elapsed: 00:15:36
                               ETA: 00:17:24

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 109571 steps/s (collection: 0.803s, learning 0.095s)
             Mean action noise std: 3.67
          Mean value_function loss: 42.9167
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 21.3472
                       Mean reward: 880.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7749
     Episode_Reward/lifting_object: 173.5963
      Episode_Reward/object_height: 0.0631
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 0.90s
                      Time elapsed: 00:15:37
                               ETA: 00:17:22

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 106112 steps/s (collection: 0.803s, learning 0.124s)
             Mean action noise std: 3.68
          Mean value_function loss: 27.7048
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 21.3623
                       Mean reward: 867.43
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 172.1130
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 0.93s
                      Time elapsed: 00:15:38
                               ETA: 00:17:21

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 108820 steps/s (collection: 0.789s, learning 0.114s)
             Mean action noise std: 3.68
          Mean value_function loss: 24.8315
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 21.3788
                       Mean reward: 859.48
               Mean episode length: 249.08
    Episode_Reward/reaching_object: 0.7716
     Episode_Reward/lifting_object: 171.8431
      Episode_Reward/object_height: 0.0625
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 0.90s
                      Time elapsed: 00:15:38
                               ETA: 00:17:20

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 107573 steps/s (collection: 0.824s, learning 0.090s)
             Mean action noise std: 3.68
          Mean value_function loss: 27.1794
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 21.3878
                       Mean reward: 870.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7800
     Episode_Reward/lifting_object: 174.4424
      Episode_Reward/object_height: 0.0634
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 0.91s
                      Time elapsed: 00:15:39
                               ETA: 00:17:19

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 110839 steps/s (collection: 0.781s, learning 0.106s)
             Mean action noise std: 3.69
          Mean value_function loss: 31.8313
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.3938
                       Mean reward: 846.86
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 172.4605
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 0.89s
                      Time elapsed: 00:15:40
                               ETA: 00:17:18

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 107468 steps/s (collection: 0.783s, learning 0.132s)
             Mean action noise std: 3.69
          Mean value_function loss: 25.9000
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.4027
                       Mean reward: 860.05
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7787
     Episode_Reward/lifting_object: 173.0877
      Episode_Reward/object_height: 0.0628
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 0.91s
                      Time elapsed: 00:15:41
                               ETA: 00:17:17

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 113247 steps/s (collection: 0.760s, learning 0.108s)
             Mean action noise std: 3.70
          Mean value_function loss: 35.3139
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 21.4129
                       Mean reward: 857.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7753
     Episode_Reward/lifting_object: 172.3793
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 0.87s
                      Time elapsed: 00:15:42
                               ETA: 00:17:16

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 111133 steps/s (collection: 0.755s, learning 0.129s)
             Mean action noise std: 3.71
          Mean value_function loss: 27.0584
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.4300
                       Mean reward: 846.79
               Mean episode length: 246.95
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 171.8232
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 0.88s
                      Time elapsed: 00:15:43
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 111924 steps/s (collection: 0.770s, learning 0.108s)
             Mean action noise std: 3.71
          Mean value_function loss: 38.5494
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 21.4463
                       Mean reward: 886.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7752
     Episode_Reward/lifting_object: 173.1766
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 0.88s
                      Time elapsed: 00:15:44
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 109165 steps/s (collection: 0.787s, learning 0.114s)
             Mean action noise std: 3.72
          Mean value_function loss: 41.3807
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 21.4587
                       Mean reward: 861.70
               Mean episode length: 249.97
    Episode_Reward/reaching_object: 0.7762
     Episode_Reward/lifting_object: 172.0168
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 0.90s
                      Time elapsed: 00:15:45
                               ETA: 00:17:13

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 115399 steps/s (collection: 0.758s, learning 0.094s)
             Mean action noise std: 3.73
          Mean value_function loss: 37.8571
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 21.4769
                       Mean reward: 868.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: 172.1809
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 0.85s
                      Time elapsed: 00:15:46
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 110905 steps/s (collection: 0.783s, learning 0.104s)
             Mean action noise std: 3.73
          Mean value_function loss: 34.2864
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 21.4880
                       Mean reward: 858.52
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 169.4884
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 0.89s
                      Time elapsed: 00:15:46
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 109049 steps/s (collection: 0.790s, learning 0.112s)
             Mean action noise std: 3.74
          Mean value_function loss: 34.1846
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 21.4987
                       Mean reward: 866.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7741
     Episode_Reward/lifting_object: 172.6268
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 0.90s
                      Time elapsed: 00:15:47
                               ETA: 00:17:09

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 111915 steps/s (collection: 0.778s, learning 0.101s)
             Mean action noise std: 3.74
          Mean value_function loss: 41.5607
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 21.5109
                       Mean reward: 866.30
               Mean episode length: 246.98
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 172.9167
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 0.88s
                      Time elapsed: 00:15:48
                               ETA: 00:17:08

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 105705 steps/s (collection: 0.775s, learning 0.155s)
             Mean action noise std: 3.75
          Mean value_function loss: 33.6743
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 21.5174
                       Mean reward: 887.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 175.5359
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 0.93s
                      Time elapsed: 00:15:49
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 110152 steps/s (collection: 0.765s, learning 0.127s)
             Mean action noise std: 3.76
          Mean value_function loss: 45.6247
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 21.5335
                       Mean reward: 874.04
               Mean episode length: 249.36
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 171.4183
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 0.89s
                      Time elapsed: 00:15:50
                               ETA: 00:17:06

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 107887 steps/s (collection: 0.779s, learning 0.133s)
             Mean action noise std: 3.77
          Mean value_function loss: 49.9475
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 21.5540
                       Mean reward: 863.57
               Mean episode length: 249.75
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: 173.3688
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 0.91s
                      Time elapsed: 00:15:51
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 108766 steps/s (collection: 0.788s, learning 0.116s)
             Mean action noise std: 3.77
          Mean value_function loss: 52.5931
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 21.5659
                       Mean reward: 875.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7766
     Episode_Reward/lifting_object: 173.7928
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 0.90s
                      Time elapsed: 00:15:52
                               ETA: 00:17:04

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 108258 steps/s (collection: 0.800s, learning 0.108s)
             Mean action noise std: 3.78
          Mean value_function loss: 45.6426
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 21.5766
                       Mean reward: 851.29
               Mean episode length: 246.82
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 170.8677
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 0.91s
                      Time elapsed: 00:15:53
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 101065 steps/s (collection: 0.844s, learning 0.129s)
             Mean action noise std: 3.79
          Mean value_function loss: 49.8020
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.5927
                       Mean reward: 850.50
               Mean episode length: 246.51
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 171.1314
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 0.97s
                      Time elapsed: 00:15:54
                               ETA: 00:17:02

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 106181 steps/s (collection: 0.808s, learning 0.118s)
             Mean action noise std: 3.79
          Mean value_function loss: 46.9407
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 21.6047
                       Mean reward: 872.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 172.4215
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 0.93s
                      Time elapsed: 00:15:55
                               ETA: 00:17:01

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 111199 steps/s (collection: 0.787s, learning 0.097s)
             Mean action noise std: 3.80
          Mean value_function loss: 51.0653
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 21.6194
                       Mean reward: 866.99
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 173.7051
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 0.88s
                      Time elapsed: 00:15:56
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 113980 steps/s (collection: 0.762s, learning 0.101s)
             Mean action noise std: 3.80
          Mean value_function loss: 48.4993
               Mean surrogate loss: 0.0142
                 Mean entropy loss: 21.6280
                       Mean reward: 864.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 169.9521
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 0.86s
                      Time elapsed: 00:15:56
                               ETA: 00:16:59

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 102586 steps/s (collection: 0.831s, learning 0.127s)
             Mean action noise std: 3.80
          Mean value_function loss: 45.2636
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 21.6293
                       Mean reward: 821.86
               Mean episode length: 243.75
    Episode_Reward/reaching_object: 0.7519
     Episode_Reward/lifting_object: 168.5549
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 0.96s
                      Time elapsed: 00:15:57
                               ETA: 00:16:58

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 112660 steps/s (collection: 0.771s, learning 0.101s)
             Mean action noise std: 3.81
          Mean value_function loss: 54.5698
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 21.6361
                       Mean reward: 853.79
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 171.5847
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 0.87s
                      Time elapsed: 00:15:58
                               ETA: 00:16:56

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 105789 steps/s (collection: 0.806s, learning 0.124s)
             Mean action noise std: 3.81
          Mean value_function loss: 54.3145
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 21.6501
                       Mean reward: 848.83
               Mean episode length: 247.44
    Episode_Reward/reaching_object: 0.7663
     Episode_Reward/lifting_object: 170.9352
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 0.93s
                      Time elapsed: 00:15:59
                               ETA: 00:16:55

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 108739 steps/s (collection: 0.788s, learning 0.116s)
             Mean action noise std: 3.82
          Mean value_function loss: 68.4837
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 21.6686
                       Mean reward: 850.18
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 170.6342
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 0.90s
                      Time elapsed: 00:16:00
                               ETA: 00:16:54

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 106105 steps/s (collection: 0.762s, learning 0.164s)
             Mean action noise std: 3.83
          Mean value_function loss: 44.9057
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 21.6874
                       Mean reward: 833.26
               Mean episode length: 245.71
    Episode_Reward/reaching_object: 0.7539
     Episode_Reward/lifting_object: 167.8739
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 0.93s
                      Time elapsed: 00:16:01
                               ETA: 00:16:53

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 101579 steps/s (collection: 0.815s, learning 0.153s)
             Mean action noise std: 3.84
          Mean value_function loss: 56.3568
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 21.7157
                       Mean reward: 826.41
               Mean episode length: 247.58
    Episode_Reward/reaching_object: 0.7501
     Episode_Reward/lifting_object: 167.6822
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 0.97s
                      Time elapsed: 00:16:02
                               ETA: 00:16:52

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 110412 steps/s (collection: 0.785s, learning 0.105s)
             Mean action noise std: 3.85
          Mean value_function loss: 50.5170
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 21.7261
                       Mean reward: 844.92
               Mean episode length: 244.44
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 169.1530
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 0.89s
                      Time elapsed: 00:16:03
                               ETA: 00:16:51

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 103267 steps/s (collection: 0.836s, learning 0.115s)
             Mean action noise std: 3.85
          Mean value_function loss: 42.3763
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.7358
                       Mean reward: 857.15
               Mean episode length: 249.46
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 171.9767
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 0.95s
                      Time elapsed: 00:16:04
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 108148 steps/s (collection: 0.819s, learning 0.090s)
             Mean action noise std: 3.86
          Mean value_function loss: 58.4775
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 21.7527
                       Mean reward: 864.84
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 170.5763
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 0.91s
                      Time elapsed: 00:16:05
                               ETA: 00:16:49

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 108088 steps/s (collection: 0.821s, learning 0.089s)
             Mean action noise std: 3.86
          Mean value_function loss: 55.3211
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 21.7648
                       Mean reward: 839.05
               Mean episode length: 246.65
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 168.1263
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 0.91s
                      Time elapsed: 00:16:06
                               ETA: 00:16:48

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 109497 steps/s (collection: 0.785s, learning 0.113s)
             Mean action noise std: 3.87
          Mean value_function loss: 53.8769
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 21.7784
                       Mean reward: 844.63
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7538
     Episode_Reward/lifting_object: 167.8209
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 0.90s
                      Time elapsed: 00:16:06
                               ETA: 00:16:47

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 107250 steps/s (collection: 0.828s, learning 0.088s)
             Mean action noise std: 3.88
          Mean value_function loss: 51.6795
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 21.7895
                       Mean reward: 840.40
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 169.0402
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 0.92s
                      Time elapsed: 00:16:07
                               ETA: 00:16:46

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 111090 steps/s (collection: 0.795s, learning 0.090s)
             Mean action noise std: 3.88
          Mean value_function loss: 47.7152
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 21.8020
                       Mean reward: 861.15
               Mean episode length: 249.62
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 169.5149
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 0.88s
                      Time elapsed: 00:16:08
                               ETA: 00:16:45

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 108866 steps/s (collection: 0.799s, learning 0.104s)
             Mean action noise std: 3.88
          Mean value_function loss: 48.6545
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 21.8060
                       Mean reward: 838.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 169.1123
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 0.90s
                      Time elapsed: 00:16:09
                               ETA: 00:16:44

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 106337 steps/s (collection: 0.805s, learning 0.119s)
             Mean action noise std: 3.89
          Mean value_function loss: 45.1678
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 21.8135
                       Mean reward: 842.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7509
     Episode_Reward/lifting_object: 168.6254
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 0.92s
                      Time elapsed: 00:16:10
                               ETA: 00:16:43

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 112500 steps/s (collection: 0.774s, learning 0.100s)
             Mean action noise std: 3.89
          Mean value_function loss: 39.6318
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 21.8169
                       Mean reward: 871.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 170.3986
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 0.87s
                      Time elapsed: 00:16:11
                               ETA: 00:16:42

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 101545 steps/s (collection: 0.821s, learning 0.147s)
             Mean action noise std: 3.89
          Mean value_function loss: 43.8218
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 21.8259
                       Mean reward: 843.49
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 169.6145
      Episode_Reward/object_height: 0.0621
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 0.97s
                      Time elapsed: 00:16:12
                               ETA: 00:16:41

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 110978 steps/s (collection: 0.754s, learning 0.132s)
             Mean action noise std: 3.90
          Mean value_function loss: 40.1653
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 21.8406
                       Mean reward: 863.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7685
     Episode_Reward/lifting_object: 170.9992
      Episode_Reward/object_height: 0.0627
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 0.89s
                      Time elapsed: 00:16:13
                               ETA: 00:16:39

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 106605 steps/s (collection: 0.804s, learning 0.118s)
             Mean action noise std: 3.91
          Mean value_function loss: 52.3455
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 21.8525
                       Mean reward: 838.10
               Mean episode length: 247.93
    Episode_Reward/reaching_object: 0.7628
     Episode_Reward/lifting_object: 168.7485
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 0.92s
                      Time elapsed: 00:16:14
                               ETA: 00:16:38

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 108156 steps/s (collection: 0.804s, learning 0.105s)
             Mean action noise std: 3.91
          Mean value_function loss: 49.1182
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 21.8619
                       Mean reward: 831.84
               Mean episode length: 247.86
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 171.3885
      Episode_Reward/object_height: 0.0623
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 0.91s
                      Time elapsed: 00:16:15
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 106950 steps/s (collection: 0.814s, learning 0.106s)
             Mean action noise std: 3.92
          Mean value_function loss: 47.9657
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 21.8735
                       Mean reward: 866.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 170.4789
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 0.92s
                      Time elapsed: 00:16:16
                               ETA: 00:16:36

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 107878 steps/s (collection: 0.824s, learning 0.088s)
             Mean action noise std: 3.93
          Mean value_function loss: 46.7752
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 21.8914
                       Mean reward: 837.69
               Mean episode length: 246.57
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 168.4817
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 0.91s
                      Time elapsed: 00:16:17
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 111934 steps/s (collection: 0.787s, learning 0.091s)
             Mean action noise std: 3.93
          Mean value_function loss: 45.8810
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 21.9044
                       Mean reward: 865.94
               Mean episode length: 247.93
    Episode_Reward/reaching_object: 0.7701
     Episode_Reward/lifting_object: 171.9374
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 0.88s
                      Time elapsed: 00:16:17
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 112053 steps/s (collection: 0.775s, learning 0.103s)
             Mean action noise std: 3.94
          Mean value_function loss: 46.4767
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 21.9154
                       Mean reward: 867.52
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7657
     Episode_Reward/lifting_object: 170.7198
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 0.88s
                      Time elapsed: 00:16:18
                               ETA: 00:16:33

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 101340 steps/s (collection: 0.872s, learning 0.098s)
             Mean action noise std: 3.95
          Mean value_function loss: 39.8782
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 21.9250
                       Mean reward: 828.47
               Mean episode length: 249.88
    Episode_Reward/reaching_object: 0.7553
     Episode_Reward/lifting_object: 168.6361
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 0.97s
                      Time elapsed: 00:16:19
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 108224 steps/s (collection: 0.817s, learning 0.091s)
             Mean action noise std: 3.95
          Mean value_function loss: 48.8531
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 21.9442
                       Mean reward: 850.91
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 170.6666
      Episode_Reward/object_height: 0.0618
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 0.91s
                      Time elapsed: 00:16:20
                               ETA: 00:16:31

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 106846 steps/s (collection: 0.768s, learning 0.152s)
             Mean action noise std: 3.96
          Mean value_function loss: 56.9129
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 21.9589
                       Mean reward: 861.44
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7755
     Episode_Reward/lifting_object: 172.1923
      Episode_Reward/object_height: 0.0622
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 0.92s
                      Time elapsed: 00:16:21
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 111348 steps/s (collection: 0.766s, learning 0.117s)
             Mean action noise std: 3.97
          Mean value_function loss: 73.4701
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 21.9754
                       Mean reward: 855.92
               Mean episode length: 249.96
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 171.9536
      Episode_Reward/object_height: 0.0620
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 0.88s
                      Time elapsed: 00:16:22
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 112736 steps/s (collection: 0.758s, learning 0.114s)
             Mean action noise std: 3.97
          Mean value_function loss: 70.0174
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 21.9902
                       Mean reward: 858.65
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7540
     Episode_Reward/lifting_object: 168.3012
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 0.87s
                      Time elapsed: 00:16:23
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 110989 steps/s (collection: 0.788s, learning 0.098s)
             Mean action noise std: 3.98
          Mean value_function loss: 61.0377
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 22.0022
                       Mean reward: 861.60
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 172.5977
      Episode_Reward/object_height: 0.0626
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 0.89s
                      Time elapsed: 00:16:24
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 112938 steps/s (collection: 0.782s, learning 0.089s)
             Mean action noise std: 3.99
          Mean value_function loss: 60.2006
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 22.0273
                       Mean reward: 854.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 170.0182
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 0.87s
                      Time elapsed: 00:16:25
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 34288 steps/s (collection: 2.752s, learning 0.115s)
             Mean action noise std: 4.00
          Mean value_function loss: 53.5260
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 22.0474
                       Mean reward: 864.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 170.7843
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 2.87s
                      Time elapsed: 00:16:27
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 31942 steps/s (collection: 2.961s, learning 0.117s)
             Mean action noise std: 4.01
          Mean value_function loss: 51.7525
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 22.0625
                       Mean reward: 869.87
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.7678
     Episode_Reward/lifting_object: 171.1631
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 3.08s
                      Time elapsed: 00:16:31
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 30387 steps/s (collection: 3.105s, learning 0.131s)
             Mean action noise std: 4.01
          Mean value_function loss: 49.0539
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 22.0726
                       Mean reward: 852.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7479
     Episode_Reward/lifting_object: 167.6922
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 3.24s
                      Time elapsed: 00:16:34
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 28995 steps/s (collection: 3.258s, learning 0.132s)
             Mean action noise std: 4.02
          Mean value_function loss: 44.5155
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.0877
                       Mean reward: 844.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7500
     Episode_Reward/lifting_object: 167.8263
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 3.39s
                      Time elapsed: 00:16:37
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 29111 steps/s (collection: 3.247s, learning 0.130s)
             Mean action noise std: 4.03
          Mean value_function loss: 52.1584
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 22.1042
                       Mean reward: 841.65
               Mean episode length: 249.71
    Episode_Reward/reaching_object: 0.7478
     Episode_Reward/lifting_object: 167.1896
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 3.38s
                      Time elapsed: 00:16:41
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 30007 steps/s (collection: 3.142s, learning 0.134s)
             Mean action noise std: 4.04
          Mean value_function loss: 56.9963
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 22.1215
                       Mean reward: 830.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7338
     Episode_Reward/lifting_object: 163.5113
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 3.28s
                      Time elapsed: 00:16:44
                               ETA: 00:16:33

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 31826 steps/s (collection: 2.967s, learning 0.122s)
             Mean action noise std: 4.04
          Mean value_function loss: 60.3185
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 22.1328
                       Mean reward: 863.16
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7458
     Episode_Reward/lifting_object: 166.1705
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 3.09s
                      Time elapsed: 00:16:47
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 29769 steps/s (collection: 3.167s, learning 0.135s)
             Mean action noise std: 4.05
          Mean value_function loss: 54.2499
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 22.1464
                       Mean reward: 856.26
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 168.2126
      Episode_Reward/object_height: 0.0610
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 3.30s
                      Time elapsed: 00:16:50
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 24749 steps/s (collection: 3.815s, learning 0.157s)
             Mean action noise std: 4.05
          Mean value_function loss: 55.5874
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 22.1509
                       Mean reward: 855.59
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 170.0312
      Episode_Reward/object_height: 0.0615
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 3.97s
                      Time elapsed: 00:16:54
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 106010 steps/s (collection: 0.789s, learning 0.139s)
             Mean action noise std: 4.06
          Mean value_function loss: 74.7722
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 22.1633
                       Mean reward: 872.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 172.2442
      Episode_Reward/object_height: 0.0624
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 0.93s
                      Time elapsed: 00:16:55
                               ETA: 00:16:36

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 98871 steps/s (collection: 0.832s, learning 0.163s)
             Mean action noise std: 4.06
          Mean value_function loss: 93.9120
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 22.1756
                       Mean reward: 851.76
               Mean episode length: 246.63
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 169.5634
      Episode_Reward/object_height: 0.0616
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 0.99s
                      Time elapsed: 00:16:56
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 81541 steps/s (collection: 0.992s, learning 0.213s)
             Mean action noise std: 4.07
          Mean value_function loss: 123.8340
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 22.1868
                       Mean reward: 856.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7581
     Episode_Reward/lifting_object: 169.9500
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 1.21s
                      Time elapsed: 00:16:57
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 74232 steps/s (collection: 1.124s, learning 0.201s)
             Mean action noise std: 4.08
          Mean value_function loss: 127.1223
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 22.2086
                       Mean reward: 861.09
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 169.3926
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 1.32s
                      Time elapsed: 00:16:59
                               ETA: 00:16:33

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 69684 steps/s (collection: 1.204s, learning 0.207s)
             Mean action noise std: 4.08
          Mean value_function loss: 140.3753
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 22.2144
                       Mean reward: 836.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 168.2829
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 1.41s
                      Time elapsed: 00:17:00
                               ETA: 00:16:33

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 75415 steps/s (collection: 1.072s, learning 0.231s)
             Mean action noise std: 4.09
          Mean value_function loss: 134.4880
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 22.2214
                       Mean reward: 857.73
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7551
     Episode_Reward/lifting_object: 168.2552
      Episode_Reward/object_height: 0.0606
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 1.30s
                      Time elapsed: 00:17:01
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 78837 steps/s (collection: 1.100s, learning 0.147s)
             Mean action noise std: 4.09
          Mean value_function loss: 159.9461
               Mean surrogate loss: 0.0258
                 Mean entropy loss: 22.2289
                       Mean reward: 865.17
               Mean episode length: 248.95
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 170.2622
      Episode_Reward/object_height: 0.0619
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 1.25s
                      Time elapsed: 00:17:03
                               ETA: 00:16:31

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 91784 steps/s (collection: 0.899s, learning 0.172s)
             Mean action noise std: 4.09
          Mean value_function loss: 117.8029
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 22.2294
                       Mean reward: 867.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7541
     Episode_Reward/lifting_object: 167.9065
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 1.07s
                      Time elapsed: 00:17:04
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 94920 steps/s (collection: 0.900s, learning 0.136s)
             Mean action noise std: 4.09
          Mean value_function loss: 96.4290
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 22.2307
                       Mean reward: 809.72
               Mean episode length: 245.98
    Episode_Reward/reaching_object: 0.7455
     Episode_Reward/lifting_object: 168.2193
      Episode_Reward/object_height: 0.0614
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 1.04s
                      Time elapsed: 00:17:05
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 89551 steps/s (collection: 0.948s, learning 0.150s)
             Mean action noise std: 4.09
          Mean value_function loss: 86.5591
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 22.2349
                       Mean reward: 746.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7180
     Episode_Reward/lifting_object: 161.6601
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 1.10s
                      Time elapsed: 00:17:06
                               ETA: 00:16:29

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 94291 steps/s (collection: 0.865s, learning 0.177s)
             Mean action noise std: 4.09
          Mean value_function loss: 101.8067
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 22.2389
                       Mean reward: 766.22
               Mean episode length: 249.11
    Episode_Reward/reaching_object: 0.6624
     Episode_Reward/lifting_object: 149.8907
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 1.04s
                      Time elapsed: 00:17:07
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 90510 steps/s (collection: 0.898s, learning 0.188s)
             Mean action noise std: 4.10
          Mean value_function loss: 79.6233
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.2440
                       Mean reward: 761.21
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.6745
     Episode_Reward/lifting_object: 151.7145
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 1.09s
                      Time elapsed: 00:17:08
                               ETA: 00:16:27

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 99017 steps/s (collection: 0.819s, learning 0.174s)
             Mean action noise std: 4.11
          Mean value_function loss: 60.8157
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.2526
                       Mean reward: 791.71
               Mean episode length: 245.67
    Episode_Reward/reaching_object: 0.6876
     Episode_Reward/lifting_object: 156.3758
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 0.99s
                      Time elapsed: 00:17:09
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 96437 steps/s (collection: 0.860s, learning 0.159s)
             Mean action noise std: 4.11
          Mean value_function loss: 59.8690
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 22.2668
                       Mean reward: 768.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.6895
     Episode_Reward/lifting_object: 155.1913
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 1.02s
                      Time elapsed: 00:17:10
                               ETA: 00:16:25

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 90975 steps/s (collection: 0.900s, learning 0.181s)
             Mean action noise std: 4.12
          Mean value_function loss: 48.3533
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 22.2823
                       Mean reward: 786.29
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.6936
     Episode_Reward/lifting_object: 155.6855
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 1.08s
                      Time elapsed: 00:17:11
                               ETA: 00:16:24

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 90751 steps/s (collection: 0.885s, learning 0.199s)
             Mean action noise std: 4.13
          Mean value_function loss: 45.2864
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 22.2966
                       Mean reward: 835.64
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7342
     Episode_Reward/lifting_object: 164.6249
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 1.08s
                      Time elapsed: 00:17:12
                               ETA: 00:16:23

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 92733 steps/s (collection: 0.874s, learning 0.186s)
             Mean action noise std: 4.14
          Mean value_function loss: 55.3891
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 22.3097
                       Mean reward: 832.00
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7527
     Episode_Reward/lifting_object: 168.5192
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 1.06s
                      Time elapsed: 00:17:13
                               ETA: 00:16:22

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 92109 steps/s (collection: 0.900s, learning 0.167s)
             Mean action noise std: 4.15
          Mean value_function loss: 49.0783
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.3355
                       Mean reward: 860.76
               Mean episode length: 249.76
    Episode_Reward/reaching_object: 0.7492
     Episode_Reward/lifting_object: 167.7655
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 1.07s
                      Time elapsed: 00:17:14
                               ETA: 00:16:21

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 96091 steps/s (collection: 0.866s, learning 0.157s)
             Mean action noise std: 4.17
          Mean value_function loss: 45.2359
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 22.3668
                       Mean reward: 868.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7507
     Episode_Reward/lifting_object: 168.4467
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 1.02s
                      Time elapsed: 00:17:15
                               ETA: 00:16:20

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 94712 steps/s (collection: 0.866s, learning 0.172s)
             Mean action noise std: 4.18
          Mean value_function loss: 53.6673
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 22.3910
                       Mean reward: 827.41
               Mean episode length: 249.71
    Episode_Reward/reaching_object: 0.7421
     Episode_Reward/lifting_object: 166.5723
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 1.04s
                      Time elapsed: 00:17:16
                               ETA: 00:16:19

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 99010 steps/s (collection: 0.858s, learning 0.135s)
             Mean action noise std: 4.19
          Mean value_function loss: 43.9604
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 22.4097
                       Mean reward: 853.35
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 170.6976
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 0.99s
                      Time elapsed: 00:17:17
                               ETA: 00:16:18

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 88944 steps/s (collection: 0.942s, learning 0.163s)
             Mean action noise std: 4.20
          Mean value_function loss: 55.0903
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 22.4276
                       Mean reward: 860.80
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 171.2501
      Episode_Reward/object_height: 0.0612
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 1.11s
                      Time elapsed: 00:17:18
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 97961 steps/s (collection: 0.847s, learning 0.157s)
             Mean action noise std: 4.20
          Mean value_function loss: 53.3995
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 22.4410
                       Mean reward: 856.98
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 170.3604
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 1.00s
                      Time elapsed: 00:17:19
                               ETA: 00:16:16

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 92315 steps/s (collection: 0.902s, learning 0.163s)
             Mean action noise std: 4.21
          Mean value_function loss: 52.7422
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 22.4512
                       Mean reward: 864.82
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 172.7949
      Episode_Reward/object_height: 0.0617
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 1.06s
                      Time elapsed: 00:17:20
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 109635 steps/s (collection: 0.786s, learning 0.111s)
             Mean action noise std: 4.22
          Mean value_function loss: 51.9829
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 22.4734
                       Mean reward: 871.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 172.6980
      Episode_Reward/object_height: 0.0613
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 0.90s
                      Time elapsed: 00:17:21
                               ETA: 00:16:14

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 112179 steps/s (collection: 0.749s, learning 0.128s)
             Mean action noise std: 4.23
          Mean value_function loss: 46.9865
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.4893
                       Mean reward: 853.82
               Mean episode length: 247.40
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 170.4158
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 0.88s
                      Time elapsed: 00:17:22
                               ETA: 00:16:13

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 109480 steps/s (collection: 0.805s, learning 0.093s)
             Mean action noise std: 4.24
          Mean value_function loss: 36.8540
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 22.5050
                       Mean reward: 843.82
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.7563
     Episode_Reward/lifting_object: 170.3032
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 0.90s
                      Time elapsed: 00:17:23
                               ETA: 00:16:12

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 106072 steps/s (collection: 0.834s, learning 0.093s)
             Mean action noise std: 4.25
          Mean value_function loss: 42.4119
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.5264
                       Mean reward: 846.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7476
     Episode_Reward/lifting_object: 168.2400
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 0.93s
                      Time elapsed: 00:17:24
                               ETA: 00:16:10

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 110414 steps/s (collection: 0.798s, learning 0.093s)
             Mean action noise std: 4.26
          Mean value_function loss: 34.4990
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 22.5438
                       Mean reward: 836.28
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7412
     Episode_Reward/lifting_object: 168.9177
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 0.89s
                      Time elapsed: 00:17:25
                               ETA: 00:16:09

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 118910 steps/s (collection: 0.740s, learning 0.087s)
             Mean action noise std: 4.27
          Mean value_function loss: 35.9317
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 22.5577
                       Mean reward: 865.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7435
     Episode_Reward/lifting_object: 168.3482
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 0.83s
                      Time elapsed: 00:17:26
                               ETA: 00:16:08

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 112304 steps/s (collection: 0.784s, learning 0.091s)
             Mean action noise std: 4.27
          Mean value_function loss: 38.9195
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 22.5664
                       Mean reward: 798.88
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 169.5030
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 0.88s
                      Time elapsed: 00:17:27
                               ETA: 00:16:07

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 108077 steps/s (collection: 0.783s, learning 0.126s)
             Mean action noise std: 4.28
          Mean value_function loss: 37.4491
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 22.5741
                       Mean reward: 856.86
               Mean episode length: 249.73
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 169.7686
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 0.91s
                      Time elapsed: 00:17:28
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 94750 steps/s (collection: 0.861s, learning 0.177s)
             Mean action noise std: 4.28
          Mean value_function loss: 41.1929
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 22.5817
                       Mean reward: 859.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7486
     Episode_Reward/lifting_object: 169.2883
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 1.04s
                      Time elapsed: 00:17:29
                               ETA: 00:16:05

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 96412 steps/s (collection: 0.866s, learning 0.154s)
             Mean action noise std: 4.28
          Mean value_function loss: 28.8163
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 22.5890
                       Mean reward: 858.45
               Mean episode length: 246.12
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 169.2578
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 1.02s
                      Time elapsed: 00:17:30
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 99161 steps/s (collection: 0.816s, learning 0.175s)
             Mean action noise std: 4.29
          Mean value_function loss: 33.8555
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 22.5975
                       Mean reward: 877.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 170.9819
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 0.99s
                      Time elapsed: 00:17:31
                               ETA: 00:16:03

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 105724 steps/s (collection: 0.789s, learning 0.141s)
             Mean action noise std: 4.29
          Mean value_function loss: 33.3215
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 22.6083
                       Mean reward: 869.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 171.7792
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 0.93s
                      Time elapsed: 00:17:32
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 105638 steps/s (collection: 0.819s, learning 0.112s)
             Mean action noise std: 4.30
          Mean value_function loss: 40.1827
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 22.6210
                       Mean reward: 862.45
               Mean episode length: 249.28
    Episode_Reward/reaching_object: 0.7689
     Episode_Reward/lifting_object: 172.1753
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 0.93s
                      Time elapsed: 00:17:32
                               ETA: 00:16:01

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 95802 steps/s (collection: 0.848s, learning 0.179s)
             Mean action noise std: 4.30
          Mean value_function loss: 34.0046
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 22.6295
                       Mean reward: 874.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7751
     Episode_Reward/lifting_object: 172.9253
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 1.03s
                      Time elapsed: 00:17:33
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 104995 steps/s (collection: 0.784s, learning 0.152s)
             Mean action noise std: 4.31
          Mean value_function loss: 36.5596
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 22.6370
                       Mean reward: 853.86
               Mean episode length: 248.53
    Episode_Reward/reaching_object: 0.7736
     Episode_Reward/lifting_object: 172.1117
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 0.94s
                      Time elapsed: 00:17:34
                               ETA: 00:15:59

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 100795 steps/s (collection: 0.786s, learning 0.190s)
             Mean action noise std: 4.31
          Mean value_function loss: 38.9182
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 22.6480
                       Mean reward: 870.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7720
     Episode_Reward/lifting_object: 171.2296
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 0.98s
                      Time elapsed: 00:17:35
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 106049 steps/s (collection: 0.774s, learning 0.153s)
             Mean action noise std: 4.32
          Mean value_function loss: 40.9396
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 22.6604
                       Mean reward: 856.26
               Mean episode length: 247.15
    Episode_Reward/reaching_object: 0.7668
     Episode_Reward/lifting_object: 172.1039
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 0.93s
                      Time elapsed: 00:17:36
                               ETA: 00:15:57

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 98311 steps/s (collection: 0.794s, learning 0.206s)
             Mean action noise std: 4.33
          Mean value_function loss: 42.3787
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 22.6740
                       Mean reward: 878.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 172.4812
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 1.00s
                      Time elapsed: 00:17:37
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 103411 steps/s (collection: 0.766s, learning 0.184s)
             Mean action noise std: 4.33
          Mean value_function loss: 38.1193
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 22.6817
                       Mean reward: 871.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 170.9027
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 0.95s
                      Time elapsed: 00:17:38
                               ETA: 00:15:55

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 111412 steps/s (collection: 0.772s, learning 0.110s)
             Mean action noise std: 4.34
          Mean value_function loss: 37.4672
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 22.6889
                       Mean reward: 866.18
               Mean episode length: 249.58
    Episode_Reward/reaching_object: 0.7706
     Episode_Reward/lifting_object: 172.5879
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 0.88s
                      Time elapsed: 00:17:39
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 102389 steps/s (collection: 0.812s, learning 0.149s)
             Mean action noise std: 4.35
          Mean value_function loss: 40.1461
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 22.7089
                       Mean reward: 856.70
               Mean episode length: 247.15
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 171.4369
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 0.96s
                      Time elapsed: 00:17:40
                               ETA: 00:15:52

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 106803 steps/s (collection: 0.799s, learning 0.122s)
             Mean action noise std: 4.36
          Mean value_function loss: 31.3355
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 22.7269
                       Mean reward: 860.53
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 170.2742
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 0.92s
                      Time elapsed: 00:17:41
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 105082 steps/s (collection: 0.805s, learning 0.131s)
             Mean action noise std: 4.37
          Mean value_function loss: 28.2286
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 22.7405
                       Mean reward: 880.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 172.4079
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 0.94s
                      Time elapsed: 00:17:42
                               ETA: 00:15:50

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 111336 steps/s (collection: 0.795s, learning 0.088s)
             Mean action noise std: 4.37
          Mean value_function loss: 30.6076
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 22.7531
                       Mean reward: 862.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 171.6294
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 0.88s
                      Time elapsed: 00:17:43
                               ETA: 00:15:49

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 106467 steps/s (collection: 0.784s, learning 0.140s)
             Mean action noise std: 4.37
          Mean value_function loss: 36.9741
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 22.7573
                       Mean reward: 860.30
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 172.1798
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 0.92s
                      Time elapsed: 00:17:44
                               ETA: 00:15:48

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 104067 steps/s (collection: 0.824s, learning 0.121s)
             Mean action noise std: 4.38
          Mean value_function loss: 33.2206
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 22.7659
                       Mean reward: 872.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 172.1657
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 0.94s
                      Time elapsed: 00:17:45
                               ETA: 00:15:47

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 108645 steps/s (collection: 0.775s, learning 0.130s)
             Mean action noise std: 4.39
          Mean value_function loss: 38.4346
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.7792
                       Mean reward: 861.85
               Mean episode length: 247.50
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 172.0537
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 0.90s
                      Time elapsed: 00:17:46
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 102598 steps/s (collection: 0.820s, learning 0.138s)
             Mean action noise std: 4.40
          Mean value_function loss: 33.5917
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.7968
                       Mean reward: 868.73
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 172.0822
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 0.96s
                      Time elapsed: 00:17:47
                               ETA: 00:15:45

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 101761 steps/s (collection: 0.799s, learning 0.167s)
             Mean action noise std: 4.41
          Mean value_function loss: 38.3865
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 22.8138
                       Mean reward: 849.64
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 170.6230
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 0.97s
                      Time elapsed: 00:17:48
                               ETA: 00:15:44

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 103913 steps/s (collection: 0.823s, learning 0.123s)
             Mean action noise std: 4.41
          Mean value_function loss: 44.7792
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 22.8284
                       Mean reward: 859.42
               Mean episode length: 247.02
    Episode_Reward/reaching_object: 0.7708
     Episode_Reward/lifting_object: 171.6981
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 0.95s
                      Time elapsed: 00:17:48
                               ETA: 00:15:43

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 101976 steps/s (collection: 0.830s, learning 0.134s)
             Mean action noise std: 4.42
          Mean value_function loss: 40.0875
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 22.8406
                       Mean reward: 882.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7786
     Episode_Reward/lifting_object: 173.5137
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 0.96s
                      Time elapsed: 00:17:49
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 99071 steps/s (collection: 0.817s, learning 0.176s)
             Mean action noise std: 4.43
          Mean value_function loss: 39.2099
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 22.8590
                       Mean reward: 835.82
               Mean episode length: 243.02
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 170.8272
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 0.99s
                      Time elapsed: 00:17:50
                               ETA: 00:15:41

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 96823 steps/s (collection: 0.818s, learning 0.197s)
             Mean action noise std: 4.44
          Mean value_function loss: 45.3121
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 22.8800
                       Mean reward: 867.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 172.2548
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 1.02s
                      Time elapsed: 00:17:51
                               ETA: 00:15:40

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 100714 steps/s (collection: 0.831s, learning 0.145s)
             Mean action noise std: 4.45
          Mean value_function loss: 37.1664
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 22.8903
                       Mean reward: 846.23
               Mean episode length: 249.54
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 171.3046
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 0.98s
                      Time elapsed: 00:17:52
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 98880 steps/s (collection: 0.824s, learning 0.171s)
             Mean action noise std: 4.45
          Mean value_function loss: 40.3881
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.9008
                       Mean reward: 856.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 171.0250
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 0.99s
                      Time elapsed: 00:17:53
                               ETA: 00:15:38

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 104044 steps/s (collection: 0.803s, learning 0.142s)
             Mean action noise std: 4.47
          Mean value_function loss: 51.6739
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 22.9202
                       Mean reward: 877.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7765
     Episode_Reward/lifting_object: 172.8649
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 0.94s
                      Time elapsed: 00:17:54
                               ETA: 00:15:37

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 101553 steps/s (collection: 0.821s, learning 0.147s)
             Mean action noise std: 4.48
          Mean value_function loss: 55.4305
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 22.9415
                       Mean reward: 870.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7702
     Episode_Reward/lifting_object: 170.8548
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 0.97s
                      Time elapsed: 00:17:55
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 103464 steps/s (collection: 0.826s, learning 0.124s)
             Mean action noise std: 4.48
          Mean value_function loss: 64.7817
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 22.9567
                       Mean reward: 858.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 170.1662
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 0.95s
                      Time elapsed: 00:17:56
                               ETA: 00:15:35

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 103633 steps/s (collection: 0.793s, learning 0.155s)
             Mean action noise std: 4.49
          Mean value_function loss: 66.6887
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 22.9647
                       Mean reward: 842.80
               Mean episode length: 249.55
    Episode_Reward/reaching_object: 0.7631
     Episode_Reward/lifting_object: 169.9964
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 0.95s
                      Time elapsed: 00:17:57
                               ETA: 00:15:33

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 96847 steps/s (collection: 0.846s, learning 0.169s)
             Mean action noise std: 4.49
          Mean value_function loss: 63.5268
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 22.9694
                       Mean reward: 838.49
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 170.0773
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 1.02s
                      Time elapsed: 00:17:58
                               ETA: 00:15:32

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 102895 steps/s (collection: 0.838s, learning 0.117s)
             Mean action noise std: 4.49
          Mean value_function loss: 57.8709
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 22.9746
                       Mean reward: 858.28
               Mean episode length: 246.27
    Episode_Reward/reaching_object: 0.7587
     Episode_Reward/lifting_object: 168.6581
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 0.96s
                      Time elapsed: 00:17:59
                               ETA: 00:15:31

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 109246 steps/s (collection: 0.795s, learning 0.105s)
             Mean action noise std: 4.50
          Mean value_function loss: 54.2687
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 22.9818
                       Mean reward: 864.89
               Mean episode length: 247.06
    Episode_Reward/reaching_object: 0.7544
     Episode_Reward/lifting_object: 168.2937
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 0.90s
                      Time elapsed: 00:18:00
                               ETA: 00:15:30

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 109356 steps/s (collection: 0.802s, learning 0.097s)
             Mean action noise std: 4.51
          Mean value_function loss: 36.2476
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 22.9951
                       Mean reward: 868.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 171.4525
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 0.90s
                      Time elapsed: 00:18:01
                               ETA: 00:15:29

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 101561 steps/s (collection: 0.818s, learning 0.150s)
             Mean action noise std: 4.52
          Mean value_function loss: 43.1305
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 23.0122
                       Mean reward: 845.57
               Mean episode length: 249.76
    Episode_Reward/reaching_object: 0.7586
     Episode_Reward/lifting_object: 170.5711
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 0.97s
                      Time elapsed: 00:18:02
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 104361 steps/s (collection: 0.819s, learning 0.123s)
             Mean action noise std: 4.52
          Mean value_function loss: 38.2361
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.0276
                       Mean reward: 833.64
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.7513
     Episode_Reward/lifting_object: 167.6163
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 0.94s
                      Time elapsed: 00:18:03
                               ETA: 00:15:27

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 106111 steps/s (collection: 0.811s, learning 0.115s)
             Mean action noise std: 4.53
          Mean value_function loss: 37.6022
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 23.0381
                       Mean reward: 852.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7479
     Episode_Reward/lifting_object: 169.7503
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 0.93s
                      Time elapsed: 00:18:04
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 103655 steps/s (collection: 0.822s, learning 0.127s)
             Mean action noise std: 4.54
          Mean value_function loss: 34.6857
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 23.0547
                       Mean reward: 805.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7412
     Episode_Reward/lifting_object: 168.6447
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 0.95s
                      Time elapsed: 00:18:05
                               ETA: 00:15:25

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 104335 steps/s (collection: 0.789s, learning 0.153s)
             Mean action noise std: 4.55
          Mean value_function loss: 38.1021
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.0671
                       Mean reward: 852.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7337
     Episode_Reward/lifting_object: 167.8531
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 0.94s
                      Time elapsed: 00:18:06
                               ETA: 00:15:24

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 106810 steps/s (collection: 0.812s, learning 0.109s)
             Mean action noise std: 4.55
          Mean value_function loss: 33.5344
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.0809
                       Mean reward: 844.56
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7269
     Episode_Reward/lifting_object: 164.2094
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 0.92s
                      Time elapsed: 00:18:07
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 109315 steps/s (collection: 0.785s, learning 0.115s)
             Mean action noise std: 4.56
          Mean value_function loss: 38.5497
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 23.0923
                       Mean reward: 855.63
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.7535
     Episode_Reward/lifting_object: 171.0121
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 0.90s
                      Time elapsed: 00:18:08
                               ETA: 00:15:22

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 101143 steps/s (collection: 0.834s, learning 0.138s)
             Mean action noise std: 4.56
          Mean value_function loss: 38.3948
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 23.0983
                       Mean reward: 848.63
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 171.3664
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 0.97s
                      Time elapsed: 00:18:09
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 106864 steps/s (collection: 0.800s, learning 0.120s)
             Mean action noise std: 4.57
          Mean value_function loss: 33.5441
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 23.1084
                       Mean reward: 856.13
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7700
     Episode_Reward/lifting_object: 171.7337
      Episode_Reward/object_height: 0.0608
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 0.92s
                      Time elapsed: 00:18:09
                               ETA: 00:15:20

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 105043 steps/s (collection: 0.819s, learning 0.117s)
             Mean action noise std: 4.57
          Mean value_function loss: 32.6126
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 23.1173
                       Mean reward: 877.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 170.2267
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 0.94s
                      Time elapsed: 00:18:10
                               ETA: 00:15:19

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 107131 steps/s (collection: 0.791s, learning 0.127s)
             Mean action noise std: 4.58
          Mean value_function loss: 32.0711
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.1295
                       Mean reward: 854.31
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 169.9858
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 0.92s
                      Time elapsed: 00:18:11
                               ETA: 00:15:18

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 109841 steps/s (collection: 0.781s, learning 0.114s)
             Mean action noise std: 4.59
          Mean value_function loss: 33.6334
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.1426
                       Mean reward: 867.89
               Mean episode length: 249.88
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 173.0071
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 0.89s
                      Time elapsed: 00:18:12
                               ETA: 00:15:16

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 101979 steps/s (collection: 0.789s, learning 0.175s)
             Mean action noise std: 4.59
          Mean value_function loss: 40.0198
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 23.1513
                       Mean reward: 846.85
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7577
     Episode_Reward/lifting_object: 168.6554
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 0.96s
                      Time elapsed: 00:18:13
                               ETA: 00:15:15

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 96275 steps/s (collection: 0.823s, learning 0.198s)
             Mean action noise std: 4.60
          Mean value_function loss: 45.6897
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 23.1640
                       Mean reward: 868.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7739
     Episode_Reward/lifting_object: 171.7746
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 1.02s
                      Time elapsed: 00:18:14
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 113402 steps/s (collection: 0.754s, learning 0.113s)
             Mean action noise std: 4.61
          Mean value_function loss: 41.8043
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.1746
                       Mean reward: 861.51
               Mean episode length: 249.55
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 171.0934
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 0.87s
                      Time elapsed: 00:18:15
                               ETA: 00:15:13

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 116926 steps/s (collection: 0.747s, learning 0.094s)
             Mean action noise std: 4.61
          Mean value_function loss: 35.1272
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 23.1839
                       Mean reward: 870.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7678
     Episode_Reward/lifting_object: 171.1311
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 0.84s
                      Time elapsed: 00:18:16
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 114388 steps/s (collection: 0.761s, learning 0.098s)
             Mean action noise std: 4.62
          Mean value_function loss: 26.7051
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 23.1983
                       Mean reward: 880.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7760
     Episode_Reward/lifting_object: 173.4885
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 0.86s
                      Time elapsed: 00:18:17
                               ETA: 00:15:11

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 111535 steps/s (collection: 0.781s, learning 0.101s)
             Mean action noise std: 4.63
          Mean value_function loss: 25.1002
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 23.2074
                       Mean reward: 871.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7822
     Episode_Reward/lifting_object: 175.1786
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 0.88s
                      Time elapsed: 00:18:18
                               ETA: 00:15:10

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 114500 steps/s (collection: 0.770s, learning 0.089s)
             Mean action noise std: 4.63
          Mean value_function loss: 23.9485
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 23.2208
                       Mean reward: 850.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7738
     Episode_Reward/lifting_object: 172.8516
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 0.86s
                      Time elapsed: 00:18:18
                               ETA: 00:15:09

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 112239 steps/s (collection: 0.788s, learning 0.088s)
             Mean action noise std: 4.64
          Mean value_function loss: 26.6842
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 23.2358
                       Mean reward: 861.86
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 171.5307
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 0.88s
                      Time elapsed: 00:18:19
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 115753 steps/s (collection: 0.753s, learning 0.096s)
             Mean action noise std: 4.65
          Mean value_function loss: 41.1411
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 23.2460
                       Mean reward: 869.54
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7743
     Episode_Reward/lifting_object: 172.5052
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 0.85s
                      Time elapsed: 00:18:20
                               ETA: 00:15:07

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 110024 steps/s (collection: 0.754s, learning 0.140s)
             Mean action noise std: 4.65
          Mean value_function loss: 28.6144
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 23.2519
                       Mean reward: 870.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 169.7606
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 0.89s
                      Time elapsed: 00:18:21
                               ETA: 00:15:05

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 113719 steps/s (collection: 0.764s, learning 0.100s)
             Mean action noise std: 4.66
          Mean value_function loss: 37.0646
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 23.2611
                       Mean reward: 873.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 170.8946
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 0.86s
                      Time elapsed: 00:18:22
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 112432 steps/s (collection: 0.765s, learning 0.110s)
             Mean action noise std: 4.67
          Mean value_function loss: 33.0647
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 23.2723
                       Mean reward: 854.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 170.8814
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 0.87s
                      Time elapsed: 00:18:23
                               ETA: 00:15:03

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 97172 steps/s (collection: 0.858s, learning 0.153s)
             Mean action noise std: 4.67
          Mean value_function loss: 34.1031
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.2858
                       Mean reward: 878.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 171.9070
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 1.01s
                      Time elapsed: 00:18:24
                               ETA: 00:15:02

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 101624 steps/s (collection: 0.819s, learning 0.148s)
             Mean action noise std: 4.68
          Mean value_function loss: 32.7063
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 23.2986
                       Mean reward: 879.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 173.2595
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 0.97s
                      Time elapsed: 00:18:25
                               ETA: 00:15:01

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 103931 steps/s (collection: 0.837s, learning 0.109s)
             Mean action noise std: 4.69
          Mean value_function loss: 32.1129
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 23.3110
                       Mean reward: 863.19
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.7758
     Episode_Reward/lifting_object: 172.7848
      Episode_Reward/object_height: 0.0594
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 0.95s
                      Time elapsed: 00:18:26
                               ETA: 00:15:00

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 108251 steps/s (collection: 0.791s, learning 0.117s)
             Mean action noise std: 4.70
          Mean value_function loss: 31.5632
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.3209
                       Mean reward: 857.16
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 171.7594
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 0.91s
                      Time elapsed: 00:18:27
                               ETA: 00:14:59

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 101221 steps/s (collection: 0.814s, learning 0.157s)
             Mean action noise std: 4.70
          Mean value_function loss: 36.0403
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 23.3357
                       Mean reward: 860.28
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 172.6193
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 0.97s
                      Time elapsed: 00:18:28
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 106934 steps/s (collection: 0.818s, learning 0.101s)
             Mean action noise std: 4.71
          Mean value_function loss: 28.5756
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 23.3461
                       Mean reward: 873.28
               Mean episode length: 249.49
    Episode_Reward/reaching_object: 0.7689
     Episode_Reward/lifting_object: 172.4960
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 0.92s
                      Time elapsed: 00:18:29
                               ETA: 00:14:57

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 107765 steps/s (collection: 0.815s, learning 0.097s)
             Mean action noise std: 4.71
          Mean value_function loss: 30.2222
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 23.3528
                       Mean reward: 863.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7748
     Episode_Reward/lifting_object: 173.1349
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 0.91s
                      Time elapsed: 00:18:29
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 108866 steps/s (collection: 0.788s, learning 0.115s)
             Mean action noise std: 4.72
          Mean value_function loss: 36.7972
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 23.3655
                       Mean reward: 846.52
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 171.6344
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 0.90s
                      Time elapsed: 00:18:30
                               ETA: 00:14:55

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 117984 steps/s (collection: 0.747s, learning 0.086s)
             Mean action noise std: 4.73
          Mean value_function loss: 32.9062
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 23.3769
                       Mean reward: 869.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7779
     Episode_Reward/lifting_object: 172.8911
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 0.83s
                      Time elapsed: 00:18:31
                               ETA: 00:14:54

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 107886 steps/s (collection: 0.764s, learning 0.148s)
             Mean action noise std: 4.74
          Mean value_function loss: 44.9212
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 23.3881
                       Mean reward: 861.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 172.5860
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 0.91s
                      Time elapsed: 00:18:32
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 104716 steps/s (collection: 0.817s, learning 0.122s)
             Mean action noise std: 4.74
          Mean value_function loss: 37.5827
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.4013
                       Mean reward: 867.14
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 171.8566
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 0.94s
                      Time elapsed: 00:18:33
                               ETA: 00:14:52

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 112092 steps/s (collection: 0.772s, learning 0.105s)
             Mean action noise std: 4.75
          Mean value_function loss: 31.8089
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 23.4085
                       Mean reward: 880.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7762
     Episode_Reward/lifting_object: 172.2541
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 0.88s
                      Time elapsed: 00:18:34
                               ETA: 00:14:50

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 114045 steps/s (collection: 0.770s, learning 0.092s)
             Mean action noise std: 4.75
          Mean value_function loss: 35.4288
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 23.4204
                       Mean reward: 862.90
               Mean episode length: 249.17
    Episode_Reward/reaching_object: 0.7725
     Episode_Reward/lifting_object: 172.3051
      Episode_Reward/object_height: 0.0590
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 0.86s
                      Time elapsed: 00:18:35
                               ETA: 00:14:49

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 115061 steps/s (collection: 0.768s, learning 0.087s)
             Mean action noise std: 4.76
          Mean value_function loss: 33.4331
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 23.4275
                       Mean reward: 855.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7715
     Episode_Reward/lifting_object: 171.7751
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 0.85s
                      Time elapsed: 00:18:36
                               ETA: 00:14:48

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 111644 steps/s (collection: 0.793s, learning 0.087s)
             Mean action noise std: 4.76
          Mean value_function loss: 34.8616
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 23.4413
                       Mean reward: 857.61
               Mean episode length: 247.53
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 172.7042
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 0.88s
                      Time elapsed: 00:18:37
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 114368 steps/s (collection: 0.761s, learning 0.099s)
             Mean action noise std: 4.77
          Mean value_function loss: 28.9246
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.4518
                       Mean reward: 874.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7790
     Episode_Reward/lifting_object: 173.2402
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 0.86s
                      Time elapsed: 00:18:37
                               ETA: 00:14:46

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 111530 steps/s (collection: 0.746s, learning 0.135s)
             Mean action noise std: 4.77
          Mean value_function loss: 26.4925
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 23.4593
                       Mean reward: 877.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 172.5885
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 0.88s
                      Time elapsed: 00:18:38
                               ETA: 00:14:45

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 110277 steps/s (collection: 0.764s, learning 0.128s)
             Mean action noise std: 4.78
          Mean value_function loss: 26.6876
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 23.4695
                       Mean reward: 849.58
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 171.5594
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 0.89s
                      Time elapsed: 00:18:39
                               ETA: 00:14:44

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 108413 steps/s (collection: 0.777s, learning 0.130s)
             Mean action noise std: 4.80
          Mean value_function loss: 28.5897
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 23.4900
                       Mean reward: 827.23
               Mean episode length: 247.18
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 169.0468
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 0.91s
                      Time elapsed: 00:18:40
                               ETA: 00:14:43

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 96413 steps/s (collection: 0.813s, learning 0.207s)
             Mean action noise std: 4.80
          Mean value_function loss: 46.3166
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 23.5037
                       Mean reward: 860.85
               Mean episode length: 249.44
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 170.7537
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 1.02s
                      Time elapsed: 00:18:41
                               ETA: 00:14:42

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 111984 steps/s (collection: 0.790s, learning 0.088s)
             Mean action noise std: 4.81
          Mean value_function loss: 51.3085
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 23.5161
                       Mean reward: 859.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 171.5907
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 0.88s
                      Time elapsed: 00:18:42
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 112672 steps/s (collection: 0.780s, learning 0.093s)
             Mean action noise std: 4.82
          Mean value_function loss: 46.4450
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.5253
                       Mean reward: 869.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7733
     Episode_Reward/lifting_object: 172.0194
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 0.87s
                      Time elapsed: 00:18:43
                               ETA: 00:14:40

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 109214 steps/s (collection: 0.793s, learning 0.107s)
             Mean action noise std: 4.82
          Mean value_function loss: 39.6102
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.5411
                       Mean reward: 876.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7736
     Episode_Reward/lifting_object: 172.1198
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 0.90s
                      Time elapsed: 00:18:44
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 112955 steps/s (collection: 0.774s, learning 0.096s)
             Mean action noise std: 4.84
          Mean value_function loss: 48.6943
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 23.5581
                       Mean reward: 873.64
               Mean episode length: 249.93
    Episode_Reward/reaching_object: 0.7790
     Episode_Reward/lifting_object: 172.9993
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 0.87s
                      Time elapsed: 00:18:45
                               ETA: 00:14:37

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 106346 steps/s (collection: 0.809s, learning 0.116s)
             Mean action noise std: 4.84
          Mean value_function loss: 50.6794
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 23.5768
                       Mean reward: 863.38
               Mean episode length: 249.50
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 171.7190
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 0.92s
                      Time elapsed: 00:18:46
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 106215 steps/s (collection: 0.832s, learning 0.093s)
             Mean action noise std: 4.85
          Mean value_function loss: 52.1307
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 23.5883
                       Mean reward: 871.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7850
     Episode_Reward/lifting_object: 174.2244
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0605
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 0.93s
                      Time elapsed: 00:18:46
                               ETA: 00:14:35

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 111258 steps/s (collection: 0.798s, learning 0.086s)
             Mean action noise std: 4.86
          Mean value_function loss: 47.7840
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 23.6008
                       Mean reward: 878.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7770
     Episode_Reward/lifting_object: 172.2327
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 0.88s
                      Time elapsed: 00:18:47
                               ETA: 00:14:34

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 115091 steps/s (collection: 0.761s, learning 0.093s)
             Mean action noise std: 4.87
          Mean value_function loss: 47.7120
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 23.6149
                       Mean reward: 856.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7639
     Episode_Reward/lifting_object: 169.3441
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 0.85s
                      Time elapsed: 00:18:48
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 106410 steps/s (collection: 0.749s, learning 0.175s)
             Mean action noise std: 4.87
          Mean value_function loss: 35.7396
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 23.6251
                       Mean reward: 841.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7404
     Episode_Reward/lifting_object: 166.5826
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 0.92s
                      Time elapsed: 00:18:49
                               ETA: 00:14:32

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 114191 steps/s (collection: 0.772s, learning 0.089s)
             Mean action noise std: 4.88
          Mean value_function loss: 40.2265
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 23.6323
                       Mean reward: 816.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7523
     Episode_Reward/lifting_object: 167.7219
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 0.86s
                      Time elapsed: 00:18:50
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 114911 steps/s (collection: 0.757s, learning 0.099s)
             Mean action noise std: 4.88
          Mean value_function loss: 37.0813
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 23.6351
                       Mean reward: 813.15
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 169.7408
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 0.86s
                      Time elapsed: 00:18:51
                               ETA: 00:14:30

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 110258 steps/s (collection: 0.784s, learning 0.108s)
             Mean action noise std: 4.88
          Mean value_function loss: 37.1488
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 23.6382
                       Mean reward: 862.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7674
     Episode_Reward/lifting_object: 171.5862
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 0.89s
                      Time elapsed: 00:18:52
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 103691 steps/s (collection: 0.812s, learning 0.136s)
             Mean action noise std: 4.88
          Mean value_function loss: 32.2353
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 23.6413
                       Mean reward: 853.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 169.9658
      Episode_Reward/object_height: 0.0564
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 0.95s
                      Time elapsed: 00:18:53
                               ETA: 00:14:28

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 113960 steps/s (collection: 0.765s, learning 0.097s)
             Mean action noise std: 4.89
          Mean value_function loss: 29.0612
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 23.6486
                       Mean reward: 876.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 172.2623
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 0.86s
                      Time elapsed: 00:18:54
                               ETA: 00:14:27

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 114890 steps/s (collection: 0.768s, learning 0.088s)
             Mean action noise std: 4.90
          Mean value_function loss: 25.3885
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 23.6620
                       Mean reward: 862.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7741
     Episode_Reward/lifting_object: 172.3252
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 0.86s
                      Time elapsed: 00:18:54
                               ETA: 00:14:25

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 118202 steps/s (collection: 0.745s, learning 0.087s)
             Mean action noise std: 4.90
          Mean value_function loss: 25.7609
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 23.6705
                       Mean reward: 851.10
               Mean episode length: 247.44
    Episode_Reward/reaching_object: 0.7689
     Episode_Reward/lifting_object: 172.5035
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 0.83s
                      Time elapsed: 00:18:55
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 114422 steps/s (collection: 0.752s, learning 0.108s)
             Mean action noise std: 4.91
          Mean value_function loss: 27.8657
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.6822
                       Mean reward: 872.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7748
     Episode_Reward/lifting_object: 172.6635
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0637
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 0.86s
                      Time elapsed: 00:18:56
                               ETA: 00:14:23

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 115747 steps/s (collection: 0.763s, learning 0.087s)
             Mean action noise std: 4.92
          Mean value_function loss: 25.0639
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 23.7017
                       Mean reward: 874.13
               Mean episode length: 249.79
    Episode_Reward/reaching_object: 0.7755
     Episode_Reward/lifting_object: 172.9715
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 0.85s
                      Time elapsed: 00:18:57
                               ETA: 00:14:22

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 119048 steps/s (collection: 0.736s, learning 0.090s)
             Mean action noise std: 4.93
          Mean value_function loss: 24.0010
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 23.7132
                       Mean reward: 862.17
               Mean episode length: 249.87
    Episode_Reward/reaching_object: 0.7748
     Episode_Reward/lifting_object: 172.6763
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 0.83s
                      Time elapsed: 00:18:58
                               ETA: 00:14:21

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 113311 steps/s (collection: 0.772s, learning 0.095s)
             Mean action noise std: 4.93
          Mean value_function loss: 28.5287
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 23.7194
                       Mean reward: 880.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7774
     Episode_Reward/lifting_object: 173.2118
      Episode_Reward/object_height: 0.0583
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 0.87s
                      Time elapsed: 00:18:59
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 117584 steps/s (collection: 0.748s, learning 0.088s)
             Mean action noise std: 4.94
          Mean value_function loss: 24.7796
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 23.7278
                       Mean reward: 866.97
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7776
     Episode_Reward/lifting_object: 172.6807
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0648
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 0.84s
                      Time elapsed: 00:18:59
                               ETA: 00:14:19

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 114061 steps/s (collection: 0.775s, learning 0.087s)
             Mean action noise std: 4.94
          Mean value_function loss: 25.4896
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.7327
                       Mean reward: 869.39
               Mean episode length: 249.48
    Episode_Reward/reaching_object: 0.7749
     Episode_Reward/lifting_object: 172.8699
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0650
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 0.86s
                      Time elapsed: 00:19:00
                               ETA: 00:14:18

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 117185 steps/s (collection: 0.750s, learning 0.089s)
             Mean action noise std: 4.94
          Mean value_function loss: 24.7668
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 23.7371
                       Mean reward: 870.54
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7766
     Episode_Reward/lifting_object: 173.6976
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0654
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 0.84s
                      Time elapsed: 00:19:01
                               ETA: 00:14:17

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 114452 steps/s (collection: 0.758s, learning 0.101s)
             Mean action noise std: 4.94
          Mean value_function loss: 36.2777
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 23.7429
                       Mean reward: 869.25
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 0.7791
     Episode_Reward/lifting_object: 173.3917
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 0.86s
                      Time elapsed: 00:19:02
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 116084 steps/s (collection: 0.755s, learning 0.092s)
             Mean action noise std: 4.95
          Mean value_function loss: 35.8743
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 23.7519
                       Mean reward: 853.23
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7703
     Episode_Reward/lifting_object: 170.8980
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 0.85s
                      Time elapsed: 00:19:03
                               ETA: 00:14:14

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 116229 steps/s (collection: 0.753s, learning 0.093s)
             Mean action noise std: 4.96
          Mean value_function loss: 41.2866
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 23.7635
                       Mean reward: 859.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7750
     Episode_Reward/lifting_object: 172.6132
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 0.85s
                      Time elapsed: 00:19:04
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 116525 steps/s (collection: 0.749s, learning 0.095s)
             Mean action noise std: 4.97
          Mean value_function loss: 36.8059
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 23.7823
                       Mean reward: 857.88
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 0.7812
     Episode_Reward/lifting_object: 173.9480
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 0.84s
                      Time elapsed: 00:19:05
                               ETA: 00:14:12

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 109162 steps/s (collection: 0.787s, learning 0.113s)
             Mean action noise std: 4.98
          Mean value_function loss: 33.1248
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 23.8010
                       Mean reward: 878.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7737
     Episode_Reward/lifting_object: 173.5611
      Episode_Reward/object_height: 0.0589
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 0.90s
                      Time elapsed: 00:19:05
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 109413 steps/s (collection: 0.780s, learning 0.118s)
             Mean action noise std: 4.98
          Mean value_function loss: 38.6751
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 23.8065
                       Mean reward: 865.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7787
     Episode_Reward/lifting_object: 172.9090
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0661
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 0.90s
                      Time elapsed: 00:19:06
                               ETA: 00:14:10

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 113207 steps/s (collection: 0.780s, learning 0.088s)
             Mean action noise std: 5.00
          Mean value_function loss: 29.2506
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 23.8172
                       Mean reward: 868.08
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: 171.6813
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 0.87s
                      Time elapsed: 00:19:07
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 113633 steps/s (collection: 0.748s, learning 0.118s)
             Mean action noise std: 5.00
          Mean value_function loss: 29.7369
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 23.8325
                       Mean reward: 875.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7741
     Episode_Reward/lifting_object: 171.0171
      Episode_Reward/object_height: 0.0579
        Episode_Reward/action_rate: -0.0668
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 0.87s
                      Time elapsed: 00:19:08
                               ETA: 00:14:08

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 116213 steps/s (collection: 0.754s, learning 0.092s)
             Mean action noise std: 5.01
          Mean value_function loss: 28.1058
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 23.8436
                       Mean reward: 856.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7755
     Episode_Reward/lifting_object: 172.1136
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 0.85s
                      Time elapsed: 00:19:09
                               ETA: 00:14:07

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 117618 steps/s (collection: 0.740s, learning 0.096s)
             Mean action noise std: 5.02
          Mean value_function loss: 30.2103
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 23.8553
                       Mean reward: 843.47
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.7715
     Episode_Reward/lifting_object: 170.1197
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0670
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 0.84s
                      Time elapsed: 00:19:10
                               ETA: 00:14:05

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 115337 steps/s (collection: 0.764s, learning 0.088s)
             Mean action noise std: 5.03
          Mean value_function loss: 34.3301
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 23.8763
                       Mean reward: 860.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 171.0475
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 0.85s
                      Time elapsed: 00:19:11
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 115329 steps/s (collection: 0.762s, learning 0.091s)
             Mean action noise std: 5.04
          Mean value_function loss: 29.2679
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 23.8951
                       Mean reward: 853.65
               Mean episode length: 244.71
    Episode_Reward/reaching_object: 0.7743
     Episode_Reward/lifting_object: 172.4015
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 0.85s
                      Time elapsed: 00:19:11
                               ETA: 00:14:03

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 114105 steps/s (collection: 0.775s, learning 0.086s)
             Mean action noise std: 5.05
          Mean value_function loss: 36.9331
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 23.9083
                       Mean reward: 876.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7765
     Episode_Reward/lifting_object: 172.4620
      Episode_Reward/object_height: 0.0593
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 0.86s
                      Time elapsed: 00:19:12
                               ETA: 00:14:02

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 110529 steps/s (collection: 0.770s, learning 0.119s)
             Mean action noise std: 5.06
          Mean value_function loss: 28.4437
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 23.9272
                       Mean reward: 864.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 172.3763
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 0.89s
                      Time elapsed: 00:19:13
                               ETA: 00:14:01

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 111426 steps/s (collection: 0.750s, learning 0.132s)
             Mean action noise std: 5.07
          Mean value_function loss: 31.1993
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 23.9395
                       Mean reward: 852.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 171.7678
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 0.88s
                      Time elapsed: 00:19:14
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 112437 steps/s (collection: 0.755s, learning 0.119s)
             Mean action noise std: 5.07
          Mean value_function loss: 27.7062
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 23.9501
                       Mean reward: 864.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7788
     Episode_Reward/lifting_object: 172.8595
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0687
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 0.87s
                      Time elapsed: 00:19:15
                               ETA: 00:13:59

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 116422 steps/s (collection: 0.757s, learning 0.088s)
             Mean action noise std: 5.08
          Mean value_function loss: 29.9471
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 23.9614
                       Mean reward: 856.41
               Mean episode length: 249.66
    Episode_Reward/reaching_object: 0.7784
     Episode_Reward/lifting_object: 173.1633
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 0.84s
                      Time elapsed: 00:19:16
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 103706 steps/s (collection: 0.805s, learning 0.143s)
             Mean action noise std: 5.09
          Mean value_function loss: 35.0663
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 23.9769
                       Mean reward: 857.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 171.6540
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0696
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 0.95s
                      Time elapsed: 00:19:17
                               ETA: 00:13:57

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 111173 steps/s (collection: 0.783s, learning 0.102s)
             Mean action noise std: 5.10
          Mean value_function loss: 28.4143
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 23.9924
                       Mean reward: 878.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 170.9007
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 0.88s
                      Time elapsed: 00:19:18
                               ETA: 00:13:56

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 113837 steps/s (collection: 0.772s, learning 0.092s)
             Mean action noise std: 5.11
          Mean value_function loss: 28.2180
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.0006
                       Mean reward: 862.17
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7814
     Episode_Reward/lifting_object: 173.4191
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 0.86s
                      Time elapsed: 00:19:19
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 112571 steps/s (collection: 0.783s, learning 0.090s)
             Mean action noise std: 5.12
          Mean value_function loss: 25.9573
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 24.0135
                       Mean reward: 872.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 172.0273
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 0.87s
                      Time elapsed: 00:19:19
                               ETA: 00:13:54

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 107781 steps/s (collection: 0.819s, learning 0.093s)
             Mean action noise std: 5.13
          Mean value_function loss: 36.1854
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 24.0292
                       Mean reward: 864.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: 172.0409
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 0.91s
                      Time elapsed: 00:19:20
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 110633 steps/s (collection: 0.778s, learning 0.111s)
             Mean action noise std: 5.14
          Mean value_function loss: 30.4660
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 24.0391
                       Mean reward: 873.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7725
     Episode_Reward/lifting_object: 172.4140
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 0.89s
                      Time elapsed: 00:19:21
                               ETA: 00:13:51

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 106684 steps/s (collection: 0.765s, learning 0.157s)
             Mean action noise std: 5.14
          Mean value_function loss: 31.2977
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 24.0454
                       Mean reward: 859.76
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 170.7786
      Episode_Reward/object_height: 0.0597
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 0.92s
                      Time elapsed: 00:19:22
                               ETA: 00:13:50

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 108574 steps/s (collection: 0.761s, learning 0.144s)
             Mean action noise std: 5.16
          Mean value_function loss: 33.0190
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 24.0598
                       Mean reward: 863.81
               Mean episode length: 249.14
    Episode_Reward/reaching_object: 0.7715
     Episode_Reward/lifting_object: 172.5004
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 0.91s
                      Time elapsed: 00:19:23
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 118738 steps/s (collection: 0.742s, learning 0.085s)
             Mean action noise std: 5.17
          Mean value_function loss: 28.9335
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 24.0786
                       Mean reward: 871.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 172.8383
      Episode_Reward/object_height: 0.0605
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 0.83s
                      Time elapsed: 00:19:24
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 108180 steps/s (collection: 0.819s, learning 0.090s)
             Mean action noise std: 5.17
          Mean value_function loss: 29.3998
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 24.0916
                       Mean reward: 869.24
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 0.7684
     Episode_Reward/lifting_object: 172.0042
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 0.91s
                      Time elapsed: 00:19:25
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 113545 steps/s (collection: 0.760s, learning 0.106s)
             Mean action noise std: 5.18
          Mean value_function loss: 34.0954
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 24.1007
                       Mean reward: 860.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7759
     Episode_Reward/lifting_object: 173.3410
      Episode_Reward/object_height: 0.0609
        Episode_Reward/action_rate: -0.0687
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 0.87s
                      Time elapsed: 00:19:26
                               ETA: 00:13:46

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 98474 steps/s (collection: 0.845s, learning 0.154s)
             Mean action noise std: 5.18
          Mean value_function loss: 31.9141
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 24.1084
                       Mean reward: 877.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7713
     Episode_Reward/lifting_object: 172.8004
      Episode_Reward/object_height: 0.0607
        Episode_Reward/action_rate: -0.0688
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 1.00s
                      Time elapsed: 00:19:27
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 108523 steps/s (collection: 0.805s, learning 0.101s)
             Mean action noise std: 5.18
          Mean value_function loss: 31.4708
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.1098
                       Mean reward: 864.90
               Mean episode length: 249.81
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 173.7461
      Episode_Reward/object_height: 0.0611
        Episode_Reward/action_rate: -0.0693
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 0.91s
                      Time elapsed: 00:19:28
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 108083 steps/s (collection: 0.785s, learning 0.124s)
             Mean action noise std: 5.19
          Mean value_function loss: 36.0426
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 24.1141
                       Mean reward: 844.79
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 170.7256
      Episode_Reward/object_height: 0.0600
        Episode_Reward/action_rate: -0.0692
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 0.91s
                      Time elapsed: 00:19:28
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 109357 steps/s (collection: 0.803s, learning 0.096s)
             Mean action noise std: 5.20
          Mean value_function loss: 33.8995
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 24.1248
                       Mean reward: 863.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 171.7448
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 0.90s
                      Time elapsed: 00:19:29
                               ETA: 00:13:42

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 109812 steps/s (collection: 0.782s, learning 0.114s)
             Mean action noise std: 5.20
          Mean value_function loss: 42.5089
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 24.1378
                       Mean reward: 859.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7734
     Episode_Reward/lifting_object: 172.0412
      Episode_Reward/object_height: 0.0602
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 0.90s
                      Time elapsed: 00:19:30
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 112791 steps/s (collection: 0.764s, learning 0.108s)
             Mean action noise std: 5.21
          Mean value_function loss: 31.4149
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 24.1483
                       Mean reward: 872.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7750
     Episode_Reward/lifting_object: 172.8423
      Episode_Reward/object_height: 0.0604
        Episode_Reward/action_rate: -0.0692
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 0.87s
                      Time elapsed: 00:19:31
                               ETA: 00:13:40

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 109480 steps/s (collection: 0.764s, learning 0.134s)
             Mean action noise std: 5.22
          Mean value_function loss: 28.8042
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 24.1633
                       Mean reward: 856.63
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 170.6922
      Episode_Reward/object_height: 0.0595
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 0.90s
                      Time elapsed: 00:19:32
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 110031 steps/s (collection: 0.779s, learning 0.114s)
             Mean action noise std: 5.23
          Mean value_function loss: 34.2349
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 24.1746
                       Mean reward: 858.33
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 171.8107
      Episode_Reward/object_height: 0.0596
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 0.89s
                      Time elapsed: 00:19:33
                               ETA: 00:13:38

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 107438 steps/s (collection: 0.809s, learning 0.106s)
             Mean action noise std: 5.23
          Mean value_function loss: 28.1556
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 24.1856
                       Mean reward: 881.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7689
     Episode_Reward/lifting_object: 171.7137
      Episode_Reward/object_height: 0.0598
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 0.91s
                      Time elapsed: 00:19:34
                               ETA: 00:13:37

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 108000 steps/s (collection: 0.817s, learning 0.093s)
             Mean action noise std: 5.24
          Mean value_function loss: 30.3540
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 24.1922
                       Mean reward: 851.77
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 172.7954
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 0.91s
                      Time elapsed: 00:19:35
                               ETA: 00:13:35

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 112320 steps/s (collection: 0.779s, learning 0.097s)
             Mean action noise std: 5.24
          Mean value_function loss: 30.3282
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 24.2006
                       Mean reward: 850.24
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 171.6377
      Episode_Reward/object_height: 0.0601
        Episode_Reward/action_rate: -0.0704
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 0.88s
                      Time elapsed: 00:19:36
                               ETA: 00:13:34

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 113805 steps/s (collection: 0.775s, learning 0.089s)
             Mean action noise std: 5.25
          Mean value_function loss: 27.0883
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 24.2095
                       Mean reward: 864.49
               Mean episode length: 249.83
    Episode_Reward/reaching_object: 0.7682
     Episode_Reward/lifting_object: 171.9337
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0708
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 0.86s
                      Time elapsed: 00:19:36
                               ETA: 00:13:33

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 112235 steps/s (collection: 0.784s, learning 0.092s)
             Mean action noise std: 5.25
          Mean value_function loss: 29.7526
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 24.2161
                       Mean reward: 866.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 172.5382
      Episode_Reward/object_height: 0.0599
        Episode_Reward/action_rate: -0.0709
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 0.88s
                      Time elapsed: 00:19:37
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 114828 steps/s (collection: 0.758s, learning 0.099s)
             Mean action noise std: 5.26
          Mean value_function loss: 31.7247
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.2255
                       Mean reward: 877.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7749
     Episode_Reward/lifting_object: 173.3103
      Episode_Reward/object_height: 0.0603
        Episode_Reward/action_rate: -0.0714
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 0.86s
                      Time elapsed: 00:19:38
                               ETA: 00:13:31

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 102995 steps/s (collection: 0.782s, learning 0.173s)
             Mean action noise std: 5.26
          Mean value_function loss: 46.6660
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 24.2342
                       Mean reward: 863.80
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.7735
     Episode_Reward/lifting_object: 171.1606
      Episode_Reward/object_height: 0.0591
        Episode_Reward/action_rate: -0.0716
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 0.95s
                      Time elapsed: 00:19:39
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 110483 steps/s (collection: 0.788s, learning 0.102s)
             Mean action noise std: 5.27
          Mean value_function loss: 44.4084
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 24.2432
                       Mean reward: 856.56
               Mean episode length: 246.21
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 171.2442
      Episode_Reward/object_height: 0.0587
        Episode_Reward/action_rate: -0.0715
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 0.89s
                      Time elapsed: 00:19:40
                               ETA: 00:13:29

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 106938 steps/s (collection: 0.786s, learning 0.133s)
             Mean action noise std: 5.27
          Mean value_function loss: 31.2582
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 24.2499
                       Mean reward: 880.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7791
     Episode_Reward/lifting_object: 171.4579
      Episode_Reward/object_height: 0.0584
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 0.92s
                      Time elapsed: 00:19:41
                               ETA: 00:13:28

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 111372 steps/s (collection: 0.783s, learning 0.100s)
             Mean action noise std: 5.28
          Mean value_function loss: 26.1359
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 24.2590
                       Mean reward: 862.70
               Mean episode length: 249.53
    Episode_Reward/reaching_object: 0.7799
     Episode_Reward/lifting_object: 172.9563
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 0.88s
                      Time elapsed: 00:19:42
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 109358 steps/s (collection: 0.798s, learning 0.101s)
             Mean action noise std: 5.29
          Mean value_function loss: 44.8405
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 24.2664
                       Mean reward: 876.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7760
     Episode_Reward/lifting_object: 172.9680
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 0.90s
                      Time elapsed: 00:19:43
                               ETA: 00:13:26

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 112569 steps/s (collection: 0.753s, learning 0.120s)
             Mean action noise std: 5.29
          Mean value_function loss: 37.3451
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 24.2778
                       Mean reward: 861.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7774
     Episode_Reward/lifting_object: 172.4692
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 0.87s
                      Time elapsed: 00:19:44
                               ETA: 00:13:25

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 101081 steps/s (collection: 0.841s, learning 0.131s)
             Mean action noise std: 5.30
          Mean value_function loss: 27.4669
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 24.2882
                       Mean reward: 864.31
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.7733
     Episode_Reward/lifting_object: 170.7673
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 0.97s
                      Time elapsed: 00:19:45
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 103447 steps/s (collection: 0.806s, learning 0.145s)
             Mean action noise std: 5.31
          Mean value_function loss: 30.5357
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 24.3030
                       Mean reward: 853.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7772
     Episode_Reward/lifting_object: 173.5249
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0735
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 0.95s
                      Time elapsed: 00:19:46
                               ETA: 00:13:23

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 103690 steps/s (collection: 0.809s, learning 0.139s)
             Mean action noise std: 5.31
          Mean value_function loss: 37.9712
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 24.3099
                       Mean reward: 861.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 170.4878
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0739
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 0.95s
                      Time elapsed: 00:19:46
                               ETA: 00:13:22

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 95865 steps/s (collection: 0.832s, learning 0.194s)
             Mean action noise std: 5.32
          Mean value_function loss: 32.8287
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 24.3210
                       Mean reward: 839.94
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 170.9345
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0743
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 1.03s
                      Time elapsed: 00:19:48
                               ETA: 00:13:21

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 99150 steps/s (collection: 0.867s, learning 0.125s)
             Mean action noise std: 5.32
          Mean value_function loss: 40.3507
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 24.3305
                       Mean reward: 881.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7769
     Episode_Reward/lifting_object: 173.5070
      Episode_Reward/object_height: 0.0582
        Episode_Reward/action_rate: -0.0742
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 0.99s
                      Time elapsed: 00:19:49
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 106100 steps/s (collection: 0.810s, learning 0.117s)
             Mean action noise std: 5.33
          Mean value_function loss: 39.8249
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 24.3360
                       Mean reward: 860.80
               Mean episode length: 247.94
    Episode_Reward/reaching_object: 0.7711
     Episode_Reward/lifting_object: 171.4488
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 0.93s
                      Time elapsed: 00:19:49
                               ETA: 00:13:19

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 106858 steps/s (collection: 0.795s, learning 0.125s)
             Mean action noise std: 5.34
          Mean value_function loss: 28.5058
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 24.3490
                       Mean reward: 847.25
               Mean episode length: 247.93
    Episode_Reward/reaching_object: 0.7664
     Episode_Reward/lifting_object: 171.6302
      Episode_Reward/object_height: 0.0573
        Episode_Reward/action_rate: -0.0742
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 0.92s
                      Time elapsed: 00:19:50
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 98455 steps/s (collection: 0.817s, learning 0.181s)
             Mean action noise std: 5.35
          Mean value_function loss: 25.8920
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 24.3637
                       Mean reward: 864.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7796
     Episode_Reward/lifting_object: 174.1065
      Episode_Reward/object_height: 0.0580
        Episode_Reward/action_rate: -0.0746
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 1.00s
                      Time elapsed: 00:19:51
                               ETA: 00:13:17

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 99547 steps/s (collection: 0.863s, learning 0.124s)
             Mean action noise std: 5.36
          Mean value_function loss: 32.2878
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.3814
                       Mean reward: 871.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7767
     Episode_Reward/lifting_object: 173.3807
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0751
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 0.99s
                      Time elapsed: 00:19:52
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 105129 steps/s (collection: 0.799s, learning 0.137s)
             Mean action noise std: 5.36
          Mean value_function loss: 33.4182
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 24.3909
                       Mean reward: 876.31
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 171.2480
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0749
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 0.94s
                      Time elapsed: 00:19:53
                               ETA: 00:13:15

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 105829 steps/s (collection: 0.797s, learning 0.132s)
             Mean action noise std: 5.37
          Mean value_function loss: 30.4910
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.3966
                       Mean reward: 842.28
               Mean episode length: 248.47
    Episode_Reward/reaching_object: 0.7648
     Episode_Reward/lifting_object: 169.8935
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 0.93s
                      Time elapsed: 00:19:54
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 100141 steps/s (collection: 0.806s, learning 0.175s)
             Mean action noise std: 5.37
          Mean value_function loss: 35.1766
               Mean surrogate loss: 0.0171
                 Mean entropy loss: 24.4040
                       Mean reward: 863.80
               Mean episode length: 249.80
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 170.0891
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 0.98s
                      Time elapsed: 00:19:55
                               ETA: 00:13:13

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 103784 steps/s (collection: 0.807s, learning 0.141s)
             Mean action noise std: 5.37
          Mean value_function loss: 41.3895
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 24.4053
                       Mean reward: 867.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 172.7910
      Episode_Reward/object_height: 0.0576
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 0.95s
                      Time elapsed: 00:19:56
                               ETA: 00:13:12

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 87670 steps/s (collection: 0.893s, learning 0.228s)
             Mean action noise std: 5.38
          Mean value_function loss: 51.2410
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 24.4108
                       Mean reward: 866.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7810
     Episode_Reward/lifting_object: 173.0265
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0751
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 1.12s
                      Time elapsed: 00:19:57
                               ETA: 00:13:11

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 101719 steps/s (collection: 0.831s, learning 0.136s)
             Mean action noise std: 5.38
          Mean value_function loss: 53.4641
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.4190
                       Mean reward: 868.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7794
     Episode_Reward/lifting_object: 172.2576
      Episode_Reward/object_height: 0.0578
        Episode_Reward/action_rate: -0.0750
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 0.97s
                      Time elapsed: 00:19:58
                               ETA: 00:13:10

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 104407 steps/s (collection: 0.826s, learning 0.116s)
             Mean action noise std: 5.39
          Mean value_function loss: 54.3539
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 24.4288
                       Mean reward: 852.87
               Mean episode length: 247.06
    Episode_Reward/reaching_object: 0.7803
     Episode_Reward/lifting_object: 172.9681
      Episode_Reward/object_height: 0.0586
        Episode_Reward/action_rate: -0.0750
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 0.94s
                      Time elapsed: 00:19:59
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 109454 steps/s (collection: 0.792s, learning 0.106s)
             Mean action noise std: 5.40
          Mean value_function loss: 52.2107
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 24.4404
                       Mean reward: 845.08
               Mean episode length: 246.60
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 170.5473
      Episode_Reward/object_height: 0.0577
        Episode_Reward/action_rate: -0.0753
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 0.90s
                      Time elapsed: 00:20:00
                               ETA: 00:13:08

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 104704 steps/s (collection: 0.792s, learning 0.147s)
             Mean action noise std: 5.41
          Mean value_function loss: 44.7593
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 24.4597
                       Mean reward: 841.17
               Mean episode length: 249.33
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 170.5503
      Episode_Reward/object_height: 0.0581
        Episode_Reward/action_rate: -0.0749
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 0.94s
                      Time elapsed: 00:20:01
                               ETA: 00:13:07

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 86467 steps/s (collection: 0.927s, learning 0.210s)
             Mean action noise std: 5.41
          Mean value_function loss: 47.4842
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 24.4705
                       Mean reward: 869.76
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 171.4162
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 1.14s
                      Time elapsed: 00:20:02
                               ETA: 00:13:06

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 99184 steps/s (collection: 0.820s, learning 0.171s)
             Mean action noise std: 5.42
          Mean value_function loss: 50.6751
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 24.4751
                       Mean reward: 870.26
               Mean episode length: 249.20
    Episode_Reward/reaching_object: 0.7740
     Episode_Reward/lifting_object: 173.2278
      Episode_Reward/object_height: 0.0588
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 0.99s
                      Time elapsed: 00:20:03
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 94125 steps/s (collection: 0.854s, learning 0.191s)
             Mean action noise std: 5.43
          Mean value_function loss: 61.6646
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 24.4891
                       Mean reward: 851.19
               Mean episode length: 247.83
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 171.9355
      Episode_Reward/object_height: 0.0585
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 1.04s
                      Time elapsed: 00:20:04
                               ETA: 00:13:04

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 98358 steps/s (collection: 0.868s, learning 0.131s)
             Mean action noise std: 5.43
          Mean value_function loss: 40.3027
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 24.5007
                       Mean reward: 834.08
               Mean episode length: 249.81
    Episode_Reward/reaching_object: 0.7558
     Episode_Reward/lifting_object: 170.3668
      Episode_Reward/object_height: 0.0574
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 1.00s
                      Time elapsed: 00:20:05
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 102332 steps/s (collection: 0.828s, learning 0.133s)
             Mean action noise std: 5.44
          Mean value_function loss: 37.8681
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 24.5064
                       Mean reward: 858.02
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 0.7385
     Episode_Reward/lifting_object: 165.5717
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0757
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 0.96s
                      Time elapsed: 00:20:06
                               ETA: 00:13:02

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 95475 steps/s (collection: 0.889s, learning 0.141s)
             Mean action noise std: 5.45
          Mean value_function loss: 45.5275
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 24.5162
                       Mean reward: 851.29
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7529
     Episode_Reward/lifting_object: 167.5999
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0762
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 1.03s
                      Time elapsed: 00:20:07
                               ETA: 00:13:01

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 99808 steps/s (collection: 0.821s, learning 0.164s)
             Mean action noise std: 5.45
          Mean value_function loss: 36.3068
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 24.5295
                       Mean reward: 853.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 169.7490
      Episode_Reward/object_height: 0.0565
        Episode_Reward/action_rate: -0.0760
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 0.98s
                      Time elapsed: 00:20:08
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 100020 steps/s (collection: 0.834s, learning 0.149s)
             Mean action noise std: 5.46
          Mean value_function loss: 30.0669
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 24.5367
                       Mean reward: 854.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 170.9255
      Episode_Reward/object_height: 0.0568
        Episode_Reward/action_rate: -0.0764
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 0.98s
                      Time elapsed: 00:20:09
                               ETA: 00:12:59

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 95306 steps/s (collection: 0.858s, learning 0.173s)
             Mean action noise std: 5.47
          Mean value_function loss: 34.6610
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 24.5475
                       Mean reward: 850.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 170.9215
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0763
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 1.03s
                      Time elapsed: 00:20:10
                               ETA: 00:12:58

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 97770 steps/s (collection: 0.852s, learning 0.154s)
             Mean action noise std: 5.48
          Mean value_function loss: 39.8539
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 24.5623
                       Mean reward: 862.09
               Mean episode length: 247.87
    Episode_Reward/reaching_object: 0.7556
     Episode_Reward/lifting_object: 169.5927
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0766
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 1.01s
                      Time elapsed: 00:20:11
                               ETA: 00:12:57

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 101105 steps/s (collection: 0.806s, learning 0.166s)
             Mean action noise std: 5.48
          Mean value_function loss: 32.4249
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 24.5668
                       Mean reward: 866.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 171.5110
      Episode_Reward/object_height: 0.0570
        Episode_Reward/action_rate: -0.0763
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 0.97s
                      Time elapsed: 00:20:12
                               ETA: 00:12:56

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 99961 steps/s (collection: 0.825s, learning 0.158s)
             Mean action noise std: 5.49
          Mean value_function loss: 37.4446
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.5771
                       Mean reward: 857.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7616
     Episode_Reward/lifting_object: 169.8560
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0764
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 0.98s
                      Time elapsed: 00:20:13
                               ETA: 00:12:55

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 99894 steps/s (collection: 0.812s, learning 0.172s)
             Mean action noise std: 5.49
          Mean value_function loss: 30.6743
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 24.5909
                       Mean reward: 865.33
               Mean episode length: 248.62
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 171.1360
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 0.98s
                      Time elapsed: 00:20:14
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 94902 steps/s (collection: 0.855s, learning 0.181s)
             Mean action noise std: 5.50
          Mean value_function loss: 41.4867
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 24.5989
                       Mean reward: 858.98
               Mean episode length: 249.81
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 172.0604
      Episode_Reward/object_height: 0.0572
        Episode_Reward/action_rate: -0.0763
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 1.04s
                      Time elapsed: 00:20:15
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 96465 steps/s (collection: 0.880s, learning 0.139s)
             Mean action noise std: 5.51
          Mean value_function loss: 45.3751
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 24.6101
                       Mean reward: 861.17
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 170.8879
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0761
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 1.02s
                      Time elapsed: 00:20:16
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 102316 steps/s (collection: 0.839s, learning 0.122s)
             Mean action noise std: 5.52
          Mean value_function loss: 35.1782
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 24.6214
                       Mean reward: 871.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 171.7722
      Episode_Reward/object_height: 0.0569
        Episode_Reward/action_rate: -0.0768
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 0.96s
                      Time elapsed: 00:20:17
                               ETA: 00:12:51

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 101250 steps/s (collection: 0.814s, learning 0.157s)
             Mean action noise std: 5.53
          Mean value_function loss: 29.4805
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 24.6386
                       Mean reward: 875.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 171.3193
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 0.97s
                      Time elapsed: 00:20:18
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 92450 steps/s (collection: 0.852s, learning 0.211s)
             Mean action noise std: 5.53
          Mean value_function loss: 35.7225
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 24.6472
                       Mean reward: 859.61
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 171.1423
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0770
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 1.06s
                      Time elapsed: 00:20:19
                               ETA: 00:12:49

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 97969 steps/s (collection: 0.832s, learning 0.171s)
             Mean action noise std: 5.54
          Mean value_function loss: 35.2834
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 24.6563
                       Mean reward: 850.31
               Mean episode length: 246.32
    Episode_Reward/reaching_object: 0.7650
     Episode_Reward/lifting_object: 172.2114
      Episode_Reward/object_height: 0.0571
        Episode_Reward/action_rate: -0.0777
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 1.00s
                      Time elapsed: 00:20:20
                               ETA: 00:12:48

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 96035 steps/s (collection: 0.860s, learning 0.164s)
             Mean action noise std: 5.54
          Mean value_function loss: 33.6595
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 24.6688
                       Mean reward: 869.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 171.4768
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 1.02s
                      Time elapsed: 00:20:21
                               ETA: 00:12:47

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 84147 steps/s (collection: 1.002s, learning 0.166s)
             Mean action noise std: 5.55
          Mean value_function loss: 27.2885
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 24.6755
                       Mean reward: 846.73
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 169.2922
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 1.17s
                      Time elapsed: 00:20:22
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 92876 steps/s (collection: 0.880s, learning 0.178s)
             Mean action noise std: 5.55
          Mean value_function loss: 35.6341
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 24.6767
                       Mean reward: 846.02
               Mean episode length: 245.20
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 169.8396
      Episode_Reward/object_height: 0.0560
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 1.06s
                      Time elapsed: 00:20:23
                               ETA: 00:12:45

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 105829 steps/s (collection: 0.822s, learning 0.107s)
             Mean action noise std: 5.56
          Mean value_function loss: 34.7925
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 24.6813
                       Mean reward: 850.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 171.3369
      Episode_Reward/object_height: 0.0566
        Episode_Reward/action_rate: -0.0791
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 0.93s
                      Time elapsed: 00:20:24
                               ETA: 00:12:44

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 109409 steps/s (collection: 0.788s, learning 0.110s)
             Mean action noise std: 5.56
          Mean value_function loss: 36.1833
               Mean surrogate loss: 0.0142
                 Mean entropy loss: 24.6897
                       Mean reward: 850.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7750
     Episode_Reward/lifting_object: 173.2718
      Episode_Reward/object_height: 0.0575
        Episode_Reward/action_rate: -0.0798
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 0.90s
                      Time elapsed: 00:20:25
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 104239 steps/s (collection: 0.772s, learning 0.171s)
             Mean action noise std: 5.56
          Mean value_function loss: 58.1057
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 24.6914
                       Mean reward: 871.34
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.7747
     Episode_Reward/lifting_object: 172.7979
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0794
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 0.94s
                      Time elapsed: 00:20:26
                               ETA: 00:12:42

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 110669 steps/s (collection: 0.773s, learning 0.116s)
             Mean action noise std: 5.56
          Mean value_function loss: 51.3275
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 24.6933
                       Mean reward: 876.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 170.0596
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0795
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 0.89s
                      Time elapsed: 00:20:27
                               ETA: 00:12:41

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 111024 steps/s (collection: 0.782s, learning 0.104s)
             Mean action noise std: 5.57
          Mean value_function loss: 46.3850
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 24.6981
                       Mean reward: 859.97
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.7716
     Episode_Reward/lifting_object: 172.7994
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0791
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 0.89s
                      Time elapsed: 00:20:28
                               ETA: 00:12:40

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 112300 steps/s (collection: 0.766s, learning 0.110s)
             Mean action noise std: 5.57
          Mean value_function loss: 29.0355
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 24.7052
                       Mean reward: 875.52
               Mean episode length: 249.94
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 174.1187
      Episode_Reward/object_height: 0.0567
        Episode_Reward/action_rate: -0.0797
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 0.88s
                      Time elapsed: 00:20:29
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 104531 steps/s (collection: 0.793s, learning 0.148s)
             Mean action noise std: 5.58
          Mean value_function loss: 32.8855
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 24.7166
                       Mean reward: 868.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 170.8451
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0797
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 0.94s
                      Time elapsed: 00:20:30
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 95115 steps/s (collection: 0.882s, learning 0.151s)
             Mean action noise std: 5.59
          Mean value_function loss: 38.1598
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 24.7299
                       Mean reward: 878.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 171.1155
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0798
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 1.03s
                      Time elapsed: 00:20:31
                               ETA: 00:12:37

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 102614 steps/s (collection: 0.816s, learning 0.142s)
             Mean action noise std: 5.60
          Mean value_function loss: 30.9929
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 24.7456
                       Mean reward: 860.51
               Mean episode length: 249.91
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 171.1795
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0802
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 0.96s
                      Time elapsed: 00:20:32
                               ETA: 00:12:36

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 97921 steps/s (collection: 0.857s, learning 0.147s)
             Mean action noise std: 5.61
          Mean value_function loss: 34.2845
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 24.7604
                       Mean reward: 866.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: 172.0862
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 1.00s
                      Time elapsed: 00:20:33
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 103152 steps/s (collection: 0.841s, learning 0.112s)
             Mean action noise std: 5.62
          Mean value_function loss: 40.8690
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 24.7750
                       Mean reward: 863.51
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 173.0826
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 0.95s
                      Time elapsed: 00:20:34
                               ETA: 00:12:34

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 91636 steps/s (collection: 0.921s, learning 0.152s)
             Mean action noise std: 5.63
          Mean value_function loss: 36.8710
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 24.7930
                       Mean reward: 824.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7515
     Episode_Reward/lifting_object: 168.2637
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0817
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 1.07s
                      Time elapsed: 00:20:35
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 107793 steps/s (collection: 0.786s, learning 0.126s)
             Mean action noise std: 5.63
          Mean value_function loss: 39.5676
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 24.7992
                       Mean reward: 853.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 169.9880
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0816
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 0.91s
                      Time elapsed: 00:20:36
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 94571 steps/s (collection: 0.846s, learning 0.193s)
             Mean action noise std: 5.64
          Mean value_function loss: 35.5077
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 24.8069
                       Mean reward: 853.40
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7660
     Episode_Reward/lifting_object: 172.1698
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0822
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 1.04s
                      Time elapsed: 00:20:37
                               ETA: 00:12:31

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 107620 steps/s (collection: 0.794s, learning 0.120s)
             Mean action noise std: 5.66
          Mean value_function loss: 29.2059
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 24.8238
                       Mean reward: 861.67
               Mean episode length: 248.92
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 172.1895
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.0829
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 0.91s
                      Time elapsed: 00:20:38
                               ETA: 00:12:30

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 97192 steps/s (collection: 0.852s, learning 0.160s)
             Mean action noise std: 5.66
          Mean value_function loss: 27.8831
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 24.8381
                       Mean reward: 878.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7793
     Episode_Reward/lifting_object: 173.5543
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0829
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 1.01s
                      Time elapsed: 00:20:39
                               ETA: 00:12:29

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 101686 steps/s (collection: 0.848s, learning 0.119s)
             Mean action noise std: 5.67
          Mean value_function loss: 27.6062
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 24.8480
                       Mean reward: 862.40
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 171.6357
      Episode_Reward/object_height: 0.0545
        Episode_Reward/action_rate: -0.0831
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 0.97s
                      Time elapsed: 00:20:40
                               ETA: 00:12:28

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 104864 steps/s (collection: 0.803s, learning 0.135s)
             Mean action noise std: 5.68
          Mean value_function loss: 42.2400
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 24.8602
                       Mean reward: 855.51
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.7752
     Episode_Reward/lifting_object: 173.3588
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0833
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 0.94s
                      Time elapsed: 00:20:41
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 99531 steps/s (collection: 0.816s, learning 0.172s)
             Mean action noise std: 5.69
          Mean value_function loss: 32.0596
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 24.8805
                       Mean reward: 871.53
               Mean episode length: 248.97
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 170.4438
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.0837
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 0.99s
                      Time elapsed: 00:20:42
                               ETA: 00:12:26

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 101303 steps/s (collection: 0.865s, learning 0.105s)
             Mean action noise std: 5.71
          Mean value_function loss: 36.6193
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 24.9011
                       Mean reward: 854.08
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7628
     Episode_Reward/lifting_object: 169.9947
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0838
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 0.97s
                      Time elapsed: 00:20:43
                               ETA: 00:12:25

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 102503 steps/s (collection: 0.843s, learning 0.116s)
             Mean action noise std: 5.72
          Mean value_function loss: 42.9408
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 24.9194
                       Mean reward: 861.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7703
     Episode_Reward/lifting_object: 170.9315
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0842
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 0.96s
                      Time elapsed: 00:20:43
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 108516 steps/s (collection: 0.791s, learning 0.115s)
             Mean action noise std: 5.73
          Mean value_function loss: 36.5638
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 24.9278
                       Mean reward: 859.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 170.2433
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0844
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 0.91s
                      Time elapsed: 00:20:44
                               ETA: 00:12:23

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 96826 steps/s (collection: 0.843s, learning 0.172s)
             Mean action noise std: 5.74
          Mean value_function loss: 43.9176
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 24.9401
                       Mean reward: 877.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7762
     Episode_Reward/lifting_object: 172.7794
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0843
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 1.02s
                      Time elapsed: 00:20:45
                               ETA: 00:12:22

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 101389 steps/s (collection: 0.852s, learning 0.118s)
             Mean action noise std: 5.74
          Mean value_function loss: 39.4750
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 24.9507
                       Mean reward: 872.75
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.7752
     Episode_Reward/lifting_object: 173.4117
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0842
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 0.97s
                      Time elapsed: 00:20:46
                               ETA: 00:12:21

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 102581 steps/s (collection: 0.842s, learning 0.116s)
             Mean action noise std: 5.75
          Mean value_function loss: 42.8164
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 24.9614
                       Mean reward: 851.58
               Mean episode length: 247.41
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 172.5278
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0842
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 0.96s
                      Time elapsed: 00:20:47
                               ETA: 00:12:20

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 105157 steps/s (collection: 0.835s, learning 0.100s)
             Mean action noise std: 5.76
          Mean value_function loss: 39.4450
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 24.9740
                       Mean reward: 878.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7758
     Episode_Reward/lifting_object: 173.2305
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0848
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 0.93s
                      Time elapsed: 00:20:48
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 106694 steps/s (collection: 0.815s, learning 0.106s)
             Mean action noise std: 5.77
          Mean value_function loss: 39.4900
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 24.9845
                       Mean reward: 862.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 173.0542
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0852
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 0.92s
                      Time elapsed: 00:20:49
                               ETA: 00:12:18

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 103471 steps/s (collection: 0.818s, learning 0.132s)
             Mean action noise std: 5.78
          Mean value_function loss: 40.5747
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 25.0017
                       Mean reward: 877.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7776
     Episode_Reward/lifting_object: 174.2153
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 0.95s
                      Time elapsed: 00:20:50
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 95199 steps/s (collection: 0.863s, learning 0.169s)
             Mean action noise std: 5.78
          Mean value_function loss: 42.3145
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 25.0131
                       Mean reward: 860.48
               Mean episode length: 247.75
    Episode_Reward/reaching_object: 0.7635
     Episode_Reward/lifting_object: 170.9310
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 1.03s
                      Time elapsed: 00:20:51
                               ETA: 00:12:16

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 102078 steps/s (collection: 0.839s, learning 0.124s)
             Mean action noise std: 5.79
          Mean value_function loss: 37.6671
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 25.0234
                       Mean reward: 861.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 168.9444
      Episode_Reward/object_height: 0.0545
        Episode_Reward/action_rate: -0.0858
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 0.96s
                      Time elapsed: 00:20:52
                               ETA: 00:12:15

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 107736 steps/s (collection: 0.773s, learning 0.140s)
             Mean action noise std: 5.80
          Mean value_function loss: 46.9031
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 25.0392
                       Mean reward: 851.24
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7701
     Episode_Reward/lifting_object: 173.1106
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.0855
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 0.91s
                      Time elapsed: 00:20:53
                               ETA: 00:12:14

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 110027 steps/s (collection: 0.771s, learning 0.122s)
             Mean action noise std: 5.81
          Mean value_function loss: 40.8890
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.0510
                       Mean reward: 868.06
               Mean episode length: 246.54
    Episode_Reward/reaching_object: 0.7566
     Episode_Reward/lifting_object: 171.8914
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.0860
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 0.89s
                      Time elapsed: 00:20:54
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 107539 steps/s (collection: 0.793s, learning 0.121s)
             Mean action noise std: 5.82
          Mean value_function loss: 34.5869
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 25.0616
                       Mean reward: 852.44
               Mean episode length: 246.49
    Episode_Reward/reaching_object: 0.7589
     Episode_Reward/lifting_object: 169.6497
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0858
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 0.91s
                      Time elapsed: 00:20:55
                               ETA: 00:12:11

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 112485 steps/s (collection: 0.770s, learning 0.104s)
             Mean action noise std: 5.83
          Mean value_function loss: 33.0617
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 25.0724
                       Mean reward: 872.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 173.0216
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.0866
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 0.87s
                      Time elapsed: 00:20:56
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 102988 steps/s (collection: 0.863s, learning 0.092s)
             Mean action noise std: 5.83
          Mean value_function loss: 28.3519
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 25.0839
                       Mean reward: 842.29
               Mean episode length: 249.97
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 171.3565
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 0.95s
                      Time elapsed: 00:20:57
                               ETA: 00:12:09

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 110522 steps/s (collection: 0.789s, learning 0.100s)
             Mean action noise std: 5.84
          Mean value_function loss: 36.5450
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 25.0946
                       Mean reward: 865.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 171.1190
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0869
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 0.89s
                      Time elapsed: 00:20:58
                               ETA: 00:12:08

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 108871 steps/s (collection: 0.784s, learning 0.119s)
             Mean action noise std: 5.86
          Mean value_function loss: 40.9320
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 25.1145
                       Mean reward: 845.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 170.9281
      Episode_Reward/object_height: 0.0545
        Episode_Reward/action_rate: -0.0870
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 0.90s
                      Time elapsed: 00:20:58
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 109237 steps/s (collection: 0.805s, learning 0.095s)
             Mean action noise std: 5.87
          Mean value_function loss: 48.8642
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 25.1280
                       Mean reward: 866.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 171.4564
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 0.90s
                      Time elapsed: 00:20:59
                               ETA: 00:12:06

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 109062 steps/s (collection: 0.803s, learning 0.099s)
             Mean action noise std: 5.87
          Mean value_function loss: 44.6441
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 25.1354
                       Mean reward: 877.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 172.4037
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0877
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 0.90s
                      Time elapsed: 00:21:00
                               ETA: 00:12:05

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 94559 steps/s (collection: 0.860s, learning 0.180s)
             Mean action noise std: 5.88
          Mean value_function loss: 46.2420
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 25.1435
                       Mean reward: 862.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7640
     Episode_Reward/lifting_object: 170.4671
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 1.04s
                      Time elapsed: 00:21:01
                               ETA: 00:12:04

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 107978 steps/s (collection: 0.820s, learning 0.091s)
             Mean action noise std: 5.89
          Mean value_function loss: 48.6198
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 25.1543
                       Mean reward: 843.23
               Mean episode length: 247.90
    Episode_Reward/reaching_object: 0.7603
     Episode_Reward/lifting_object: 169.1320
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.0885
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 0.91s
                      Time elapsed: 00:21:02
                               ETA: 00:12:03

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 108260 steps/s (collection: 0.809s, learning 0.099s)
             Mean action noise std: 5.90
          Mean value_function loss: 36.5605
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 25.1676
                       Mean reward: 873.36
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7792
     Episode_Reward/lifting_object: 173.0403
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 0.91s
                      Time elapsed: 00:21:03
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 98116 steps/s (collection: 0.811s, learning 0.191s)
             Mean action noise std: 5.90
          Mean value_function loss: 35.7690
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 25.1759
                       Mean reward: 850.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 171.0210
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.0893
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 1.00s
                      Time elapsed: 00:21:04
                               ETA: 00:12:01

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 100310 steps/s (collection: 0.859s, learning 0.121s)
             Mean action noise std: 5.91
          Mean value_function loss: 41.0039
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.1868
                       Mean reward: 875.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7758
     Episode_Reward/lifting_object: 173.6879
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 0.98s
                      Time elapsed: 00:21:05
                               ETA: 00:12:00

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 104475 steps/s (collection: 0.804s, learning 0.137s)
             Mean action noise std: 5.92
          Mean value_function loss: 42.2351
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 25.2016
                       Mean reward: 865.82
               Mean episode length: 249.15
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 172.4096
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 0.94s
                      Time elapsed: 00:21:06
                               ETA: 00:11:59

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 98941 steps/s (collection: 0.898s, learning 0.096s)
             Mean action noise std: 5.92
          Mean value_function loss: 30.6776
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 25.2031
                       Mean reward: 872.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 171.9459
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.0902
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 0.99s
                      Time elapsed: 00:21:07
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 89367 steps/s (collection: 0.939s, learning 0.161s)
             Mean action noise std: 5.92
          Mean value_function loss: 46.9736
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 25.2074
                       Mean reward: 858.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 169.7380
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.0915
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 1.10s
                      Time elapsed: 00:21:08
                               ETA: 00:11:57

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 91683 steps/s (collection: 0.897s, learning 0.176s)
             Mean action noise std: 5.93
          Mean value_function loss: 33.4026
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 25.2138
                       Mean reward: 835.72
               Mean episode length: 245.43
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 169.8513
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.0912
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 1.07s
                      Time elapsed: 00:21:09
                               ETA: 00:11:56

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 98594 steps/s (collection: 0.866s, learning 0.131s)
             Mean action noise std: 5.93
          Mean value_function loss: 36.0614
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 25.2221
                       Mean reward: 879.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 172.3037
      Episode_Reward/object_height: 0.0527
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 1.00s
                      Time elapsed: 00:21:10
                               ETA: 00:11:55

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 91072 steps/s (collection: 0.924s, learning 0.156s)
             Mean action noise std: 5.94
          Mean value_function loss: 36.9449
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 25.2302
                       Mean reward: 868.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 170.5159
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.0925
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 1.08s
                      Time elapsed: 00:21:11
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 99453 steps/s (collection: 0.870s, learning 0.119s)
             Mean action noise std: 5.95
          Mean value_function loss: 39.9163
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 25.2420
                       Mean reward: 878.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7711
     Episode_Reward/lifting_object: 172.0987
      Episode_Reward/object_height: 0.0527
        Episode_Reward/action_rate: -0.0920
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 0.99s
                      Time elapsed: 00:21:12
                               ETA: 00:11:53

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 94257 steps/s (collection: 0.899s, learning 0.144s)
             Mean action noise std: 5.96
          Mean value_function loss: 27.4447
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 25.2500
                       Mean reward: 849.35
               Mean episode length: 248.91
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 172.0468
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 1.04s
                      Time elapsed: 00:21:13
                               ETA: 00:11:52

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 89480 steps/s (collection: 0.945s, learning 0.154s)
             Mean action noise std: 5.97
          Mean value_function loss: 25.1217
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 25.2618
                       Mean reward: 870.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 173.2771
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.0935
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 1.10s
                      Time elapsed: 00:21:14
                               ETA: 00:11:51

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 91535 steps/s (collection: 0.890s, learning 0.184s)
             Mean action noise std: 5.98
          Mean value_function loss: 26.3946
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 25.2755
                       Mean reward: 853.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 172.0013
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.0936
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 1.07s
                      Time elapsed: 00:21:16
                               ETA: 00:11:50

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 95104 steps/s (collection: 0.869s, learning 0.165s)
             Mean action noise std: 5.98
          Mean value_function loss: 30.3064
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 25.2901
                       Mean reward: 859.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 171.9593
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.0935
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 1.03s
                      Time elapsed: 00:21:17
                               ETA: 00:11:50

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 95999 steps/s (collection: 0.874s, learning 0.150s)
             Mean action noise std: 5.99
          Mean value_function loss: 36.2534
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 25.2974
                       Mean reward: 861.91
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 171.4508
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.0940
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 1.02s
                      Time elapsed: 00:21:18
                               ETA: 00:11:49

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 100381 steps/s (collection: 0.857s, learning 0.122s)
             Mean action noise std: 6.00
          Mean value_function loss: 50.9738
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 25.3101
                       Mean reward: 867.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7749
     Episode_Reward/lifting_object: 172.3255
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 0.98s
                      Time elapsed: 00:21:19
                               ETA: 00:11:48

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 99808 steps/s (collection: 0.846s, learning 0.139s)
             Mean action noise std: 6.01
          Mean value_function loss: 43.1856
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 25.3208
                       Mean reward: 852.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 171.5896
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.0949
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 0.98s
                      Time elapsed: 00:21:20
                               ETA: 00:11:47

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 93092 steps/s (collection: 0.889s, learning 0.167s)
             Mean action noise std: 6.02
          Mean value_function loss: 34.7693
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 25.3339
                       Mean reward: 857.50
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7778
     Episode_Reward/lifting_object: 172.6023
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.0956
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 1.06s
                      Time elapsed: 00:21:21
                               ETA: 00:11:46

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 103366 steps/s (collection: 0.856s, learning 0.095s)
             Mean action noise std: 6.02
          Mean value_function loss: 38.6029
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 25.3398
                       Mean reward: 868.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 170.8378
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.0957
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 0.95s
                      Time elapsed: 00:21:22
                               ETA: 00:11:45

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 100066 steps/s (collection: 0.861s, learning 0.122s)
             Mean action noise std: 6.03
          Mean value_function loss: 23.7349
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 25.3471
                       Mean reward: 858.34
               Mean episode length: 249.39
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 171.5647
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.0952
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 0.98s
                      Time elapsed: 00:21:23
                               ETA: 00:11:44

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 89537 steps/s (collection: 0.944s, learning 0.154s)
             Mean action noise std: 6.04
          Mean value_function loss: 25.1210
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 25.3649
                       Mean reward: 868.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7765
     Episode_Reward/lifting_object: 173.2318
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.0959
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 1.10s
                      Time elapsed: 00:21:24
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 79889 steps/s (collection: 1.051s, learning 0.180s)
             Mean action noise std: 6.05
          Mean value_function loss: 29.1741
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.3751
                       Mean reward: 875.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7762
     Episode_Reward/lifting_object: 173.0333
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0961
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 1.23s
                      Time elapsed: 00:21:25
                               ETA: 00:11:42

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 83520 steps/s (collection: 1.019s, learning 0.158s)
             Mean action noise std: 6.06
          Mean value_function loss: 31.9635
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 25.3865
                       Mean reward: 848.94
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.7710
     Episode_Reward/lifting_object: 170.9315
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.0959
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 1.18s
                      Time elapsed: 00:21:26
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 88622 steps/s (collection: 0.928s, learning 0.182s)
             Mean action noise std: 6.07
          Mean value_function loss: 28.0296
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.3972
                       Mean reward: 870.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 172.4048
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0963
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 1.11s
                      Time elapsed: 00:21:27
                               ETA: 00:11:40

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 83734 steps/s (collection: 0.998s, learning 0.176s)
             Mean action noise std: 6.07
          Mean value_function loss: 36.8629
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 25.4056
                       Mean reward: 868.45
               Mean episode length: 247.62
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 170.6525
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0970
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 1.17s
                      Time elapsed: 00:21:28
                               ETA: 00:11:39

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 90725 steps/s (collection: 0.919s, learning 0.165s)
             Mean action noise std: 6.08
          Mean value_function loss: 37.4245
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 25.4141
                       Mean reward: 856.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 171.3517
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.0977
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 1.08s
                      Time elapsed: 00:21:29
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 92910 steps/s (collection: 0.870s, learning 0.188s)
             Mean action noise std: 6.08
          Mean value_function loss: 23.7956
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 25.4192
                       Mean reward: 874.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7737
     Episode_Reward/lifting_object: 172.8066
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0980
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 1.06s
                      Time elapsed: 00:21:30
                               ETA: 00:11:37

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 87263 steps/s (collection: 1.003s, learning 0.124s)
             Mean action noise std: 6.09
          Mean value_function loss: 25.8152
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 25.4247
                       Mean reward: 871.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7772
     Episode_Reward/lifting_object: 173.6346
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0979
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 1.13s
                      Time elapsed: 00:21:32
                               ETA: 00:11:36

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 97764 steps/s (collection: 0.882s, learning 0.123s)
             Mean action noise std: 6.09
          Mean value_function loss: 30.9678
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 25.4304
                       Mean reward: 864.40
               Mean episode length: 248.66
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 172.7118
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.0982
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 1.01s
                      Time elapsed: 00:21:33
                               ETA: 00:11:35

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 88898 steps/s (collection: 0.948s, learning 0.158s)
             Mean action noise std: 6.10
          Mean value_function loss: 34.0911
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 25.4406
                       Mean reward: 877.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7768
     Episode_Reward/lifting_object: 173.8063
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.0985
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 1.11s
                      Time elapsed: 00:21:34
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 103472 steps/s (collection: 0.833s, learning 0.117s)
             Mean action noise std: 6.10
          Mean value_function loss: 25.7336
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 25.4469
                       Mean reward: 868.34
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7663
     Episode_Reward/lifting_object: 170.6247
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.0984
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 0.95s
                      Time elapsed: 00:21:35
                               ETA: 00:11:33

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 98090 steps/s (collection: 0.895s, learning 0.107s)
             Mean action noise std: 6.11
          Mean value_function loss: 39.8472
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 25.4530
                       Mean reward: 851.99
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.7663
     Episode_Reward/lifting_object: 170.5053
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.0993
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 1.00s
                      Time elapsed: 00:21:36
                               ETA: 00:11:32

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 92517 steps/s (collection: 0.845s, learning 0.217s)
             Mean action noise std: 6.11
          Mean value_function loss: 29.7425
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 25.4553
                       Mean reward: 856.34
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7733
     Episode_Reward/lifting_object: 172.4421
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0989
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 1.06s
                      Time elapsed: 00:21:37
                               ETA: 00:11:31

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 104819 steps/s (collection: 0.839s, learning 0.099s)
             Mean action noise std: 6.12
          Mean value_function loss: 30.2935
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 25.4626
                       Mean reward: 879.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 172.4485
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.0992
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 0.94s
                      Time elapsed: 00:21:38
                               ETA: 00:11:30

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 102602 steps/s (collection: 0.826s, learning 0.133s)
             Mean action noise std: 6.12
          Mean value_function loss: 42.4693
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 25.4705
                       Mean reward: 868.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7787
     Episode_Reward/lifting_object: 172.9904
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0999
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 0.96s
                      Time elapsed: 00:21:39
                               ETA: 00:11:29

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 106267 steps/s (collection: 0.788s, learning 0.138s)
             Mean action noise std: 6.13
          Mean value_function loss: 29.9108
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 25.4799
                       Mean reward: 854.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 172.0253
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.0997
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 0.93s
                      Time elapsed: 00:21:40
                               ETA: 00:11:28

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 101752 steps/s (collection: 0.854s, learning 0.112s)
             Mean action noise std: 6.13
          Mean value_function loss: 26.2651
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 25.4882
                       Mean reward: 871.63
               Mean episode length: 248.61
    Episode_Reward/reaching_object: 0.7763
     Episode_Reward/lifting_object: 173.1537
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0992
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 0.97s
                      Time elapsed: 00:21:40
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 112893 steps/s (collection: 0.764s, learning 0.107s)
             Mean action noise std: 6.14
          Mean value_function loss: 41.8816
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 25.4923
                       Mean reward: 870.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7713
     Episode_Reward/lifting_object: 172.3831
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.1000
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 0.87s
                      Time elapsed: 00:21:41
                               ETA: 00:11:26

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 102814 steps/s (collection: 0.830s, learning 0.127s)
             Mean action noise std: 6.15
          Mean value_function loss: 35.0585
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 25.5017
                       Mean reward: 841.62
               Mean episode length: 248.36
    Episode_Reward/reaching_object: 0.7627
     Episode_Reward/lifting_object: 170.1332
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.0994
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 0.96s
                      Time elapsed: 00:21:42
                               ETA: 00:11:25

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 110643 steps/s (collection: 0.788s, learning 0.101s)
             Mean action noise std: 6.16
          Mean value_function loss: 41.2667
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 25.5103
                       Mean reward: 877.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7738
     Episode_Reward/lifting_object: 173.0038
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.1000
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 0.89s
                      Time elapsed: 00:21:43
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 109231 steps/s (collection: 0.810s, learning 0.090s)
             Mean action noise std: 6.16
          Mean value_function loss: 40.3428
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 25.5178
                       Mean reward: 871.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7693
     Episode_Reward/lifting_object: 170.5604
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.1007
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 0.90s
                      Time elapsed: 00:21:44
                               ETA: 00:11:23

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 105131 steps/s (collection: 0.817s, learning 0.118s)
             Mean action noise std: 6.17
          Mean value_function loss: 33.2141
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 25.5232
                       Mean reward: 868.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 171.0087
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.0999
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 0.94s
                      Time elapsed: 00:21:45
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 106147 steps/s (collection: 0.816s, learning 0.110s)
             Mean action noise std: 6.17
          Mean value_function loss: 34.9809
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 25.5296
                       Mean reward: 857.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7749
     Episode_Reward/lifting_object: 172.5439
      Episode_Reward/object_height: 0.0545
        Episode_Reward/action_rate: -0.1000
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 0.93s
                      Time elapsed: 00:21:46
                               ETA: 00:11:21

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 108929 steps/s (collection: 0.789s, learning 0.113s)
             Mean action noise std: 6.18
          Mean value_function loss: 36.2704
               Mean surrogate loss: 0.0139
                 Mean entropy loss: 25.5338
                       Mean reward: 860.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7705
     Episode_Reward/lifting_object: 171.5656
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0999
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 0.90s
                      Time elapsed: 00:21:47
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 106573 steps/s (collection: 0.799s, learning 0.124s)
             Mean action noise std: 6.18
          Mean value_function loss: 46.1402
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 25.5348
                       Mean reward: 859.20
               Mean episode length: 247.47
    Episode_Reward/reaching_object: 0.7700
     Episode_Reward/lifting_object: 172.2599
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.0993
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 0.92s
                      Time elapsed: 00:21:48
                               ETA: 00:11:19

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 105365 steps/s (collection: 0.788s, learning 0.145s)
             Mean action noise std: 6.18
          Mean value_function loss: 53.9102
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 25.5381
                       Mean reward: 868.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7783
     Episode_Reward/lifting_object: 172.6808
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.1000
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 0.93s
                      Time elapsed: 00:21:49
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 99097 steps/s (collection: 0.830s, learning 0.162s)
             Mean action noise std: 6.18
          Mean value_function loss: 56.1800
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 25.5402
                       Mean reward: 860.93
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 0.7807
     Episode_Reward/lifting_object: 172.8387
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.0999
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 0.99s
                      Time elapsed: 00:21:50
                               ETA: 00:11:17

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 106053 steps/s (collection: 0.815s, learning 0.112s)
             Mean action noise std: 6.18
          Mean value_function loss: 61.3371
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 25.5443
                       Mean reward: 850.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 171.2603
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.0996
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 0.93s
                      Time elapsed: 00:21:51
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 104971 steps/s (collection: 0.820s, learning 0.116s)
             Mean action noise std: 6.19
          Mean value_function loss: 59.3539
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 25.5497
                       Mean reward: 837.90
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 169.9389
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.0996
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 0.94s
                      Time elapsed: 00:21:52
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 111841 steps/s (collection: 0.775s, learning 0.104s)
             Mean action noise std: 6.20
          Mean value_function loss: 54.9850
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 25.5601
                       Mean reward: 869.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 171.9994
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.0997
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 0.88s
                      Time elapsed: 00:21:52
                               ETA: 00:11:14

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 104288 steps/s (collection: 0.831s, learning 0.111s)
             Mean action noise std: 6.21
          Mean value_function loss: 58.1285
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 25.5745
                       Mean reward: 860.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 172.0958
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.1001
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 0.94s
                      Time elapsed: 00:21:53
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 106344 steps/s (collection: 0.807s, learning 0.117s)
             Mean action noise std: 6.21
          Mean value_function loss: 55.4572
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 25.5848
                       Mean reward: 856.05
               Mean episode length: 247.92
    Episode_Reward/reaching_object: 0.7713
     Episode_Reward/lifting_object: 172.8115
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.0997
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 0.92s
                      Time elapsed: 00:21:54
                               ETA: 00:11:12

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 100528 steps/s (collection: 0.849s, learning 0.129s)
             Mean action noise std: 6.22
          Mean value_function loss: 47.9724
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 25.5965
                       Mean reward: 868.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 169.9057
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.1003
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 0.98s
                      Time elapsed: 00:21:55
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 102873 steps/s (collection: 0.857s, learning 0.099s)
             Mean action noise std: 6.23
          Mean value_function loss: 45.2244
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.6031
                       Mean reward: 824.82
               Mean episode length: 248.62
    Episode_Reward/reaching_object: 0.7559
     Episode_Reward/lifting_object: 170.5160
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.1004
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 0.96s
                      Time elapsed: 00:21:56
                               ETA: 00:11:10

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 105641 steps/s (collection: 0.829s, learning 0.102s)
             Mean action noise std: 6.23
          Mean value_function loss: 71.7910
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 25.6085
                       Mean reward: 819.04
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.7319
     Episode_Reward/lifting_object: 163.9430
      Episode_Reward/object_height: 0.0516
        Episode_Reward/action_rate: -0.1015
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 0.93s
                      Time elapsed: 00:21:57
                               ETA: 00:11:09

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 99667 steps/s (collection: 0.889s, learning 0.098s)
             Mean action noise std: 6.24
          Mean value_function loss: 56.7921
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 25.6164
                       Mean reward: 855.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7436
     Episode_Reward/lifting_object: 168.4515
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.1009
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 0.99s
                      Time elapsed: 00:21:58
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 104411 steps/s (collection: 0.846s, learning 0.095s)
             Mean action noise std: 6.25
          Mean value_function loss: 56.4796
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 25.6290
                       Mean reward: 833.16
               Mean episode length: 247.39
    Episode_Reward/reaching_object: 0.7365
     Episode_Reward/lifting_object: 165.1899
      Episode_Reward/object_height: 0.0522
        Episode_Reward/action_rate: -0.1013
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 0.94s
                      Time elapsed: 00:21:59
                               ETA: 00:11:07

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 107190 steps/s (collection: 0.818s, learning 0.099s)
             Mean action noise std: 6.26
          Mean value_function loss: 46.0062
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 25.6420
                       Mean reward: 846.72
               Mean episode length: 247.07
    Episode_Reward/reaching_object: 0.7540
     Episode_Reward/lifting_object: 168.4805
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.1005
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 0.92s
                      Time elapsed: 00:22:00
                               ETA: 00:11:06

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 85159 steps/s (collection: 0.953s, learning 0.202s)
             Mean action noise std: 6.27
          Mean value_function loss: 40.5832
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 25.6528
                       Mean reward: 820.90
               Mean episode length: 249.58
    Episode_Reward/reaching_object: 0.7514
     Episode_Reward/lifting_object: 168.3367
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.1015
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 1.15s
                      Time elapsed: 00:22:01
                               ETA: 00:11:05

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 97818 steps/s (collection: 0.906s, learning 0.099s)
             Mean action noise std: 6.28
          Mean value_function loss: 43.3673
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 25.6643
                       Mean reward: 872.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 171.2674
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.1020
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 1.00s
                      Time elapsed: 00:22:02
                               ETA: 00:11:04

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 107382 steps/s (collection: 0.824s, learning 0.092s)
             Mean action noise std: 6.29
          Mean value_function loss: 40.5388
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 25.6775
                       Mean reward: 839.74
               Mean episode length: 249.17
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 169.0399
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.1020
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 0.92s
                      Time elapsed: 00:22:03
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 43776 steps/s (collection: 2.147s, learning 0.099s)
             Mean action noise std: 6.29
          Mean value_function loss: 37.9671
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 25.6867
                       Mean reward: 863.85
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 171.7994
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.1017
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 2.25s
                      Time elapsed: 00:22:05
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 31001 steps/s (collection: 3.042s, learning 0.129s)
             Mean action noise std: 6.30
          Mean value_function loss: 34.5862
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 25.6914
                       Mean reward: 849.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7622
     Episode_Reward/lifting_object: 170.5405
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.1032
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 3.17s
                      Time elapsed: 00:22:09
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 31396 steps/s (collection: 3.016s, learning 0.115s)
             Mean action noise std: 6.30
          Mean value_function loss: 30.8913
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 25.6957
                       Mean reward: 838.81
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 169.3538
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.1032
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 3.13s
                      Time elapsed: 00:22:12
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 30467 steps/s (collection: 3.091s, learning 0.135s)
             Mean action noise std: 6.31
          Mean value_function loss: 33.0029
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.7060
                       Mean reward: 861.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 172.2537
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.1037
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 3.23s
                      Time elapsed: 00:22:15
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 30450 steps/s (collection: 3.104s, learning 0.125s)
             Mean action noise std: 6.33
          Mean value_function loss: 32.7893
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 25.7247
                       Mean reward: 858.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7632
     Episode_Reward/lifting_object: 170.6962
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.1040
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 3.23s
                      Time elapsed: 00:22:18
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 29692 steps/s (collection: 3.166s, learning 0.145s)
             Mean action noise std: 6.33
          Mean value_function loss: 50.3632
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 25.7364
                       Mean reward: 876.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 172.2347
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.1036
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 3.31s
                      Time elapsed: 00:22:21
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 29400 steps/s (collection: 3.149s, learning 0.195s)
             Mean action noise std: 6.34
          Mean value_function loss: 35.9766
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 25.7430
                       Mean reward: 858.53
               Mean episode length: 248.52
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 171.3140
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.1048
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 3.34s
                      Time elapsed: 00:22:25
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 29821 steps/s (collection: 3.180s, learning 0.117s)
             Mean action noise std: 6.34
          Mean value_function loss: 40.4141
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 25.7525
                       Mean reward: 846.24
               Mean episode length: 249.28
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 171.7982
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.1055
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 3.30s
                      Time elapsed: 00:22:28
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 21491 steps/s (collection: 4.448s, learning 0.126s)
             Mean action noise std: 6.36
          Mean value_function loss: 37.2803
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.7669
                       Mean reward: 867.12
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 171.1298
      Episode_Reward/object_height: 0.0545
        Episode_Reward/action_rate: -0.1050
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 4.57s
                      Time elapsed: 00:22:33
                               ETA: 00:11:04

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 106728 steps/s (collection: 0.809s, learning 0.112s)
             Mean action noise std: 6.36
          Mean value_function loss: 38.0783
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 25.7831
                       Mean reward: 874.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7682
     Episode_Reward/lifting_object: 171.7695
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.1061
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 0.92s
                      Time elapsed: 00:22:34
                               ETA: 00:11:03

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 119607 steps/s (collection: 0.736s, learning 0.086s)
             Mean action noise std: 6.37
          Mean value_function loss: 39.0332
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 25.7886
                       Mean reward: 852.76
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.7754
     Episode_Reward/lifting_object: 173.1694
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.1063
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 0.82s
                      Time elapsed: 00:22:34
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 112040 steps/s (collection: 0.783s, learning 0.094s)
             Mean action noise std: 6.38
          Mean value_function loss: 53.8436
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 25.8011
                       Mean reward: 870.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7725
     Episode_Reward/lifting_object: 172.7418
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.1066
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 0.88s
                      Time elapsed: 00:22:35
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 109626 steps/s (collection: 0.773s, learning 0.124s)
             Mean action noise std: 6.40
          Mean value_function loss: 30.8563
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 25.8188
                       Mean reward: 871.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 170.4648
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.1070
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 0.90s
                      Time elapsed: 00:22:36
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 110331 steps/s (collection: 0.738s, learning 0.153s)
             Mean action noise std: 6.40
          Mean value_function loss: 41.2539
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 25.8324
                       Mean reward: 864.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 170.6584
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.1086
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 0.89s
                      Time elapsed: 00:22:37
                               ETA: 00:10:59

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 112047 steps/s (collection: 0.748s, learning 0.130s)
             Mean action noise std: 6.41
          Mean value_function loss: 29.2956
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 25.8375
                       Mean reward: 861.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 170.1115
      Episode_Reward/object_height: 0.0554
        Episode_Reward/action_rate: -0.1088
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 0.88s
                      Time elapsed: 00:22:38
                               ETA: 00:10:58

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 108650 steps/s (collection: 0.770s, learning 0.135s)
             Mean action noise std: 6.42
          Mean value_function loss: 42.1644
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 25.8495
                       Mean reward: 875.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7715
     Episode_Reward/lifting_object: 171.5679
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.1097
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 0.90s
                      Time elapsed: 00:22:39
                               ETA: 00:10:56

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 110090 steps/s (collection: 0.788s, learning 0.105s)
             Mean action noise std: 6.43
          Mean value_function loss: 40.5429
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 25.8615
                       Mean reward: 837.88
               Mean episode length: 246.28
    Episode_Reward/reaching_object: 0.7715
     Episode_Reward/lifting_object: 171.0567
      Episode_Reward/object_height: 0.0555
        Episode_Reward/action_rate: -0.1096
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 0.89s
                      Time elapsed: 00:22:40
                               ETA: 00:10:55

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 96352 steps/s (collection: 0.855s, learning 0.165s)
             Mean action noise std: 6.43
          Mean value_function loss: 41.4568
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.8670
                       Mean reward: 846.46
               Mean episode length: 247.54
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 170.9172
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.1092
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 1.02s
                      Time elapsed: 00:22:41
                               ETA: 00:10:54

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 106394 steps/s (collection: 0.818s, learning 0.106s)
             Mean action noise std: 6.43
          Mean value_function loss: 34.7675
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 25.8691
                       Mean reward: 848.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: 172.2794
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.1101
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 0.92s
                      Time elapsed: 00:22:42
                               ETA: 00:10:53

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 113032 steps/s (collection: 0.760s, learning 0.110s)
             Mean action noise std: 6.44
          Mean value_function loss: 31.2465
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 25.8724
                       Mean reward: 861.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 171.5240
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.1112
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 0.87s
                      Time elapsed: 00:22:43
                               ETA: 00:10:52

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 107670 steps/s (collection: 0.814s, learning 0.099s)
             Mean action noise std: 6.44
          Mean value_function loss: 38.6525
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 25.8807
                       Mean reward: 853.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 170.1221
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.1106
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 0.91s
                      Time elapsed: 00:22:43
                               ETA: 00:10:51

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 113291 steps/s (collection: 0.779s, learning 0.089s)
             Mean action noise std: 6.45
          Mean value_function loss: 34.9603
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.8895
                       Mean reward: 860.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7592
     Episode_Reward/lifting_object: 170.8415
      Episode_Reward/object_height: 0.0557
        Episode_Reward/action_rate: -0.1107
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 0.87s
                      Time elapsed: 00:22:44
                               ETA: 00:10:50

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 107994 steps/s (collection: 0.809s, learning 0.102s)
             Mean action noise std: 6.46
          Mean value_function loss: 31.5358
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 25.8994
                       Mean reward: 869.19
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.7740
     Episode_Reward/lifting_object: 173.0463
      Episode_Reward/object_height: 0.0563
        Episode_Reward/action_rate: -0.1112
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 0.91s
                      Time elapsed: 00:22:45
                               ETA: 00:10:49

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 112169 steps/s (collection: 0.776s, learning 0.100s)
             Mean action noise std: 6.46
          Mean value_function loss: 30.8520
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 25.9023
                       Mean reward: 874.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 172.4817
      Episode_Reward/object_height: 0.0562
        Episode_Reward/action_rate: -0.1123
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 0.88s
                      Time elapsed: 00:22:46
                               ETA: 00:10:48

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 113906 steps/s (collection: 0.766s, learning 0.097s)
             Mean action noise std: 6.46
          Mean value_function loss: 38.6013
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 25.9060
                       Mean reward: 862.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 172.8746
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.1128
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 0.86s
                      Time elapsed: 00:22:47
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 110068 steps/s (collection: 0.766s, learning 0.127s)
             Mean action noise std: 6.47
          Mean value_function loss: 36.2244
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.9124
                       Mean reward: 882.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7715
     Episode_Reward/lifting_object: 172.1708
      Episode_Reward/object_height: 0.0558
        Episode_Reward/action_rate: -0.1127
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 0.89s
                      Time elapsed: 00:22:48
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 113212 steps/s (collection: 0.773s, learning 0.096s)
             Mean action noise std: 6.47
          Mean value_function loss: 46.1176
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 25.9180
                       Mean reward: 859.28
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7641
     Episode_Reward/lifting_object: 170.9891
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.1134
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 0.87s
                      Time elapsed: 00:22:49
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 104605 steps/s (collection: 0.805s, learning 0.135s)
             Mean action noise std: 6.48
          Mean value_function loss: 34.4258
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 25.9241
                       Mean reward: 864.46
               Mean episode length: 247.95
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 169.2260
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.1122
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 0.94s
                      Time elapsed: 00:22:50
                               ETA: 00:10:44

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 104767 steps/s (collection: 0.842s, learning 0.096s)
             Mean action noise std: 6.49
          Mean value_function loss: 40.4878
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 25.9295
                       Mean reward: 877.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7786
     Episode_Reward/lifting_object: 174.6393
      Episode_Reward/object_height: 0.0561
        Episode_Reward/action_rate: -0.1134
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 0.94s
                      Time elapsed: 00:22:51
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 112749 steps/s (collection: 0.764s, learning 0.108s)
             Mean action noise std: 6.49
          Mean value_function loss: 34.0091
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 25.9368
                       Mean reward: 840.98
               Mean episode length: 247.70
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 171.2825
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.1145
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 0.87s
                      Time elapsed: 00:22:51
                               ETA: 00:10:42

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 110722 steps/s (collection: 0.775s, learning 0.113s)
             Mean action noise std: 6.50
          Mean value_function loss: 32.5793
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 25.9465
                       Mean reward: 855.30
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 171.1964
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.1140
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 0.89s
                      Time elapsed: 00:22:52
                               ETA: 00:10:41

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 107025 steps/s (collection: 0.803s, learning 0.115s)
             Mean action noise std: 6.51
          Mean value_function loss: 26.7339
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 25.9528
                       Mean reward: 863.98
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7706
     Episode_Reward/lifting_object: 171.6025
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.1146
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 0.92s
                      Time elapsed: 00:22:53
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 112411 steps/s (collection: 0.781s, learning 0.093s)
             Mean action noise std: 6.51
          Mean value_function loss: 44.4015
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 25.9638
                       Mean reward: 876.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7763
     Episode_Reward/lifting_object: 173.1512
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.1149
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 0.87s
                      Time elapsed: 00:22:54
                               ETA: 00:10:39

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 111936 steps/s (collection: 0.770s, learning 0.109s)
             Mean action noise std: 6.52
          Mean value_function loss: 27.8950
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 25.9693
                       Mean reward: 856.32
               Mean episode length: 247.45
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 170.9739
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.1141
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 0.88s
                      Time elapsed: 00:22:55
                               ETA: 00:10:37

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 115875 steps/s (collection: 0.745s, learning 0.103s)
             Mean action noise std: 6.53
          Mean value_function loss: 26.5228
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 25.9785
                       Mean reward: 880.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7724
     Episode_Reward/lifting_object: 172.3507
      Episode_Reward/object_height: 0.0545
        Episode_Reward/action_rate: -0.1141
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 0.85s
                      Time elapsed: 00:22:56
                               ETA: 00:10:36

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 115362 steps/s (collection: 0.755s, learning 0.097s)
             Mean action noise std: 6.53
          Mean value_function loss: 29.4261
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 25.9879
                       Mean reward: 874.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7756
     Episode_Reward/lifting_object: 171.9150
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.1147
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 0.85s
                      Time elapsed: 00:22:57
                               ETA: 00:10:35

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 113396 steps/s (collection: 0.757s, learning 0.110s)
             Mean action noise std: 6.54
          Mean value_function loss: 29.5404
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 25.9964
                       Mean reward: 878.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7774
     Episode_Reward/lifting_object: 173.7970
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.1148
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 0.87s
                      Time elapsed: 00:22:58
                               ETA: 00:10:34

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 109660 steps/s (collection: 0.796s, learning 0.100s)
             Mean action noise std: 6.55
          Mean value_function loss: 32.5237
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.0034
                       Mean reward: 860.64
               Mean episode length: 249.14
    Episode_Reward/reaching_object: 0.7743
     Episode_Reward/lifting_object: 171.8578
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.1152
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 0.90s
                      Time elapsed: 00:22:58
                               ETA: 00:10:33

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 111450 steps/s (collection: 0.790s, learning 0.092s)
             Mean action noise std: 6.56
          Mean value_function loss: 27.3863
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 26.0156
                       Mean reward: 855.15
               Mean episode length: 249.23
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 170.5455
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.1151
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 0.88s
                      Time elapsed: 00:22:59
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 114743 steps/s (collection: 0.761s, learning 0.096s)
             Mean action noise std: 6.56
          Mean value_function loss: 27.0695
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 26.0248
                       Mean reward: 874.03
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7773
     Episode_Reward/lifting_object: 171.6197
      Episode_Reward/object_height: 0.0538
        Episode_Reward/action_rate: -0.1162
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 0.86s
                      Time elapsed: 00:23:00
                               ETA: 00:10:31

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 111003 steps/s (collection: 0.771s, learning 0.115s)
             Mean action noise std: 6.58
          Mean value_function loss: 20.5330
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 26.0383
                       Mean reward: 861.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7750
     Episode_Reward/lifting_object: 173.1614
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.1169
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 0.89s
                      Time elapsed: 00:23:01
                               ETA: 00:10:30

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 110385 steps/s (collection: 0.772s, learning 0.118s)
             Mean action noise std: 6.59
          Mean value_function loss: 34.9119
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 26.0599
                       Mean reward: 875.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7760
     Episode_Reward/lifting_object: 173.9345
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.1166
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 0.89s
                      Time elapsed: 00:23:02
                               ETA: 00:10:29

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 111680 steps/s (collection: 0.727s, learning 0.153s)
             Mean action noise std: 6.60
          Mean value_function loss: 26.4713
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 26.0700
                       Mean reward: 867.10
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7760
     Episode_Reward/lifting_object: 172.0891
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.1172
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 0.88s
                      Time elapsed: 00:23:03
                               ETA: 00:10:28

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 115350 steps/s (collection: 0.765s, learning 0.087s)
             Mean action noise std: 6.61
          Mean value_function loss: 30.0977
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 26.0821
                       Mean reward: 872.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7743
     Episode_Reward/lifting_object: 172.3778
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.1172
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 0.85s
                      Time elapsed: 00:23:04
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 118160 steps/s (collection: 0.735s, learning 0.097s)
             Mean action noise std: 6.62
          Mean value_function loss: 40.2099
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 26.0922
                       Mean reward: 874.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 173.0668
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.1177
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 0.83s
                      Time elapsed: 00:23:05
                               ETA: 00:10:26

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 95794 steps/s (collection: 0.859s, learning 0.168s)
             Mean action noise std: 6.62
          Mean value_function loss: 36.2121
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 26.0958
                       Mean reward: 864.81
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.7672
     Episode_Reward/lifting_object: 172.1117
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.1171
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 1.03s
                      Time elapsed: 00:23:06
                               ETA: 00:10:25

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 107599 steps/s (collection: 0.815s, learning 0.099s)
             Mean action noise std: 6.62
          Mean value_function loss: 29.6024
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 26.1000
                       Mean reward: 869.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 171.8655
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.1173
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 0.91s
                      Time elapsed: 00:23:07
                               ETA: 00:10:24

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 113578 steps/s (collection: 0.772s, learning 0.094s)
             Mean action noise std: 6.63
          Mean value_function loss: 27.4019
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 26.1023
                       Mean reward: 883.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7791
     Episode_Reward/lifting_object: 173.9704
      Episode_Reward/object_height: 0.0538
        Episode_Reward/action_rate: -0.1180
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 0.87s
                      Time elapsed: 00:23:07
                               ETA: 00:10:23

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 112875 steps/s (collection: 0.782s, learning 0.089s)
             Mean action noise std: 6.64
          Mean value_function loss: 29.3993
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.1111
                       Mean reward: 867.89
               Mean episode length: 247.06
    Episode_Reward/reaching_object: 0.7717
     Episode_Reward/lifting_object: 172.3149
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.1177
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 0.87s
                      Time elapsed: 00:23:08
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 111524 steps/s (collection: 0.787s, learning 0.095s)
             Mean action noise std: 6.64
          Mean value_function loss: 20.0367
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 26.1214
                       Mean reward: 870.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7713
     Episode_Reward/lifting_object: 173.0634
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.1187
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 0.88s
                      Time elapsed: 00:23:09
                               ETA: 00:10:20

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 109946 steps/s (collection: 0.772s, learning 0.122s)
             Mean action noise std: 6.65
          Mean value_function loss: 23.8374
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 26.1315
                       Mean reward: 873.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7778
     Episode_Reward/lifting_object: 174.1882
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.1193
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 0.89s
                      Time elapsed: 00:23:10
                               ETA: 00:10:19

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 112062 steps/s (collection: 0.753s, learning 0.125s)
             Mean action noise std: 6.66
          Mean value_function loss: 27.5347
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 26.1437
                       Mean reward: 877.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7758
     Episode_Reward/lifting_object: 173.6503
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.1184
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 0.88s
                      Time elapsed: 00:23:11
                               ETA: 00:10:18

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 110454 steps/s (collection: 0.762s, learning 0.128s)
             Mean action noise std: 6.67
          Mean value_function loss: 33.8986
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 26.1541
                       Mean reward: 874.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7757
     Episode_Reward/lifting_object: 173.1012
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.1188
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 0.89s
                      Time elapsed: 00:23:12
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 113436 steps/s (collection: 0.769s, learning 0.098s)
             Mean action noise std: 6.68
          Mean value_function loss: 25.7920
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 26.1638
                       Mean reward: 844.48
               Mean episode length: 246.86
    Episode_Reward/reaching_object: 0.7636
     Episode_Reward/lifting_object: 170.5649
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.1188
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 0.87s
                      Time elapsed: 00:23:13
                               ETA: 00:10:16

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 119034 steps/s (collection: 0.731s, learning 0.095s)
             Mean action noise std: 6.69
          Mean value_function loss: 32.0584
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 26.1792
                       Mean reward: 869.30
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 173.1577
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.1186
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 0.83s
                      Time elapsed: 00:23:13
                               ETA: 00:10:15

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 114065 steps/s (collection: 0.760s, learning 0.102s)
             Mean action noise std: 6.70
          Mean value_function loss: 29.6604
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 26.1905
                       Mean reward: 849.28
               Mean episode length: 247.25
    Episode_Reward/reaching_object: 0.7711
     Episode_Reward/lifting_object: 171.7932
      Episode_Reward/object_height: 0.0527
        Episode_Reward/action_rate: -0.1191
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 0.86s
                      Time elapsed: 00:23:14
                               ETA: 00:10:14

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 109145 steps/s (collection: 0.778s, learning 0.123s)
             Mean action noise std: 6.71
          Mean value_function loss: 28.3722
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 26.2009
                       Mean reward: 870.91
               Mean episode length: 249.00
    Episode_Reward/reaching_object: 0.7809
     Episode_Reward/lifting_object: 173.5568
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.1199
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 0.90s
                      Time elapsed: 00:23:15
                               ETA: 00:10:13

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 103074 steps/s (collection: 0.829s, learning 0.125s)
             Mean action noise std: 6.72
          Mean value_function loss: 31.3207
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 26.2087
                       Mean reward: 866.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7755
     Episode_Reward/lifting_object: 172.9714
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.1202
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 0.95s
                      Time elapsed: 00:23:16
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 112928 steps/s (collection: 0.772s, learning 0.099s)
             Mean action noise std: 6.73
          Mean value_function loss: 23.9833
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 26.2214
                       Mean reward: 876.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7790
     Episode_Reward/lifting_object: 173.8582
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.1209
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 0.87s
                      Time elapsed: 00:23:17
                               ETA: 00:10:11

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 111096 steps/s (collection: 0.752s, learning 0.133s)
             Mean action noise std: 6.74
          Mean value_function loss: 27.7549
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 26.2329
                       Mean reward: 866.61
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.7830
     Episode_Reward/lifting_object: 173.8785
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.1207
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 0.88s
                      Time elapsed: 00:23:18
                               ETA: 00:10:10

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 110334 steps/s (collection: 0.771s, learning 0.120s)
             Mean action noise std: 6.75
          Mean value_function loss: 24.1491
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 26.2478
                       Mean reward: 872.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7808
     Episode_Reward/lifting_object: 173.8705
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.1215
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 0.89s
                      Time elapsed: 00:23:19
                               ETA: 00:10:09

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 108979 steps/s (collection: 0.784s, learning 0.118s)
             Mean action noise std: 6.75
          Mean value_function loss: 25.1459
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 26.2517
                       Mean reward: 874.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7776
     Episode_Reward/lifting_object: 173.0442
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.1220
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 0.90s
                      Time elapsed: 00:23:20
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 112598 steps/s (collection: 0.753s, learning 0.120s)
             Mean action noise std: 6.75
          Mean value_function loss: 20.8858
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.2548
                       Mean reward: 869.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7772
     Episode_Reward/lifting_object: 173.4500
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.1218
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 0.87s
                      Time elapsed: 00:23:21
                               ETA: 00:10:07

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 112270 steps/s (collection: 0.774s, learning 0.101s)
             Mean action noise std: 6.76
          Mean value_function loss: 33.1063
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 26.2564
                       Mean reward: 858.15
               Mean episode length: 247.25
    Episode_Reward/reaching_object: 0.7774
     Episode_Reward/lifting_object: 172.4987
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.1212
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 0.88s
                      Time elapsed: 00:23:22
                               ETA: 00:10:06

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 115810 steps/s (collection: 0.734s, learning 0.115s)
             Mean action noise std: 6.76
          Mean value_function loss: 26.1888
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 26.2598
                       Mean reward: 862.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7759
     Episode_Reward/lifting_object: 171.6873
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.1208
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 0.85s
                      Time elapsed: 00:23:22
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 115433 steps/s (collection: 0.760s, learning 0.092s)
             Mean action noise std: 6.77
          Mean value_function loss: 25.5672
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 26.2662
                       Mean reward: 877.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7819
     Episode_Reward/lifting_object: 173.0622
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.1211
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 0.85s
                      Time elapsed: 00:23:23
                               ETA: 00:10:04

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 113653 steps/s (collection: 0.765s, learning 0.100s)
             Mean action noise std: 6.78
          Mean value_function loss: 25.7914
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 26.2780
                       Mean reward: 866.88
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.7756
     Episode_Reward/lifting_object: 172.0049
      Episode_Reward/object_height: 0.0538
        Episode_Reward/action_rate: -0.1200
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 0.86s
                      Time elapsed: 00:23:24
                               ETA: 00:10:02

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 110284 steps/s (collection: 0.773s, learning 0.119s)
             Mean action noise std: 6.79
          Mean value_function loss: 31.3363
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 26.2935
                       Mean reward: 880.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7785
     Episode_Reward/lifting_object: 172.9006
      Episode_Reward/object_height: 0.0542
        Episode_Reward/action_rate: -0.1193
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 0.89s
                      Time elapsed: 00:23:25
                               ETA: 00:10:01

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 107087 steps/s (collection: 0.784s, learning 0.134s)
             Mean action noise std: 6.80
          Mean value_function loss: 24.2254
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.3033
                       Mean reward: 839.62
               Mean episode length: 246.83
    Episode_Reward/reaching_object: 0.7758
     Episode_Reward/lifting_object: 171.9259
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.1188
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 0.92s
                      Time elapsed: 00:23:26
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 115955 steps/s (collection: 0.748s, learning 0.099s)
             Mean action noise std: 6.80
          Mean value_function loss: 28.2220
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 26.3089
                       Mean reward: 864.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7783
     Episode_Reward/lifting_object: 172.6536
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.1193
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 0.85s
                      Time elapsed: 00:23:27
                               ETA: 00:09:59

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 113304 steps/s (collection: 0.767s, learning 0.101s)
             Mean action noise std: 6.81
          Mean value_function loss: 28.4826
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 26.3111
                       Mean reward: 863.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7819
     Episode_Reward/lifting_object: 173.4928
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.1186
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 0.87s
                      Time elapsed: 00:23:28
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 118873 steps/s (collection: 0.726s, learning 0.100s)
             Mean action noise std: 6.82
          Mean value_function loss: 28.8282
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 26.3189
                       Mean reward: 867.65
               Mean episode length: 248.56
    Episode_Reward/reaching_object: 0.7823
     Episode_Reward/lifting_object: 173.8438
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.1193
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 0.83s
                      Time elapsed: 00:23:28
                               ETA: 00:09:57

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 114895 steps/s (collection: 0.768s, learning 0.088s)
             Mean action noise std: 6.82
          Mean value_function loss: 22.5278
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 26.3298
                       Mean reward: 865.29
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7868
     Episode_Reward/lifting_object: 173.4080
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.1189
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 0.86s
                      Time elapsed: 00:23:29
                               ETA: 00:09:56

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 112291 steps/s (collection: 0.769s, learning 0.107s)
             Mean action noise std: 6.83
          Mean value_function loss: 38.6211
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 26.3370
                       Mean reward: 875.63
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.7800
     Episode_Reward/lifting_object: 173.0138
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.1199
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 0.88s
                      Time elapsed: 00:23:30
                               ETA: 00:09:55

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 110649 steps/s (collection: 0.770s, learning 0.118s)
             Mean action noise std: 6.84
          Mean value_function loss: 26.2960
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 26.3457
                       Mean reward: 874.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7790
     Episode_Reward/lifting_object: 173.2549
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.1199
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 0.89s
                      Time elapsed: 00:23:31
                               ETA: 00:09:54

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 115874 steps/s (collection: 0.735s, learning 0.114s)
             Mean action noise std: 6.86
          Mean value_function loss: 29.4575
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 26.3626
                       Mean reward: 870.67
               Mean episode length: 248.92
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 172.3472
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.1197
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 0.85s
                      Time elapsed: 00:23:32
                               ETA: 00:09:53

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 111561 steps/s (collection: 0.755s, learning 0.127s)
             Mean action noise std: 6.86
          Mean value_function loss: 37.2545
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 26.3787
                       Mean reward: 874.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7847
     Episode_Reward/lifting_object: 173.7093
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.1214
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 0.88s
                      Time elapsed: 00:23:33
                               ETA: 00:09:52

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 115663 steps/s (collection: 0.747s, learning 0.103s)
             Mean action noise std: 6.87
          Mean value_function loss: 29.0708
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 26.3838
                       Mean reward: 867.95
               Mean episode length: 249.21
    Episode_Reward/reaching_object: 0.7797
     Episode_Reward/lifting_object: 173.4387
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.1211
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 0.85s
                      Time elapsed: 00:23:34
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 114674 steps/s (collection: 0.739s, learning 0.119s)
             Mean action noise std: 6.87
          Mean value_function loss: 37.1012
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 26.3900
                       Mean reward: 872.02
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7759
     Episode_Reward/lifting_object: 171.7375
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.1222
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 0.86s
                      Time elapsed: 00:23:34
                               ETA: 00:09:50

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 106806 steps/s (collection: 0.814s, learning 0.106s)
             Mean action noise std: 6.88
          Mean value_function loss: 30.7482
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 26.3936
                       Mean reward: 859.94
               Mean episode length: 246.73
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 170.7989
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.1217
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 0.92s
                      Time elapsed: 00:23:35
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 114022 steps/s (collection: 0.776s, learning 0.086s)
             Mean action noise std: 6.88
          Mean value_function loss: 27.1057
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 26.4014
                       Mean reward: 877.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7725
     Episode_Reward/lifting_object: 172.4664
      Episode_Reward/object_height: 0.0545
        Episode_Reward/action_rate: -0.1230
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 0.86s
                      Time elapsed: 00:23:36
                               ETA: 00:09:48

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 116424 steps/s (collection: 0.753s, learning 0.092s)
             Mean action noise std: 6.89
          Mean value_function loss: 30.7333
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 26.4065
                       Mean reward: 857.76
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 170.9321
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.1237
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 0.84s
                      Time elapsed: 00:23:37
                               ETA: 00:09:47

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 110983 steps/s (collection: 0.751s, learning 0.135s)
             Mean action noise std: 6.90
          Mean value_function loss: 27.8618
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 26.4126
                       Mean reward: 854.79
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 170.9778
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.1236
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 0.89s
                      Time elapsed: 00:23:38
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 112054 steps/s (collection: 0.761s, learning 0.117s)
             Mean action noise std: 6.91
          Mean value_function loss: 26.5778
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 26.4234
                       Mean reward: 853.08
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 172.7645
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.1246
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 0.88s
                      Time elapsed: 00:23:39
                               ETA: 00:09:44

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 117662 steps/s (collection: 0.729s, learning 0.107s)
             Mean action noise std: 6.92
          Mean value_function loss: 31.1742
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 26.4350
                       Mean reward: 861.43
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 173.9297
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.1241
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 0.84s
                      Time elapsed: 00:23:40
                               ETA: 00:09:43

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 113989 steps/s (collection: 0.768s, learning 0.094s)
             Mean action noise std: 6.93
          Mean value_function loss: 34.1986
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 26.4518
                       Mean reward: 854.54
               Mean episode length: 246.17
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 170.8685
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.1244
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 0.86s
                      Time elapsed: 00:23:41
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 114446 steps/s (collection: 0.758s, learning 0.101s)
             Mean action noise std: 6.93
          Mean value_function loss: 30.9929
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 26.4616
                       Mean reward: 873.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7789
     Episode_Reward/lifting_object: 172.1466
      Episode_Reward/object_height: 0.0545
        Episode_Reward/action_rate: -0.1238
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 0.86s
                      Time elapsed: 00:23:41
                               ETA: 00:09:41

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 111652 steps/s (collection: 0.776s, learning 0.104s)
             Mean action noise std: 6.94
          Mean value_function loss: 40.1470
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 26.4668
                       Mean reward: 855.54
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.7713
     Episode_Reward/lifting_object: 171.3657
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.1238
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 0.88s
                      Time elapsed: 00:23:42
                               ETA: 00:09:40

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 115865 steps/s (collection: 0.748s, learning 0.100s)
             Mean action noise std: 6.94
          Mean value_function loss: 32.8320
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 26.4721
                       Mean reward: 856.53
               Mean episode length: 244.94
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 173.3575
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.1238
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 0.85s
                      Time elapsed: 00:23:43
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 111658 steps/s (collection: 0.766s, learning 0.115s)
             Mean action noise std: 6.95
          Mean value_function loss: 41.3658
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.4818
                       Mean reward: 880.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 171.7937
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.1241
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 0.88s
                      Time elapsed: 00:23:44
                               ETA: 00:09:38

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 110196 steps/s (collection: 0.758s, learning 0.134s)
             Mean action noise std: 6.96
          Mean value_function loss: 40.6430
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 26.4896
                       Mean reward: 853.68
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 170.3053
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.1234
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 0.89s
                      Time elapsed: 00:23:45
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 119604 steps/s (collection: 0.735s, learning 0.087s)
             Mean action noise std: 6.96
          Mean value_function loss: 30.0113
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.4929
                       Mean reward: 864.28
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.7693
     Episode_Reward/lifting_object: 171.8993
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.1233
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 0.82s
                      Time elapsed: 00:23:46
                               ETA: 00:09:36

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 117403 steps/s (collection: 0.745s, learning 0.092s)
             Mean action noise std: 6.97
          Mean value_function loss: 28.5355
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 26.4972
                       Mean reward: 878.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7747
     Episode_Reward/lifting_object: 172.9963
      Episode_Reward/object_height: 0.0546
        Episode_Reward/action_rate: -0.1241
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 0.84s
                      Time elapsed: 00:23:47
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 114184 steps/s (collection: 0.758s, learning 0.103s)
             Mean action noise std: 6.98
          Mean value_function loss: 33.3986
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.5079
                       Mean reward: 858.63
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 0.7576
     Episode_Reward/lifting_object: 171.5826
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.1240
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 0.86s
                      Time elapsed: 00:23:47
                               ETA: 00:09:34

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 111687 steps/s (collection: 0.780s, learning 0.100s)
             Mean action noise std: 7.00
          Mean value_function loss: 39.7751
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.5266
                       Mean reward: 858.59
               Mean episode length: 249.12
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 171.1778
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.1237
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 0.88s
                      Time elapsed: 00:23:48
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 99615 steps/s (collection: 0.817s, learning 0.169s)
             Mean action noise std: 7.01
          Mean value_function loss: 33.5640
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.5422
                       Mean reward: 861.17
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 171.4488
      Episode_Reward/object_height: 0.0538
        Episode_Reward/action_rate: -0.1236
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 0.99s
                      Time elapsed: 00:23:49
                               ETA: 00:09:32

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 116685 steps/s (collection: 0.749s, learning 0.094s)
             Mean action noise std: 7.02
          Mean value_function loss: 41.1882
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 26.5513
                       Mean reward: 840.06
               Mean episode length: 249.03
    Episode_Reward/reaching_object: 0.7670
     Episode_Reward/lifting_object: 171.3388
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.1235
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 0.84s
                      Time elapsed: 00:23:50
                               ETA: 00:09:31

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 110864 steps/s (collection: 0.773s, learning 0.114s)
             Mean action noise std: 7.02
          Mean value_function loss: 25.9720
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 26.5597
                       Mean reward: 860.04
               Mean episode length: 247.17
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 172.3347
      Episode_Reward/object_height: 0.0538
        Episode_Reward/action_rate: -0.1239
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 0.89s
                      Time elapsed: 00:23:51
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 115342 steps/s (collection: 0.753s, learning 0.099s)
             Mean action noise std: 7.04
          Mean value_function loss: 29.2734
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.5726
                       Mean reward: 872.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 171.9341
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.1240
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 0.85s
                      Time elapsed: 00:23:52
                               ETA: 00:09:29

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 113933 steps/s (collection: 0.733s, learning 0.130s)
             Mean action noise std: 7.05
          Mean value_function loss: 25.9686
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 26.5875
                       Mean reward: 871.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: 172.7698
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.1249
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 0.86s
                      Time elapsed: 00:23:53
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 114336 steps/s (collection: 0.774s, learning 0.086s)
             Mean action noise std: 7.05
          Mean value_function loss: 29.5333
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 26.5959
                       Mean reward: 871.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 172.4649
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.1250
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 0.86s
                      Time elapsed: 00:23:54
                               ETA: 00:09:27

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 101957 steps/s (collection: 0.817s, learning 0.147s)
             Mean action noise std: 7.06
          Mean value_function loss: 27.1165
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 26.6056
                       Mean reward: 876.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7742
     Episode_Reward/lifting_object: 173.6321
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.1246
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 0.96s
                      Time elapsed: 00:23:55
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 105124 steps/s (collection: 0.841s, learning 0.094s)
             Mean action noise std: 7.07
          Mean value_function loss: 22.2355
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 26.6168
                       Mean reward: 858.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 171.3945
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.1258
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 0.94s
                      Time elapsed: 00:23:56
                               ETA: 00:09:25

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 112567 steps/s (collection: 0.786s, learning 0.087s)
             Mean action noise std: 7.08
          Mean value_function loss: 32.6377
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 26.6290
                       Mean reward: 877.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7793
     Episode_Reward/lifting_object: 173.9397
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.1259
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 0.87s
                      Time elapsed: 00:23:56
                               ETA: 00:09:23

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 114134 steps/s (collection: 0.774s, learning 0.088s)
             Mean action noise std: 7.09
          Mean value_function loss: 42.5332
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 26.6355
                       Mean reward: 879.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7820
     Episode_Reward/lifting_object: 173.3806
      Episode_Reward/object_height: 0.0527
        Episode_Reward/action_rate: -0.1264
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 0.86s
                      Time elapsed: 00:23:57
                               ETA: 00:09:22

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 110051 steps/s (collection: 0.791s, learning 0.103s)
             Mean action noise std: 7.10
          Mean value_function loss: 30.4489
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 26.6432
                       Mean reward: 869.40
               Mean episode length: 246.79
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 170.2927
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.1264
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 0.89s
                      Time elapsed: 00:23:58
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 116633 steps/s (collection: 0.745s, learning 0.098s)
             Mean action noise std: 7.11
          Mean value_function loss: 39.2424
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 26.6557
                       Mean reward: 863.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7844
     Episode_Reward/lifting_object: 173.5410
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.1272
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 0.84s
                      Time elapsed: 00:23:59
                               ETA: 00:09:20

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 107804 steps/s (collection: 0.749s, learning 0.163s)
             Mean action noise std: 7.11
          Mean value_function loss: 45.2622
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 26.6594
                       Mean reward: 851.38
               Mean episode length: 247.91
    Episode_Reward/reaching_object: 0.7734
     Episode_Reward/lifting_object: 171.4728
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.1277
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 0.91s
                      Time elapsed: 00:24:00
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 109301 steps/s (collection: 0.782s, learning 0.117s)
             Mean action noise std: 7.11
          Mean value_function loss: 31.3559
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 26.6620
                       Mean reward: 858.02
               Mean episode length: 246.84
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 170.8099
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.1267
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 0.90s
                      Time elapsed: 00:24:01
                               ETA: 00:09:18

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 117332 steps/s (collection: 0.748s, learning 0.090s)
             Mean action noise std: 7.12
          Mean value_function loss: 36.0357
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.6722
                       Mean reward: 875.26
               Mean episode length: 248.93
    Episode_Reward/reaching_object: 0.7743
     Episode_Reward/lifting_object: 172.1941
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.1281
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 0.84s
                      Time elapsed: 00:24:02
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 108097 steps/s (collection: 0.799s, learning 0.111s)
             Mean action noise std: 7.13
          Mean value_function loss: 34.9001
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 26.6841
                       Mean reward: 868.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7758
     Episode_Reward/lifting_object: 172.1071
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.1289
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 0.91s
                      Time elapsed: 00:24:03
                               ETA: 00:09:16

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 113974 steps/s (collection: 0.769s, learning 0.094s)
             Mean action noise std: 7.14
          Mean value_function loss: 42.3311
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 26.6910
                       Mean reward: 847.18
               Mean episode length: 246.21
    Episode_Reward/reaching_object: 0.7713
     Episode_Reward/lifting_object: 171.7198
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.1285
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 0.86s
                      Time elapsed: 00:24:03
                               ETA: 00:09:15

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 108243 steps/s (collection: 0.816s, learning 0.093s)
             Mean action noise std: 7.15
          Mean value_function loss: 38.9456
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 26.7014
                       Mean reward: 863.97
               Mean episode length: 249.94
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 171.6802
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.1297
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 0.91s
                      Time elapsed: 00:24:04
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 115566 steps/s (collection: 0.741s, learning 0.110s)
             Mean action noise std: 7.15
          Mean value_function loss: 42.9496
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 26.7125
                       Mean reward: 850.79
               Mean episode length: 247.60
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 171.6170
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.1293
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 0.85s
                      Time elapsed: 00:24:05
                               ETA: 00:09:13

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 101881 steps/s (collection: 0.817s, learning 0.148s)
             Mean action noise std: 7.16
          Mean value_function loss: 50.0629
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.7185
                       Mean reward: 867.29
               Mean episode length: 248.71
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 172.1441
      Episode_Reward/object_height: 0.0527
        Episode_Reward/action_rate: -0.1294
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 0.96s
                      Time elapsed: 00:24:06
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 104012 steps/s (collection: 0.829s, learning 0.116s)
             Mean action noise std: 7.17
          Mean value_function loss: 45.1547
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 26.7266
                       Mean reward: 843.92
               Mean episode length: 248.79
    Episode_Reward/reaching_object: 0.7715
     Episode_Reward/lifting_object: 171.9676
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.1295
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 0.95s
                      Time elapsed: 00:24:07
                               ETA: 00:09:11

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 105799 steps/s (collection: 0.790s, learning 0.139s)
             Mean action noise std: 7.18
          Mean value_function loss: 35.8558
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 26.7355
                       Mean reward: 863.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 172.8691
      Episode_Reward/object_height: 0.0518
        Episode_Reward/action_rate: -0.1304
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 0.93s
                      Time elapsed: 00:24:08
                               ETA: 00:09:10

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 104554 steps/s (collection: 0.791s, learning 0.150s)
             Mean action noise std: 7.18
          Mean value_function loss: 47.2962
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 26.7444
                       Mean reward: 855.57
               Mean episode length: 248.71
    Episode_Reward/reaching_object: 0.7790
     Episode_Reward/lifting_object: 172.2051
      Episode_Reward/object_height: 0.0518
        Episode_Reward/action_rate: -0.1302
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 0.94s
                      Time elapsed: 00:24:09
                               ETA: 00:09:09

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 105019 steps/s (collection: 0.813s, learning 0.123s)
             Mean action noise std: 7.19
          Mean value_function loss: 41.6993
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 26.7489
                       Mean reward: 876.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 172.9883
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.1302
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 0.94s
                      Time elapsed: 00:24:10
                               ETA: 00:09:08

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 110125 steps/s (collection: 0.752s, learning 0.141s)
             Mean action noise std: 7.19
          Mean value_function loss: 32.1725
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 26.7574
                       Mean reward: 851.92
               Mean episode length: 246.63
    Episode_Reward/reaching_object: 0.7682
     Episode_Reward/lifting_object: 170.5895
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.1302
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 0.89s
                      Time elapsed: 00:24:11
                               ETA: 00:09:07

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 107574 steps/s (collection: 0.815s, learning 0.099s)
             Mean action noise std: 7.20
          Mean value_function loss: 40.5091
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 26.7661
                       Mean reward: 837.34
               Mean episode length: 246.82
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 167.8852
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.1318
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 0.91s
                      Time elapsed: 00:24:12
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 101677 steps/s (collection: 0.805s, learning 0.162s)
             Mean action noise std: 7.20
          Mean value_function loss: 32.3567
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 26.7735
                       Mean reward: 836.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7700
     Episode_Reward/lifting_object: 170.5679
      Episode_Reward/object_height: 0.0506
        Episode_Reward/action_rate: -0.1321
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 0.97s
                      Time elapsed: 00:24:13
                               ETA: 00:09:05

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 97990 steps/s (collection: 0.853s, learning 0.150s)
             Mean action noise std: 7.21
          Mean value_function loss: 39.9057
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.7744
                       Mean reward: 867.82
               Mean episode length: 249.24
    Episode_Reward/reaching_object: 0.7753
     Episode_Reward/lifting_object: 171.7732
      Episode_Reward/object_height: 0.0511
        Episode_Reward/action_rate: -0.1327
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 1.00s
                      Time elapsed: 00:24:14
                               ETA: 00:09:04

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 110610 steps/s (collection: 0.800s, learning 0.089s)
             Mean action noise std: 7.21
          Mean value_function loss: 24.6304
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 26.7783
                       Mean reward: 850.36
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 170.7506
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.1333
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 0.89s
                      Time elapsed: 00:24:15
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 109474 steps/s (collection: 0.808s, learning 0.090s)
             Mean action noise std: 7.22
          Mean value_function loss: 33.8052
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 26.7833
                       Mean reward: 829.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 170.9729
      Episode_Reward/object_height: 0.0506
        Episode_Reward/action_rate: -0.1330
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 0.90s
                      Time elapsed: 00:24:15
                               ETA: 00:09:02

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 106612 steps/s (collection: 0.794s, learning 0.128s)
             Mean action noise std: 7.22
          Mean value_function loss: 25.7572
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 26.7885
                       Mean reward: 861.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 171.7733
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.1331
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 0.92s
                      Time elapsed: 00:24:16
                               ETA: 00:09:01

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 107218 steps/s (collection: 0.802s, learning 0.115s)
             Mean action noise std: 7.23
          Mean value_function loss: 34.1072
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 26.7950
                       Mean reward: 856.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 170.4438
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.1340
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 0.92s
                      Time elapsed: 00:24:17
                               ETA: 00:09:00

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 108495 steps/s (collection: 0.814s, learning 0.092s)
             Mean action noise std: 7.24
          Mean value_function loss: 39.6714
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.8053
                       Mean reward: 853.00
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 171.8766
      Episode_Reward/object_height: 0.0506
        Episode_Reward/action_rate: -0.1329
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 0.91s
                      Time elapsed: 00:24:18
                               ETA: 00:08:59

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 103293 steps/s (collection: 0.834s, learning 0.118s)
             Mean action noise std: 7.25
          Mean value_function loss: 25.6925
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 26.8129
                       Mean reward: 841.40
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 171.1671
      Episode_Reward/object_height: 0.0506
        Episode_Reward/action_rate: -0.1332
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 0.95s
                      Time elapsed: 00:24:19
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 103940 steps/s (collection: 0.779s, learning 0.166s)
             Mean action noise std: 7.26
          Mean value_function loss: 28.7705
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 26.8256
                       Mean reward: 847.27
               Mean episode length: 247.52
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 171.0189
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.1339
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 0.95s
                      Time elapsed: 00:24:20
                               ETA: 00:08:57

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 102212 steps/s (collection: 0.872s, learning 0.090s)
             Mean action noise std: 7.26
          Mean value_function loss: 40.9146
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 26.8336
                       Mean reward: 874.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7756
     Episode_Reward/lifting_object: 173.6891
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.1348
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 0.96s
                      Time elapsed: 00:24:21
                               ETA: 00:08:56

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 99777 steps/s (collection: 0.828s, learning 0.158s)
             Mean action noise std: 7.27
          Mean value_function loss: 39.4337
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 26.8417
                       Mean reward: 875.83
               Mean episode length: 249.67
    Episode_Reward/reaching_object: 0.7773
     Episode_Reward/lifting_object: 173.8227
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.1341
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 0.99s
                      Time elapsed: 00:24:22
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 110583 steps/s (collection: 0.773s, learning 0.116s)
             Mean action noise std: 7.28
          Mean value_function loss: 26.2502
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.8505
                       Mean reward: 879.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7790
     Episode_Reward/lifting_object: 173.7131
      Episode_Reward/object_height: 0.0514
        Episode_Reward/action_rate: -0.1341
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 0.89s
                      Time elapsed: 00:24:23
                               ETA: 00:08:54

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 104236 steps/s (collection: 0.784s, learning 0.160s)
             Mean action noise std: 7.29
          Mean value_function loss: 29.0920
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 26.8591
                       Mean reward: 868.60
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7767
     Episode_Reward/lifting_object: 172.2453
      Episode_Reward/object_height: 0.0508
        Episode_Reward/action_rate: -0.1345
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 0.94s
                      Time elapsed: 00:24:24
                               ETA: 00:08:53

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 104413 steps/s (collection: 0.786s, learning 0.156s)
             Mean action noise std: 7.29
          Mean value_function loss: 26.3138
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 26.8681
                       Mean reward: 863.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 172.2376
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.1353
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 0.94s
                      Time elapsed: 00:24:25
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 107150 steps/s (collection: 0.778s, learning 0.140s)
             Mean action noise std: 7.30
          Mean value_function loss: 33.7228
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 26.8763
                       Mean reward: 875.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7773
     Episode_Reward/lifting_object: 173.3590
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.1349
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 0.92s
                      Time elapsed: 00:24:26
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 96480 steps/s (collection: 0.846s, learning 0.173s)
             Mean action noise std: 7.31
          Mean value_function loss: 38.7494
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 26.8870
                       Mean reward: 873.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 172.6519
      Episode_Reward/object_height: 0.0511
        Episode_Reward/action_rate: -0.1337
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 1.02s
                      Time elapsed: 00:24:27
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 115839 steps/s (collection: 0.760s, learning 0.089s)
             Mean action noise std: 7.31
          Mean value_function loss: 25.9559
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.8921
                       Mean reward: 871.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7760
     Episode_Reward/lifting_object: 173.3248
      Episode_Reward/object_height: 0.0514
        Episode_Reward/action_rate: -0.1339
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 0.85s
                      Time elapsed: 00:24:28
                               ETA: 00:08:48

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 112127 steps/s (collection: 0.764s, learning 0.113s)
             Mean action noise std: 7.32
          Mean value_function loss: 36.4291
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 26.8971
                       Mean reward: 863.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7762
     Episode_Reward/lifting_object: 173.1917
      Episode_Reward/object_height: 0.0514
        Episode_Reward/action_rate: -0.1339
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 0.88s
                      Time elapsed: 00:24:28
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 110286 steps/s (collection: 0.791s, learning 0.101s)
             Mean action noise std: 7.33
          Mean value_function loss: 27.8294
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 26.9064
                       Mean reward: 859.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7673
     Episode_Reward/lifting_object: 171.0670
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.1337
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 18.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 0.89s
                      Time elapsed: 00:24:29
                               ETA: 00:08:46

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 109016 steps/s (collection: 0.791s, learning 0.111s)
             Mean action noise std: 7.34
          Mean value_function loss: 34.6847
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 26.9127
                       Mean reward: 838.39
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7695
     Episode_Reward/lifting_object: 172.4513
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.1334
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 0.90s
                      Time elapsed: 00:24:30
                               ETA: 00:08:45

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 112352 steps/s (collection: 0.765s, learning 0.110s)
             Mean action noise std: 7.35
          Mean value_function loss: 27.0164
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 26.9241
                       Mean reward: 871.35
               Mean episode length: 247.97
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 171.3901
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.1331
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 0.87s
                      Time elapsed: 00:24:31
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 110162 steps/s (collection: 0.796s, learning 0.096s)
             Mean action noise std: 7.35
          Mean value_function loss: 33.7976
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 26.9339
                       Mean reward: 873.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7823
     Episode_Reward/lifting_object: 174.7432
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.1334
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 0.89s
                      Time elapsed: 00:24:32
                               ETA: 00:08:43

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 111283 steps/s (collection: 0.782s, learning 0.102s)
             Mean action noise std: 7.36
          Mean value_function loss: 29.8787
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 26.9410
                       Mean reward: 870.20
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 173.4393
      Episode_Reward/object_height: 0.0514
        Episode_Reward/action_rate: -0.1338
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 0.88s
                      Time elapsed: 00:24:33
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 116992 steps/s (collection: 0.745s, learning 0.095s)
             Mean action noise std: 7.37
          Mean value_function loss: 31.7875
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 26.9533
                       Mean reward: 867.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 170.9456
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.1340
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 0.84s
                      Time elapsed: 00:24:34
                               ETA: 00:08:41

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 99703 steps/s (collection: 0.799s, learning 0.187s)
             Mean action noise std: 7.38
          Mean value_function loss: 35.8081
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 26.9648
                       Mean reward: 869.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 169.3607
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.1333
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 0.99s
                      Time elapsed: 00:24:35
                               ETA: 00:08:40

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 111875 steps/s (collection: 0.766s, learning 0.113s)
             Mean action noise std: 7.39
          Mean value_function loss: 39.0768
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 26.9779
                       Mean reward: 837.78
               Mean episode length: 246.45
    Episode_Reward/reaching_object: 0.7754
     Episode_Reward/lifting_object: 171.6093
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.1331
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 0.88s
                      Time elapsed: 00:24:36
                               ETA: 00:08:39

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 111892 steps/s (collection: 0.782s, learning 0.097s)
             Mean action noise std: 7.40
          Mean value_function loss: 43.2040
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 26.9887
                       Mean reward: 860.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7749
     Episode_Reward/lifting_object: 172.1729
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.1339
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 0.88s
                      Time elapsed: 00:24:36
                               ETA: 00:08:38

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 110161 steps/s (collection: 0.777s, learning 0.115s)
             Mean action noise std: 7.41
          Mean value_function loss: 29.0955
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 26.9943
                       Mean reward: 835.82
               Mean episode length: 246.47
    Episode_Reward/reaching_object: 0.7768
     Episode_Reward/lifting_object: 171.8313
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.1331
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 0.89s
                      Time elapsed: 00:24:37
                               ETA: 00:08:37

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 111856 steps/s (collection: 0.778s, learning 0.101s)
             Mean action noise std: 7.42
          Mean value_function loss: 31.7743
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 27.0024
                       Mean reward: 875.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7751
     Episode_Reward/lifting_object: 173.1470
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.1342
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 0.88s
                      Time elapsed: 00:24:38
                               ETA: 00:08:36

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 109334 steps/s (collection: 0.798s, learning 0.101s)
             Mean action noise std: 7.42
          Mean value_function loss: 28.9840
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.0112
                       Mean reward: 879.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7762
     Episode_Reward/lifting_object: 173.6630
      Episode_Reward/object_height: 0.0518
        Episode_Reward/action_rate: -0.1337
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 0.90s
                      Time elapsed: 00:24:39
                               ETA: 00:08:35

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 113027 steps/s (collection: 0.777s, learning 0.093s)
             Mean action noise std: 7.43
          Mean value_function loss: 27.5819
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 27.0212
                       Mean reward: 875.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7813
     Episode_Reward/lifting_object: 174.0633
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.1349
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 0.87s
                      Time elapsed: 00:24:40
                               ETA: 00:08:34

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 108857 steps/s (collection: 0.800s, learning 0.103s)
             Mean action noise std: 7.44
          Mean value_function loss: 25.5519
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 27.0326
                       Mean reward: 872.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7762
     Episode_Reward/lifting_object: 173.5249
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.1338
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 0.90s
                      Time elapsed: 00:24:41
                               ETA: 00:08:33

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 107307 steps/s (collection: 0.780s, learning 0.136s)
             Mean action noise std: 7.45
          Mean value_function loss: 22.8094
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.0401
                       Mean reward: 864.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7813
     Episode_Reward/lifting_object: 173.0951
      Episode_Reward/object_height: 0.0508
        Episode_Reward/action_rate: -0.1344
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 0.92s
                      Time elapsed: 00:24:42
                               ETA: 00:08:32

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 111935 steps/s (collection: 0.748s, learning 0.131s)
             Mean action noise std: 7.45
          Mean value_function loss: 24.0994
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 27.0455
                       Mean reward: 867.20
               Mean episode length: 249.99
    Episode_Reward/reaching_object: 0.7784
     Episode_Reward/lifting_object: 173.6670
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.1349
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 0.88s
                      Time elapsed: 00:24:43
                               ETA: 00:08:31

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 103420 steps/s (collection: 0.795s, learning 0.156s)
             Mean action noise std: 7.46
          Mean value_function loss: 35.6961
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 27.0532
                       Mean reward: 872.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7748
     Episode_Reward/lifting_object: 173.9718
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.1348
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 0.95s
                      Time elapsed: 00:24:44
                               ETA: 00:08:30

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 118322 steps/s (collection: 0.745s, learning 0.086s)
             Mean action noise std: 7.46
          Mean value_function loss: 34.0627
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.0589
                       Mean reward: 861.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7693
     Episode_Reward/lifting_object: 171.8465
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.1347
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 0.83s
                      Time elapsed: 00:24:45
                               ETA: 00:08:29

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 113273 steps/s (collection: 0.774s, learning 0.094s)
             Mean action noise std: 7.47
          Mean value_function loss: 41.7579
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 27.0670
                       Mean reward: 870.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7743
     Episode_Reward/lifting_object: 173.0102
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.1347
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 0.87s
                      Time elapsed: 00:24:45
                               ETA: 00:08:28

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 107327 steps/s (collection: 0.820s, learning 0.096s)
             Mean action noise std: 7.48
          Mean value_function loss: 35.0430
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.0781
                       Mean reward: 867.50
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 170.4038
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1341
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 0.92s
                      Time elapsed: 00:24:46
                               ETA: 00:08:27

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 113536 steps/s (collection: 0.758s, learning 0.108s)
             Mean action noise std: 7.49
          Mean value_function loss: 33.4287
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 27.0899
                       Mean reward: 860.74
               Mean episode length: 246.23
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 169.3150
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.1340
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 0.87s
                      Time elapsed: 00:24:47
                               ETA: 00:08:26

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 109254 steps/s (collection: 0.786s, learning 0.114s)
             Mean action noise std: 7.50
          Mean value_function loss: 31.9455
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.0975
                       Mean reward: 875.15
               Mean episode length: 248.66
    Episode_Reward/reaching_object: 0.7739
     Episode_Reward/lifting_object: 171.8847
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.1346
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 0.90s
                      Time elapsed: 00:24:48
                               ETA: 00:08:25

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 105557 steps/s (collection: 0.774s, learning 0.158s)
             Mean action noise std: 7.51
          Mean value_function loss: 25.1841
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.1058
                       Mean reward: 862.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 170.3273
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.1350
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 0.93s
                      Time elapsed: 00:24:49
                               ETA: 00:08:24

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 111285 steps/s (collection: 0.759s, learning 0.125s)
             Mean action noise std: 7.52
          Mean value_function loss: 21.0553
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 27.1145
                       Mean reward: 868.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7752
     Episode_Reward/lifting_object: 171.8593
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.1358
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 0.88s
                      Time elapsed: 00:24:50
                               ETA: 00:08:23

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 109895 steps/s (collection: 0.783s, learning 0.112s)
             Mean action noise std: 7.52
          Mean value_function loss: 27.8814
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 27.1212
                       Mean reward: 866.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7799
     Episode_Reward/lifting_object: 173.7256
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.1348
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 0.89s
                      Time elapsed: 00:24:51
                               ETA: 00:08:22

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 99425 steps/s (collection: 0.777s, learning 0.212s)
             Mean action noise std: 7.53
          Mean value_function loss: 23.5986
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.1324
                       Mean reward: 871.07
               Mean episode length: 248.91
    Episode_Reward/reaching_object: 0.7710
     Episode_Reward/lifting_object: 172.4676
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.1352
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 0.99s
                      Time elapsed: 00:24:52
                               ETA: 00:08:21

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 110413 steps/s (collection: 0.770s, learning 0.120s)
             Mean action noise std: 7.54
          Mean value_function loss: 28.8012
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 27.1430
                       Mean reward: 869.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 172.9829
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.1346
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 0.89s
                      Time elapsed: 00:24:53
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 107982 steps/s (collection: 0.810s, learning 0.100s)
             Mean action noise std: 7.55
          Mean value_function loss: 40.4257
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.1545
                       Mean reward: 869.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7601
     Episode_Reward/lifting_object: 171.0247
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1353
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 0.91s
                      Time elapsed: 00:24:54
                               ETA: 00:08:19

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 105174 steps/s (collection: 0.825s, learning 0.110s)
             Mean action noise std: 7.56
          Mean value_function loss: 36.4424
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 27.1627
                       Mean reward: 849.49
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 171.2069
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.1356
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 0.93s
                      Time elapsed: 00:24:54
                               ETA: 00:08:17

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 114139 steps/s (collection: 0.764s, learning 0.097s)
             Mean action noise std: 7.57
          Mean value_function loss: 40.0894
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 27.1730
                       Mean reward: 874.27
               Mean episode length: 249.53
    Episode_Reward/reaching_object: 0.7782
     Episode_Reward/lifting_object: 172.2142
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.1362
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 0.86s
                      Time elapsed: 00:24:55
                               ETA: 00:08:16

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 111971 steps/s (collection: 0.791s, learning 0.087s)
             Mean action noise std: 7.58
          Mean value_function loss: 37.4219
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 27.1814
                       Mean reward: 869.30
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 0.7810
     Episode_Reward/lifting_object: 172.9996
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.1352
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 0.88s
                      Time elapsed: 00:24:56
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 114961 steps/s (collection: 0.750s, learning 0.105s)
             Mean action noise std: 7.58
          Mean value_function loss: 33.1140
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.1929
                       Mean reward: 860.58
               Mean episode length: 248.72
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 172.9932
      Episode_Reward/object_height: 0.0506
        Episode_Reward/action_rate: -0.1349
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 0.86s
                      Time elapsed: 00:24:57
                               ETA: 00:08:14

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 114118 steps/s (collection: 0.763s, learning 0.098s)
             Mean action noise std: 7.60
          Mean value_function loss: 36.9033
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.2016
                       Mean reward: 848.42
               Mean episode length: 245.91
    Episode_Reward/reaching_object: 0.7760
     Episode_Reward/lifting_object: 173.0899
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.1353
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 0.86s
                      Time elapsed: 00:24:58
                               ETA: 00:08:13

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 112030 steps/s (collection: 0.758s, learning 0.119s)
             Mean action noise std: 7.61
          Mean value_function loss: 28.5640
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 27.2151
                       Mean reward: 843.07
               Mean episode length: 245.41
    Episode_Reward/reaching_object: 0.7708
     Episode_Reward/lifting_object: 170.8975
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.1353
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 0.88s
                      Time elapsed: 00:24:59
                               ETA: 00:08:12

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 116060 steps/s (collection: 0.745s, learning 0.102s)
             Mean action noise std: 7.61
          Mean value_function loss: 29.3781
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 27.2217
                       Mean reward: 864.23
               Mean episode length: 248.29
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 170.0375
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.1359
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 0.85s
                      Time elapsed: 00:25:00
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 112982 steps/s (collection: 0.755s, learning 0.116s)
             Mean action noise std: 7.62
          Mean value_function loss: 28.1308
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.2259
                       Mean reward: 840.13
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.7753
     Episode_Reward/lifting_object: 171.9627
      Episode_Reward/object_height: 0.0508
        Episode_Reward/action_rate: -0.1366
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 0.87s
                      Time elapsed: 00:25:01
                               ETA: 00:08:10

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 110853 steps/s (collection: 0.769s, learning 0.118s)
             Mean action noise std: 7.63
          Mean value_function loss: 34.3535
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.2319
                       Mean reward: 873.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7775
     Episode_Reward/lifting_object: 171.5815
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.1379
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 0.89s
                      Time elapsed: 00:25:01
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 110998 steps/s (collection: 0.778s, learning 0.108s)
             Mean action noise std: 7.63
          Mean value_function loss: 35.0935
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.2428
                       Mean reward: 849.89
               Mean episode length: 248.64
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 168.6054
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.1373
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 0.89s
                      Time elapsed: 00:25:02
                               ETA: 00:08:08

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 110161 steps/s (collection: 0.796s, learning 0.096s)
             Mean action noise std: 7.64
          Mean value_function loss: 33.2315
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 27.2478
                       Mean reward: 865.14
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 0.7732
     Episode_Reward/lifting_object: 171.0570
      Episode_Reward/object_height: 0.0514
        Episode_Reward/action_rate: -0.1369
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 0.89s
                      Time elapsed: 00:25:03
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 114670 steps/s (collection: 0.755s, learning 0.103s)
             Mean action noise std: 7.64
          Mean value_function loss: 28.4369
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.2544
                       Mean reward: 866.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7781
     Episode_Reward/lifting_object: 171.9111
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.1389
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 0.86s
                      Time elapsed: 00:25:04
                               ETA: 00:08:06

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 112683 steps/s (collection: 0.761s, learning 0.112s)
             Mean action noise std: 7.65
          Mean value_function loss: 26.9919
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.2616
                       Mean reward: 873.99
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.7802
     Episode_Reward/lifting_object: 173.1629
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.1390
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 0.87s
                      Time elapsed: 00:25:05
                               ETA: 00:08:05

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 107367 steps/s (collection: 0.806s, learning 0.110s)
             Mean action noise std: 7.66
          Mean value_function loss: 34.8852
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.2672
                       Mean reward: 873.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7846
     Episode_Reward/lifting_object: 174.1774
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.1400
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 0.92s
                      Time elapsed: 00:25:06
                               ETA: 00:08:04

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 107938 steps/s (collection: 0.775s, learning 0.136s)
             Mean action noise std: 7.66
          Mean value_function loss: 31.2452
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 27.2687
                       Mean reward: 871.58
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 171.5173
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.1390
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 0.91s
                      Time elapsed: 00:25:07
                               ETA: 00:08:03

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 111600 steps/s (collection: 0.769s, learning 0.112s)
             Mean action noise std: 7.67
          Mean value_function loss: 29.7220
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 27.2739
                       Mean reward: 869.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7751
     Episode_Reward/lifting_object: 172.4949
      Episode_Reward/object_height: 0.0522
        Episode_Reward/action_rate: -0.1410
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 0.88s
                      Time elapsed: 00:25:08
                               ETA: 00:08:02

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 116846 steps/s (collection: 0.748s, learning 0.094s)
             Mean action noise std: 7.67
          Mean value_function loss: 31.5390
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.2799
                       Mean reward: 861.43
               Mean episode length: 246.87
    Episode_Reward/reaching_object: 0.7740
     Episode_Reward/lifting_object: 171.4668
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.1408
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 0.84s
                      Time elapsed: 00:25:08
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 110849 steps/s (collection: 0.789s, learning 0.098s)
             Mean action noise std: 7.68
          Mean value_function loss: 22.2986
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.2850
                       Mean reward: 872.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7861
     Episode_Reward/lifting_object: 173.6948
      Episode_Reward/object_height: 0.0522
        Episode_Reward/action_rate: -0.1411
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 0.89s
                      Time elapsed: 00:25:09
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 110556 steps/s (collection: 0.788s, learning 0.101s)
             Mean action noise std: 7.69
          Mean value_function loss: 25.9826
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 27.2949
                       Mean reward: 858.11
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 171.6802
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.1411
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 0.89s
                      Time elapsed: 00:25:10
                               ETA: 00:07:59

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 115147 steps/s (collection: 0.762s, learning 0.092s)
             Mean action noise std: 7.69
          Mean value_function loss: 29.4292
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.3022
                       Mean reward: 868.44
               Mean episode length: 249.33
    Episode_Reward/reaching_object: 0.7759
     Episode_Reward/lifting_object: 173.3915
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.1410
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 0.85s
                      Time elapsed: 00:25:11
                               ETA: 00:07:58

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 112414 steps/s (collection: 0.779s, learning 0.095s)
             Mean action noise std: 7.70
          Mean value_function loss: 31.0370
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 27.3119
                       Mean reward: 869.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7753
     Episode_Reward/lifting_object: 173.1441
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.1421
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 0.87s
                      Time elapsed: 00:25:12
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 112600 steps/s (collection: 0.761s, learning 0.112s)
             Mean action noise std: 7.71
          Mean value_function loss: 35.8167
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.3198
                       Mean reward: 871.42
               Mean episode length: 249.92
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 172.9119
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.1415
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 0.87s
                      Time elapsed: 00:25:13
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 105219 steps/s (collection: 0.772s, learning 0.163s)
             Mean action noise std: 7.71
          Mean value_function loss: 35.8403
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.3217
                       Mean reward: 871.83
               Mean episode length: 248.78
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 173.1389
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.1410
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 0.93s
                      Time elapsed: 00:25:14
                               ETA: 00:07:55

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 107399 steps/s (collection: 0.780s, learning 0.135s)
             Mean action noise std: 7.71
          Mean value_function loss: 20.9743
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.3232
                       Mean reward: 869.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 172.8911
      Episode_Reward/object_height: 0.0527
        Episode_Reward/action_rate: -0.1421
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 0.92s
                      Time elapsed: 00:25:15
                               ETA: 00:07:54

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 107576 steps/s (collection: 0.779s, learning 0.135s)
             Mean action noise std: 7.71
          Mean value_function loss: 31.6551
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.3253
                       Mean reward: 853.79
               Mean episode length: 247.92
    Episode_Reward/reaching_object: 0.7664
     Episode_Reward/lifting_object: 171.4923
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.1415
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 0.91s
                      Time elapsed: 00:25:16
                               ETA: 00:07:53

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 107097 steps/s (collection: 0.819s, learning 0.098s)
             Mean action noise std: 7.72
          Mean value_function loss: 34.9253
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.3310
                       Mean reward: 866.10
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.7752
     Episode_Reward/lifting_object: 172.1422
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.1417
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 0.92s
                      Time elapsed: 00:25:17
                               ETA: 00:07:52

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 111352 steps/s (collection: 0.774s, learning 0.109s)
             Mean action noise std: 7.73
          Mean value_function loss: 26.6374
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.3394
                       Mean reward: 875.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7750
     Episode_Reward/lifting_object: 171.9413
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.1419
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 0.88s
                      Time elapsed: 00:25:17
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 115111 steps/s (collection: 0.759s, learning 0.095s)
             Mean action noise std: 7.73
          Mean value_function loss: 27.9469
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.3454
                       Mean reward: 873.21
               Mean episode length: 248.68
    Episode_Reward/reaching_object: 0.7818
     Episode_Reward/lifting_object: 173.3704
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.1422
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 0.85s
                      Time elapsed: 00:25:18
                               ETA: 00:07:50

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 99453 steps/s (collection: 0.856s, learning 0.132s)
             Mean action noise std: 7.74
          Mean value_function loss: 28.6210
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.3509
                       Mean reward: 881.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7812
     Episode_Reward/lifting_object: 173.2549
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.1428
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 0.99s
                      Time elapsed: 00:25:19
                               ETA: 00:07:49

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 106432 steps/s (collection: 0.790s, learning 0.133s)
             Mean action noise std: 7.75
          Mean value_function loss: 24.3294
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.3632
                       Mean reward: 864.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 171.4840
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.1436
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 0.92s
                      Time elapsed: 00:25:20
                               ETA: 00:07:48

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 110374 steps/s (collection: 0.777s, learning 0.114s)
             Mean action noise std: 7.76
          Mean value_function loss: 25.9025
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 27.3709
                       Mean reward: 853.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 171.3716
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.1445
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 0.89s
                      Time elapsed: 00:25:21
                               ETA: 00:07:47

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 107289 steps/s (collection: 0.781s, learning 0.135s)
             Mean action noise std: 7.77
          Mean value_function loss: 28.6910
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 27.3766
                       Mean reward: 853.48
               Mean episode length: 247.93
    Episode_Reward/reaching_object: 0.7753
     Episode_Reward/lifting_object: 171.8967
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.1437
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 0.92s
                      Time elapsed: 00:25:22
                               ETA: 00:07:46

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 107509 steps/s (collection: 0.821s, learning 0.094s)
             Mean action noise std: 7.78
          Mean value_function loss: 29.8339
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.3883
                       Mean reward: 880.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7810
     Episode_Reward/lifting_object: 173.7518
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.1436
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 0.91s
                      Time elapsed: 00:25:23
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 109318 steps/s (collection: 0.776s, learning 0.124s)
             Mean action noise std: 7.78
          Mean value_function loss: 34.8605
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.3936
                       Mean reward: 873.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7807
     Episode_Reward/lifting_object: 173.4958
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.1437
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 0.90s
                      Time elapsed: 00:25:24
                               ETA: 00:07:44

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 108573 steps/s (collection: 0.770s, learning 0.135s)
             Mean action noise std: 7.79
          Mean value_function loss: 24.6994
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 27.3999
                       Mean reward: 877.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7768
     Episode_Reward/lifting_object: 173.0498
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.1440
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 0.91s
                      Time elapsed: 00:25:25
                               ETA: 00:07:43

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 105659 steps/s (collection: 0.787s, learning 0.144s)
             Mean action noise std: 7.79
          Mean value_function loss: 31.3081
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.4048
                       Mean reward: 876.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7837
     Episode_Reward/lifting_object: 173.6181
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.1441
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 0.93s
                      Time elapsed: 00:25:26
                               ETA: 00:07:42

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 112426 steps/s (collection: 0.763s, learning 0.111s)
             Mean action noise std: 7.80
          Mean value_function loss: 27.3341
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 27.4112
                       Mean reward: 875.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7780
     Episode_Reward/lifting_object: 172.6493
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.1432
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 0.87s
                      Time elapsed: 00:25:27
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 113499 steps/s (collection: 0.776s, learning 0.090s)
             Mean action noise std: 7.81
          Mean value_function loss: 25.9613
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 27.4191
                       Mean reward: 871.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7717
     Episode_Reward/lifting_object: 171.9941
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.1451
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 0.87s
                      Time elapsed: 00:25:27
                               ETA: 00:07:39

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 109607 steps/s (collection: 0.792s, learning 0.105s)
             Mean action noise std: 7.82
          Mean value_function loss: 22.3300
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 27.4283
                       Mean reward: 852.42
               Mean episode length: 246.10
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 172.1865
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.1450
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 0.90s
                      Time elapsed: 00:25:28
                               ETA: 00:07:38

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 110630 steps/s (collection: 0.782s, learning 0.107s)
             Mean action noise std: 7.82
          Mean value_function loss: 35.5031
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 27.4357
                       Mean reward: 861.04
               Mean episode length: 249.22
    Episode_Reward/reaching_object: 0.7775
     Episode_Reward/lifting_object: 173.0013
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.1457
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 0.89s
                      Time elapsed: 00:25:29
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 113785 steps/s (collection: 0.759s, learning 0.104s)
             Mean action noise std: 7.83
          Mean value_function loss: 41.6729
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 27.4416
                       Mean reward: 867.30
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7810
     Episode_Reward/lifting_object: 173.5390
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.1465
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 0.86s
                      Time elapsed: 00:25:30
                               ETA: 00:07:36

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 111472 steps/s (collection: 0.778s, learning 0.104s)
             Mean action noise std: 7.84
          Mean value_function loss: 33.1703
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 27.4500
                       Mean reward: 865.23
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: 173.3174
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.1473
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 0.88s
                      Time elapsed: 00:25:31
                               ETA: 00:07:35

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 109223 steps/s (collection: 0.808s, learning 0.092s)
             Mean action noise std: 7.84
          Mean value_function loss: 33.0738
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 27.4596
                       Mean reward: 859.27
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 0.7741
     Episode_Reward/lifting_object: 173.3344
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.1471
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 0.90s
                      Time elapsed: 00:25:32
                               ETA: 00:07:34

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 109722 steps/s (collection: 0.782s, learning 0.114s)
             Mean action noise std: 7.86
          Mean value_function loss: 31.8349
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 27.4678
                       Mean reward: 872.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7742
     Episode_Reward/lifting_object: 172.8354
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.1486
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 0.90s
                      Time elapsed: 00:25:33
                               ETA: 00:07:33

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 107148 steps/s (collection: 0.745s, learning 0.173s)
             Mean action noise std: 7.87
          Mean value_function loss: 42.8757
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 27.4814
                       Mean reward: 881.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7823
     Episode_Reward/lifting_object: 173.1145
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.1494
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 0.92s
                      Time elapsed: 00:25:34
                               ETA: 00:07:32

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 115289 steps/s (collection: 0.757s, learning 0.096s)
             Mean action noise std: 7.88
          Mean value_function loss: 31.4982
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 27.4920
                       Mean reward: 869.31
               Mean episode length: 248.77
    Episode_Reward/reaching_object: 0.7840
     Episode_Reward/lifting_object: 174.2912
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.1483
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 0.85s
                      Time elapsed: 00:25:34
                               ETA: 00:07:31

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 111851 steps/s (collection: 0.784s, learning 0.095s)
             Mean action noise std: 7.89
          Mean value_function loss: 46.5753
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.5007
                       Mean reward: 855.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7757
     Episode_Reward/lifting_object: 172.3999
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.1505
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 0.88s
                      Time elapsed: 00:25:35
                               ETA: 00:07:30

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 110148 steps/s (collection: 0.791s, learning 0.101s)
             Mean action noise std: 7.90
          Mean value_function loss: 40.8380
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 27.5120
                       Mean reward: 862.07
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7785
     Episode_Reward/lifting_object: 173.0137
      Episode_Reward/object_height: 0.0522
        Episode_Reward/action_rate: -0.1493
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 0.89s
                      Time elapsed: 00:25:36
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 109498 steps/s (collection: 0.776s, learning 0.122s)
             Mean action noise std: 7.91
          Mean value_function loss: 28.5720
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 27.5213
                       Mean reward: 873.11
               Mean episode length: 249.79
    Episode_Reward/reaching_object: 0.7821
     Episode_Reward/lifting_object: 172.3380
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.1502
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 0.90s
                      Time elapsed: 00:25:37
                               ETA: 00:07:28

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 109123 steps/s (collection: 0.810s, learning 0.091s)
             Mean action noise std: 7.91
          Mean value_function loss: 45.7424
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 27.5313
                       Mean reward: 853.96
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.7754
     Episode_Reward/lifting_object: 171.7969
      Episode_Reward/object_height: 0.0518
        Episode_Reward/action_rate: -0.1509
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 0.90s
                      Time elapsed: 00:25:38
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 112805 steps/s (collection: 0.765s, learning 0.107s)
             Mean action noise std: 7.92
          Mean value_function loss: 46.9062
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 27.5407
                       Mean reward: 869.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7711
     Episode_Reward/lifting_object: 170.6086
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.1513
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 0.87s
                      Time elapsed: 00:25:39
                               ETA: 00:07:26

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 110275 steps/s (collection: 0.779s, learning 0.112s)
             Mean action noise std: 7.93
          Mean value_function loss: 45.6962
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 27.5500
                       Mean reward: 865.10
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.7705
     Episode_Reward/lifting_object: 171.1468
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.1506
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 0.89s
                      Time elapsed: 00:25:40
                               ETA: 00:07:25

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 109075 steps/s (collection: 0.771s, learning 0.130s)
             Mean action noise std: 7.94
          Mean value_function loss: 39.2617
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.5573
                       Mean reward: 877.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7726
     Episode_Reward/lifting_object: 171.4183
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.1523
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 0.90s
                      Time elapsed: 00:25:41
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 105554 steps/s (collection: 0.772s, learning 0.160s)
             Mean action noise std: 7.95
          Mean value_function loss: 53.5938
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 27.5694
                       Mean reward: 842.00
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 169.9075
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.1530
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 0.93s
                      Time elapsed: 00:25:42
                               ETA: 00:07:23

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 110148 steps/s (collection: 0.766s, learning 0.126s)
             Mean action noise std: 7.95
          Mean value_function loss: 40.5821
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 27.5769
                       Mean reward: 827.58
               Mean episode length: 246.04
    Episode_Reward/reaching_object: 0.7602
     Episode_Reward/lifting_object: 167.9099
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.1542
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 0.89s
                      Time elapsed: 00:25:43
                               ETA: 00:07:22

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 112895 steps/s (collection: 0.763s, learning 0.108s)
             Mean action noise std: 7.96
          Mean value_function loss: 54.8291
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.5796
                       Mean reward: 854.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 171.7028
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.1548
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 0.87s
                      Time elapsed: 00:25:43
                               ETA: 00:07:21

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 113336 steps/s (collection: 0.780s, learning 0.088s)
             Mean action noise std: 7.97
          Mean value_function loss: 43.8416
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 27.5855
                       Mean reward: 847.63
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7646
     Episode_Reward/lifting_object: 168.7089
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.1547
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 0.87s
                      Time elapsed: 00:25:44
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 113120 steps/s (collection: 0.772s, learning 0.097s)
             Mean action noise std: 7.97
          Mean value_function loss: 62.1603
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 27.5921
                       Mean reward: 874.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7743
     Episode_Reward/lifting_object: 172.5623
      Episode_Reward/object_height: 0.0518
        Episode_Reward/action_rate: -0.1534
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 0.87s
                      Time elapsed: 00:25:45
                               ETA: 00:07:19

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 108391 steps/s (collection: 0.783s, learning 0.124s)
             Mean action noise std: 7.98
          Mean value_function loss: 51.6653
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 27.5993
                       Mean reward: 858.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 168.4330
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.1535
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 0.91s
                      Time elapsed: 00:25:46
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 109791 steps/s (collection: 0.808s, learning 0.087s)
             Mean action noise std: 7.98
          Mean value_function loss: 52.5728
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 27.6044
                       Mean reward: 839.44
               Mean episode length: 247.06
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 169.2946
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.1542
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 0.90s
                      Time elapsed: 00:25:47
                               ETA: 00:07:17

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 107668 steps/s (collection: 0.781s, learning 0.132s)
             Mean action noise std: 8.00
          Mean value_function loss: 44.4929
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.6125
                       Mean reward: 846.73
               Mean episode length: 246.79
    Episode_Reward/reaching_object: 0.7543
     Episode_Reward/lifting_object: 169.9633
      Episode_Reward/object_height: 0.0514
        Episode_Reward/action_rate: -0.1524
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 0.91s
                      Time elapsed: 00:25:48
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 106312 steps/s (collection: 0.809s, learning 0.116s)
             Mean action noise std: 8.00
          Mean value_function loss: 51.7957
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 27.6200
                       Mean reward: 850.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 170.4073
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.1560
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 0.92s
                      Time elapsed: 00:25:49
                               ETA: 00:07:15

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 112207 steps/s (collection: 0.774s, learning 0.102s)
             Mean action noise std: 8.01
          Mean value_function loss: 49.3079
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 27.6238
                       Mean reward: 838.36
               Mean episode length: 248.21
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 168.1676
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.1560
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 0.88s
                      Time elapsed: 00:25:50
                               ETA: 00:07:14

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 111267 steps/s (collection: 0.774s, learning 0.109s)
             Mean action noise std: 8.01
          Mean value_function loss: 40.9086
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 27.6280
                       Mean reward: 862.20
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.7619
     Episode_Reward/lifting_object: 170.6830
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.1553
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 0.88s
                      Time elapsed: 00:25:51
                               ETA: 00:07:13

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 110356 steps/s (collection: 0.786s, learning 0.105s)
             Mean action noise std: 8.02
          Mean value_function loss: 49.6919
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 27.6341
                       Mean reward: 843.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7617
     Episode_Reward/lifting_object: 170.5497
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.1563
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 0.89s
                      Time elapsed: 00:25:51
                               ETA: 00:07:12

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 110537 steps/s (collection: 0.794s, learning 0.096s)
             Mean action noise std: 8.02
          Mean value_function loss: 50.6443
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.6358
                       Mean reward: 857.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7496
     Episode_Reward/lifting_object: 167.4432
      Episode_Reward/object_height: 0.0508
        Episode_Reward/action_rate: -0.1560
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 0.89s
                      Time elapsed: 00:25:52
                               ETA: 00:07:11

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 112900 steps/s (collection: 0.774s, learning 0.097s)
             Mean action noise std: 8.03
          Mean value_function loss: 50.6484
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 27.6405
                       Mean reward: 842.74
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 170.5652
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.1558
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 0.87s
                      Time elapsed: 00:25:53
                               ETA: 00:07:10

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 112155 steps/s (collection: 0.780s, learning 0.096s)
             Mean action noise std: 8.03
          Mean value_function loss: 47.2292
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 27.6486
                       Mean reward: 869.44
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 0.7528
     Episode_Reward/lifting_object: 169.8234
      Episode_Reward/object_height: 0.0516
        Episode_Reward/action_rate: -0.1563
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 0.88s
                      Time elapsed: 00:25:54
                               ETA: 00:07:09

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 110662 steps/s (collection: 0.789s, learning 0.100s)
             Mean action noise std: 8.04
          Mean value_function loss: 44.2707
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.6561
                       Mean reward: 858.83
               Mean episode length: 249.25
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 171.8392
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.1562
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 0.89s
                      Time elapsed: 00:25:55
                               ETA: 00:07:08

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 113889 steps/s (collection: 0.774s, learning 0.090s)
             Mean action noise std: 8.06
          Mean value_function loss: 42.8898
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 27.6669
                       Mean reward: 850.68
               Mean episode length: 246.56
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 169.1449
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.1569
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 0.86s
                      Time elapsed: 00:25:56
                               ETA: 00:07:07

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 107305 steps/s (collection: 0.794s, learning 0.123s)
             Mean action noise std: 8.06
          Mean value_function loss: 38.9612
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 27.6753
                       Mean reward: 857.86
               Mean episode length: 249.39
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 170.6087
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.1580
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 0.92s
                      Time elapsed: 00:25:57
                               ETA: 00:07:06

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 108483 steps/s (collection: 0.793s, learning 0.113s)
             Mean action noise std: 8.07
          Mean value_function loss: 33.0747
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.6787
                       Mean reward: 877.74
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 172.5086
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.1580
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 0.91s
                      Time elapsed: 00:25:58
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 112811 steps/s (collection: 0.769s, learning 0.102s)
             Mean action noise std: 8.08
          Mean value_function loss: 39.5632
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 27.6876
                       Mean reward: 862.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7633
     Episode_Reward/lifting_object: 171.6904
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.1599
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 0.87s
                      Time elapsed: 00:25:59
                               ETA: 00:07:04

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 110668 steps/s (collection: 0.769s, learning 0.119s)
             Mean action noise std: 8.08
          Mean value_function loss: 38.5877
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 27.6966
                       Mean reward: 858.72
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 170.0434
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.1610
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 0.89s
                      Time elapsed: 00:25:59
                               ETA: 00:07:03

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 108222 steps/s (collection: 0.792s, learning 0.116s)
             Mean action noise std: 8.09
          Mean value_function loss: 46.3245
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 27.6999
                       Mean reward: 853.30
               Mean episode length: 249.05
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 169.6382
      Episode_Reward/object_height: 0.0508
        Episode_Reward/action_rate: -0.1617
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 0.91s
                      Time elapsed: 00:26:00
                               ETA: 00:07:02

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 112193 steps/s (collection: 0.773s, learning 0.103s)
             Mean action noise std: 8.10
          Mean value_function loss: 41.6635
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 27.7067
                       Mean reward: 837.53
               Mean episode length: 246.55
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 169.0635
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.1606
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 0.88s
                      Time elapsed: 00:26:01
                               ETA: 00:07:01

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 108604 steps/s (collection: 0.782s, learning 0.124s)
             Mean action noise std: 8.10
          Mean value_function loss: 37.2560
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 27.7118
                       Mean reward: 832.60
               Mean episode length: 247.36
    Episode_Reward/reaching_object: 0.7643
     Episode_Reward/lifting_object: 170.3553
      Episode_Reward/object_height: 0.0506
        Episode_Reward/action_rate: -0.1608
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 0.91s
                      Time elapsed: 00:26:02
                               ETA: 00:07:00

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 96863 steps/s (collection: 0.898s, learning 0.117s)
             Mean action noise std: 8.11
          Mean value_function loss: 32.5226
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 27.7158
                       Mean reward: 845.39
               Mean episode length: 244.26
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: 169.8480
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.1623
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 1.01s
                      Time elapsed: 00:26:03
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 110124 steps/s (collection: 0.786s, learning 0.107s)
             Mean action noise std: 8.11
          Mean value_function loss: 37.0634
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 27.7234
                       Mean reward: 870.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7757
     Episode_Reward/lifting_object: 172.4798
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.1636
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 0.89s
                      Time elapsed: 00:26:04
                               ETA: 00:06:58

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 107010 steps/s (collection: 0.803s, learning 0.115s)
             Mean action noise std: 8.12
          Mean value_function loss: 32.2178
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 27.7292
                       Mean reward: 855.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7739
     Episode_Reward/lifting_object: 171.2070
      Episode_Reward/object_height: 0.0506
        Episode_Reward/action_rate: -0.1631
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 0.92s
                      Time elapsed: 00:26:05
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 103688 steps/s (collection: 0.830s, learning 0.118s)
             Mean action noise std: 8.13
          Mean value_function loss: 31.4590
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 27.7372
                       Mean reward: 881.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7778
     Episode_Reward/lifting_object: 173.4387
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.1639
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 0.95s
                      Time elapsed: 00:26:06
                               ETA: 00:06:56

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 113392 steps/s (collection: 0.776s, learning 0.091s)
             Mean action noise std: 8.14
          Mean value_function loss: 31.0934
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 27.7442
                       Mean reward: 872.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7834
     Episode_Reward/lifting_object: 173.2422
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.1637
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 0.87s
                      Time elapsed: 00:26:07
                               ETA: 00:06:55

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 105914 steps/s (collection: 0.775s, learning 0.154s)
             Mean action noise std: 8.15
          Mean value_function loss: 37.0547
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 27.7534
                       Mean reward: 871.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7786
     Episode_Reward/lifting_object: 172.2941
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.1631
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 0.93s
                      Time elapsed: 00:26:08
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 115139 steps/s (collection: 0.755s, learning 0.099s)
             Mean action noise std: 8.16
          Mean value_function loss: 18.4482
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 27.7641
                       Mean reward: 872.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7757
     Episode_Reward/lifting_object: 171.1140
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.1635
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 0.85s
                      Time elapsed: 00:26:09
                               ETA: 00:06:53

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 106279 steps/s (collection: 0.794s, learning 0.131s)
             Mean action noise std: 8.16
          Mean value_function loss: 48.5936
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 27.7735
                       Mean reward: 858.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7751
     Episode_Reward/lifting_object: 172.1894
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.1637
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 0.92s
                      Time elapsed: 00:26:09
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 105044 steps/s (collection: 0.798s, learning 0.138s)
             Mean action noise std: 8.17
          Mean value_function loss: 36.6360
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 27.7774
                       Mean reward: 860.02
               Mean episode length: 247.12
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 173.0473
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.1629
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 0.94s
                      Time elapsed: 00:26:10
                               ETA: 00:06:51

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 108557 steps/s (collection: 0.812s, learning 0.094s)
             Mean action noise std: 8.18
          Mean value_function loss: 45.9563
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 27.7875
                       Mean reward: 840.90
               Mean episode length: 247.61
    Episode_Reward/reaching_object: 0.7600
     Episode_Reward/lifting_object: 169.9811
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.1644
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 0.91s
                      Time elapsed: 00:26:11
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 110358 steps/s (collection: 0.782s, learning 0.109s)
             Mean action noise std: 8.19
          Mean value_function loss: 34.7497
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 27.7987
                       Mean reward: 877.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7726
     Episode_Reward/lifting_object: 172.8523
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.1637
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 0.89s
                      Time elapsed: 00:26:12
                               ETA: 00:06:49

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 99173 steps/s (collection: 0.862s, learning 0.129s)
             Mean action noise std: 8.19
          Mean value_function loss: 35.9081
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 27.8033
                       Mean reward: 867.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7799
     Episode_Reward/lifting_object: 173.1639
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.1645
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 0.99s
                      Time elapsed: 00:26:13
                               ETA: 00:06:48

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 109309 steps/s (collection: 0.809s, learning 0.090s)
             Mean action noise std: 8.19
          Mean value_function loss: 43.4189
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 27.8064
                       Mean reward: 875.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7737
     Episode_Reward/lifting_object: 172.3046
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.1635
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 0.90s
                      Time elapsed: 00:26:14
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 109287 steps/s (collection: 0.800s, learning 0.100s)
             Mean action noise std: 8.20
          Mean value_function loss: 42.7590
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 27.8136
                       Mean reward: 851.71
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.7789
     Episode_Reward/lifting_object: 172.1047
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.1643
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 0.90s
                      Time elapsed: 00:26:15
                               ETA: 00:06:45

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 107245 steps/s (collection: 0.812s, learning 0.105s)
             Mean action noise std: 8.21
          Mean value_function loss: 36.5989
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 27.8207
                       Mean reward: 862.40
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.7736
     Episode_Reward/lifting_object: 170.8893
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1639
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 0.92s
                      Time elapsed: 00:26:16
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 106124 steps/s (collection: 0.817s, learning 0.109s)
             Mean action noise std: 8.22
          Mean value_function loss: 39.9833
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.8266
                       Mean reward: 861.14
               Mean episode length: 247.31
    Episode_Reward/reaching_object: 0.7767
     Episode_Reward/lifting_object: 171.9975
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.1636
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 0.93s
                      Time elapsed: 00:26:17
                               ETA: 00:06:43

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 111140 steps/s (collection: 0.791s, learning 0.093s)
             Mean action noise std: 8.22
          Mean value_function loss: 37.2699
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 27.8336
                       Mean reward: 866.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7685
     Episode_Reward/lifting_object: 171.6439
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1639
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 0.88s
                      Time elapsed: 00:26:18
                               ETA: 00:06:42

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 109423 steps/s (collection: 0.780s, learning 0.118s)
             Mean action noise std: 8.23
          Mean value_function loss: 44.1830
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.8371
                       Mean reward: 853.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7725
     Episode_Reward/lifting_object: 170.3207
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.1640
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 0.90s
                      Time elapsed: 00:26:19
                               ETA: 00:06:41

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 109023 steps/s (collection: 0.778s, learning 0.124s)
             Mean action noise std: 8.24
          Mean value_function loss: 39.3523
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 27.8456
                       Mean reward: 852.15
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 0.7611
     Episode_Reward/lifting_object: 168.7938
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.1643
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 0.90s
                      Time elapsed: 00:26:20
                               ETA: 00:06:40

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 112949 steps/s (collection: 0.781s, learning 0.089s)
             Mean action noise std: 8.25
          Mean value_function loss: 32.5771
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 27.8592
                       Mean reward: 858.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 170.3868
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.1643
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 0.87s
                      Time elapsed: 00:26:20
                               ETA: 00:06:39

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 111121 steps/s (collection: 0.791s, learning 0.094s)
             Mean action noise std: 8.27
          Mean value_function loss: 32.2645
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.8720
                       Mean reward: 861.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 169.9251
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.1648
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 0.88s
                      Time elapsed: 00:26:21
                               ETA: 00:06:38

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 100251 steps/s (collection: 0.873s, learning 0.108s)
             Mean action noise std: 8.28
          Mean value_function loss: 38.7200
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 27.8878
                       Mean reward: 857.23
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: 171.6068
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.1654
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 0.98s
                      Time elapsed: 00:26:22
                               ETA: 00:06:37

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 110344 steps/s (collection: 0.796s, learning 0.095s)
             Mean action noise std: 8.29
          Mean value_function loss: 48.5026
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 27.9014
                       Mean reward: 879.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7810
     Episode_Reward/lifting_object: 173.4474
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1656
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 0.89s
                      Time elapsed: 00:26:23
                               ETA: 00:06:36

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 110131 steps/s (collection: 0.798s, learning 0.094s)
             Mean action noise std: 8.30
          Mean value_function loss: 42.5093
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 27.9136
                       Mean reward: 874.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 170.1614
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.1664
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 0.89s
                      Time elapsed: 00:26:24
                               ETA: 00:06:35

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 113926 steps/s (collection: 0.773s, learning 0.090s)
             Mean action noise std: 8.31
          Mean value_function loss: 61.3600
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 27.9209
                       Mean reward: 840.84
               Mean episode length: 249.79
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 172.1641
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1670
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 0.86s
                      Time elapsed: 00:26:25
                               ETA: 00:06:34

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 111134 steps/s (collection: 0.769s, learning 0.115s)
             Mean action noise std: 8.32
          Mean value_function loss: 56.4214
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 27.9258
                       Mean reward: 858.02
               Mean episode length: 246.21
    Episode_Reward/reaching_object: 0.7734
     Episode_Reward/lifting_object: 171.0658
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.1668
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 0.88s
                      Time elapsed: 00:26:26
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 98581 steps/s (collection: 0.821s, learning 0.176s)
             Mean action noise std: 8.33
          Mean value_function loss: 52.5463
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.9353
                       Mean reward: 865.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7645
     Episode_Reward/lifting_object: 169.9563
      Episode_Reward/object_height: 0.0480
        Episode_Reward/action_rate: -0.1670
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 1.00s
                      Time elapsed: 00:26:27
                               ETA: 00:06:32

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 100592 steps/s (collection: 0.873s, learning 0.104s)
             Mean action noise std: 8.33
          Mean value_function loss: 43.0420
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 27.9423
                       Mean reward: 841.83
               Mean episode length: 246.58
    Episode_Reward/reaching_object: 0.7615
     Episode_Reward/lifting_object: 170.3700
      Episode_Reward/object_height: 0.0484
        Episode_Reward/action_rate: -0.1670
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 0.98s
                      Time elapsed: 00:26:28
                               ETA: 00:06:31

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 110732 steps/s (collection: 0.774s, learning 0.114s)
             Mean action noise std: 8.34
          Mean value_function loss: 43.3008
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 27.9458
                       Mean reward: 861.01
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 171.5814
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1682
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 0.89s
                      Time elapsed: 00:26:29
                               ETA: 00:06:30

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 109985 steps/s (collection: 0.762s, learning 0.132s)
             Mean action noise std: 8.35
          Mean value_function loss: 43.6918
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.9564
                       Mean reward: 845.64
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 0.7605
     Episode_Reward/lifting_object: 170.2687
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.1679
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 0.89s
                      Time elapsed: 00:26:30
                               ETA: 00:06:29

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 104166 steps/s (collection: 0.788s, learning 0.156s)
             Mean action noise std: 8.36
          Mean value_function loss: 39.1454
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 27.9690
                       Mean reward: 842.79
               Mean episode length: 248.32
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 170.2495
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.1687
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 0.94s
                      Time elapsed: 00:26:30
                               ETA: 00:06:28

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 111291 steps/s (collection: 0.788s, learning 0.095s)
             Mean action noise std: 8.37
          Mean value_function loss: 52.3096
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 27.9727
                       Mean reward: 836.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7667
     Episode_Reward/lifting_object: 170.4016
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.1688
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 0.88s
                      Time elapsed: 00:26:31
                               ETA: 00:06:27

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 109968 steps/s (collection: 0.803s, learning 0.091s)
             Mean action noise std: 8.38
          Mean value_function loss: 45.3893
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 27.9834
                       Mean reward: 868.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7609
     Episode_Reward/lifting_object: 170.1728
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.1693
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 0.89s
                      Time elapsed: 00:26:32
                               ETA: 00:06:26

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 100324 steps/s (collection: 0.835s, learning 0.145s)
             Mean action noise std: 8.38
          Mean value_function loss: 46.0632
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 27.9904
                       Mean reward: 829.42
               Mean episode length: 249.18
    Episode_Reward/reaching_object: 0.7503
     Episode_Reward/lifting_object: 167.9416
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.1704
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 0.98s
                      Time elapsed: 00:26:33
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 109460 steps/s (collection: 0.791s, learning 0.107s)
             Mean action noise std: 8.39
          Mean value_function loss: 31.7043
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 27.9941
                       Mean reward: 853.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7574
     Episode_Reward/lifting_object: 169.5099
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.1692
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 0.90s
                      Time elapsed: 00:26:34
                               ETA: 00:06:24

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 108158 steps/s (collection: 0.814s, learning 0.095s)
             Mean action noise std: 8.39
          Mean value_function loss: 31.3449
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 28.0011
                       Mean reward: 882.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7591
     Episode_Reward/lifting_object: 170.5209
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1700
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 0.91s
                      Time elapsed: 00:26:35
                               ETA: 00:06:23

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 111928 steps/s (collection: 0.770s, learning 0.109s)
             Mean action noise std: 8.40
          Mean value_function loss: 35.1175
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.0062
                       Mean reward: 865.07
               Mean episode length: 249.77
    Episode_Reward/reaching_object: 0.7701
     Episode_Reward/lifting_object: 171.8373
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.1700
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 0.88s
                      Time elapsed: 00:26:36
                               ETA: 00:06:22

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 104591 steps/s (collection: 0.821s, learning 0.119s)
             Mean action noise std: 8.40
          Mean value_function loss: 32.8777
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 28.0101
                       Mean reward: 865.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 171.8285
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.1705
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 0.94s
                      Time elapsed: 00:26:37
                               ETA: 00:06:21

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 110913 steps/s (collection: 0.792s, learning 0.094s)
             Mean action noise std: 8.41
          Mean value_function loss: 31.8265
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.0146
                       Mean reward: 863.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7675
     Episode_Reward/lifting_object: 171.0190
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.1704
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 0.89s
                      Time elapsed: 00:26:38
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 101194 steps/s (collection: 0.832s, learning 0.139s)
             Mean action noise std: 8.41
          Mean value_function loss: 29.7046
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 28.0210
                       Mean reward: 874.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7689
     Episode_Reward/lifting_object: 171.9668
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.1707
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 0.97s
                      Time elapsed: 00:26:39
                               ETA: 00:06:19

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 104971 steps/s (collection: 0.784s, learning 0.153s)
             Mean action noise std: 8.42
          Mean value_function loss: 30.1651
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 28.0249
                       Mean reward: 848.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: 169.1715
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.1729
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 0.94s
                      Time elapsed: 00:26:40
                               ETA: 00:06:18

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 110113 steps/s (collection: 0.784s, learning 0.109s)
             Mean action noise std: 8.42
          Mean value_function loss: 24.1872
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 28.0284
                       Mean reward: 862.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 171.9134
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.1718
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 0.89s
                      Time elapsed: 00:26:41
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 110483 steps/s (collection: 0.788s, learning 0.102s)
             Mean action noise std: 8.43
          Mean value_function loss: 36.8732
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 28.0334
                       Mean reward: 854.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7733
     Episode_Reward/lifting_object: 171.6951
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1726
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 0.89s
                      Time elapsed: 00:26:41
                               ETA: 00:06:16

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 107243 steps/s (collection: 0.803s, learning 0.114s)
             Mean action noise std: 8.43
          Mean value_function loss: 30.1805
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.0378
                       Mean reward: 859.47
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7747
     Episode_Reward/lifting_object: 172.3512
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.1714
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 0.92s
                      Time elapsed: 00:26:42
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 109770 steps/s (collection: 0.802s, learning 0.094s)
             Mean action noise std: 8.45
          Mean value_function loss: 23.3368
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 28.0468
                       Mean reward: 855.24
               Mean episode length: 249.94
    Episode_Reward/reaching_object: 0.7804
     Episode_Reward/lifting_object: 172.9999
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.1727
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 0.90s
                      Time elapsed: 00:26:43
                               ETA: 00:06:14

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 107638 steps/s (collection: 0.811s, learning 0.102s)
             Mean action noise std: 8.45
          Mean value_function loss: 28.4455
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 28.0561
                       Mean reward: 863.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 170.9399
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1740
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 0.91s
                      Time elapsed: 00:26:44
                               ETA: 00:06:13

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 109530 steps/s (collection: 0.797s, learning 0.100s)
             Mean action noise std: 8.46
          Mean value_function loss: 30.0480
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 28.0623
                       Mean reward: 868.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7814
     Episode_Reward/lifting_object: 173.3292
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1729
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 0.90s
                      Time elapsed: 00:26:45
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 107515 steps/s (collection: 0.812s, learning 0.103s)
             Mean action noise std: 8.48
          Mean value_function loss: 30.5641
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 28.0746
                       Mean reward: 875.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7790
     Episode_Reward/lifting_object: 173.0980
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.1735
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 0.91s
                      Time elapsed: 00:26:46
                               ETA: 00:06:11

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 110289 steps/s (collection: 0.796s, learning 0.096s)
             Mean action noise std: 8.49
          Mean value_function loss: 22.2394
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.0937
                       Mean reward: 848.35
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 171.0249
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1727
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 0.89s
                      Time elapsed: 00:26:47
                               ETA: 00:06:10

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 99348 steps/s (collection: 0.840s, learning 0.149s)
             Mean action noise std: 8.50
          Mean value_function loss: 31.8181
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.1016
                       Mean reward: 874.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7822
     Episode_Reward/lifting_object: 173.7838
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.1733
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 0.99s
                      Time elapsed: 00:26:48
                               ETA: 00:06:09

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 110483 steps/s (collection: 0.796s, learning 0.094s)
             Mean action noise std: 8.51
          Mean value_function loss: 24.1698
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.1107
                       Mean reward: 875.99
               Mean episode length: 249.87
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 172.7707
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.1748
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 0.89s
                      Time elapsed: 00:26:49
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 107102 steps/s (collection: 0.782s, learning 0.136s)
             Mean action noise std: 8.52
          Mean value_function loss: 32.8431
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 28.1238
                       Mean reward: 869.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7831
     Episode_Reward/lifting_object: 173.9432
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.1744
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 0.92s
                      Time elapsed: 00:26:50
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 108752 steps/s (collection: 0.798s, learning 0.106s)
             Mean action noise std: 8.52
          Mean value_function loss: 30.6566
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.1282
                       Mean reward: 873.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7781
     Episode_Reward/lifting_object: 172.3461
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.1742
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 0.90s
                      Time elapsed: 00:26:51
                               ETA: 00:06:06

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 108444 steps/s (collection: 0.799s, learning 0.107s)
             Mean action noise std: 8.53
          Mean value_function loss: 31.9502
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.1354
                       Mean reward: 874.87
               Mean episode length: 249.04
    Episode_Reward/reaching_object: 0.7814
     Episode_Reward/lifting_object: 174.0905
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.1745
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 0.91s
                      Time elapsed: 00:26:51
                               ETA: 00:06:05

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 106456 steps/s (collection: 0.806s, learning 0.117s)
             Mean action noise std: 8.54
          Mean value_function loss: 29.3479
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.1442
                       Mean reward: 870.98
               Mean episode length: 249.26
    Episode_Reward/reaching_object: 0.7790
     Episode_Reward/lifting_object: 173.0880
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1742
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 0.92s
                      Time elapsed: 00:26:52
                               ETA: 00:06:04

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 113594 steps/s (collection: 0.776s, learning 0.089s)
             Mean action noise std: 8.55
          Mean value_function loss: 28.0046
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.1508
                       Mean reward: 864.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7651
     Episode_Reward/lifting_object: 171.4323
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1765
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 0.87s
                      Time elapsed: 00:26:53
                               ETA: 00:06:03

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 108710 steps/s (collection: 0.805s, learning 0.100s)
             Mean action noise std: 8.55
          Mean value_function loss: 27.1784
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 28.1571
                       Mean reward: 860.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7642
     Episode_Reward/lifting_object: 171.0431
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.1773
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 0.90s
                      Time elapsed: 00:26:54
                               ETA: 00:06:02

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 111725 steps/s (collection: 0.774s, learning 0.106s)
             Mean action noise std: 8.55
          Mean value_function loss: 27.4035
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.1587
                       Mean reward: 864.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7818
     Episode_Reward/lifting_object: 173.0062
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1772
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 0.88s
                      Time elapsed: 00:26:55
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 109780 steps/s (collection: 0.782s, learning 0.113s)
             Mean action noise std: 8.56
          Mean value_function loss: 26.3212
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.1608
                       Mean reward: 872.61
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7825
     Episode_Reward/lifting_object: 174.4276
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1768
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 0.90s
                      Time elapsed: 00:26:56
                               ETA: 00:06:00

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 109732 steps/s (collection: 0.792s, learning 0.104s)
             Mean action noise std: 8.56
          Mean value_function loss: 34.3274
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 28.1646
                       Mean reward: 856.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7808
     Episode_Reward/lifting_object: 172.8044
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.1779
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 0.90s
                      Time elapsed: 00:26:57
                               ETA: 00:05:59

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 112490 steps/s (collection: 0.780s, learning 0.094s)
             Mean action noise std: 8.57
          Mean value_function loss: 34.4279
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 28.1688
                       Mean reward: 874.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7788
     Episode_Reward/lifting_object: 173.4038
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.1776
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 0.87s
                      Time elapsed: 00:26:58
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 103536 steps/s (collection: 0.818s, learning 0.132s)
             Mean action noise std: 8.57
          Mean value_function loss: 34.1560
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 28.1745
                       Mean reward: 848.41
               Mean episode length: 246.25
    Episode_Reward/reaching_object: 0.7786
     Episode_Reward/lifting_object: 172.1853
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1773
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 0.95s
                      Time elapsed: 00:26:59
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 106803 steps/s (collection: 0.787s, learning 0.133s)
             Mean action noise std: 8.58
          Mean value_function loss: 33.2689
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 28.1826
                       Mean reward: 861.57
               Mean episode length: 247.54
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 173.6279
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.1776
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 0.92s
                      Time elapsed: 00:27:00
                               ETA: 00:05:56

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 102509 steps/s (collection: 0.774s, learning 0.185s)
             Mean action noise std: 8.59
          Mean value_function loss: 37.7939
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.1897
                       Mean reward: 865.39
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 172.7309
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.1778
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 0.96s
                      Time elapsed: 00:27:01
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 106108 steps/s (collection: 0.789s, learning 0.138s)
             Mean action noise std: 8.60
          Mean value_function loss: 42.1476
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.1970
                       Mean reward: 836.00
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7665
     Episode_Reward/lifting_object: 170.2744
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1786
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 0.93s
                      Time elapsed: 00:27:01
                               ETA: 00:05:54

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 109678 steps/s (collection: 0.797s, learning 0.100s)
             Mean action noise std: 8.60
          Mean value_function loss: 31.3750
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.2030
                       Mean reward: 877.44
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7811
     Episode_Reward/lifting_object: 173.8115
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.1776
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 0.90s
                      Time elapsed: 00:27:02
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 104253 steps/s (collection: 0.825s, learning 0.118s)
             Mean action noise std: 8.61
          Mean value_function loss: 25.0632
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 28.2104
                       Mean reward: 866.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7777
     Episode_Reward/lifting_object: 173.3901
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.1795
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 0.94s
                      Time elapsed: 00:27:03
                               ETA: 00:05:52

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 110216 steps/s (collection: 0.783s, learning 0.109s)
             Mean action noise std: 8.62
          Mean value_function loss: 35.4095
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 28.2149
                       Mean reward: 875.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7768
     Episode_Reward/lifting_object: 172.6820
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1782
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 0.89s
                      Time elapsed: 00:27:04
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 106244 steps/s (collection: 0.821s, learning 0.104s)
             Mean action noise std: 8.62
          Mean value_function loss: 23.0395
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.2192
                       Mean reward: 863.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7662
     Episode_Reward/lifting_object: 170.9802
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.1788
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 0.93s
                      Time elapsed: 00:27:05
                               ETA: 00:05:50

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 111018 steps/s (collection: 0.794s, learning 0.091s)
             Mean action noise std: 8.64
          Mean value_function loss: 31.8792
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.2304
                       Mean reward: 872.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: 173.4539
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.1788
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 0.89s
                      Time elapsed: 00:27:06
                               ETA: 00:05:49

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 108646 steps/s (collection: 0.805s, learning 0.100s)
             Mean action noise std: 8.64
          Mean value_function loss: 31.6865
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.2386
                       Mean reward: 837.21
               Mean episode length: 247.73
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 171.6705
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1797
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 0.90s
                      Time elapsed: 00:27:07
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 108823 steps/s (collection: 0.801s, learning 0.103s)
             Mean action noise std: 8.66
          Mean value_function loss: 30.1260
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 28.2490
                       Mean reward: 856.32
               Mean episode length: 247.85
    Episode_Reward/reaching_object: 0.7750
     Episode_Reward/lifting_object: 172.0950
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.1781
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 0.90s
                      Time elapsed: 00:27:08
                               ETA: 00:05:47

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 114306 steps/s (collection: 0.771s, learning 0.089s)
             Mean action noise std: 8.66
          Mean value_function loss: 24.4884
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.2600
                       Mean reward: 867.76
               Mean episode length: 249.19
    Episode_Reward/reaching_object: 0.7647
     Episode_Reward/lifting_object: 171.5005
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.1788
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 0.86s
                      Time elapsed: 00:27:09
                               ETA: 00:05:46

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 105709 steps/s (collection: 0.789s, learning 0.141s)
             Mean action noise std: 8.67
          Mean value_function loss: 33.8911
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 28.2690
                       Mean reward: 863.06
               Mean episode length: 249.79
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 173.7672
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.1793
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 0.93s
                      Time elapsed: 00:27:10
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 107743 steps/s (collection: 0.797s, learning 0.115s)
             Mean action noise std: 8.68
          Mean value_function loss: 38.6806
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.2757
                       Mean reward: 863.49
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.7671
     Episode_Reward/lifting_object: 172.3120
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.1789
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 0.91s
                      Time elapsed: 00:27:11
                               ETA: 00:05:44

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 106234 steps/s (collection: 0.770s, learning 0.156s)
             Mean action noise std: 8.69
          Mean value_function loss: 38.2794
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.2800
                       Mean reward: 881.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7767
     Episode_Reward/lifting_object: 173.4337
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1787
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 0.93s
                      Time elapsed: 00:27:11
                               ETA: 00:05:43

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 109856 steps/s (collection: 0.785s, learning 0.110s)
             Mean action noise std: 8.69
          Mean value_function loss: 31.1190
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 28.2886
                       Mean reward: 861.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7766
     Episode_Reward/lifting_object: 173.1659
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.1788
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 0.89s
                      Time elapsed: 00:27:12
                               ETA: 00:05:42

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 111171 steps/s (collection: 0.794s, learning 0.091s)
             Mean action noise std: 8.71
          Mean value_function loss: 46.6924
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 28.2991
                       Mean reward: 867.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7843
     Episode_Reward/lifting_object: 174.5666
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.1788
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 0.88s
                      Time elapsed: 00:27:13
                               ETA: 00:05:41

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 110469 steps/s (collection: 0.793s, learning 0.097s)
             Mean action noise std: 8.72
          Mean value_function loss: 27.4839
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 28.3106
                       Mean reward: 857.58
               Mean episode length: 248.08
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 170.6032
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.1797
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 0.89s
                      Time elapsed: 00:27:14
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 108091 steps/s (collection: 0.804s, learning 0.106s)
             Mean action noise std: 8.73
          Mean value_function loss: 34.9947
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 28.3209
                       Mean reward: 861.04
               Mean episode length: 248.82
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 172.4714
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.1801
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 0.91s
                      Time elapsed: 00:27:15
                               ETA: 00:05:39

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 105714 steps/s (collection: 0.824s, learning 0.106s)
             Mean action noise std: 8.73
          Mean value_function loss: 32.4299
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.3249
                       Mean reward: 859.79
               Mean episode length: 245.93
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 171.7897
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.1796
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 0.93s
                      Time elapsed: 00:27:16
                               ETA: 00:05:38

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 110043 steps/s (collection: 0.799s, learning 0.094s)
             Mean action noise std: 8.74
          Mean value_function loss: 39.6454
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.3289
                       Mean reward: 877.39
               Mean episode length: 249.83
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 172.7859
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.1805
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 0.89s
                      Time elapsed: 00:27:17
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 102131 steps/s (collection: 0.847s, learning 0.116s)
             Mean action noise std: 8.75
          Mean value_function loss: 37.6400
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.3354
                       Mean reward: 858.39
               Mean episode length: 249.18
    Episode_Reward/reaching_object: 0.7597
     Episode_Reward/lifting_object: 170.1700
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.1807
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 0.96s
                      Time elapsed: 00:27:18
                               ETA: 00:05:36

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 90299 steps/s (collection: 0.897s, learning 0.192s)
             Mean action noise std: 8.76
          Mean value_function loss: 39.7019
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 28.3461
                       Mean reward: 871.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 172.2360
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.1820
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 1.09s
                      Time elapsed: 00:27:19
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 109566 steps/s (collection: 0.803s, learning 0.094s)
             Mean action noise std: 8.77
          Mean value_function loss: 47.4129
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 28.3558
                       Mean reward: 879.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7749
     Episode_Reward/lifting_object: 173.3051
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.1823
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 0.90s
                      Time elapsed: 00:27:20
                               ETA: 00:05:34

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 108998 steps/s (collection: 0.811s, learning 0.091s)
             Mean action noise std: 8.78
          Mean value_function loss: 40.4544
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.3652
                       Mean reward: 863.51
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 171.9944
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.1832
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 0.90s
                      Time elapsed: 00:27:21
                               ETA: 00:05:33

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 105324 steps/s (collection: 0.828s, learning 0.105s)
             Mean action noise std: 8.78
          Mean value_function loss: 28.4975
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.3722
                       Mean reward: 856.64
               Mean episode length: 248.99
    Episode_Reward/reaching_object: 0.7520
     Episode_Reward/lifting_object: 169.5244
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.1838
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 0.93s
                      Time elapsed: 00:27:22
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 94487 steps/s (collection: 0.821s, learning 0.219s)
             Mean action noise std: 8.79
          Mean value_function loss: 31.6286
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 28.3795
                       Mean reward: 858.05
               Mean episode length: 249.58
    Episode_Reward/reaching_object: 0.7598
     Episode_Reward/lifting_object: 170.9243
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.1853
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 1.04s
                      Time elapsed: 00:27:23
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 106235 steps/s (collection: 0.805s, learning 0.120s)
             Mean action noise std: 8.80
          Mean value_function loss: 34.5659
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 28.3861
                       Mean reward: 866.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 173.1763
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.1841
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 0.93s
                      Time elapsed: 00:27:24
                               ETA: 00:05:30

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 61110 steps/s (collection: 1.509s, learning 0.100s)
             Mean action noise std: 8.80
          Mean value_function loss: 32.1704
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 28.3913
                       Mean reward: 861.84
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.7676
     Episode_Reward/lifting_object: 170.9559
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.1852
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 1.61s
                      Time elapsed: 00:27:25
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 30930 steps/s (collection: 3.053s, learning 0.125s)
             Mean action noise std: 8.81
          Mean value_function loss: 29.1821
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.3991
                       Mean reward: 879.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 173.3102
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.1856
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 3.18s
                      Time elapsed: 00:27:28
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 31762 steps/s (collection: 2.964s, learning 0.131s)
             Mean action noise std: 8.82
          Mean value_function loss: 26.5984
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 28.4065
                       Mean reward: 871.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 170.7565
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.1869
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 3.09s
                      Time elapsed: 00:27:31
                               ETA: 00:05:28

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 30431 steps/s (collection: 3.102s, learning 0.129s)
             Mean action noise std: 8.83
          Mean value_function loss: 23.9209
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.4131
                       Mean reward: 861.15
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.7756
     Episode_Reward/lifting_object: 172.2624
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.1872
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 3.23s
                      Time elapsed: 00:27:35
                               ETA: 00:05:28

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 29990 steps/s (collection: 3.148s, learning 0.130s)
             Mean action noise std: 8.83
          Mean value_function loss: 33.0858
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 28.4167
                       Mean reward: 872.07
               Mean episode length: 248.97
    Episode_Reward/reaching_object: 0.7762
     Episode_Reward/lifting_object: 171.9552
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.1873
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 3.28s
                      Time elapsed: 00:27:38
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 32123 steps/s (collection: 2.943s, learning 0.117s)
             Mean action noise std: 8.84
          Mean value_function loss: 27.1139
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.4200
                       Mean reward: 859.62
               Mean episode length: 248.28
    Episode_Reward/reaching_object: 0.7790
     Episode_Reward/lifting_object: 172.6508
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.1872
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 3.06s
                      Time elapsed: 00:27:41
                               ETA: 00:05:26

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 29801 steps/s (collection: 3.160s, learning 0.139s)
             Mean action noise std: 8.84
          Mean value_function loss: 24.3661
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 28.4254
                       Mean reward: 879.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7792
     Episode_Reward/lifting_object: 172.9784
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.1883
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 3.30s
                      Time elapsed: 00:27:44
                               ETA: 00:05:26

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 30035 steps/s (collection: 3.093s, learning 0.180s)
             Mean action noise std: 8.85
          Mean value_function loss: 29.9675
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.4329
                       Mean reward: 872.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7823
     Episode_Reward/lifting_object: 173.9687
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.1885
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 3.27s
                      Time elapsed: 00:27:48
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 29972 steps/s (collection: 3.156s, learning 0.124s)
             Mean action noise std: 8.86
          Mean value_function loss: 41.3402
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 28.4418
                       Mean reward: 877.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 172.5320
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.1896
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 3.28s
                      Time elapsed: 00:27:51
                               ETA: 00:05:25

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 30377 steps/s (collection: 3.124s, learning 0.112s)
             Mean action noise std: 8.87
          Mean value_function loss: 28.5780
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 28.4481
                       Mean reward: 865.24
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 170.8851
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.1875
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 3.24s
                      Time elapsed: 00:27:54
                               ETA: 00:05:24

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 113372 steps/s (collection: 0.778s, learning 0.090s)
             Mean action noise std: 8.87
          Mean value_function loss: 36.9012
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.4549
                       Mean reward: 864.59
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7763
     Episode_Reward/lifting_object: 171.7322
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.1888
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 0.87s
                      Time elapsed: 00:27:55
                               ETA: 00:05:23

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 84413 steps/s (collection: 0.977s, learning 0.188s)
             Mean action noise std: 8.88
          Mean value_function loss: 32.1726
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.4602
                       Mean reward: 841.33
               Mean episode length: 248.39
    Episode_Reward/reaching_object: 0.7716
     Episode_Reward/lifting_object: 171.9591
      Episode_Reward/object_height: 0.0549
        Episode_Reward/action_rate: -0.1896
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 1.16s
                      Time elapsed: 00:27:56
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 102101 steps/s (collection: 0.875s, learning 0.088s)
             Mean action noise std: 8.89
          Mean value_function loss: 27.2114
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.4661
                       Mean reward: 864.98
               Mean episode length: 247.93
    Episode_Reward/reaching_object: 0.7791
     Episode_Reward/lifting_object: 173.7586
      Episode_Reward/object_height: 0.0559
        Episode_Reward/action_rate: -0.1897
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 0.96s
                      Time elapsed: 00:27:57
                               ETA: 00:05:21

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 90850 steps/s (collection: 0.957s, learning 0.125s)
             Mean action noise std: 8.89
          Mean value_function loss: 27.5117
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 28.4706
                       Mean reward: 881.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7685
     Episode_Reward/lifting_object: 172.5674
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.1908
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 1.08s
                      Time elapsed: 00:27:58
                               ETA: 00:05:20

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 105373 steps/s (collection: 0.837s, learning 0.096s)
             Mean action noise std: 8.90
          Mean value_function loss: 23.4356
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.4755
                       Mean reward: 866.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7708
     Episode_Reward/lifting_object: 171.3133
      Episode_Reward/object_height: 0.0551
        Episode_Reward/action_rate: -0.1900
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 0.93s
                      Time elapsed: 00:27:59
                               ETA: 00:05:19

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 103539 steps/s (collection: 0.832s, learning 0.117s)
             Mean action noise std: 8.91
          Mean value_function loss: 20.6650
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 28.4847
                       Mean reward: 872.01
               Mean episode length: 248.94
    Episode_Reward/reaching_object: 0.7796
     Episode_Reward/lifting_object: 172.5383
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.1912
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 0.95s
                      Time elapsed: 00:28:00
                               ETA: 00:05:18

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 102546 steps/s (collection: 0.842s, learning 0.116s)
             Mean action noise std: 8.92
          Mean value_function loss: 33.6827
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.4934
                       Mean reward: 873.09
               Mean episode length: 249.17
    Episode_Reward/reaching_object: 0.7767
     Episode_Reward/lifting_object: 172.3715
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.1916
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 0.96s
                      Time elapsed: 00:28:01
                               ETA: 00:05:17

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 103698 steps/s (collection: 0.836s, learning 0.112s)
             Mean action noise std: 8.93
          Mean value_function loss: 30.4628
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.5034
                       Mean reward: 861.98
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7733
     Episode_Reward/lifting_object: 171.4662
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.1927
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 0.95s
                      Time elapsed: 00:28:02
                               ETA: 00:05:16

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 100888 steps/s (collection: 0.862s, learning 0.112s)
             Mean action noise std: 8.94
          Mean value_function loss: 25.7051
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 28.5107
                       Mean reward: 867.45
               Mean episode length: 248.32
    Episode_Reward/reaching_object: 0.7654
     Episode_Reward/lifting_object: 170.6339
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.1931
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 0.97s
                      Time elapsed: 00:28:03
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 106236 steps/s (collection: 0.829s, learning 0.096s)
             Mean action noise std: 8.94
          Mean value_function loss: 29.2246
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 28.5176
                       Mean reward: 875.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 172.7618
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.1936
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 0.93s
                      Time elapsed: 00:28:04
                               ETA: 00:05:14

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 103120 steps/s (collection: 0.833s, learning 0.120s)
             Mean action noise std: 8.94
          Mean value_function loss: 27.0046
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.5188
                       Mean reward: 849.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 173.9935
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.1961
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 0.95s
                      Time elapsed: 00:28:05
                               ETA: 00:05:13

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 109240 steps/s (collection: 0.794s, learning 0.106s)
             Mean action noise std: 8.94
          Mean value_function loss: 30.5523
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 28.5201
                       Mean reward: 876.42
               Mean episode length: 249.67
    Episode_Reward/reaching_object: 0.7754
     Episode_Reward/lifting_object: 172.9734
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.1956
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 0.90s
                      Time elapsed: 00:28:06
                               ETA: 00:05:12

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 103535 steps/s (collection: 0.796s, learning 0.154s)
             Mean action noise std: 8.95
          Mean value_function loss: 23.4087
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 28.5229
                       Mean reward: 848.99
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 171.6147
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.1963
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 0.95s
                      Time elapsed: 00:28:07
                               ETA: 00:05:11

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 102744 steps/s (collection: 0.796s, learning 0.161s)
             Mean action noise std: 8.95
          Mean value_function loss: 35.2784
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 28.5246
                       Mean reward: 877.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 173.6716
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.1970
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 0.96s
                      Time elapsed: 00:28:08
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 111458 steps/s (collection: 0.764s, learning 0.118s)
             Mean action noise std: 8.96
          Mean value_function loss: 28.6309
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 28.5291
                       Mean reward: 860.77
               Mean episode length: 246.54
    Episode_Reward/reaching_object: 0.7697
     Episode_Reward/lifting_object: 172.2229
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.1965
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 0.88s
                      Time elapsed: 00:28:09
                               ETA: 00:05:09

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 106304 steps/s (collection: 0.807s, learning 0.118s)
             Mean action noise std: 8.96
          Mean value_function loss: 17.5389
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 28.5377
                       Mean reward: 850.20
               Mean episode length: 248.95
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 170.6917
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.1990
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 0.92s
                      Time elapsed: 00:28:09
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 108707 steps/s (collection: 0.807s, learning 0.097s)
             Mean action noise std: 8.97
          Mean value_function loss: 30.4117
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.5421
                       Mean reward: 871.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7800
     Episode_Reward/lifting_object: 172.6636
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.1988
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 0.90s
                      Time elapsed: 00:28:10
                               ETA: 00:05:07

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 105924 steps/s (collection: 0.806s, learning 0.122s)
             Mean action noise std: 8.99
          Mean value_function loss: 32.5258
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 28.5519
                       Mean reward: 847.24
               Mean episode length: 245.87
    Episode_Reward/reaching_object: 0.7726
     Episode_Reward/lifting_object: 173.0894
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.1979
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 0.93s
                      Time elapsed: 00:28:11
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 103185 steps/s (collection: 0.818s, learning 0.134s)
             Mean action noise std: 9.00
          Mean value_function loss: 27.3192
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 28.5646
                       Mean reward: 875.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7805
     Episode_Reward/lifting_object: 174.8301
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.1994
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 0.95s
                      Time elapsed: 00:28:12
                               ETA: 00:05:05

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 98972 steps/s (collection: 0.869s, learning 0.124s)
             Mean action noise std: 9.00
          Mean value_function loss: 26.8006
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.5711
                       Mean reward: 863.20
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 171.5423
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.2005
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 0.99s
                      Time elapsed: 00:28:13
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 101918 steps/s (collection: 0.840s, learning 0.125s)
             Mean action noise std: 9.01
          Mean value_function loss: 23.3518
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.5763
                       Mean reward: 883.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7758
     Episode_Reward/lifting_object: 172.7067
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.2003
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 0.96s
                      Time elapsed: 00:28:14
                               ETA: 00:05:03

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 107595 steps/s (collection: 0.814s, learning 0.100s)
             Mean action noise std: 9.02
          Mean value_function loss: 30.7503
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 28.5845
                       Mean reward: 875.34
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 172.8113
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.2004
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 0.91s
                      Time elapsed: 00:28:15
                               ETA: 00:05:02

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 107537 steps/s (collection: 0.816s, learning 0.098s)
             Mean action noise std: 9.02
          Mean value_function loss: 25.9997
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.5903
                       Mean reward: 880.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7737
     Episode_Reward/lifting_object: 171.4554
      Episode_Reward/object_height: 0.0522
        Episode_Reward/action_rate: -0.2010
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 0.91s
                      Time elapsed: 00:28:16
                               ETA: 00:05:01

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 110538 steps/s (collection: 0.783s, learning 0.106s)
             Mean action noise std: 9.04
          Mean value_function loss: 23.2185
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 28.6028
                       Mean reward: 863.34
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: 173.0997
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.2001
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 0.89s
                      Time elapsed: 00:28:17
                               ETA: 00:05:00

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 109257 steps/s (collection: 0.799s, learning 0.101s)
             Mean action noise std: 9.05
          Mean value_function loss: 34.6588
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 28.6133
                       Mean reward: 875.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7871
     Episode_Reward/lifting_object: 173.7128
      Episode_Reward/object_height: 0.0527
        Episode_Reward/action_rate: -0.2026
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 0.90s
                      Time elapsed: 00:28:18
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 114318 steps/s (collection: 0.766s, learning 0.094s)
             Mean action noise std: 9.06
          Mean value_function loss: 30.4475
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.6212
                       Mean reward: 845.40
               Mean episode length: 244.24
    Episode_Reward/reaching_object: 0.7713
     Episode_Reward/lifting_object: 172.2848
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.2021
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 0.86s
                      Time elapsed: 00:28:19
                               ETA: 00:04:58

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 108221 steps/s (collection: 0.779s, learning 0.130s)
             Mean action noise std: 9.07
          Mean value_function loss: 25.1499
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.6321
                       Mean reward: 833.09
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7701
     Episode_Reward/lifting_object: 170.6159
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.2031
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 0.91s
                      Time elapsed: 00:28:20
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 111225 steps/s (collection: 0.785s, learning 0.099s)
             Mean action noise std: 9.08
          Mean value_function loss: 32.9775
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.6441
                       Mean reward: 874.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7726
     Episode_Reward/lifting_object: 171.3739
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.2042
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 0.88s
                      Time elapsed: 00:28:20
                               ETA: 00:04:56

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 113234 steps/s (collection: 0.777s, learning 0.091s)
             Mean action noise std: 9.09
          Mean value_function loss: 28.8298
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 28.6551
                       Mean reward: 882.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7851
     Episode_Reward/lifting_object: 174.3248
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.2035
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 0.87s
                      Time elapsed: 00:28:21
                               ETA: 00:04:55

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 108328 steps/s (collection: 0.812s, learning 0.096s)
             Mean action noise std: 9.10
          Mean value_function loss: 33.6717
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 28.6644
                       Mean reward: 866.27
               Mean episode length: 248.27
    Episode_Reward/reaching_object: 0.7813
     Episode_Reward/lifting_object: 173.3818
      Episode_Reward/object_height: 0.0518
        Episode_Reward/action_rate: -0.2050
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 0.91s
                      Time elapsed: 00:28:22
                               ETA: 00:04:54

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 115940 steps/s (collection: 0.758s, learning 0.090s)
             Mean action noise std: 9.11
          Mean value_function loss: 30.6161
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.6718
                       Mean reward: 869.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 172.3433
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.2039
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 0.85s
                      Time elapsed: 00:28:23
                               ETA: 00:04:53

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 110096 steps/s (collection: 0.806s, learning 0.087s)
             Mean action noise std: 9.11
          Mean value_function loss: 23.3140
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 28.6768
                       Mean reward: 855.82
               Mean episode length: 245.35
    Episode_Reward/reaching_object: 0.7705
     Episode_Reward/lifting_object: 172.4960
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.2043
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 0.89s
                      Time elapsed: 00:28:24
                               ETA: 00:04:52

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 113459 steps/s (collection: 0.771s, learning 0.096s)
             Mean action noise std: 9.13
          Mean value_function loss: 26.6671
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 28.6885
                       Mean reward: 867.93
               Mean episode length: 249.18
    Episode_Reward/reaching_object: 0.7784
     Episode_Reward/lifting_object: 172.8824
      Episode_Reward/object_height: 0.0518
        Episode_Reward/action_rate: -0.2065
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 0.87s
                      Time elapsed: 00:28:25
                               ETA: 00:04:51

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 110330 steps/s (collection: 0.800s, learning 0.091s)
             Mean action noise std: 9.14
          Mean value_function loss: 25.4587
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 28.7010
                       Mean reward: 861.28
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 170.1736
      Episode_Reward/object_height: 0.0511
        Episode_Reward/action_rate: -0.2086
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 0.89s
                      Time elapsed: 00:28:26
                               ETA: 00:04:50

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 113843 steps/s (collection: 0.765s, learning 0.098s)
             Mean action noise std: 9.15
          Mean value_function loss: 37.2294
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 28.7152
                       Mean reward: 853.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 172.7303
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.2094
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 0.86s
                      Time elapsed: 00:28:27
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 114930 steps/s (collection: 0.758s, learning 0.097s)
             Mean action noise std: 9.17
          Mean value_function loss: 33.7359
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 28.7250
                       Mean reward: 872.00
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 172.6058
      Episode_Reward/object_height: 0.0516
        Episode_Reward/action_rate: -0.2083
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 0.86s
                      Time elapsed: 00:28:27
                               ETA: 00:04:48

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 115979 steps/s (collection: 0.760s, learning 0.088s)
             Mean action noise std: 9.18
          Mean value_function loss: 42.9606
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 28.7363
                       Mean reward: 866.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 173.5719
      Episode_Reward/object_height: 0.0516
        Episode_Reward/action_rate: -0.2073
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 0.85s
                      Time elapsed: 00:28:28
                               ETA: 00:04:47

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 113365 steps/s (collection: 0.778s, learning 0.090s)
             Mean action noise std: 9.19
          Mean value_function loss: 51.7723
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.7440
                       Mean reward: 871.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7726
     Episode_Reward/lifting_object: 171.5970
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.2096
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 0.87s
                      Time elapsed: 00:28:29
                               ETA: 00:04:46

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 113258 steps/s (collection: 0.770s, learning 0.098s)
             Mean action noise std: 9.20
          Mean value_function loss: 50.7598
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 28.7547
                       Mean reward: 866.38
               Mean episode length: 249.19
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 172.8157
      Episode_Reward/object_height: 0.0498
        Episode_Reward/action_rate: -0.2110
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 0.87s
                      Time elapsed: 00:28:30
                               ETA: 00:04:45

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 113185 steps/s (collection: 0.781s, learning 0.088s)
             Mean action noise std: 9.21
          Mean value_function loss: 41.1825
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 28.7636
                       Mean reward: 861.33
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 172.5404
      Episode_Reward/object_height: 0.0499
        Episode_Reward/action_rate: -0.2106
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 0.87s
                      Time elapsed: 00:28:31
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 113408 steps/s (collection: 0.779s, learning 0.088s)
             Mean action noise std: 9.21
          Mean value_function loss: 45.5570
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 28.7661
                       Mean reward: 874.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: 173.1078
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.2122
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 0.87s
                      Time elapsed: 00:28:32
                               ETA: 00:04:43

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 106083 steps/s (collection: 0.803s, learning 0.124s)
             Mean action noise std: 9.22
          Mean value_function loss: 44.3891
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 28.7706
                       Mean reward: 845.02
               Mean episode length: 246.20
    Episode_Reward/reaching_object: 0.7593
     Episode_Reward/lifting_object: 169.6972
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.2100
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 0.93s
                      Time elapsed: 00:28:33
                               ETA: 00:04:42

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 117140 steps/s (collection: 0.749s, learning 0.091s)
             Mean action noise std: 9.23
          Mean value_function loss: 37.5828
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.7769
                       Mean reward: 821.01
               Mean episode length: 245.32
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 170.4034
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.2122
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 0.84s
                      Time elapsed: 00:28:34
                               ETA: 00:04:41

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 105710 steps/s (collection: 0.794s, learning 0.136s)
             Mean action noise std: 9.23
          Mean value_function loss: 37.8033
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 28.7809
                       Mean reward: 870.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 172.4316
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.2157
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 0.93s
                      Time elapsed: 00:28:34
                               ETA: 00:04:40

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 102076 steps/s (collection: 0.863s, learning 0.100s)
             Mean action noise std: 9.24
          Mean value_function loss: 30.2543
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 28.7871
                       Mean reward: 836.66
               Mean episode length: 244.05
    Episode_Reward/reaching_object: 0.7568
     Episode_Reward/lifting_object: 170.7330
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.2141
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 0.96s
                      Time elapsed: 00:28:35
                               ETA: 00:04:39

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 115458 steps/s (collection: 0.763s, learning 0.089s)
             Mean action noise std: 9.25
          Mean value_function loss: 35.6670
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 28.7969
                       Mean reward: 840.75
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7477
     Episode_Reward/lifting_object: 168.9793
      Episode_Reward/object_height: 0.0470
        Episode_Reward/action_rate: -0.2174
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 0.85s
                      Time elapsed: 00:28:36
                               ETA: 00:04:38

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 106212 steps/s (collection: 0.804s, learning 0.121s)
             Mean action noise std: 9.27
          Mean value_function loss: 30.6207
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 28.8080
                       Mean reward: 852.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7508
     Episode_Reward/lifting_object: 170.7317
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.2175
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 0.93s
                      Time elapsed: 00:28:37
                               ETA: 00:04:37

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 112888 steps/s (collection: 0.778s, learning 0.093s)
             Mean action noise std: 9.27
          Mean value_function loss: 30.1107
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 28.8175
                       Mean reward: 822.22
               Mean episode length: 248.18
    Episode_Reward/reaching_object: 0.7408
     Episode_Reward/lifting_object: 167.3682
      Episode_Reward/object_height: 0.0470
        Episode_Reward/action_rate: -0.2190
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 0.87s
                      Time elapsed: 00:28:38
                               ETA: 00:04:36

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 117384 steps/s (collection: 0.744s, learning 0.094s)
             Mean action noise std: 9.28
          Mean value_function loss: 36.6090
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.8211
                       Mean reward: 853.88
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7509
     Episode_Reward/lifting_object: 170.3915
      Episode_Reward/object_height: 0.0477
        Episode_Reward/action_rate: -0.2180
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 0.84s
                      Time elapsed: 00:28:39
                               ETA: 00:04:35

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 110437 steps/s (collection: 0.788s, learning 0.103s)
             Mean action noise std: 9.29
          Mean value_function loss: 23.4833
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 28.8290
                       Mean reward: 852.05
               Mean episode length: 249.97
    Episode_Reward/reaching_object: 0.7580
     Episode_Reward/lifting_object: 171.0232
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.2182
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 0.89s
                      Time elapsed: 00:28:40
                               ETA: 00:04:34

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 117679 steps/s (collection: 0.744s, learning 0.091s)
             Mean action noise std: 9.29
          Mean value_function loss: 39.5801
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.8352
                       Mean reward: 872.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 172.0903
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.2184
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 0.84s
                      Time elapsed: 00:28:41
                               ETA: 00:04:33

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 115632 steps/s (collection: 0.730s, learning 0.120s)
             Mean action noise std: 9.30
          Mean value_function loss: 29.1234
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 28.8432
                       Mean reward: 866.33
               Mean episode length: 248.05
    Episode_Reward/reaching_object: 0.7483
     Episode_Reward/lifting_object: 169.1929
      Episode_Reward/object_height: 0.0478
        Episode_Reward/action_rate: -0.2171
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 0.85s
                      Time elapsed: 00:28:41
                               ETA: 00:04:32

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 115989 steps/s (collection: 0.762s, learning 0.086s)
             Mean action noise std: 9.31
          Mean value_function loss: 24.4415
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.8481
                       Mean reward: 862.83
               Mean episode length: 246.48
    Episode_Reward/reaching_object: 0.7501
     Episode_Reward/lifting_object: 170.8875
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.2172
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 0.85s
                      Time elapsed: 00:28:42
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 118341 steps/s (collection: 0.742s, learning 0.089s)
             Mean action noise std: 9.32
          Mean value_function loss: 29.0430
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 28.8552
                       Mean reward: 878.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 173.2206
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.2193
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 0.83s
                      Time elapsed: 00:28:43
                               ETA: 00:04:30

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 114272 steps/s (collection: 0.757s, learning 0.104s)
             Mean action noise std: 9.33
          Mean value_function loss: 24.5546
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.8641
                       Mean reward: 881.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 171.1045
      Episode_Reward/object_height: 0.0481
        Episode_Reward/action_rate: -0.2179
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 0.86s
                      Time elapsed: 00:28:44
                               ETA: 00:04:28

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 115116 steps/s (collection: 0.762s, learning 0.092s)
             Mean action noise std: 9.33
          Mean value_function loss: 26.9938
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 28.8704
                       Mean reward: 859.08
               Mean episode length: 248.69
    Episode_Reward/reaching_object: 0.7726
     Episode_Reward/lifting_object: 172.0851
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.2173
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 0.85s
                      Time elapsed: 00:28:45
                               ETA: 00:04:27

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 114398 steps/s (collection: 0.756s, learning 0.104s)
             Mean action noise std: 9.34
          Mean value_function loss: 23.0159
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.8799
                       Mean reward: 867.13
               Mean episode length: 249.14
    Episode_Reward/reaching_object: 0.7659
     Episode_Reward/lifting_object: 172.9604
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.2172
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 0.86s
                      Time elapsed: 00:28:46
                               ETA: 00:04:26

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 107480 steps/s (collection: 0.777s, learning 0.137s)
             Mean action noise std: 9.35
          Mean value_function loss: 21.3665
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 28.8902
                       Mean reward: 852.17
               Mean episode length: 249.54
    Episode_Reward/reaching_object: 0.7649
     Episode_Reward/lifting_object: 171.4598
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.2183
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 0.91s
                      Time elapsed: 00:28:47
                               ETA: 00:04:25

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 104360 steps/s (collection: 0.795s, learning 0.147s)
             Mean action noise std: 9.36
          Mean value_function loss: 27.7485
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 28.8977
                       Mean reward: 868.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7692
     Episode_Reward/lifting_object: 172.9389
      Episode_Reward/object_height: 0.0497
        Episode_Reward/action_rate: -0.2172
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 0.94s
                      Time elapsed: 00:28:48
                               ETA: 00:04:24

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 110477 steps/s (collection: 0.751s, learning 0.139s)
             Mean action noise std: 9.37
          Mean value_function loss: 25.3534
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 28.9050
                       Mean reward: 876.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7522
     Episode_Reward/lifting_object: 168.4264
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.2205
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 0.89s
                      Time elapsed: 00:28:48
                               ETA: 00:04:23

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 118820 steps/s (collection: 0.731s, learning 0.097s)
             Mean action noise std: 9.38
          Mean value_function loss: 32.2184
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 28.9159
                       Mean reward: 867.20
               Mean episode length: 247.35
    Episode_Reward/reaching_object: 0.7698
     Episode_Reward/lifting_object: 173.7753
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.2180
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 0.83s
                      Time elapsed: 00:28:49
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 114174 steps/s (collection: 0.761s, learning 0.100s)
             Mean action noise std: 9.39
          Mean value_function loss: 27.9375
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.9218
                       Mean reward: 867.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7737
     Episode_Reward/lifting_object: 172.8128
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.2191
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 0.86s
                      Time elapsed: 00:28:50
                               ETA: 00:04:21

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 114286 steps/s (collection: 0.765s, learning 0.095s)
             Mean action noise std: 9.39
          Mean value_function loss: 24.1147
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 28.9275
                       Mean reward: 873.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7682
     Episode_Reward/lifting_object: 172.0845
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.2200
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 0.86s
                      Time elapsed: 00:28:51
                               ETA: 00:04:20

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 112946 steps/s (collection: 0.756s, learning 0.114s)
             Mean action noise std: 9.40
          Mean value_function loss: 25.5902
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.9290
                       Mean reward: 880.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7772
     Episode_Reward/lifting_object: 173.6708
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.2195
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 0.87s
                      Time elapsed: 00:28:52
                               ETA: 00:04:19

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 108155 steps/s (collection: 0.802s, learning 0.107s)
             Mean action noise std: 9.40
          Mean value_function loss: 34.5737
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 28.9338
                       Mean reward: 871.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7843
     Episode_Reward/lifting_object: 174.1527
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.2210
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 0.91s
                      Time elapsed: 00:28:53
                               ETA: 00:04:18

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 117528 steps/s (collection: 0.739s, learning 0.097s)
             Mean action noise std: 9.41
          Mean value_function loss: 30.8331
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 28.9400
                       Mean reward: 867.87
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7753
     Episode_Reward/lifting_object: 173.5189
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.2196
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 0.84s
                      Time elapsed: 00:28:54
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 112626 steps/s (collection: 0.759s, learning 0.114s)
             Mean action noise std: 9.42
          Mean value_function loss: 27.0528
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 28.9486
                       Mean reward: 879.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7769
     Episode_Reward/lifting_object: 174.4540
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.2208
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 0.87s
                      Time elapsed: 00:28:55
                               ETA: 00:04:16

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 116451 steps/s (collection: 0.746s, learning 0.098s)
             Mean action noise std: 9.43
          Mean value_function loss: 35.5884
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 28.9565
                       Mean reward: 860.30
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7710
     Episode_Reward/lifting_object: 171.4813
      Episode_Reward/object_height: 0.0514
        Episode_Reward/action_rate: -0.2208
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 0.84s
                      Time elapsed: 00:28:55
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 113911 steps/s (collection: 0.762s, learning 0.101s)
             Mean action noise std: 9.43
          Mean value_function loss: 38.4457
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 28.9647
                       Mean reward: 828.90
               Mean episode length: 244.16
    Episode_Reward/reaching_object: 0.7720
     Episode_Reward/lifting_object: 171.1863
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.2210
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 0.86s
                      Time elapsed: 00:28:56
                               ETA: 00:04:14

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 113467 steps/s (collection: 0.771s, learning 0.095s)
             Mean action noise std: 9.44
          Mean value_function loss: 32.5067
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 28.9661
                       Mean reward: 872.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7788
     Episode_Reward/lifting_object: 172.5178
      Episode_Reward/object_height: 0.0516
        Episode_Reward/action_rate: -0.2202
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 0.87s
                      Time elapsed: 00:28:57
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 111286 steps/s (collection: 0.780s, learning 0.103s)
             Mean action noise std: 9.44
          Mean value_function loss: 29.9913
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 28.9714
                       Mean reward: 869.91
               Mean episode length: 248.99
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 172.2390
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.2227
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 0.88s
                      Time elapsed: 00:28:58
                               ETA: 00:04:12

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 114693 steps/s (collection: 0.767s, learning 0.090s)
             Mean action noise std: 9.45
          Mean value_function loss: 26.2309
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 28.9786
                       Mean reward: 853.50
               Mean episode length: 247.69
    Episode_Reward/reaching_object: 0.7792
     Episode_Reward/lifting_object: 172.9306
      Episode_Reward/object_height: 0.0518
        Episode_Reward/action_rate: -0.2227
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 0.86s
                      Time elapsed: 00:28:59
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 111959 steps/s (collection: 0.769s, learning 0.109s)
             Mean action noise std: 9.46
          Mean value_function loss: 22.3807
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 28.9860
                       Mean reward: 873.19
               Mean episode length: 249.04
    Episode_Reward/reaching_object: 0.7809
     Episode_Reward/lifting_object: 172.9134
      Episode_Reward/object_height: 0.0518
        Episode_Reward/action_rate: -0.2230
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 0.88s
                      Time elapsed: 00:29:00
                               ETA: 00:04:10

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 108844 steps/s (collection: 0.767s, learning 0.136s)
             Mean action noise std: 9.47
          Mean value_function loss: 25.3551
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 28.9938
                       Mean reward: 885.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7766
     Episode_Reward/lifting_object: 172.7471
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.2224
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 0.90s
                      Time elapsed: 00:29:01
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 110383 steps/s (collection: 0.779s, learning 0.112s)
             Mean action noise std: 9.48
          Mean value_function loss: 23.3297
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.0038
                       Mean reward: 871.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7812
     Episode_Reward/lifting_object: 172.9595
      Episode_Reward/object_height: 0.0516
        Episode_Reward/action_rate: -0.2233
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 0.89s
                      Time elapsed: 00:29:02
                               ETA: 00:04:08

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 110282 steps/s (collection: 0.772s, learning 0.119s)
             Mean action noise std: 9.49
          Mean value_function loss: 36.2075
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.0104
                       Mean reward: 879.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7832
     Episode_Reward/lifting_object: 174.7277
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.2225
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 0.89s
                      Time elapsed: 00:29:02
                               ETA: 00:04:07

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 116924 steps/s (collection: 0.754s, learning 0.087s)
             Mean action noise std: 9.50
          Mean value_function loss: 23.4087
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 29.0189
                       Mean reward: 857.38
               Mean episode length: 249.10
    Episode_Reward/reaching_object: 0.7579
     Episode_Reward/lifting_object: 170.8591
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.2248
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 0.84s
                      Time elapsed: 00:29:03
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 115663 steps/s (collection: 0.754s, learning 0.096s)
             Mean action noise std: 9.50
          Mean value_function loss: 27.0710
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 29.0255
                       Mean reward: 867.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7700
     Episode_Reward/lifting_object: 172.8044
      Episode_Reward/object_height: 0.0511
        Episode_Reward/action_rate: -0.2251
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 0.85s
                      Time elapsed: 00:29:04
                               ETA: 00:04:05

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 117558 steps/s (collection: 0.748s, learning 0.088s)
             Mean action noise std: 9.51
          Mean value_function loss: 25.2808
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.0324
                       Mean reward: 877.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7682
     Episode_Reward/lifting_object: 171.9065
      Episode_Reward/object_height: 0.0506
        Episode_Reward/action_rate: -0.2240
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 0.84s
                      Time elapsed: 00:29:05
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 106526 steps/s (collection: 0.792s, learning 0.131s)
             Mean action noise std: 9.52
          Mean value_function loss: 32.5957
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 29.0396
                       Mean reward: 880.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7762
     Episode_Reward/lifting_object: 173.6290
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.2231
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 0.92s
                      Time elapsed: 00:29:06
                               ETA: 00:04:03

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 113682 steps/s (collection: 0.753s, learning 0.112s)
             Mean action noise std: 9.52
          Mean value_function loss: 38.1221
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 29.0466
                       Mean reward: 859.87
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7780
     Episode_Reward/lifting_object: 173.3943
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.2224
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 0.86s
                      Time elapsed: 00:29:07
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 112527 steps/s (collection: 0.743s, learning 0.130s)
             Mean action noise std: 9.53
          Mean value_function loss: 28.7114
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 29.0476
                       Mean reward: 851.67
               Mean episode length: 249.47
    Episode_Reward/reaching_object: 0.7753
     Episode_Reward/lifting_object: 172.5007
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.2236
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 0.87s
                      Time elapsed: 00:29:08
                               ETA: 00:04:01

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 115403 steps/s (collection: 0.733s, learning 0.119s)
             Mean action noise std: 9.53
          Mean value_function loss: 22.7302
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.0525
                       Mean reward: 872.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7763
     Episode_Reward/lifting_object: 172.3004
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.2239
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 0.85s
                      Time elapsed: 00:29:08
                               ETA: 00:04:00

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 117478 steps/s (collection: 0.751s, learning 0.086s)
             Mean action noise std: 9.54
          Mean value_function loss: 24.2250
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 29.0588
                       Mean reward: 862.88
               Mean episode length: 246.91
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 171.7427
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.2247
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 0.84s
                      Time elapsed: 00:29:09
                               ETA: 00:03:59

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 111943 steps/s (collection: 0.783s, learning 0.095s)
             Mean action noise std: 9.54
          Mean value_function loss: 22.9019
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.0620
                       Mean reward: 857.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7770
     Episode_Reward/lifting_object: 174.0500
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.2247
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 0.88s
                      Time elapsed: 00:29:10
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 116959 steps/s (collection: 0.735s, learning 0.105s)
             Mean action noise std: 9.55
          Mean value_function loss: 17.7443
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 29.0637
                       Mean reward: 867.90
               Mean episode length: 249.39
    Episode_Reward/reaching_object: 0.7731
     Episode_Reward/lifting_object: 172.5453
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.2248
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 0.84s
                      Time elapsed: 00:29:11
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 111516 steps/s (collection: 0.779s, learning 0.102s)
             Mean action noise std: 9.55
          Mean value_function loss: 21.5121
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 29.0693
                       Mean reward: 868.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7841
     Episode_Reward/lifting_object: 174.2034
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.2242
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 0.88s
                      Time elapsed: 00:29:12
                               ETA: 00:03:56

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 111718 steps/s (collection: 0.763s, learning 0.117s)
             Mean action noise std: 9.56
          Mean value_function loss: 19.7792
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.0726
                       Mean reward: 860.58
               Mean episode length: 249.90
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 172.5542
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.2245
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 0.88s
                      Time elapsed: 00:29:13
                               ETA: 00:03:55

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 108392 steps/s (collection: 0.790s, learning 0.117s)
             Mean action noise std: 9.56
          Mean value_function loss: 24.7746
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.0770
                       Mean reward: 866.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7702
     Episode_Reward/lifting_object: 171.8270
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.2252
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 0.91s
                      Time elapsed: 00:29:14
                               ETA: 00:03:54

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 113545 steps/s (collection: 0.754s, learning 0.112s)
             Mean action noise std: 9.57
          Mean value_function loss: 22.2994
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 29.0836
                       Mean reward: 867.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7768
     Episode_Reward/lifting_object: 173.1041
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.2255
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 0.87s
                      Time elapsed: 00:29:15
                               ETA: 00:03:53

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 117472 steps/s (collection: 0.741s, learning 0.096s)
             Mean action noise std: 9.58
          Mean value_function loss: 35.5663
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 29.0907
                       Mean reward: 856.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7852
     Episode_Reward/lifting_object: 173.0155
      Episode_Reward/object_height: 0.0483
        Episode_Reward/action_rate: -0.2256
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 0.84s
                      Time elapsed: 00:29:15
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 115274 steps/s (collection: 0.752s, learning 0.101s)
             Mean action noise std: 9.59
          Mean value_function loss: 24.7812
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 29.0972
                       Mean reward: 878.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7768
     Episode_Reward/lifting_object: 173.7065
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.2240
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 0.85s
                      Time elapsed: 00:29:16
                               ETA: 00:03:51

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 109362 steps/s (collection: 0.792s, learning 0.107s)
             Mean action noise std: 9.60
          Mean value_function loss: 24.1019
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 29.1012
                       Mean reward: 885.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7837
     Episode_Reward/lifting_object: 174.3940
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.2254
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 0.90s
                      Time elapsed: 00:29:17
                               ETA: 00:03:50

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 116035 steps/s (collection: 0.752s, learning 0.095s)
             Mean action noise std: 9.60
          Mean value_function loss: 22.2270
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.1081
                       Mean reward: 870.63
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7778
     Episode_Reward/lifting_object: 173.5501
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.2255
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 0.85s
                      Time elapsed: 00:29:18
                               ETA: 00:03:49

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 112021 steps/s (collection: 0.776s, learning 0.102s)
             Mean action noise std: 9.61
          Mean value_function loss: 25.3680
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 29.1149
                       Mean reward: 866.34
               Mean episode length: 248.06
    Episode_Reward/reaching_object: 0.7794
     Episode_Reward/lifting_object: 174.3552
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.2267
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 0.88s
                      Time elapsed: 00:29:19
                               ETA: 00:03:48

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 117822 steps/s (collection: 0.732s, learning 0.103s)
             Mean action noise std: 9.61
          Mean value_function loss: 27.2767
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 29.1203
                       Mean reward: 878.29
               Mean episode length: 249.41
    Episode_Reward/reaching_object: 0.7809
     Episode_Reward/lifting_object: 173.8163
      Episode_Reward/object_height: 0.0486
        Episode_Reward/action_rate: -0.2260
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 0.83s
                      Time elapsed: 00:29:20
                               ETA: 00:03:47

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 109434 steps/s (collection: 0.763s, learning 0.135s)
             Mean action noise std: 9.62
          Mean value_function loss: 16.7203
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.1217
                       Mean reward: 878.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7841
     Episode_Reward/lifting_object: 175.0970
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.2258
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 0.90s
                      Time elapsed: 00:29:21
                               ETA: 00:03:46

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 111856 steps/s (collection: 0.749s, learning 0.130s)
             Mean action noise std: 9.62
          Mean value_function loss: 26.8450
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.1236
                       Mean reward: 867.33
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.7773
     Episode_Reward/lifting_object: 173.7863
      Episode_Reward/object_height: 0.0487
        Episode_Reward/action_rate: -0.2261
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 0.88s
                      Time elapsed: 00:29:21
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 115317 steps/s (collection: 0.744s, learning 0.108s)
             Mean action noise std: 9.63
          Mean value_function loss: 30.7286
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.1275
                       Mean reward: 879.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7634
     Episode_Reward/lifting_object: 171.3668
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.2261
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 0.85s
                      Time elapsed: 00:29:22
                               ETA: 00:03:44

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 110836 steps/s (collection: 0.796s, learning 0.091s)
             Mean action noise std: 9.63
          Mean value_function loss: 25.1642
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 29.1341
                       Mean reward: 877.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7712
     Episode_Reward/lifting_object: 173.2814
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.2261
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 0.89s
                      Time elapsed: 00:29:23
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 113596 steps/s (collection: 0.760s, learning 0.105s)
             Mean action noise std: 9.63
          Mean value_function loss: 22.3059
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.1365
                       Mean reward: 871.80
               Mean episode length: 249.08
    Episode_Reward/reaching_object: 0.7757
     Episode_Reward/lifting_object: 173.6601
      Episode_Reward/object_height: 0.0494
        Episode_Reward/action_rate: -0.2267
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 0.87s
                      Time elapsed: 00:29:24
                               ETA: 00:03:42

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 113069 steps/s (collection: 0.744s, learning 0.125s)
             Mean action noise std: 9.64
          Mean value_function loss: 26.5438
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.1423
                       Mean reward: 875.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7816
     Episode_Reward/lifting_object: 172.8947
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.2268
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 0.87s
                      Time elapsed: 00:29:25
                               ETA: 00:03:41

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 111324 steps/s (collection: 0.785s, learning 0.099s)
             Mean action noise std: 9.65
          Mean value_function loss: 29.5714
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 29.1494
                       Mean reward: 862.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7848
     Episode_Reward/lifting_object: 173.5000
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.2287
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 0.88s
                      Time elapsed: 00:29:26
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 109550 steps/s (collection: 0.788s, learning 0.110s)
             Mean action noise std: 9.66
          Mean value_function loss: 26.1693
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 29.1557
                       Mean reward: 866.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7755
     Episode_Reward/lifting_object: 174.1278
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.2291
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 0.90s
                      Time elapsed: 00:29:27
                               ETA: 00:03:39

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 105852 steps/s (collection: 0.802s, learning 0.126s)
             Mean action noise std: 9.67
          Mean value_function loss: 28.6369
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 29.1689
                       Mean reward: 865.24
               Mean episode length: 247.85
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 173.2257
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.2275
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 0.93s
                      Time elapsed: 00:29:28
                               ETA: 00:03:38

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 112810 steps/s (collection: 0.756s, learning 0.116s)
             Mean action noise std: 9.67
          Mean value_function loss: 26.4098
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 29.1722
                       Mean reward: 865.61
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7736
     Episode_Reward/lifting_object: 173.6542
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.2301
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 0.87s
                      Time elapsed: 00:29:29
                               ETA: 00:03:37

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 113581 steps/s (collection: 0.734s, learning 0.131s)
             Mean action noise std: 9.68
          Mean value_function loss: 36.7548
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 29.1744
                       Mean reward: 866.63
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.7720
     Episode_Reward/lifting_object: 171.3186
      Episode_Reward/object_height: 0.0502
        Episode_Reward/action_rate: -0.2300
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 0.87s
                      Time elapsed: 00:29:29
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 108967 steps/s (collection: 0.811s, learning 0.092s)
             Mean action noise std: 9.68
          Mean value_function loss: 19.0474
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 29.1792
                       Mean reward: 858.25
               Mean episode length: 248.11
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 172.7433
      Episode_Reward/object_height: 0.0511
        Episode_Reward/action_rate: -0.2304
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 0.90s
                      Time elapsed: 00:29:30
                               ETA: 00:03:35

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 110281 steps/s (collection: 0.794s, learning 0.097s)
             Mean action noise std: 9.69
          Mean value_function loss: 28.6682
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 29.1843
                       Mean reward: 883.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7666
     Episode_Reward/lifting_object: 173.1306
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.2316
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 0.89s
                      Time elapsed: 00:29:31
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 114958 steps/s (collection: 0.764s, learning 0.091s)
             Mean action noise std: 9.70
          Mean value_function loss: 21.7304
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 29.1891
                       Mean reward: 866.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 172.1201
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.2304
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 0.86s
                      Time elapsed: 00:29:32
                               ETA: 00:03:33

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 115626 steps/s (collection: 0.760s, learning 0.090s)
             Mean action noise std: 9.70
          Mean value_function loss: 26.7481
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.1934
                       Mean reward: 864.89
               Mean episode length: 249.91
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 172.6834
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.2320
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 0.85s
                      Time elapsed: 00:29:33
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 110270 steps/s (collection: 0.793s, learning 0.099s)
             Mean action noise std: 9.71
          Mean value_function loss: 20.4645
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.1994
                       Mean reward: 877.12
               Mean episode length: 248.53
    Episode_Reward/reaching_object: 0.7805
     Episode_Reward/lifting_object: 174.2185
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.2328
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 0.89s
                      Time elapsed: 00:29:34
                               ETA: 00:03:31

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 109614 steps/s (collection: 0.784s, learning 0.113s)
             Mean action noise std: 9.71
          Mean value_function loss: 20.3112
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 29.2074
                       Mean reward: 873.67
               Mean episode length: 249.48
    Episode_Reward/reaching_object: 0.7756
     Episode_Reward/lifting_object: 174.3199
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.2318
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 0.90s
                      Time elapsed: 00:29:35
                               ETA: 00:03:30

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 108999 steps/s (collection: 0.750s, learning 0.152s)
             Mean action noise std: 9.72
          Mean value_function loss: 20.9697
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 29.2111
                       Mean reward: 853.13
               Mean episode length: 248.01
    Episode_Reward/reaching_object: 0.7758
     Episode_Reward/lifting_object: 172.9439
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.2344
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 0.90s
                      Time elapsed: 00:29:36
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 108294 steps/s (collection: 0.813s, learning 0.095s)
             Mean action noise std: 9.74
          Mean value_function loss: 22.7989
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.2205
                       Mean reward: 884.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7780
     Episode_Reward/lifting_object: 173.6837
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.2331
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 0.91s
                      Time elapsed: 00:29:36
                               ETA: 00:03:28

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 114680 steps/s (collection: 0.748s, learning 0.110s)
             Mean action noise std: 9.75
          Mean value_function loss: 29.6221
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.2324
                       Mean reward: 866.85
               Mean episode length: 247.38
    Episode_Reward/reaching_object: 0.7594
     Episode_Reward/lifting_object: 170.6774
      Episode_Reward/object_height: 0.0511
        Episode_Reward/action_rate: -0.2322
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 0.86s
                      Time elapsed: 00:29:37
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 101591 steps/s (collection: 0.805s, learning 0.163s)
             Mean action noise std: 9.76
          Mean value_function loss: 20.6779
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 29.2407
                       Mean reward: 883.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7826
     Episode_Reward/lifting_object: 175.3300
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.2320
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 0.97s
                      Time elapsed: 00:29:38
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 85797 steps/s (collection: 0.987s, learning 0.159s)
             Mean action noise std: 9.77
          Mean value_function loss: 33.7672
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 29.2485
                       Mean reward: 881.00
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7778
     Episode_Reward/lifting_object: 174.1866
      Episode_Reward/object_height: 0.0516
        Episode_Reward/action_rate: -0.2328
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 1.15s
                      Time elapsed: 00:29:39
                               ETA: 00:03:25

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 101806 steps/s (collection: 0.823s, learning 0.143s)
             Mean action noise std: 9.78
          Mean value_function loss: 34.0426
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.2601
                       Mean reward: 877.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7847
     Episode_Reward/lifting_object: 173.9654
      Episode_Reward/object_height: 0.0514
        Episode_Reward/action_rate: -0.2328
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 0.97s
                      Time elapsed: 00:29:40
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 102172 steps/s (collection: 0.837s, learning 0.125s)
             Mean action noise std: 9.79
          Mean value_function loss: 27.5188
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 29.2716
                       Mean reward: 878.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7831
     Episode_Reward/lifting_object: 173.7737
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.2329
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 0.96s
                      Time elapsed: 00:29:41
                               ETA: 00:03:23

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 106066 steps/s (collection: 0.798s, learning 0.128s)
             Mean action noise std: 9.80
          Mean value_function loss: 21.9118
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.2760
                       Mean reward: 863.12
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 0.7783
     Episode_Reward/lifting_object: 172.8040
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.2319
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 0.93s
                      Time elapsed: 00:29:42
                               ETA: 00:03:22

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 112015 steps/s (collection: 0.766s, learning 0.112s)
             Mean action noise std: 9.80
          Mean value_function loss: 31.3798
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.2793
                       Mean reward: 867.12
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.7856
     Episode_Reward/lifting_object: 174.7494
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.2332
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 0.88s
                      Time elapsed: 00:29:43
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 106223 steps/s (collection: 0.810s, learning 0.116s)
             Mean action noise std: 9.80
          Mean value_function loss: 37.3077
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 29.2817
                       Mean reward: 869.80
               Mean episode length: 248.31
    Episode_Reward/reaching_object: 0.7757
     Episode_Reward/lifting_object: 172.8464
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.2329
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 0.93s
                      Time elapsed: 00:29:44
                               ETA: 00:03:20

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 107791 steps/s (collection: 0.796s, learning 0.116s)
             Mean action noise std: 9.81
          Mean value_function loss: 28.7603
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 29.2890
                       Mean reward: 862.28
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 173.5934
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.2317
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 0.91s
                      Time elapsed: 00:29:45
                               ETA: 00:03:19

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 107810 steps/s (collection: 0.774s, learning 0.138s)
             Mean action noise std: 9.82
          Mean value_function loss: 31.4065
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 29.2974
                       Mean reward: 849.38
               Mean episode length: 246.88
    Episode_Reward/reaching_object: 0.7613
     Episode_Reward/lifting_object: 172.0054
      Episode_Reward/object_height: 0.0522
        Episode_Reward/action_rate: -0.2314
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 0.91s
                      Time elapsed: 00:29:46
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 104137 steps/s (collection: 0.835s, learning 0.109s)
             Mean action noise std: 9.83
          Mean value_function loss: 28.6740
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 29.3043
                       Mean reward: 850.95
               Mean episode length: 246.71
    Episode_Reward/reaching_object: 0.7691
     Episode_Reward/lifting_object: 172.4062
      Episode_Reward/object_height: 0.0522
        Episode_Reward/action_rate: -0.2320
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 0.94s
                      Time elapsed: 00:29:47
                               ETA: 00:03:17

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 104945 steps/s (collection: 0.834s, learning 0.102s)
             Mean action noise std: 9.84
          Mean value_function loss: 22.0223
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 29.3110
                       Mean reward: 864.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7732
     Episode_Reward/lifting_object: 171.8524
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.2330
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 0.94s
                      Time elapsed: 00:29:48
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 113266 steps/s (collection: 0.775s, learning 0.093s)
             Mean action noise std: 9.85
          Mean value_function loss: 29.2331
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.3202
                       Mean reward: 867.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 171.9243
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.2345
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 0.87s
                      Time elapsed: 00:29:49
                               ETA: 00:03:15

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 108420 steps/s (collection: 0.782s, learning 0.125s)
             Mean action noise std: 9.85
          Mean value_function loss: 27.6657
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 29.3252
                       Mean reward: 864.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7728
     Episode_Reward/lifting_object: 171.5072
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.2321
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 0.91s
                      Time elapsed: 00:29:50
                               ETA: 00:03:14

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 106676 steps/s (collection: 0.762s, learning 0.159s)
             Mean action noise std: 9.86
          Mean value_function loss: 22.1137
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 29.3303
                       Mean reward: 869.49
               Mean episode length: 248.90
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 173.8492
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.2320
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 0.92s
                      Time elapsed: 00:29:51
                               ETA: 00:03:13

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 108791 steps/s (collection: 0.759s, learning 0.145s)
             Mean action noise std: 9.86
          Mean value_function loss: 24.8724
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.3351
                       Mean reward: 866.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7743
     Episode_Reward/lifting_object: 173.9104
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.2317
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 0.90s
                      Time elapsed: 00:29:51
                               ETA: 00:03:12

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 111942 steps/s (collection: 0.763s, learning 0.115s)
             Mean action noise std: 9.88
          Mean value_function loss: 22.2981
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.3464
                       Mean reward: 851.13
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 171.7755
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.2318
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 0.88s
                      Time elapsed: 00:29:52
                               ETA: 00:03:11

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 109251 steps/s (collection: 0.784s, learning 0.116s)
             Mean action noise std: 9.88
          Mean value_function loss: 26.3658
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 29.3535
                       Mean reward: 869.34
               Mean episode length: 248.15
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: 173.5975
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.2323
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 0.90s
                      Time elapsed: 00:29:53
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 107678 steps/s (collection: 0.793s, learning 0.120s)
             Mean action noise std: 9.89
          Mean value_function loss: 20.9494
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.3595
                       Mean reward: 864.33
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 0.7714
     Episode_Reward/lifting_object: 172.6950
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.2318
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 0.91s
                      Time elapsed: 00:29:54
                               ETA: 00:03:09

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 113362 steps/s (collection: 0.774s, learning 0.093s)
             Mean action noise std: 9.90
          Mean value_function loss: 28.0748
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.3688
                       Mean reward: 873.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7781
     Episode_Reward/lifting_object: 173.4934
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.2343
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 0.87s
                      Time elapsed: 00:29:55
                               ETA: 00:03:08

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 110917 steps/s (collection: 0.799s, learning 0.087s)
             Mean action noise std: 9.90
          Mean value_function loss: 33.6408
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 29.3732
                       Mean reward: 856.70
               Mean episode length: 246.67
    Episode_Reward/reaching_object: 0.7776
     Episode_Reward/lifting_object: 173.4397
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.2321
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 0.89s
                      Time elapsed: 00:29:56
                               ETA: 00:03:07

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 106074 steps/s (collection: 0.811s, learning 0.116s)
             Mean action noise std: 9.91
          Mean value_function loss: 26.7741
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 29.3790
                       Mean reward: 884.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7692
     Episode_Reward/lifting_object: 172.2207
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.2334
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 0.93s
                      Time elapsed: 00:29:57
                               ETA: 00:03:06

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 109754 steps/s (collection: 0.786s, learning 0.110s)
             Mean action noise std: 9.93
          Mean value_function loss: 28.8951
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.3901
                       Mean reward: 867.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7822
     Episode_Reward/lifting_object: 173.5560
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.2352
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 0.90s
                      Time elapsed: 00:29:58
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 111718 steps/s (collection: 0.753s, learning 0.127s)
             Mean action noise std: 9.94
          Mean value_function loss: 22.5515
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 29.4011
                       Mean reward: 878.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7785
     Episode_Reward/lifting_object: 173.3511
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.2346
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 0.88s
                      Time elapsed: 00:29:59
                               ETA: 00:03:04

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 109125 steps/s (collection: 0.766s, learning 0.135s)
             Mean action noise std: 9.94
          Mean value_function loss: 33.5312
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 29.4071
                       Mean reward: 869.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7755
     Episode_Reward/lifting_object: 172.6720
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.2349
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 0.90s
                      Time elapsed: 00:29:59
                               ETA: 00:03:03

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 103365 steps/s (collection: 0.796s, learning 0.155s)
             Mean action noise std: 9.95
          Mean value_function loss: 37.1022
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.4125
                       Mean reward: 871.06
               Mean episode length: 249.13
    Episode_Reward/reaching_object: 0.7834
     Episode_Reward/lifting_object: 174.5127
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.2348
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 0.95s
                      Time elapsed: 00:30:00
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 112207 steps/s (collection: 0.760s, learning 0.117s)
             Mean action noise std: 9.96
          Mean value_function loss: 39.0372
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 29.4211
                       Mean reward: 869.26
               Mean episode length: 249.24
    Episode_Reward/reaching_object: 0.7763
     Episode_Reward/lifting_object: 171.8279
      Episode_Reward/object_height: 0.0511
        Episode_Reward/action_rate: -0.2353
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 0.88s
                      Time elapsed: 00:30:01
                               ETA: 00:03:01

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 105227 steps/s (collection: 0.811s, learning 0.124s)
             Mean action noise std: 9.97
          Mean value_function loss: 39.0935
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 29.4325
                       Mean reward: 854.41
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7629
     Episode_Reward/lifting_object: 170.4546
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.2341
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 0.93s
                      Time elapsed: 00:30:02
                               ETA: 00:03:00

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 108362 steps/s (collection: 0.804s, learning 0.103s)
             Mean action noise std: 9.98
          Mean value_function loss: 36.6102
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 29.4381
                       Mean reward: 868.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7801
     Episode_Reward/lifting_object: 174.6458
      Episode_Reward/object_height: 0.0516
        Episode_Reward/action_rate: -0.2367
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 0.91s
                      Time elapsed: 00:30:03
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 110648 steps/s (collection: 0.797s, learning 0.092s)
             Mean action noise std: 9.98
          Mean value_function loss: 28.0652
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.4435
                       Mean reward: 872.49
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 171.9528
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.2387
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 0.89s
                      Time elapsed: 00:30:04
                               ETA: 00:02:58

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 107216 steps/s (collection: 0.804s, learning 0.113s)
             Mean action noise std: 10.00
          Mean value_function loss: 27.0614
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 29.4507
                       Mean reward: 864.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 171.7175
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.2400
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 0.92s
                      Time elapsed: 00:30:05
                               ETA: 00:02:57

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 110651 steps/s (collection: 0.797s, learning 0.091s)
             Mean action noise std: 10.00
          Mean value_function loss: 30.9730
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 29.4599
                       Mean reward: 855.41
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.7799
     Episode_Reward/lifting_object: 172.3961
      Episode_Reward/object_height: 0.0505
        Episode_Reward/action_rate: -0.2394
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 0.89s
                      Time elapsed: 00:30:06
                               ETA: 00:02:56

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 102161 steps/s (collection: 0.867s, learning 0.095s)
             Mean action noise std: 10.01
          Mean value_function loss: 34.8425
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.4652
                       Mean reward: 874.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 173.2389
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.2391
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 0.96s
                      Time elapsed: 00:30:07
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 111658 steps/s (collection: 0.778s, learning 0.102s)
             Mean action noise std: 10.02
          Mean value_function loss: 34.7846
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 29.4720
                       Mean reward: 861.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7738
     Episode_Reward/lifting_object: 172.2932
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.2411
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 0.88s
                      Time elapsed: 00:30:08
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 103905 steps/s (collection: 0.800s, learning 0.146s)
             Mean action noise std: 10.02
          Mean value_function loss: 45.0704
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.4771
                       Mean reward: 869.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 168.8507
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.2424
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 0.95s
                      Time elapsed: 00:30:09
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 113126 steps/s (collection: 0.765s, learning 0.104s)
             Mean action noise std: 10.03
          Mean value_function loss: 36.4685
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.4839
                       Mean reward: 861.55
               Mean episode length: 248.09
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 171.2726
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.2415
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 0.87s
                      Time elapsed: 00:30:09
                               ETA: 00:02:52

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 103008 steps/s (collection: 0.779s, learning 0.176s)
             Mean action noise std: 10.04
          Mean value_function loss: 55.2781
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 29.4895
                       Mean reward: 873.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 172.6452
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.2421
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 0.95s
                      Time elapsed: 00:30:10
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 108709 steps/s (collection: 0.788s, learning 0.117s)
             Mean action noise std: 10.05
          Mean value_function loss: 42.1407
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.4941
                       Mean reward: 850.38
               Mean episode length: 246.98
    Episode_Reward/reaching_object: 0.7663
     Episode_Reward/lifting_object: 170.7922
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.2442
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 0.90s
                      Time elapsed: 00:30:11
                               ETA: 00:02:50

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 114636 steps/s (collection: 0.753s, learning 0.105s)
             Mean action noise std: 10.05
          Mean value_function loss: 32.9143
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 29.5020
                       Mean reward: 865.33
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 171.6125
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.2431
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 0.86s
                      Time elapsed: 00:30:12
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 112713 steps/s (collection: 0.784s, learning 0.088s)
             Mean action noise std: 10.06
          Mean value_function loss: 37.5060
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.5065
                       Mean reward: 855.22
               Mean episode length: 249.55
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 171.1646
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.2447
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 0.87s
                      Time elapsed: 00:30:13
                               ETA: 00:02:48

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 112494 steps/s (collection: 0.781s, learning 0.093s)
             Mean action noise std: 10.07
          Mean value_function loss: 34.1613
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 29.5139
                       Mean reward: 857.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7626
     Episode_Reward/lifting_object: 169.5577
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.2443
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 0.87s
                      Time elapsed: 00:30:14
                               ETA: 00:02:47

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 113233 steps/s (collection: 0.776s, learning 0.092s)
             Mean action noise std: 10.07
          Mean value_function loss: 46.0444
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.5186
                       Mean reward: 849.73
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.7701
     Episode_Reward/lifting_object: 172.1119
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.2441
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 0.87s
                      Time elapsed: 00:30:15
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 107997 steps/s (collection: 0.791s, learning 0.119s)
             Mean action noise std: 10.08
          Mean value_function loss: 44.2890
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 29.5230
                       Mean reward: 857.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: 169.5457
      Episode_Reward/object_height: 0.0492
        Episode_Reward/action_rate: -0.2442
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 0.91s
                      Time elapsed: 00:30:16
                               ETA: 00:02:45

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 117378 steps/s (collection: 0.752s, learning 0.085s)
             Mean action noise std: 10.08
          Mean value_function loss: 30.2719
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 29.5270
                       Mean reward: 864.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7778
     Episode_Reward/lifting_object: 172.6379
      Episode_Reward/object_height: 0.0503
        Episode_Reward/action_rate: -0.2439
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 0.84s
                      Time elapsed: 00:30:17
                               ETA: 00:02:44

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 108626 steps/s (collection: 0.754s, learning 0.151s)
             Mean action noise std: 10.09
          Mean value_function loss: 45.8602
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 29.5306
                       Mean reward: 864.12
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 170.8235
      Episode_Reward/object_height: 0.0501
        Episode_Reward/action_rate: -0.2449
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 0.90s
                      Time elapsed: 00:30:17
                               ETA: 00:02:43

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 113184 steps/s (collection: 0.747s, learning 0.121s)
             Mean action noise std: 10.09
          Mean value_function loss: 35.9388
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.5320
                       Mean reward: 862.22
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 0.7801
     Episode_Reward/lifting_object: 173.3541
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.2451
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 0.87s
                      Time elapsed: 00:30:18
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 103994 steps/s (collection: 0.816s, learning 0.129s)
             Mean action noise std: 10.09
          Mean value_function loss: 39.8890
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.5348
                       Mean reward: 859.31
               Mean episode length: 248.77
    Episode_Reward/reaching_object: 0.7704
     Episode_Reward/lifting_object: 171.6416
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.2456
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 0.95s
                      Time elapsed: 00:30:19
                               ETA: 00:02:41

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 108962 steps/s (collection: 0.788s, learning 0.115s)
             Mean action noise std: 10.10
          Mean value_function loss: 31.8392
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.5386
                       Mean reward: 860.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7710
     Episode_Reward/lifting_object: 172.1140
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.2448
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 0.90s
                      Time elapsed: 00:30:20
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 114047 steps/s (collection: 0.765s, learning 0.097s)
             Mean action noise std: 10.11
          Mean value_function loss: 24.1760
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 29.5449
                       Mean reward: 857.12
               Mean episode length: 248.93
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 171.6577
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.2448
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 0.86s
                      Time elapsed: 00:30:21
                               ETA: 00:02:39

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 111198 steps/s (collection: 0.784s, learning 0.100s)
             Mean action noise std: 10.12
          Mean value_function loss: 20.3797
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.5518
                       Mean reward: 883.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7785
     Episode_Reward/lifting_object: 174.3577
      Episode_Reward/object_height: 0.0532
        Episode_Reward/action_rate: -0.2447
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 0.88s
                      Time elapsed: 00:30:22
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 112251 steps/s (collection: 0.776s, learning 0.100s)
             Mean action noise std: 10.12
          Mean value_function loss: 27.9979
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.5610
                       Mean reward: 872.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7578
     Episode_Reward/lifting_object: 169.5924
      Episode_Reward/object_height: 0.0522
        Episode_Reward/action_rate: -0.2464
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 0.88s
                      Time elapsed: 00:30:23
                               ETA: 00:02:37

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 106502 steps/s (collection: 0.798s, learning 0.125s)
             Mean action noise std: 10.12
          Mean value_function loss: 29.2175
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.5632
                       Mean reward: 856.85
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7532
     Episode_Reward/lifting_object: 170.5168
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.2445
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 0.92s
                      Time elapsed: 00:30:24
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 111037 steps/s (collection: 0.797s, learning 0.088s)
             Mean action noise std: 10.13
          Mean value_function loss: 21.8739
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.5671
                       Mean reward: 881.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7623
     Episode_Reward/lifting_object: 172.8834
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.2449
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 0.89s
                      Time elapsed: 00:30:25
                               ETA: 00:02:35

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 110312 steps/s (collection: 0.774s, learning 0.117s)
             Mean action noise std: 10.14
          Mean value_function loss: 30.4692
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 29.5759
                       Mean reward: 866.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7661
     Episode_Reward/lifting_object: 174.7133
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.2458
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 0.89s
                      Time elapsed: 00:30:25
                               ETA: 00:02:34

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 101793 steps/s (collection: 0.850s, learning 0.116s)
             Mean action noise std: 10.14
          Mean value_function loss: 17.2575
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.5788
                       Mean reward: 857.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7604
     Episode_Reward/lifting_object: 170.0943
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.2462
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 0.97s
                      Time elapsed: 00:30:26
                               ETA: 00:02:33

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 114536 steps/s (collection: 0.752s, learning 0.107s)
             Mean action noise std: 10.15
          Mean value_function loss: 25.2631
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.5812
                       Mean reward: 871.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7757
     Episode_Reward/lifting_object: 172.4684
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.2458
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 0.86s
                      Time elapsed: 00:30:27
                               ETA: 00:02:32

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 111488 steps/s (collection: 0.783s, learning 0.099s)
             Mean action noise std: 10.15
          Mean value_function loss: 23.9909
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.5839
                       Mean reward: 858.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7748
     Episode_Reward/lifting_object: 173.5932
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.2468
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 0.88s
                      Time elapsed: 00:30:28
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 116016 steps/s (collection: 0.757s, learning 0.090s)
             Mean action noise std: 10.16
          Mean value_function loss: 26.1878
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.5898
                       Mean reward: 881.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7822
     Episode_Reward/lifting_object: 174.6602
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.2473
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 0.85s
                      Time elapsed: 00:30:29
                               ETA: 00:02:30

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 113725 steps/s (collection: 0.777s, learning 0.087s)
             Mean action noise std: 10.17
          Mean value_function loss: 29.5656
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.5967
                       Mean reward: 871.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7773
     Episode_Reward/lifting_object: 173.2353
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.2461
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 0.86s
                      Time elapsed: 00:30:30
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 111472 steps/s (collection: 0.795s, learning 0.087s)
             Mean action noise std: 10.18
          Mean value_function loss: 30.8406
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.6031
                       Mean reward: 839.58
               Mean episode length: 248.26
    Episode_Reward/reaching_object: 0.7774
     Episode_Reward/lifting_object: 172.9579
      Episode_Reward/object_height: 0.0516
        Episode_Reward/action_rate: -0.2485
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 0.88s
                      Time elapsed: 00:30:31
                               ETA: 00:02:28

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 106832 steps/s (collection: 0.798s, learning 0.122s)
             Mean action noise std: 10.19
          Mean value_function loss: 36.4049
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 29.6111
                       Mean reward: 869.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7780
     Episode_Reward/lifting_object: 173.4329
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.2498
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 0.92s
                      Time elapsed: 00:30:32
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 112895 steps/s (collection: 0.770s, learning 0.101s)
             Mean action noise std: 10.19
          Mean value_function loss: 33.8615
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.6190
                       Mean reward: 844.05
               Mean episode length: 244.41
    Episode_Reward/reaching_object: 0.7696
     Episode_Reward/lifting_object: 170.9908
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.2480
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 0.87s
                      Time elapsed: 00:30:33
                               ETA: 00:02:26

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 107477 steps/s (collection: 0.775s, learning 0.140s)
             Mean action noise std: 10.20
          Mean value_function loss: 29.5475
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.6224
                       Mean reward: 864.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7794
     Episode_Reward/lifting_object: 173.3667
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.2500
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 0.91s
                      Time elapsed: 00:30:34
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 108529 steps/s (collection: 0.775s, learning 0.131s)
             Mean action noise std: 10.21
          Mean value_function loss: 29.3469
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 29.6271
                       Mean reward: 875.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7757
     Episode_Reward/lifting_object: 173.3746
      Episode_Reward/object_height: 0.0516
        Episode_Reward/action_rate: -0.2507
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 0.91s
                      Time elapsed: 00:30:34
                               ETA: 00:02:24

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 114209 steps/s (collection: 0.750s, learning 0.111s)
             Mean action noise std: 10.21
          Mean value_function loss: 31.7407
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 29.6316
                       Mean reward: 871.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7796
     Episode_Reward/lifting_object: 172.4795
      Episode_Reward/object_height: 0.0516
        Episode_Reward/action_rate: -0.2513
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 0.86s
                      Time elapsed: 00:30:35
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 112424 steps/s (collection: 0.784s, learning 0.090s)
             Mean action noise std: 10.22
          Mean value_function loss: 26.8414
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 29.6364
                       Mean reward: 869.54
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 0.7777
     Episode_Reward/lifting_object: 173.9099
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.2499
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 0.87s
                      Time elapsed: 00:30:36
                               ETA: 00:02:22

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 115479 steps/s (collection: 0.760s, learning 0.092s)
             Mean action noise std: 10.23
          Mean value_function loss: 33.1841
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.6411
                       Mean reward: 860.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7680
     Episode_Reward/lifting_object: 171.3703
      Episode_Reward/object_height: 0.0511
        Episode_Reward/action_rate: -0.2530
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 0.85s
                      Time elapsed: 00:30:37
                               ETA: 00:02:21

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 113135 steps/s (collection: 0.770s, learning 0.099s)
             Mean action noise std: 10.24
          Mean value_function loss: 21.7272
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 29.6470
                       Mean reward: 822.79
               Mean episode length: 246.18
    Episode_Reward/reaching_object: 0.7681
     Episode_Reward/lifting_object: 170.6205
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.2511
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 0.87s
                      Time elapsed: 00:30:38
                               ETA: 00:02:20

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 113911 steps/s (collection: 0.767s, learning 0.096s)
             Mean action noise std: 10.25
          Mean value_function loss: 26.6468
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 29.6551
                       Mean reward: 882.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7822
     Episode_Reward/lifting_object: 174.2702
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.2527
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 0.86s
                      Time elapsed: 00:30:39
                               ETA: 00:02:19

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 108704 steps/s (collection: 0.803s, learning 0.102s)
             Mean action noise std: 10.26
          Mean value_function loss: 39.7452
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 29.6640
                       Mean reward: 875.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7769
     Episode_Reward/lifting_object: 173.5790
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.2515
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 0.90s
                      Time elapsed: 00:30:40
                               ETA: 00:02:18

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 115465 steps/s (collection: 0.755s, learning 0.096s)
             Mean action noise std: 10.26
          Mean value_function loss: 22.8007
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.6711
                       Mean reward: 869.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7769
     Episode_Reward/lifting_object: 172.6892
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.2506
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.4167
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 0.85s
                      Time elapsed: 00:30:40
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 110570 steps/s (collection: 0.776s, learning 0.114s)
             Mean action noise std: 10.26
          Mean value_function loss: 34.0750
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 29.6734
                       Mean reward: 857.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7715
     Episode_Reward/lifting_object: 171.0006
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.2532
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 0.89s
                      Time elapsed: 00:30:41
                               ETA: 00:02:16

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 102446 steps/s (collection: 0.820s, learning 0.139s)
             Mean action noise std: 10.27
          Mean value_function loss: 34.9749
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.6766
                       Mean reward: 869.67
               Mean episode length: 248.12
    Episode_Reward/reaching_object: 0.7757
     Episode_Reward/lifting_object: 172.3107
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.2512
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 0.96s
                      Time elapsed: 00:30:42
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 112684 steps/s (collection: 0.759s, learning 0.114s)
             Mean action noise std: 10.28
          Mean value_function loss: 22.0832
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 29.6814
                       Mean reward: 854.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7790
     Episode_Reward/lifting_object: 172.9141
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.2517
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 0.87s
                      Time elapsed: 00:30:43
                               ETA: 00:02:14

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 107622 steps/s (collection: 0.809s, learning 0.105s)
             Mean action noise std: 10.28
          Mean value_function loss: 28.9292
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.6865
                       Mean reward: 883.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7891
     Episode_Reward/lifting_object: 174.7889
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.2516
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 0.91s
                      Time elapsed: 00:30:44
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 104247 steps/s (collection: 0.786s, learning 0.157s)
             Mean action noise std: 10.29
          Mean value_function loss: 28.3430
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.6917
                       Mean reward: 864.08
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.7774
     Episode_Reward/lifting_object: 173.2281
      Episode_Reward/object_height: 0.0535
        Episode_Reward/action_rate: -0.2509
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 0.94s
                      Time elapsed: 00:30:45
                               ETA: 00:02:12

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 104722 steps/s (collection: 0.816s, learning 0.123s)
             Mean action noise std: 10.30
          Mean value_function loss: 25.8670
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.6972
                       Mean reward: 866.16
               Mean episode length: 246.89
    Episode_Reward/reaching_object: 0.7780
     Episode_Reward/lifting_object: 172.8786
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.2505
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 0.94s
                      Time elapsed: 00:30:46
                               ETA: 00:02:11

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 108565 steps/s (collection: 0.796s, learning 0.110s)
             Mean action noise std: 10.31
          Mean value_function loss: 29.9994
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.7041
                       Mean reward: 864.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7779
     Episode_Reward/lifting_object: 173.0851
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.2529
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 0.91s
                      Time elapsed: 00:30:47
                               ETA: 00:02:10

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 111751 steps/s (collection: 0.789s, learning 0.091s)
             Mean action noise std: 10.31
          Mean value_function loss: 38.4699
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 29.7100
                       Mean reward: 844.03
               Mean episode length: 246.41
    Episode_Reward/reaching_object: 0.7658
     Episode_Reward/lifting_object: 171.6398
      Episode_Reward/object_height: 0.0541
        Episode_Reward/action_rate: -0.2503
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 0.88s
                      Time elapsed: 00:30:48
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 110061 steps/s (collection: 0.788s, learning 0.105s)
             Mean action noise std: 10.31
          Mean value_function loss: 26.4733
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.7112
                       Mean reward: 848.31
               Mean episode length: 246.14
    Episode_Reward/reaching_object: 0.7569
     Episode_Reward/lifting_object: 171.5664
      Episode_Reward/object_height: 0.0548
        Episode_Reward/action_rate: -0.2528
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 0.89s
                      Time elapsed: 00:30:49
                               ETA: 00:02:08

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 110174 steps/s (collection: 0.787s, learning 0.105s)
             Mean action noise std: 10.32
          Mean value_function loss: 40.5317
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 29.7145
                       Mean reward: 865.69
               Mean episode length: 248.37
    Episode_Reward/reaching_object: 0.7624
     Episode_Reward/lifting_object: 172.0095
      Episode_Reward/object_height: 0.0547
        Episode_Reward/action_rate: -0.2534
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 0.89s
                      Time elapsed: 00:30:50
                               ETA: 00:02:07

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 106783 steps/s (collection: 0.761s, learning 0.160s)
             Mean action noise std: 10.32
          Mean value_function loss: 26.0981
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.7175
                       Mean reward: 874.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7688
     Episode_Reward/lifting_object: 172.4070
      Episode_Reward/object_height: 0.0550
        Episode_Reward/action_rate: -0.2538
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 0.92s
                      Time elapsed: 00:30:50
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 96499 steps/s (collection: 0.901s, learning 0.117s)
             Mean action noise std: 10.33
          Mean value_function loss: 33.8145
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.7226
                       Mean reward: 870.13
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.7737
     Episode_Reward/lifting_object: 174.1071
      Episode_Reward/object_height: 0.0552
        Episode_Reward/action_rate: -0.2536
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 1.02s
                      Time elapsed: 00:30:52
                               ETA: 00:02:05

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 104960 steps/s (collection: 0.761s, learning 0.176s)
             Mean action noise std: 10.34
          Mean value_function loss: 29.5992
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.7307
                       Mean reward: 866.78
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 0.7769
     Episode_Reward/lifting_object: 174.7447
      Episode_Reward/object_height: 0.0556
        Episode_Reward/action_rate: -0.2538
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 0.94s
                      Time elapsed: 00:30:52
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 115583 steps/s (collection: 0.758s, learning 0.093s)
             Mean action noise std: 10.34
          Mean value_function loss: 26.7813
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 29.7375
                       Mean reward: 881.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7807
     Episode_Reward/lifting_object: 174.8860
      Episode_Reward/object_height: 0.0553
        Episode_Reward/action_rate: -0.2562
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 0.85s
                      Time elapsed: 00:30:53
                               ETA: 00:02:03

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 113648 steps/s (collection: 0.765s, learning 0.100s)
             Mean action noise std: 10.35
          Mean value_function loss: 22.8765
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 29.7426
                       Mean reward: 851.70
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.7718
     Episode_Reward/lifting_object: 172.5976
      Episode_Reward/object_height: 0.0543
        Episode_Reward/action_rate: -0.2536
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 0.86s
                      Time elapsed: 00:30:54
                               ETA: 00:02:02

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 110842 steps/s (collection: 0.776s, learning 0.111s)
             Mean action noise std: 10.36
          Mean value_function loss: 22.3206
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.7496
                       Mean reward: 870.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7786
     Episode_Reward/lifting_object: 173.4651
      Episode_Reward/object_height: 0.0544
        Episode_Reward/action_rate: -0.2567
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 0.89s
                      Time elapsed: 00:30:55
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 113845 steps/s (collection: 0.774s, learning 0.090s)
             Mean action noise std: 10.37
          Mean value_function loss: 27.1923
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 29.7541
                       Mean reward: 866.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7779
     Episode_Reward/lifting_object: 173.9521
      Episode_Reward/object_height: 0.0538
        Episode_Reward/action_rate: -0.2563
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 0.86s
                      Time elapsed: 00:30:56
                               ETA: 00:02:00

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 111947 steps/s (collection: 0.787s, learning 0.092s)
             Mean action noise std: 10.38
          Mean value_function loss: 22.0839
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 29.7618
                       Mean reward: 877.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7762
     Episode_Reward/lifting_object: 173.2215
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.2562
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 0.88s
                      Time elapsed: 00:30:57
                               ETA: 00:01:59

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 109252 steps/s (collection: 0.796s, learning 0.104s)
             Mean action noise std: 10.39
          Mean value_function loss: 29.8585
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 29.7698
                       Mean reward: 861.76
               Mean episode length: 248.07
    Episode_Reward/reaching_object: 0.7702
     Episode_Reward/lifting_object: 172.6197
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.2577
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 0.90s
                      Time elapsed: 00:30:58
                               ETA: 00:01:58

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 114281 steps/s (collection: 0.753s, learning 0.107s)
             Mean action noise std: 10.39
          Mean value_function loss: 36.1131
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 29.7738
                       Mean reward: 873.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 172.1247
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.2579
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 0.86s
                      Time elapsed: 00:30:59
                               ETA: 00:01:57

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 108287 steps/s (collection: 0.757s, learning 0.151s)
             Mean action noise std: 10.39
          Mean value_function loss: 33.1151
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 29.7771
                       Mean reward: 862.26
               Mean episode length: 248.19
    Episode_Reward/reaching_object: 0.7717
     Episode_Reward/lifting_object: 172.0353
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.2575
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.1250
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 0.91s
                      Time elapsed: 00:30:59
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 106040 steps/s (collection: 0.768s, learning 0.159s)
             Mean action noise std: 10.40
          Mean value_function loss: 21.7929
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.7812
                       Mean reward: 858.88
               Mean episode length: 248.85
    Episode_Reward/reaching_object: 0.7800
     Episode_Reward/lifting_object: 173.6408
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.2563
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 0.93s
                      Time elapsed: 00:31:00
                               ETA: 00:01:55

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 112620 steps/s (collection: 0.762s, learning 0.111s)
             Mean action noise std: 10.40
          Mean value_function loss: 25.0968
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 29.7851
                       Mean reward: 877.70
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7797
     Episode_Reward/lifting_object: 173.2487
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.2572
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 0.87s
                      Time elapsed: 00:31:01
                               ETA: 00:01:54

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 115038 steps/s (collection: 0.758s, learning 0.097s)
             Mean action noise std: 10.41
          Mean value_function loss: 20.9231
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.7889
                       Mean reward: 876.86
               Mean episode length: 249.28
    Episode_Reward/reaching_object: 0.7849
     Episode_Reward/lifting_object: 175.0527
      Episode_Reward/object_height: 0.0533
        Episode_Reward/action_rate: -0.2565
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 0.85s
                      Time elapsed: 00:31:02
                               ETA: 00:01:53

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 112620 steps/s (collection: 0.778s, learning 0.095s)
             Mean action noise std: 10.41
          Mean value_function loss: 26.2805
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 29.7925
                       Mean reward: 878.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7834
     Episode_Reward/lifting_object: 173.3462
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.2588
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 0.87s
                      Time elapsed: 00:31:03
                               ETA: 00:01:52

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 106688 steps/s (collection: 0.809s, learning 0.113s)
             Mean action noise std: 10.42
          Mean value_function loss: 22.3858
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 29.7948
                       Mean reward: 863.69
               Mean episode length: 248.91
    Episode_Reward/reaching_object: 0.7865
     Episode_Reward/lifting_object: 174.0433
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.2598
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 0.92s
                      Time elapsed: 00:31:04
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 117139 steps/s (collection: 0.753s, learning 0.086s)
             Mean action noise std: 10.43
          Mean value_function loss: 26.5556
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 29.7998
                       Mean reward: 869.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7833
     Episode_Reward/lifting_object: 174.2395
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.2586
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 0.84s
                      Time elapsed: 00:31:05
                               ETA: 00:01:50

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 103595 steps/s (collection: 0.780s, learning 0.169s)
             Mean action noise std: 10.44
          Mean value_function loss: 24.2671
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 29.8080
                       Mean reward: 864.92
               Mean episode length: 248.43
    Episode_Reward/reaching_object: 0.7796
     Episode_Reward/lifting_object: 173.7103
      Episode_Reward/object_height: 0.0527
        Episode_Reward/action_rate: -0.2592
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 0.95s
                      Time elapsed: 00:31:06
                               ETA: 00:01:49

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 107805 steps/s (collection: 0.789s, learning 0.123s)
             Mean action noise std: 10.44
          Mean value_function loss: 21.6272
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 29.8145
                       Mean reward: 866.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7815
     Episode_Reward/lifting_object: 173.7026
      Episode_Reward/object_height: 0.0522
        Episode_Reward/action_rate: -0.2589
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.7083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 0.91s
                      Time elapsed: 00:31:07
                               ETA: 00:01:48

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 107090 steps/s (collection: 0.779s, learning 0.139s)
             Mean action noise std: 10.45
          Mean value_function loss: 27.3767
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 29.8202
                       Mean reward: 870.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7836
     Episode_Reward/lifting_object: 173.6829
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.2604
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 0.92s
                      Time elapsed: 00:31:08
                               ETA: 00:01:47

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 110723 steps/s (collection: 0.800s, learning 0.088s)
             Mean action noise std: 10.45
          Mean value_function loss: 28.7314
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 29.8232
                       Mean reward: 847.13
               Mean episode length: 242.71
    Episode_Reward/reaching_object: 0.7780
     Episode_Reward/lifting_object: 172.7465
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.2587
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 0.89s
                      Time elapsed: 00:31:08
                               ETA: 00:01:46

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 110339 steps/s (collection: 0.771s, learning 0.120s)
             Mean action noise std: 10.46
          Mean value_function loss: 28.6030
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 29.8266
                       Mean reward: 880.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7806
     Episode_Reward/lifting_object: 173.3031
      Episode_Reward/object_height: 0.0518
        Episode_Reward/action_rate: -0.2604
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 0.89s
                      Time elapsed: 00:31:09
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 108994 steps/s (collection: 0.800s, learning 0.102s)
             Mean action noise std: 10.46
          Mean value_function loss: 26.7478
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.8313
                       Mean reward: 882.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7906
     Episode_Reward/lifting_object: 175.5212
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.2604
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 0.90s
                      Time elapsed: 00:31:10
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 116343 steps/s (collection: 0.757s, learning 0.088s)
             Mean action noise std: 10.47
          Mean value_function loss: 33.2733
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.8331
                       Mean reward: 863.53
               Mean episode length: 248.33
    Episode_Reward/reaching_object: 0.7793
     Episode_Reward/lifting_object: 173.4340
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.2604
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 0.84s
                      Time elapsed: 00:31:11
                               ETA: 00:01:43

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 114802 steps/s (collection: 0.766s, learning 0.090s)
             Mean action noise std: 10.47
          Mean value_function loss: 26.0268
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.8376
                       Mean reward: 877.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7827
     Episode_Reward/lifting_object: 173.5216
      Episode_Reward/object_height: 0.0514
        Episode_Reward/action_rate: -0.2603
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 0.86s
                      Time elapsed: 00:31:12
                               ETA: 00:01:42

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 108077 steps/s (collection: 0.820s, learning 0.090s)
             Mean action noise std: 10.48
          Mean value_function loss: 26.7681
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 29.8417
                       Mean reward: 857.15
               Mean episode length: 246.41
    Episode_Reward/reaching_object: 0.7765
     Episode_Reward/lifting_object: 172.6431
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.2626
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 0.91s
                      Time elapsed: 00:31:13
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 111614 steps/s (collection: 0.780s, learning 0.101s)
             Mean action noise std: 10.48
          Mean value_function loss: 28.5069
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 29.8441
                       Mean reward: 865.35
               Mean episode length: 247.49
    Episode_Reward/reaching_object: 0.7808
     Episode_Reward/lifting_object: 173.3125
      Episode_Reward/object_height: 0.0514
        Episode_Reward/action_rate: -0.2619
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 0.88s
                      Time elapsed: 00:31:14
                               ETA: 00:01:40

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 103781 steps/s (collection: 0.807s, learning 0.140s)
             Mean action noise std: 10.49
          Mean value_function loss: 35.8872
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.8475
                       Mean reward: 874.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7898
     Episode_Reward/lifting_object: 175.0805
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.2625
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 0.95s
                      Time elapsed: 00:31:15
                               ETA: 00:01:39

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 111417 steps/s (collection: 0.795s, learning 0.087s)
             Mean action noise std: 10.49
          Mean value_function loss: 28.5325
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 29.8534
                       Mean reward: 874.52
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.7805
     Episode_Reward/lifting_object: 173.7754
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.2640
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 0.88s
                      Time elapsed: 00:31:16
                               ETA: 00:01:38

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 104722 steps/s (collection: 0.808s, learning 0.131s)
             Mean action noise std: 10.50
          Mean value_function loss: 36.5948
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.8563
                       Mean reward: 862.32
               Mean episode length: 248.60
    Episode_Reward/reaching_object: 0.7839
     Episode_Reward/lifting_object: 172.6083
      Episode_Reward/object_height: 0.0518
        Episode_Reward/action_rate: -0.2653
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 0.94s
                      Time elapsed: 00:31:16
                               ETA: 00:01:37

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 100854 steps/s (collection: 0.795s, learning 0.180s)
             Mean action noise std: 10.50
          Mean value_function loss: 30.7763
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 29.8622
                       Mean reward: 866.90
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7765
     Episode_Reward/lifting_object: 172.1808
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.2632
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 0.97s
                      Time elapsed: 00:31:17
                               ETA: 00:01:36

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 106499 steps/s (collection: 0.780s, learning 0.143s)
             Mean action noise std: 10.51
          Mean value_function loss: 26.8469
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 29.8701
                       Mean reward: 864.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7769
     Episode_Reward/lifting_object: 173.5748
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.2655
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.4167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 0.92s
                      Time elapsed: 00:31:18
                               ETA: 00:01:35

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 102182 steps/s (collection: 0.800s, learning 0.162s)
             Mean action noise std: 10.52
          Mean value_function loss: 31.9069
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 29.8767
                       Mean reward: 865.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7848
     Episode_Reward/lifting_object: 173.0149
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.2654
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 0.96s
                      Time elapsed: 00:31:19
                               ETA: 00:01:34

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 108946 steps/s (collection: 0.790s, learning 0.113s)
             Mean action noise std: 10.52
          Mean value_function loss: 24.1694
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 29.8802
                       Mean reward: 876.79
               Mean episode length: 249.41
    Episode_Reward/reaching_object: 0.7836
     Episode_Reward/lifting_object: 173.3138
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.2649
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 0.90s
                      Time elapsed: 00:31:20
                               ETA: 00:01:33

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 109675 steps/s (collection: 0.802s, learning 0.094s)
             Mean action noise std: 10.53
          Mean value_function loss: 26.7583
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 29.8839
                       Mean reward: 857.86
               Mean episode length: 249.31
    Episode_Reward/reaching_object: 0.7800
     Episode_Reward/lifting_object: 172.9494
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.2668
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 0.90s
                      Time elapsed: 00:31:21
                               ETA: 00:01:32

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 110070 steps/s (collection: 0.790s, learning 0.103s)
             Mean action noise std: 10.54
          Mean value_function loss: 28.6804
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 29.8898
                       Mean reward: 851.36
               Mean episode length: 249.34
    Episode_Reward/reaching_object: 0.7794
     Episode_Reward/lifting_object: 173.1575
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.2704
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 0.89s
                      Time elapsed: 00:31:22
                               ETA: 00:01:31

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 104209 steps/s (collection: 0.808s, learning 0.136s)
             Mean action noise std: 10.54
          Mean value_function loss: 25.9346
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.8911
                       Mean reward: 858.36
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7722
     Episode_Reward/lifting_object: 173.3798
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.2684
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 0.94s
                      Time elapsed: 00:31:23
                               ETA: 00:01:30

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 108813 steps/s (collection: 0.806s, learning 0.098s)
             Mean action noise std: 10.55
          Mean value_function loss: 23.3957
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.8950
                       Mean reward: 877.57
               Mean episode length: 249.75
    Episode_Reward/reaching_object: 0.7716
     Episode_Reward/lifting_object: 172.8030
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.2704
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 0.90s
                      Time elapsed: 00:31:24
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 105623 steps/s (collection: 0.835s, learning 0.095s)
             Mean action noise std: 10.55
          Mean value_function loss: 28.1657
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 29.9021
                       Mean reward: 851.89
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.7776
     Episode_Reward/lifting_object: 172.7543
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.2706
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 0.93s
                      Time elapsed: 00:31:25
                               ETA: 00:01:28

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 107147 steps/s (collection: 0.811s, learning 0.106s)
             Mean action noise std: 10.56
          Mean value_function loss: 28.6040
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 29.9058
                       Mean reward: 864.97
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 172.6551
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.2716
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 0.92s
                      Time elapsed: 00:31:26
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 111390 steps/s (collection: 0.784s, learning 0.099s)
             Mean action noise std: 10.56
          Mean value_function loss: 29.9287
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 29.9107
                       Mean reward: 866.54
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7685
     Episode_Reward/lifting_object: 171.2153
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.2726
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 0.88s
                      Time elapsed: 00:31:27
                               ETA: 00:01:26

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 94844 steps/s (collection: 0.831s, learning 0.205s)
             Mean action noise std: 10.57
          Mean value_function loss: 26.5382
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 29.9140
                       Mean reward: 859.78
               Mean episode length: 247.73
    Episode_Reward/reaching_object: 0.7747
     Episode_Reward/lifting_object: 172.7722
      Episode_Reward/object_height: 0.0522
        Episode_Reward/action_rate: -0.2729
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.6667
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 1.04s
                      Time elapsed: 00:31:28
                               ETA: 00:01:25

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 106759 steps/s (collection: 0.804s, learning 0.117s)
             Mean action noise std: 10.58
          Mean value_function loss: 30.9828
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 29.9202
                       Mean reward: 862.47
               Mean episode length: 244.83
    Episode_Reward/reaching_object: 0.7826
     Episode_Reward/lifting_object: 173.2016
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.2713
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 0.92s
                      Time elapsed: 00:31:29
                               ETA: 00:01:24

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 108102 steps/s (collection: 0.799s, learning 0.110s)
             Mean action noise std: 10.59
          Mean value_function loss: 32.4992
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 29.9289
                       Mean reward: 876.77
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7790
     Episode_Reward/lifting_object: 173.4476
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.2738
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 0.91s
                      Time elapsed: 00:31:29
                               ETA: 00:01:23

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 95835 steps/s (collection: 0.811s, learning 0.215s)
             Mean action noise std: 10.60
          Mean value_function loss: 34.4068
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 29.9357
                       Mean reward: 870.65
               Mean episode length: 249.31
    Episode_Reward/reaching_object: 0.7752
     Episode_Reward/lifting_object: 173.3091
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.2741
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 1.03s
                      Time elapsed: 00:31:30
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 102526 steps/s (collection: 0.793s, learning 0.166s)
             Mean action noise std: 10.61
          Mean value_function loss: 25.1557
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.9412
                       Mean reward: 867.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7801
     Episode_Reward/lifting_object: 173.0970
      Episode_Reward/object_height: 0.0524
        Episode_Reward/action_rate: -0.2751
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 0.96s
                      Time elapsed: 00:31:31
                               ETA: 00:01:21

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 105559 steps/s (collection: 0.829s, learning 0.102s)
             Mean action noise std: 10.62
          Mean value_function loss: 24.2310
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 29.9464
                       Mean reward: 868.73
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.7786
     Episode_Reward/lifting_object: 173.8316
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.2725
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 0.93s
                      Time elapsed: 00:31:32
                               ETA: 00:01:20

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 104183 steps/s (collection: 0.819s, learning 0.125s)
             Mean action noise std: 10.62
          Mean value_function loss: 36.2500
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 29.9547
                       Mean reward: 858.93
               Mean episode length: 248.02
    Episode_Reward/reaching_object: 0.7803
     Episode_Reward/lifting_object: 172.6079
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.2754
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 0.94s
                      Time elapsed: 00:31:33
                               ETA: 00:01:19

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 100696 steps/s (collection: 0.816s, learning 0.161s)
             Mean action noise std: 10.63
          Mean value_function loss: 31.6630
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 29.9584
                       Mean reward: 866.53
               Mean episode length: 248.98
    Episode_Reward/reaching_object: 0.7765
     Episode_Reward/lifting_object: 173.2037
      Episode_Reward/object_height: 0.0527
        Episode_Reward/action_rate: -0.2749
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 0.98s
                      Time elapsed: 00:31:34
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 103638 steps/s (collection: 0.773s, learning 0.175s)
             Mean action noise std: 10.64
          Mean value_function loss: 22.9036
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 29.9650
                       Mean reward: 869.25
               Mean episode length: 246.88
    Episode_Reward/reaching_object: 0.7741
     Episode_Reward/lifting_object: 172.4033
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.2742
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 0.95s
                      Time elapsed: 00:31:35
                               ETA: 00:01:17

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 106200 steps/s (collection: 0.782s, learning 0.143s)
             Mean action noise std: 10.64
          Mean value_function loss: 22.5557
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 29.9704
                       Mean reward: 836.04
               Mean episode length: 246.42
    Episode_Reward/reaching_object: 0.7690
     Episode_Reward/lifting_object: 171.3877
      Episode_Reward/object_height: 0.0522
        Episode_Reward/action_rate: -0.2739
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 0.93s
                      Time elapsed: 00:31:36
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 111883 steps/s (collection: 0.786s, learning 0.093s)
             Mean action noise std: 10.65
          Mean value_function loss: 19.2104
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 29.9775
                       Mean reward: 860.64
               Mean episode length: 247.18
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 172.9534
      Episode_Reward/object_height: 0.0530
        Episode_Reward/action_rate: -0.2736
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 0.88s
                      Time elapsed: 00:31:37
                               ETA: 00:01:15

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 110850 steps/s (collection: 0.786s, learning 0.101s)
             Mean action noise std: 10.66
          Mean value_function loss: 18.4537
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 29.9833
                       Mean reward: 873.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7709
     Episode_Reward/lifting_object: 173.3004
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.2739
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 0.89s
                      Time elapsed: 00:31:38
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 107522 steps/s (collection: 0.817s, learning 0.097s)
             Mean action noise std: 10.66
          Mean value_function loss: 22.8184
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 29.9841
                       Mean reward: 871.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7799
     Episode_Reward/lifting_object: 174.0956
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.2739
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 0.91s
                      Time elapsed: 00:31:39
                               ETA: 00:01:13

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 112855 steps/s (collection: 0.781s, learning 0.090s)
             Mean action noise std: 10.66
          Mean value_function loss: 21.9201
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 29.9857
                       Mean reward: 868.59
               Mean episode length: 249.17
    Episode_Reward/reaching_object: 0.7740
     Episode_Reward/lifting_object: 172.5422
      Episode_Reward/object_height: 0.0531
        Episode_Reward/action_rate: -0.2739
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 0.87s
                      Time elapsed: 00:31:40
                               ETA: 00:01:12

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 108215 steps/s (collection: 0.808s, learning 0.100s)
             Mean action noise std: 10.67
          Mean value_function loss: 19.6811
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 29.9902
                       Mean reward: 855.72
               Mean episode length: 248.90
    Episode_Reward/reaching_object: 0.7869
     Episode_Reward/lifting_object: 173.9153
      Episode_Reward/object_height: 0.0536
        Episode_Reward/action_rate: -0.2754
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 0.91s
                      Time elapsed: 00:31:41
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 107847 steps/s (collection: 0.814s, learning 0.098s)
             Mean action noise std: 10.67
          Mean value_function loss: 19.8225
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 29.9950
                       Mean reward: 869.69
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 0.7837
     Episode_Reward/lifting_object: 173.9436
      Episode_Reward/object_height: 0.0537
        Episode_Reward/action_rate: -0.2735
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 0.91s
                      Time elapsed: 00:31:42
                               ETA: 00:01:10

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 108066 steps/s (collection: 0.804s, learning 0.106s)
             Mean action noise std: 10.67
          Mean value_function loss: 33.8377
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 29.9958
                       Mean reward: 870.44
               Mean episode length: 248.40
    Episode_Reward/reaching_object: 0.7812
     Episode_Reward/lifting_object: 173.8913
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.2721
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 0.91s
                      Time elapsed: 00:31:42
                               ETA: 00:01:10

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 102710 steps/s (collection: 0.789s, learning 0.168s)
             Mean action noise std: 10.68
          Mean value_function loss: 19.3387
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 29.9992
                       Mean reward: 869.83
               Mean episode length: 249.07
    Episode_Reward/reaching_object: 0.7798
     Episode_Reward/lifting_object: 173.6734
      Episode_Reward/object_height: 0.0540
        Episode_Reward/action_rate: -0.2709
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 0.96s
                      Time elapsed: 00:31:43
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 102038 steps/s (collection: 0.786s, learning 0.178s)
             Mean action noise std: 10.69
          Mean value_function loss: 21.3412
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 30.0040
                       Mean reward: 867.18
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 0.7801
     Episode_Reward/lifting_object: 174.0808
      Episode_Reward/object_height: 0.0539
        Episode_Reward/action_rate: -0.2720
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 0.96s
                      Time elapsed: 00:31:44
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 99569 steps/s (collection: 0.823s, learning 0.165s)
             Mean action noise std: 10.69
          Mean value_function loss: 30.7191
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 30.0106
                       Mean reward: 857.45
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7820
     Episode_Reward/lifting_object: 173.8991
      Episode_Reward/object_height: 0.0534
        Episode_Reward/action_rate: -0.2720
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 0.99s
                      Time elapsed: 00:31:45
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 99502 steps/s (collection: 0.808s, learning 0.180s)
             Mean action noise std: 10.70
          Mean value_function loss: 25.0209
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 30.0150
                       Mean reward: 870.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7745
     Episode_Reward/lifting_object: 172.5070
      Episode_Reward/object_height: 0.0529
        Episode_Reward/action_rate: -0.2721
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 0.99s
                      Time elapsed: 00:31:46
                               ETA: 00:01:06

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 99599 steps/s (collection: 0.836s, learning 0.151s)
             Mean action noise std: 10.71
          Mean value_function loss: 33.3760
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 30.0212
                       Mean reward: 858.37
               Mean episode length: 246.23
    Episode_Reward/reaching_object: 0.7815
     Episode_Reward/lifting_object: 172.5437
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.2740
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 0.99s
                      Time elapsed: 00:31:47
                               ETA: 00:01:05

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 108014 steps/s (collection: 0.788s, learning 0.122s)
             Mean action noise std: 10.71
          Mean value_function loss: 33.1221
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 30.0261
                       Mean reward: 861.72
               Mean episode length: 246.58
    Episode_Reward/reaching_object: 0.7781
     Episode_Reward/lifting_object: 172.5688
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.2732
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 0.91s
                      Time elapsed: 00:31:48
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 109557 steps/s (collection: 0.803s, learning 0.094s)
             Mean action noise std: 10.72
          Mean value_function loss: 29.2858
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 30.0314
                       Mean reward: 858.67
               Mean episode length: 246.76
    Episode_Reward/reaching_object: 0.7805
     Episode_Reward/lifting_object: 172.9972
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.2726
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 0.90s
                      Time elapsed: 00:31:49
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 109976 steps/s (collection: 0.804s, learning 0.090s)
             Mean action noise std: 10.73
          Mean value_function loss: 36.9416
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 30.0365
                       Mean reward: 855.93
               Mean episode length: 247.98
    Episode_Reward/reaching_object: 0.7850
     Episode_Reward/lifting_object: 173.4033
      Episode_Reward/object_height: 0.0516
        Episode_Reward/action_rate: -0.2740
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 0.89s
                      Time elapsed: 00:31:50
                               ETA: 00:01:02

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 110920 steps/s (collection: 0.790s, learning 0.096s)
             Mean action noise std: 10.74
          Mean value_function loss: 31.9819
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 30.0452
                       Mean reward: 873.16
               Mean episode length: 247.38
    Episode_Reward/reaching_object: 0.7833
     Episode_Reward/lifting_object: 173.7476
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.2758
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 0.89s
                      Time elapsed: 00:31:51
                               ETA: 00:01:01

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 104455 steps/s (collection: 0.844s, learning 0.097s)
             Mean action noise std: 10.74
          Mean value_function loss: 32.4764
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 30.0517
                       Mean reward: 842.40
               Mean episode length: 244.55
    Episode_Reward/reaching_object: 0.7759
     Episode_Reward/lifting_object: 171.3500
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.2742
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 0.94s
                      Time elapsed: 00:31:52
                               ETA: 00:01:00

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 111621 steps/s (collection: 0.788s, learning 0.093s)
             Mean action noise std: 10.75
          Mean value_function loss: 22.3314
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 30.0545
                       Mean reward: 882.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7859
     Episode_Reward/lifting_object: 173.9085
      Episode_Reward/object_height: 0.0518
        Episode_Reward/action_rate: -0.2756
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 0.88s
                      Time elapsed: 00:31:53
                               ETA: 00:00:59

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 107973 steps/s (collection: 0.809s, learning 0.102s)
             Mean action noise std: 10.76
          Mean value_function loss: 36.3490
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 30.0623
                       Mean reward: 871.29
               Mean episode length: 248.04
    Episode_Reward/reaching_object: 0.7835
     Episode_Reward/lifting_object: 174.3848
      Episode_Reward/object_height: 0.0522
        Episode_Reward/action_rate: -0.2751
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 0.91s
                      Time elapsed: 00:31:54
                               ETA: 00:00:58

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 111527 steps/s (collection: 0.777s, learning 0.105s)
             Mean action noise std: 10.77
          Mean value_function loss: 36.8262
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 30.0683
                       Mean reward: 876.19
               Mean episode length: 249.74
    Episode_Reward/reaching_object: 0.7855
     Episode_Reward/lifting_object: 174.0024
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.2756
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 0.88s
                      Time elapsed: 00:31:55
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 105831 steps/s (collection: 0.820s, learning 0.109s)
             Mean action noise std: 10.77
          Mean value_function loss: 31.9337
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.0697
                       Mean reward: 857.45
               Mean episode length: 247.23
    Episode_Reward/reaching_object: 0.7766
     Episode_Reward/lifting_object: 172.3057
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.2763
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 0.93s
                      Time elapsed: 00:31:55
                               ETA: 00:00:56

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 106429 steps/s (collection: 0.798s, learning 0.126s)
             Mean action noise std: 10.77
          Mean value_function loss: 23.4424
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.0727
                       Mean reward: 866.52
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 0.7799
     Episode_Reward/lifting_object: 173.3026
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.2751
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 0.92s
                      Time elapsed: 00:31:56
                               ETA: 00:00:55

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 105799 steps/s (collection: 0.784s, learning 0.145s)
             Mean action noise std: 10.78
          Mean value_function loss: 22.9199
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 30.0795
                       Mean reward: 875.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7838
     Episode_Reward/lifting_object: 173.3607
      Episode_Reward/object_height: 0.0526
        Episode_Reward/action_rate: -0.2765
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 0.93s
                      Time elapsed: 00:31:57
                               ETA: 00:00:54

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 98505 steps/s (collection: 0.832s, learning 0.166s)
             Mean action noise std: 10.79
          Mean value_function loss: 24.0639
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 30.0851
                       Mean reward: 877.40
               Mean episode length: 249.58
    Episode_Reward/reaching_object: 0.7818
     Episode_Reward/lifting_object: 172.8246
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.2769
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 1.00s
                      Time elapsed: 00:31:58
                               ETA: 00:00:53

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 102198 steps/s (collection: 0.781s, learning 0.181s)
             Mean action noise std: 10.80
          Mean value_function loss: 25.0965
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 30.0902
                       Mean reward: 869.36
               Mean episode length: 249.29
    Episode_Reward/reaching_object: 0.7747
     Episode_Reward/lifting_object: 172.2004
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.2765
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 0.96s
                      Time elapsed: 00:31:59
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 93687 steps/s (collection: 0.832s, learning 0.218s)
             Mean action noise std: 10.80
          Mean value_function loss: 26.9480
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 30.0970
                       Mean reward: 873.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7759
     Episode_Reward/lifting_object: 172.8544
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.2779
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 1.05s
                      Time elapsed: 00:32:00
                               ETA: 00:00:51

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 104781 steps/s (collection: 0.837s, learning 0.101s)
             Mean action noise std: 10.81
          Mean value_function loss: 30.9719
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 30.1022
                       Mean reward: 878.95
               Mean episode length: 249.87
    Episode_Reward/reaching_object: 0.7890
     Episode_Reward/lifting_object: 174.4364
      Episode_Reward/object_height: 0.0528
        Episode_Reward/action_rate: -0.2764
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 0.94s
                      Time elapsed: 00:32:01
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 107461 steps/s (collection: 0.801s, learning 0.114s)
             Mean action noise std: 10.82
          Mean value_function loss: 44.1958
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 30.1074
                       Mean reward: 852.90
               Mean episode length: 247.08
    Episode_Reward/reaching_object: 0.7790
     Episode_Reward/lifting_object: 172.1493
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.2776
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 0.91s
                      Time elapsed: 00:32:02
                               ETA: 00:00:49

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 110273 steps/s (collection: 0.798s, learning 0.094s)
             Mean action noise std: 10.83
          Mean value_function loss: 33.2962
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 30.1130
                       Mean reward: 869.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7711
     Episode_Reward/lifting_object: 171.7388
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.2772
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 0.89s
                      Time elapsed: 00:32:03
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 107042 steps/s (collection: 0.820s, learning 0.099s)
             Mean action noise std: 10.84
          Mean value_function loss: 31.0273
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 30.1186
                       Mean reward: 873.21
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 0.7741
     Episode_Reward/lifting_object: 172.8374
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.2789
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 0.92s
                      Time elapsed: 00:32:04
                               ETA: 00:00:47

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 106555 steps/s (collection: 0.827s, learning 0.096s)
             Mean action noise std: 10.84
          Mean value_function loss: 40.8055
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 30.1246
                       Mean reward: 874.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7801
     Episode_Reward/lifting_object: 173.7444
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.2795
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 0.92s
                      Time elapsed: 00:32:05
                               ETA: 00:00:46

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 112515 steps/s (collection: 0.780s, learning 0.094s)
             Mean action noise std: 10.84
          Mean value_function loss: 41.4539
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 30.1263
                       Mean reward: 876.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7766
     Episode_Reward/lifting_object: 171.9337
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.2761
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 0.87s
                      Time elapsed: 00:32:06
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 102391 steps/s (collection: 0.830s, learning 0.130s)
             Mean action noise std: 10.85
          Mean value_function loss: 32.2834
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 30.1297
                       Mean reward: 872.20
               Mean episode length: 249.14
    Episode_Reward/reaching_object: 0.7841
     Episode_Reward/lifting_object: 173.9761
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.2791
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.3333
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 0.96s
                      Time elapsed: 00:32:07
                               ETA: 00:00:44

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 109264 steps/s (collection: 0.787s, learning 0.113s)
             Mean action noise std: 10.85
          Mean value_function loss: 38.5068
               Mean surrogate loss: 0.0117
                 Mean entropy loss: 30.1329
                       Mean reward: 853.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7737
     Episode_Reward/lifting_object: 171.6850
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.2813
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 0.90s
                      Time elapsed: 00:32:08
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 89922 steps/s (collection: 0.919s, learning 0.174s)
             Mean action noise std: 10.85
          Mean value_function loss: 28.1852
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 30.1333
                       Mean reward: 870.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7652
     Episode_Reward/lifting_object: 170.8085
      Episode_Reward/object_height: 0.0512
        Episode_Reward/action_rate: -0.2832
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 1.09s
                      Time elapsed: 00:32:09
                               ETA: 00:00:42

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 102849 steps/s (collection: 0.832s, learning 0.124s)
             Mean action noise std: 10.86
          Mean value_function loss: 40.9669
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 30.1350
                       Mean reward: 862.96
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 0.7788
     Episode_Reward/lifting_object: 173.3488
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.2812
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 0.96s
                      Time elapsed: 00:32:10
                               ETA: 00:00:41

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 93716 steps/s (collection: 0.863s, learning 0.186s)
             Mean action noise std: 10.86
          Mean value_function loss: 33.1140
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 30.1385
                       Mean reward: 853.14
               Mean episode length: 249.75
    Episode_Reward/reaching_object: 0.7759
     Episode_Reward/lifting_object: 173.0617
      Episode_Reward/object_height: 0.0519
        Episode_Reward/action_rate: -0.2811
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 1.05s
                      Time elapsed: 00:32:11
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 102568 steps/s (collection: 0.856s, learning 0.102s)
             Mean action noise std: 10.87
          Mean value_function loss: 32.2649
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 30.1413
                       Mean reward: 856.68
               Mean episode length: 249.43
    Episode_Reward/reaching_object: 0.7719
     Episode_Reward/lifting_object: 171.2324
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.2824
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 0.96s
                      Time elapsed: 00:32:12
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 105058 steps/s (collection: 0.828s, learning 0.108s)
             Mean action noise std: 10.87
          Mean value_function loss: 23.7195
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 30.1460
                       Mean reward: 869.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 172.4457
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.2826
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 0.94s
                      Time elapsed: 00:32:13
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 102658 steps/s (collection: 0.850s, learning 0.107s)
             Mean action noise std: 10.88
          Mean value_function loss: 35.1235
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 30.1488
                       Mean reward: 861.23
               Mean episode length: 248.84
    Episode_Reward/reaching_object: 0.7765
     Episode_Reward/lifting_object: 173.5900
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.2813
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 0.96s
                      Time elapsed: 00:32:14
                               ETA: 00:00:37

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 105128 steps/s (collection: 0.844s, learning 0.092s)
             Mean action noise std: 10.88
          Mean value_function loss: 24.1520
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 30.1518
                       Mean reward: 882.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7700
     Episode_Reward/lifting_object: 171.7491
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.2806
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 0.94s
                      Time elapsed: 00:32:15
                               ETA: 00:00:36

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 106220 steps/s (collection: 0.822s, learning 0.104s)
             Mean action noise std: 10.88
          Mean value_function loss: 31.4170
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 30.1546
                       Mean reward: 852.58
               Mean episode length: 247.93
    Episode_Reward/reaching_object: 0.7735
     Episode_Reward/lifting_object: 172.7566
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.2820
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 0.93s
                      Time elapsed: 00:32:15
                               ETA: 00:00:35

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 109513 steps/s (collection: 0.796s, learning 0.102s)
             Mean action noise std: 10.89
          Mean value_function loss: 35.4413
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 30.1574
                       Mean reward: 852.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 170.6319
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.2828
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.1250
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 0.90s
                      Time elapsed: 00:32:16
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 107403 steps/s (collection: 0.789s, learning 0.126s)
             Mean action noise std: 10.90
          Mean value_function loss: 31.3558
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 30.1632
                       Mean reward: 871.30
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.7800
     Episode_Reward/lifting_object: 172.2971
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.2816
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 0.92s
                      Time elapsed: 00:32:17
                               ETA: 00:00:33

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 92015 steps/s (collection: 0.883s, learning 0.186s)
             Mean action noise std: 10.91
          Mean value_function loss: 38.3467
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 30.1714
                       Mean reward: 854.89
               Mean episode length: 249.39
    Episode_Reward/reaching_object: 0.7739
     Episode_Reward/lifting_object: 170.1686
      Episode_Reward/object_height: 0.0506
        Episode_Reward/action_rate: -0.2833
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 1.07s
                      Time elapsed: 00:32:18
                               ETA: 00:00:32

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 91403 steps/s (collection: 0.936s, learning 0.140s)
             Mean action noise std: 10.92
          Mean value_function loss: 32.5908
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 30.1809
                       Mean reward: 864.51
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.7832
     Episode_Reward/lifting_object: 174.1947
      Episode_Reward/object_height: 0.0520
        Episode_Reward/action_rate: -0.2808
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 1.08s
                      Time elapsed: 00:32:19
                               ETA: 00:00:31

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 102956 steps/s (collection: 0.840s, learning 0.115s)
             Mean action noise std: 10.93
          Mean value_function loss: 28.9404
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 30.1846
                       Mean reward: 862.17
               Mean episode length: 248.74
    Episode_Reward/reaching_object: 0.7798
     Episode_Reward/lifting_object: 172.0354
      Episode_Reward/object_height: 0.0516
        Episode_Reward/action_rate: -0.2831
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 0.95s
                      Time elapsed: 00:32:20
                               ETA: 00:00:30

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 100732 steps/s (collection: 0.825s, learning 0.151s)
             Mean action noise std: 10.93
          Mean value_function loss: 29.2234
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 30.1915
                       Mean reward: 862.95
               Mean episode length: 246.78
    Episode_Reward/reaching_object: 0.7816
     Episode_Reward/lifting_object: 173.2852
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.2807
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 0.98s
                      Time elapsed: 00:32:21
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 104274 steps/s (collection: 0.823s, learning 0.120s)
             Mean action noise std: 10.94
          Mean value_function loss: 29.9045
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 30.1959
                       Mean reward: 882.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7833
     Episode_Reward/lifting_object: 174.4841
      Episode_Reward/object_height: 0.0525
        Episode_Reward/action_rate: -0.2814
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 0.94s
                      Time elapsed: 00:32:22
                               ETA: 00:00:28

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 108477 steps/s (collection: 0.785s, learning 0.121s)
             Mean action noise std: 10.95
          Mean value_function loss: 22.7571
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 30.2023
                       Mean reward: 865.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7746
     Episode_Reward/lifting_object: 171.9233
      Episode_Reward/object_height: 0.0517
        Episode_Reward/action_rate: -0.2823
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 0.91s
                      Time elapsed: 00:32:23
                               ETA: 00:00:27

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 102584 steps/s (collection: 0.819s, learning 0.140s)
             Mean action noise std: 10.96
          Mean value_function loss: 15.5251
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 30.2086
                       Mean reward: 879.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7779
     Episode_Reward/lifting_object: 173.7468
      Episode_Reward/object_height: 0.0523
        Episode_Reward/action_rate: -0.2821
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 0.96s
                      Time elapsed: 00:32:24
                               ETA: 00:00:26

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 110924 steps/s (collection: 0.794s, learning 0.093s)
             Mean action noise std: 10.96
          Mean value_function loss: 28.2200
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 30.2126
                       Mean reward: 884.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7821
     Episode_Reward/lifting_object: 173.1554
      Episode_Reward/object_height: 0.0521
        Episode_Reward/action_rate: -0.2819
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 0.89s
                      Time elapsed: 00:32:25
                               ETA: 00:00:25

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 111726 steps/s (collection: 0.785s, learning 0.095s)
             Mean action noise std: 10.97
          Mean value_function loss: 25.5153
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 30.2169
                       Mean reward: 875.11
               Mean episode length: 248.97
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: 172.1118
      Episode_Reward/object_height: 0.0514
        Episode_Reward/action_rate: -0.2812
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 0.88s
                      Time elapsed: 00:32:26
                               ETA: 00:00:24

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 99897 steps/s (collection: 0.871s, learning 0.113s)
             Mean action noise std: 10.97
          Mean value_function loss: 20.8160
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 30.2199
                       Mean reward: 864.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7693
     Episode_Reward/lifting_object: 171.6488
      Episode_Reward/object_height: 0.0515
        Episode_Reward/action_rate: -0.2806
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.9167
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 0.98s
                      Time elapsed: 00:32:27
                               ETA: 00:00:23

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 111009 steps/s (collection: 0.773s, learning 0.113s)
             Mean action noise std: 10.98
          Mean value_function loss: 29.1625
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 30.2219
                       Mean reward: 867.79
               Mean episode length: 249.33
    Episode_Reward/reaching_object: 0.7677
     Episode_Reward/lifting_object: 171.3979
      Episode_Reward/object_height: 0.0510
        Episode_Reward/action_rate: -0.2819
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 0.89s
                      Time elapsed: 00:32:28
                               ETA: 00:00:22

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 102972 steps/s (collection: 0.842s, learning 0.113s)
             Mean action noise std: 10.98
          Mean value_function loss: 24.8534
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 30.2258
                       Mean reward: 859.10
               Mean episode length: 249.46
    Episode_Reward/reaching_object: 0.7679
     Episode_Reward/lifting_object: 171.7151
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.2814
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 0.95s
                      Time elapsed: 00:32:29
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 108344 steps/s (collection: 0.807s, learning 0.100s)
             Mean action noise std: 11.00
          Mean value_function loss: 24.0247
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 30.2360
                       Mean reward: 861.12
               Mean episode length: 248.79
    Episode_Reward/reaching_object: 0.7685
     Episode_Reward/lifting_object: 171.4809
      Episode_Reward/object_height: 0.0504
        Episode_Reward/action_rate: -0.2818
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 0.91s
                      Time elapsed: 00:32:30
                               ETA: 00:00:20

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 107010 steps/s (collection: 0.828s, learning 0.091s)
             Mean action noise std: 11.00
          Mean value_function loss: 33.6157
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 30.2446
                       Mean reward: 853.41
               Mean episode length: 247.71
    Episode_Reward/reaching_object: 0.7744
     Episode_Reward/lifting_object: 174.1096
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.2819
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 0.92s
                      Time elapsed: 00:32:31
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 104824 steps/s (collection: 0.840s, learning 0.098s)
             Mean action noise std: 11.01
          Mean value_function loss: 46.3163
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 30.2489
                       Mean reward: 877.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7784
     Episode_Reward/lifting_object: 173.8791
      Episode_Reward/object_height: 0.0513
        Episode_Reward/action_rate: -0.2796
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 0.94s
                      Time elapsed: 00:32:32
                               ETA: 00:00:18

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 101470 steps/s (collection: 0.825s, learning 0.144s)
             Mean action noise std: 11.02
          Mean value_function loss: 30.4358
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 30.2552
                       Mean reward: 871.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 172.4945
      Episode_Reward/object_height: 0.0509
        Episode_Reward/action_rate: -0.2796
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 0.97s
                      Time elapsed: 00:32:32
                               ETA: 00:00:17

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 106139 steps/s (collection: 0.822s, learning 0.104s)
             Mean action noise std: 11.03
          Mean value_function loss: 26.1710
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 30.2630
                       Mean reward: 852.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7774
     Episode_Reward/lifting_object: 173.3465
      Episode_Reward/object_height: 0.0511
        Episode_Reward/action_rate: -0.2832
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 0.93s
                      Time elapsed: 00:32:33
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 101327 steps/s (collection: 0.837s, learning 0.133s)
             Mean action noise std: 11.04
          Mean value_function loss: 32.4562
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 30.2689
                       Mean reward: 868.95
               Mean episode length: 248.25
    Episode_Reward/reaching_object: 0.7757
     Episode_Reward/lifting_object: 173.5031
      Episode_Reward/object_height: 0.0511
        Episode_Reward/action_rate: -0.2812
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 0.97s
                      Time elapsed: 00:32:34
                               ETA: 00:00:15

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 106858 steps/s (collection: 0.820s, learning 0.100s)
             Mean action noise std: 11.04
          Mean value_function loss: 30.0730
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 30.2725
                       Mean reward: 853.31
               Mean episode length: 248.54
    Episode_Reward/reaching_object: 0.7711
     Episode_Reward/lifting_object: 172.9643
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.2811
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 0.92s
                      Time elapsed: 00:32:35
                               ETA: 00:00:14

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 103102 steps/s (collection: 0.799s, learning 0.154s)
             Mean action noise std: 11.05
          Mean value_function loss: 30.3313
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 30.2769
                       Mean reward: 877.18
               Mean episode length: 248.14
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 173.5876
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.2813
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 18.7500
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 0.95s
                      Time elapsed: 00:32:36
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 105650 steps/s (collection: 0.792s, learning 0.139s)
             Mean action noise std: 11.05
          Mean value_function loss: 37.9547
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 30.2795
                       Mean reward: 871.11
               Mean episode length: 248.10
    Episode_Reward/reaching_object: 0.7708
     Episode_Reward/lifting_object: 173.4342
      Episode_Reward/object_height: 0.0507
        Episode_Reward/action_rate: -0.2814
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 0.93s
                      Time elapsed: 00:32:37
                               ETA: 00:00:12

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 108694 steps/s (collection: 0.792s, learning 0.112s)
             Mean action noise std: 11.07
          Mean value_function loss: 37.3581
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 30.2868
                       Mean reward: 861.63
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 0.7620
     Episode_Reward/lifting_object: 171.2442
      Episode_Reward/object_height: 0.0500
        Episode_Reward/action_rate: -0.2832
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 0.90s
                      Time elapsed: 00:32:38
                               ETA: 00:00:11

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 107553 steps/s (collection: 0.801s, learning 0.113s)
             Mean action noise std: 11.07
          Mean value_function loss: 28.4586
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 30.2941
                       Mean reward: 864.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7560
     Episode_Reward/lifting_object: 170.9997
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.2843
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 0.91s
                      Time elapsed: 00:32:39
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 105042 steps/s (collection: 0.835s, learning 0.101s)
             Mean action noise std: 11.08
          Mean value_function loss: 21.2995
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 30.2990
                       Mean reward: 856.98
               Mean episode length: 248.41
    Episode_Reward/reaching_object: 0.7572
     Episode_Reward/lifting_object: 171.7369
      Episode_Reward/object_height: 0.0495
        Episode_Reward/action_rate: -0.2867
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 0.94s
                      Time elapsed: 00:32:40
                               ETA: 00:00:09

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 109261 steps/s (collection: 0.803s, learning 0.097s)
             Mean action noise std: 11.09
          Mean value_function loss: 16.5920
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 30.3057
                       Mean reward: 861.84
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 0.7738
     Episode_Reward/lifting_object: 172.3579
      Episode_Reward/object_height: 0.0490
        Episode_Reward/action_rate: -0.2838
          Episode_Reward/joint_vel: -0.0025
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 0.90s
                      Time elapsed: 00:32:41
                               ETA: 00:00:08

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 105672 steps/s (collection: 0.819s, learning 0.111s)
             Mean action noise std: 11.10
          Mean value_function loss: 27.4724
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 30.3125
                       Mean reward: 880.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7842
     Episode_Reward/lifting_object: 173.8544
      Episode_Reward/object_height: 0.0496
        Episode_Reward/action_rate: -0.2867
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 0.93s
                      Time elapsed: 00:32:42
                               ETA: 00:00:07

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 109866 steps/s (collection: 0.804s, learning 0.091s)
             Mean action noise std: 11.10
          Mean value_function loss: 24.0137
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 30.3188
                       Mean reward: 867.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7802
     Episode_Reward/lifting_object: 174.0630
      Episode_Reward/object_height: 0.0493
        Episode_Reward/action_rate: -0.2857
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 0.89s
                      Time elapsed: 00:32:43
                               ETA: 00:00:06

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 106460 steps/s (collection: 0.834s, learning 0.090s)
             Mean action noise std: 11.11
          Mean value_function loss: 17.0516
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 30.3233
                       Mean reward: 872.17
               Mean episode length: 248.03
    Episode_Reward/reaching_object: 0.7779
     Episode_Reward/lifting_object: 173.0215
      Episode_Reward/object_height: 0.0489
        Episode_Reward/action_rate: -0.2862
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 0.92s
                      Time elapsed: 00:32:44
                               ETA: 00:00:05

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 93652 steps/s (collection: 0.880s, learning 0.170s)
             Mean action noise std: 11.12
          Mean value_function loss: 26.9778
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 30.3310
                       Mean reward: 874.43
               Mean episode length: 249.63
    Episode_Reward/reaching_object: 0.7758
     Episode_Reward/lifting_object: 173.5410
      Episode_Reward/object_height: 0.0491
        Episode_Reward/action_rate: -0.2876
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 1.05s
                      Time elapsed: 00:32:45
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 103983 steps/s (collection: 0.851s, learning 0.095s)
             Mean action noise std: 11.13
          Mean value_function loss: 25.3353
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 30.3386
                       Mean reward: 851.62
               Mean episode length: 247.30
    Episode_Reward/reaching_object: 0.7799
     Episode_Reward/lifting_object: 173.3624
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.2872
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 0.95s
                      Time elapsed: 00:32:46
                               ETA: 00:00:03

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 106790 steps/s (collection: 0.809s, learning 0.111s)
             Mean action noise std: 11.14
          Mean value_function loss: 29.7177
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 30.3456
                       Mean reward: 852.18
               Mean episode length: 247.58
    Episode_Reward/reaching_object: 0.7708
     Episode_Reward/lifting_object: 171.8590
      Episode_Reward/object_height: 0.0485
        Episode_Reward/action_rate: -0.2869
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 0.92s
                      Time elapsed: 00:32:46
                               ETA: 00:00:02

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 102615 steps/s (collection: 0.829s, learning 0.129s)
             Mean action noise std: 11.15
          Mean value_function loss: 22.3941
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 30.3517
                       Mean reward: 867.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7769
     Episode_Reward/lifting_object: 172.5033
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.2853
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 0.96s
                      Time elapsed: 00:32:47
                               ETA: 00:00:01

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 104471 steps/s (collection: 0.843s, learning 0.098s)
             Mean action noise std: 11.15
          Mean value_function loss: 29.4119
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 30.3570
                       Mean reward: 872.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7815
     Episode_Reward/lifting_object: 172.8588
      Episode_Reward/object_height: 0.0488
        Episode_Reward/action_rate: -0.2878
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 0.94s
                      Time elapsed: 00:32:48
                               ETA: 00:00:00

