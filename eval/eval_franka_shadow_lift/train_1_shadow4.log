################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 10856 steps/s (collection: 8.806s, learning 0.248s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0049
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 36.9322
                       Mean reward: 0.00
               Mean episode length: 21.93
    Episode_Reward/reaching_object: 0.0009
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0003
          Episode_Reward/joint_vel: -0.0004
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 9.05s
                      Time elapsed: 00:00:09
                               ETA: 05:01:49

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 14544 steps/s (collection: 6.612s, learning 0.147s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 37.0479
                       Mean reward: 0.01
               Mean episode length: 45.42
    Episode_Reward/reaching_object: 0.0025
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0008
          Episode_Reward/joint_vel: -0.0011
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 6.76s
                      Time elapsed: 00:00:15
                               ETA: 04:23:25

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 14851 steps/s (collection: 6.472s, learning 0.147s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 37.1421
                       Mean reward: 0.01
               Mean episode length: 69.07
    Episode_Reward/reaching_object: 0.0045
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0013
          Episode_Reward/joint_vel: -0.0018
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 6.62s
                      Time elapsed: 00:00:22
                               ETA: 04:09:00

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 14890 steps/s (collection: 6.447s, learning 0.155s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 37.1949
                       Mean reward: 0.02
               Mean episode length: 93.12
    Episode_Reward/reaching_object: 0.0059
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 6.60s
                      Time elapsed: 00:00:29
                               ETA: 04:01:35

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 13925 steps/s (collection: 6.932s, learning 0.127s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 37.2238
                       Mean reward: 0.02
               Mean episode length: 117.84
    Episode_Reward/reaching_object: 0.0085
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0033
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 7.06s
                      Time elapsed: 00:00:36
                               ETA: 04:00:08

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 15044 steps/s (collection: 6.405s, learning 0.129s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 37.2641
                       Mean reward: 0.03
               Mean episode length: 141.19
    Episode_Reward/reaching_object: 0.0107
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0041
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 6.53s
                      Time elapsed: 00:00:42
                               ETA: 03:56:13

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 14556 steps/s (collection: 6.616s, learning 0.138s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 37.2948
                       Mean reward: 0.04
               Mean episode length: 165.18
    Episode_Reward/reaching_object: 0.0137
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0048
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 6.75s
                      Time elapsed: 00:00:49
                               ETA: 03:54:26

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 14986 steps/s (collection: 6.414s, learning 0.145s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 37.3170
                       Mean reward: 0.05
               Mean episode length: 189.88
    Episode_Reward/reaching_object: 0.0175
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0056
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 6.56s
                      Time elapsed: 00:00:55
                               ETA: 03:52:16

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 18471 steps/s (collection: 5.230s, learning 0.092s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 37.3345
                       Mean reward: 0.07
               Mean episode length: 213.51
    Episode_Reward/reaching_object: 0.0205
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0064
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 5.32s
                      Time elapsed: 00:01:01
                               ETA: 03:45:59

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 56057 steps/s (collection: 1.643s, learning 0.111s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 37.3500
                       Mean reward: 0.08
               Mean episode length: 236.90
    Episode_Reward/reaching_object: 0.0254
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0049
          Episode_Reward/joint_vel: -0.0071
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 1.75s
                      Time elapsed: 00:01:03
                               ETA: 03:29:06

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 55922 steps/s (collection: 1.636s, learning 0.122s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 37.3858
                       Mean reward: 0.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0325
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 1.76s
                      Time elapsed: 00:01:04
                               ETA: 03:15:18

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 57967 steps/s (collection: 1.589s, learning 0.107s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 37.4113
                       Mean reward: 0.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0358
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.70s
                      Time elapsed: 00:01:06
                               ETA: 03:03:37

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 56957 steps/s (collection: 1.625s, learning 0.101s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 37.4203
                       Mean reward: 0.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0461
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.73s
                      Time elapsed: 00:01:08
                               ETA: 02:53:48

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 57274 steps/s (collection: 1.602s, learning 0.115s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 37.4458
                       Mean reward: 0.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0552
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.72s
                      Time elapsed: 00:01:09
                               ETA: 02:45:22

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 57060 steps/s (collection: 1.628s, learning 0.095s)
             Mean action noise std: 1.03
          Mean value_function loss: 1.4495
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 37.5377
                       Mean reward: -0.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0764
     Episode_Reward/lifting_object: -0.0227
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.72s
                      Time elapsed: 00:01:11
                               ETA: 02:38:04

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 53751 steps/s (collection: 1.742s, learning 0.087s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.5546
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 37.6228
                       Mean reward: 0.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0963
     Episode_Reward/lifting_object: -0.1616
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.83s
                      Time elapsed: 00:01:13
                               ETA: 02:31:54

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 52000 steps/s (collection: 1.797s, learning 0.094s)
             Mean action noise std: 1.04
          Mean value_function loss: 1.9814
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 37.8329
                       Mean reward: -1.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1279
     Episode_Reward/lifting_object: -0.1755
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.89s
                      Time elapsed: 00:01:15
                               ETA: 02:26:34

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 51809 steps/s (collection: 1.805s, learning 0.092s)
             Mean action noise std: 1.05
          Mean value_function loss: 3.1813
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 37.9972
                       Mean reward: -1.45
               Mean episode length: 249.73
    Episode_Reward/reaching_object: 0.1472
     Episode_Reward/lifting_object: -0.1252
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.90s
                      Time elapsed: 00:01:17
                               ETA: 02:21:50

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 53366 steps/s (collection: 1.732s, learning 0.110s)
             Mean action noise std: 1.06
          Mean value_function loss: 2.5377
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 38.2009
                       Mean reward: -2.01
               Mean episode length: 249.48
    Episode_Reward/reaching_object: 0.1601
     Episode_Reward/lifting_object: -0.2585
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 1.84s
                      Time elapsed: 00:01:19
                               ETA: 02:17:30

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 51245 steps/s (collection: 1.799s, learning 0.119s)
             Mean action noise std: 1.06
          Mean value_function loss: 3.0157
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 38.3609
                       Mean reward: 0.14
               Mean episode length: 248.49
    Episode_Reward/reaching_object: 0.1974
     Episode_Reward/lifting_object: -0.3015
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.92s
                      Time elapsed: 00:01:21
                               ETA: 02:13:44

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 53576 steps/s (collection: 1.742s, learning 0.093s)
             Mean action noise std: 1.07
          Mean value_function loss: 1.7431
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.5273
                       Mean reward: -0.34
               Mean episode length: 249.08
    Episode_Reward/reaching_object: 0.2163
     Episode_Reward/lifting_object: -0.4087
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 1.83s
                      Time elapsed: 00:01:22
                               ETA: 02:10:11

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 53550 steps/s (collection: 1.743s, learning 0.093s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.3671
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 38.6482
                       Mean reward: 1.11
               Mean episode length: 249.48
    Episode_Reward/reaching_object: 0.2224
     Episode_Reward/lifting_object: -0.0559
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 1.84s
                      Time elapsed: 00:01:24
                               ETA: 02:06:57

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 52248 steps/s (collection: 1.793s, learning 0.088s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.9067
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 38.7879
                       Mean reward: 0.53
               Mean episode length: 249.09
    Episode_Reward/reaching_object: 0.2229
     Episode_Reward/lifting_object: -0.1724
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 1.88s
                      Time elapsed: 00:01:26
                               ETA: 02:04:04

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 54646 steps/s (collection: 1.711s, learning 0.088s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.1536
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 38.9072
                       Mean reward: 0.97
               Mean episode length: 249.61
    Episode_Reward/reaching_object: 0.2128
     Episode_Reward/lifting_object: -0.0647
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 1.80s
                      Time elapsed: 00:01:28
                               ETA: 02:01:18

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 55557 steps/s (collection: 1.675s, learning 0.094s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 39.0496
                       Mean reward: 0.91
               Mean episode length: 249.84
    Episode_Reward/reaching_object: 0.1918
     Episode_Reward/lifting_object: -0.0088
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0083
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 1.77s
                      Time elapsed: 00:01:30
                               ETA: 01:58:44

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 54666 steps/s (collection: 1.706s, learning 0.093s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.7700
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.1505
                       Mean reward: 0.82
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.1925
     Episode_Reward/lifting_object: -0.0937
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0084
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 1.80s
                      Time elapsed: 00:01:31
                               ETA: 01:56:23

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 56337 steps/s (collection: 1.656s, learning 0.089s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0009
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 39.1955
                       Mean reward: 0.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1509
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 1.74s
                      Time elapsed: 00:01:33
                               ETA: 01:54:08

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 54799 steps/s (collection: 1.688s, learning 0.106s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 39.3144
                       Mean reward: 0.63
               Mean episode length: 249.93
    Episode_Reward/reaching_object: 0.1455
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 1.79s
                      Time elapsed: 00:01:35
                               ETA: 01:52:07

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 53961 steps/s (collection: 1.709s, learning 0.113s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 39.3471
                       Mean reward: 0.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1375
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.82s
                      Time elapsed: 00:01:37
                               ETA: 01:50:15

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 56241 steps/s (collection: 1.647s, learning 0.101s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.4020
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.4284
                       Mean reward: 0.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1256
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.75s
                      Time elapsed: 00:01:39
                               ETA: 01:48:26

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 53198 steps/s (collection: 1.742s, learning 0.106s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0475
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.4788
                       Mean reward: 0.63
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 0.1296
     Episode_Reward/lifting_object: -0.0538
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.85s
                      Time elapsed: 00:01:40
                               ETA: 01:46:51

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 56403 steps/s (collection: 1.656s, learning 0.087s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0301
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 39.6115
                       Mean reward: 0.32
               Mean episode length: 249.93
    Episode_Reward/reaching_object: 0.1288
     Episode_Reward/lifting_object: -0.0111
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.74s
                      Time elapsed: 00:01:42
                               ETA: 01:45:14

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 52683 steps/s (collection: 1.755s, learning 0.111s)
             Mean action noise std: 1.12
          Mean value_function loss: 1.4839
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 39.7071
                       Mean reward: 0.08
               Mean episode length: 249.86
    Episode_Reward/reaching_object: 0.1484
     Episode_Reward/lifting_object: -0.1371
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.87s
                      Time elapsed: 00:01:44
                               ETA: 01:43:51

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 51032 steps/s (collection: 1.821s, learning 0.105s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0245
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 39.8657
                       Mean reward: 0.52
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 0.1637
     Episode_Reward/lifting_object: -0.0485
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.93s
                      Time elapsed: 00:01:46
                               ETA: 01:42:36

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 50776 steps/s (collection: 1.847s, learning 0.089s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.9614
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 40.1242
                       Mean reward: -0.75
               Mean episode length: 248.59
    Episode_Reward/reaching_object: 0.1748
     Episode_Reward/lifting_object: -0.1035
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.94s
                      Time elapsed: 00:01:48
                               ETA: 01:41:26

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 52543 steps/s (collection: 1.782s, learning 0.088s)
             Mean action noise std: 1.14
          Mean value_function loss: 1.8738
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 40.1975
                       Mean reward: 0.65
               Mean episode length: 247.17
    Episode_Reward/reaching_object: 0.2144
     Episode_Reward/lifting_object: -0.2451
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.87s
                      Time elapsed: 00:01:50
                               ETA: 01:40:16

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 51379 steps/s (collection: 1.804s, learning 0.110s)
             Mean action noise std: 1.14
          Mean value_function loss: 1.7943
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 40.3288
                       Mean reward: 0.70
               Mean episode length: 246.66
    Episode_Reward/reaching_object: 0.2333
     Episode_Reward/lifting_object: -0.1237
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.91s
                      Time elapsed: 00:01:52
                               ETA: 01:39:12

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 52560 steps/s (collection: 1.783s, learning 0.087s)
             Mean action noise std: 1.15
          Mean value_function loss: 1.1465
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 40.4280
                       Mean reward: 1.32
               Mean episode length: 247.21
    Episode_Reward/reaching_object: 0.2502
     Episode_Reward/lifting_object: -0.2156
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 1.87s
                      Time elapsed: 00:01:54
                               ETA: 01:38:09

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 52724 steps/s (collection: 1.774s, learning 0.091s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.6027
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 40.5564
                       Mean reward: 1.14
               Mean episode length: 246.97
    Episode_Reward/reaching_object: 0.2635
     Episode_Reward/lifting_object: -0.1847
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.86s
                      Time elapsed: 00:01:55
                               ETA: 01:37:09

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 52873 steps/s (collection: 1.770s, learning 0.090s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.0073
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 40.6610
                       Mean reward: 1.37
               Mean episode length: 247.24
    Episode_Reward/reaching_object: 0.2745
     Episode_Reward/lifting_object: -0.0606
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.86s
                      Time elapsed: 00:01:57
                               ETA: 01:36:12

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 53371 steps/s (collection: 1.756s, learning 0.086s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.1066
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 40.8109
                       Mean reward: 1.21
               Mean episode length: 247.26
    Episode_Reward/reaching_object: 0.2623
     Episode_Reward/lifting_object: -0.0196
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.84s
                      Time elapsed: 00:01:59
                               ETA: 01:35:16

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 52601 steps/s (collection: 1.779s, learning 0.090s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.3753
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 40.8562
                       Mean reward: 0.83
               Mean episode length: 245.47
    Episode_Reward/reaching_object: 0.2579
     Episode_Reward/lifting_object: -0.0845
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.87s
                      Time elapsed: 00:02:01
                               ETA: 01:34:24

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 52901 steps/s (collection: 1.764s, learning 0.095s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 40.9281
                       Mean reward: 1.28
               Mean episode length: 247.06
    Episode_Reward/reaching_object: 0.2706
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.86s
                      Time elapsed: 00:02:03
                               ETA: 01:33:34

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 52782 steps/s (collection: 1.776s, learning 0.086s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.2767
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 41.0256
                       Mean reward: 1.06
               Mean episode length: 248.72
    Episode_Reward/reaching_object: 0.2469
     Episode_Reward/lifting_object: -0.0660
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.86s
                      Time elapsed: 00:02:05
                               ETA: 01:32:47

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 51602 steps/s (collection: 1.787s, learning 0.118s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.1530
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.0783
                       Mean reward: 1.04
               Mean episode length: 248.34
    Episode_Reward/reaching_object: 0.2516
     Episode_Reward/lifting_object: -0.0648
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 1.91s
                      Time elapsed: 00:02:07
                               ETA: 01:32:03

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 52200 steps/s (collection: 1.793s, learning 0.090s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.4973
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 41.1831
                       Mean reward: 0.35
               Mean episode length: 246.63
    Episode_Reward/reaching_object: 0.2254
     Episode_Reward/lifting_object: -0.0878
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 1.88s
                      Time elapsed: 00:02:08
                               ETA: 01:31:20

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 53070 steps/s (collection: 1.765s, learning 0.088s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0312
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 41.2645
                       Mean reward: 1.09
               Mean episode length: 247.53
    Episode_Reward/reaching_object: 0.2418
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 1.85s
                      Time elapsed: 00:02:10
                               ETA: 01:30:38

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 53427 steps/s (collection: 1.755s, learning 0.085s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.2055
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.3693
                       Mean reward: 1.07
               Mean episode length: 249.00
    Episode_Reward/reaching_object: 0.2336
     Episode_Reward/lifting_object: -0.0312
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.84s
                      Time elapsed: 00:02:12
                               ETA: 01:29:57

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 52460 steps/s (collection: 1.779s, learning 0.095s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.6887
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 41.4152
                       Mean reward: 1.11
               Mean episode length: 248.44
    Episode_Reward/reaching_object: 0.2382
     Episode_Reward/lifting_object: -0.0263
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.87s
                      Time elapsed: 00:02:14
                               ETA: 01:29:18

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 52489 steps/s (collection: 1.783s, learning 0.090s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.5118
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.4962
                       Mean reward: 0.81
               Mean episode length: 247.99
    Episode_Reward/reaching_object: 0.2407
     Episode_Reward/lifting_object: -0.1712
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.87s
                      Time elapsed: 00:02:16
                               ETA: 01:28:42

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 52756 steps/s (collection: 1.770s, learning 0.094s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.2080
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.5730
                       Mean reward: 1.16
               Mean episode length: 246.89
    Episode_Reward/reaching_object: 0.2516
     Episode_Reward/lifting_object: -0.0737
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.86s
                      Time elapsed: 00:02:18
                               ETA: 01:28:06

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 51797 steps/s (collection: 1.794s, learning 0.104s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.1202
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.6654
                       Mean reward: 0.66
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 0.2502
     Episode_Reward/lifting_object: -0.0415
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.90s
                      Time elapsed: 00:02:20
                               ETA: 01:27:33

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 54043 steps/s (collection: 1.725s, learning 0.094s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.9647
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.7636
                       Mean reward: -0.33
               Mean episode length: 246.87
    Episode_Reward/reaching_object: 0.2629
     Episode_Reward/lifting_object: -0.0826
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.82s
                      Time elapsed: 00:02:21
                               ETA: 01:26:58

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 52879 steps/s (collection: 1.767s, learning 0.092s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.4614
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 41.8532
                       Mean reward: 1.15
               Mean episode length: 247.95
    Episode_Reward/reaching_object: 0.2702
     Episode_Reward/lifting_object: -0.1149
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 1.86s
                      Time elapsed: 00:02:23
                               ETA: 01:26:26

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 52513 steps/s (collection: 1.771s, learning 0.101s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.2030
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.9312
                       Mean reward: 0.95
               Mean episode length: 246.35
    Episode_Reward/reaching_object: 0.2587
     Episode_Reward/lifting_object: -0.0656
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 1.87s
                      Time elapsed: 00:02:25
                               ETA: 01:25:55

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 51922 steps/s (collection: 1.798s, learning 0.095s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.5627
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.0200
                       Mean reward: 0.39
               Mean episode length: 245.08
    Episode_Reward/reaching_object: 0.2748
     Episode_Reward/lifting_object: -0.0420
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 1.89s
                      Time elapsed: 00:02:27
                               ETA: 01:25:26

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 52412 steps/s (collection: 1.790s, learning 0.086s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.7209
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 42.0832
                       Mean reward: 0.47
               Mean episode length: 246.94
    Episode_Reward/reaching_object: 0.2757
     Episode_Reward/lifting_object: -0.0958
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 1.88s
                      Time elapsed: 00:02:29
                               ETA: 01:24:57

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 51933 steps/s (collection: 1.804s, learning 0.089s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.3353
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.1817
                       Mean reward: 1.25
               Mean episode length: 246.23
    Episode_Reward/reaching_object: 0.2963
     Episode_Reward/lifting_object: -0.0542
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.89s
                      Time elapsed: 00:02:31
                               ETA: 01:24:30

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 50284 steps/s (collection: 1.862s, learning 0.093s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.2003
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 42.2574
                       Mean reward: 1.40
               Mean episode length: 247.07
    Episode_Reward/reaching_object: 0.2877
     Episode_Reward/lifting_object: -0.1020
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.95s
                      Time elapsed: 00:02:33
                               ETA: 01:24:06

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 52416 steps/s (collection: 1.770s, learning 0.106s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.8047
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.3646
                       Mean reward: 1.28
               Mean episode length: 244.40
    Episode_Reward/reaching_object: 0.2961
     Episode_Reward/lifting_object: -0.1477
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.88s
                      Time elapsed: 00:02:35
                               ETA: 01:23:40

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 52623 steps/s (collection: 1.756s, learning 0.112s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.0738
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 42.4364
                       Mean reward: 1.33
               Mean episode length: 245.84
    Episode_Reward/reaching_object: 0.2921
     Episode_Reward/lifting_object: -0.0133
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.87s
                      Time elapsed: 00:02:37
                               ETA: 01:23:15

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 52303 steps/s (collection: 1.769s, learning 0.110s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.3978
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 42.6104
                       Mean reward: 1.38
               Mean episode length: 245.98
    Episode_Reward/reaching_object: 0.2971
     Episode_Reward/lifting_object: -0.0351
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.88s
                      Time elapsed: 00:02:38
                               ETA: 01:22:50

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 51877 steps/s (collection: 1.803s, learning 0.092s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0443
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 42.6626
                       Mean reward: 1.46
               Mean episode length: 244.39
    Episode_Reward/reaching_object: 0.2974
     Episode_Reward/lifting_object: -0.0172
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 1.89s
                      Time elapsed: 00:02:40
                               ETA: 01:22:27

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 51768 steps/s (collection: 1.801s, learning 0.097s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.2659
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 42.8264
                       Mean reward: 0.77
               Mean episode length: 243.30
    Episode_Reward/reaching_object: 0.3011
     Episode_Reward/lifting_object: -0.0646
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.90s
                      Time elapsed: 00:02:42
                               ETA: 01:22:05

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 52051 steps/s (collection: 1.802s, learning 0.087s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.1164
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 42.8796
                       Mean reward: 1.20
               Mean episode length: 244.49
    Episode_Reward/reaching_object: 0.3120
     Episode_Reward/lifting_object: -0.0358
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.89s
                      Time elapsed: 00:02:44
                               ETA: 01:21:43

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 52053 steps/s (collection: 1.800s, learning 0.089s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.6393
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 42.9535
                       Mean reward: 1.12
               Mean episode length: 242.32
    Episode_Reward/reaching_object: 0.3042
     Episode_Reward/lifting_object: -0.0835
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.89s
                      Time elapsed: 00:02:46
                               ETA: 01:21:21

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 51055 steps/s (collection: 1.835s, learning 0.091s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.2371
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 43.0087
                       Mean reward: 1.60
               Mean episode length: 243.62
    Episode_Reward/reaching_object: 0.3233
     Episode_Reward/lifting_object: -0.0903
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.93s
                      Time elapsed: 00:02:48
                               ETA: 01:21:02

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 51633 steps/s (collection: 1.813s, learning 0.091s)
             Mean action noise std: 1.27
          Mean value_function loss: 5.4647
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 43.0911
                       Mean reward: -1.62
               Mean episode length: 236.79
    Episode_Reward/reaching_object: 0.3213
     Episode_Reward/lifting_object: -0.1404
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.90s
                      Time elapsed: 00:02:50
                               ETA: 01:20:42

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 51405 steps/s (collection: 1.814s, learning 0.099s)
             Mean action noise std: 1.27
          Mean value_function loss: 1.7231
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 43.1485
                       Mean reward: 0.66
               Mean episode length: 237.49
    Episode_Reward/reaching_object: 0.3215
     Episode_Reward/lifting_object: -0.3037
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.91s
                      Time elapsed: 00:02:52
                               ETA: 01:20:23

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 51485 steps/s (collection: 1.815s, learning 0.095s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.4793
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 43.2255
                       Mean reward: 1.41
               Mean episode length: 237.37
    Episode_Reward/reaching_object: 0.3447
     Episode_Reward/lifting_object: -0.0777
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.91s
                      Time elapsed: 00:02:54
                               ETA: 01:20:04

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 53165 steps/s (collection: 1.754s, learning 0.095s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0838
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 43.2846
                       Mean reward: 1.60
               Mean episode length: 238.39
    Episode_Reward/reaching_object: 0.3355
     Episode_Reward/lifting_object: -0.0554
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.85s
                      Time elapsed: 00:02:56
                               ETA: 01:19:44

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 52753 steps/s (collection: 1.775s, learning 0.089s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.1849
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 43.4191
                       Mean reward: 1.68
               Mean episode length: 240.79
    Episode_Reward/reaching_object: 0.3435
     Episode_Reward/lifting_object: -0.0413
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.86s
                      Time elapsed: 00:02:57
                               ETA: 01:19:25

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 52512 steps/s (collection: 1.786s, learning 0.086s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.1266
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 43.4599
                       Mean reward: 1.57
               Mean episode length: 240.33
    Episode_Reward/reaching_object: 0.3482
     Episode_Reward/lifting_object: -0.0441
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 1.87s
                      Time elapsed: 00:02:59
                               ETA: 01:19:07

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 53489 steps/s (collection: 1.748s, learning 0.090s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.3315
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 43.5206
                       Mean reward: 1.37
               Mean episode length: 241.91
    Episode_Reward/reaching_object: 0.3303
     Episode_Reward/lifting_object: -0.0378
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 1.84s
                      Time elapsed: 00:03:01
                               ETA: 01:18:48

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 51080 steps/s (collection: 1.832s, learning 0.092s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.5615
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 43.5859
                       Mean reward: 1.45
               Mean episode length: 241.64
    Episode_Reward/reaching_object: 0.3317
     Episode_Reward/lifting_object: -0.1342
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.92s
                      Time elapsed: 00:03:03
                               ETA: 01:18:32

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 52869 steps/s (collection: 1.769s, learning 0.091s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.5235
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 43.6423
                       Mean reward: 1.51
               Mean episode length: 240.22
    Episode_Reward/reaching_object: 0.3258
     Episode_Reward/lifting_object: -0.0558
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 1.86s
                      Time elapsed: 00:03:05
                               ETA: 01:18:15

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 51902 steps/s (collection: 1.764s, learning 0.130s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.4688
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 43.7044
                       Mean reward: 0.45
               Mean episode length: 242.54
    Episode_Reward/reaching_object: 0.3109
     Episode_Reward/lifting_object: -0.0767
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 1.89s
                      Time elapsed: 00:03:07
                               ETA: 01:17:59

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 53649 steps/s (collection: 1.726s, learning 0.107s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0371
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 43.7683
                       Mean reward: 1.14
               Mean episode length: 244.98
    Episode_Reward/reaching_object: 0.3020
     Episode_Reward/lifting_object: -0.0299
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 1.83s
                      Time elapsed: 00:03:09
                               ETA: 01:17:42

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 52565 steps/s (collection: 1.778s, learning 0.092s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0341
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 43.8947
                       Mean reward: 1.36
               Mean episode length: 245.19
    Episode_Reward/reaching_object: 0.3066
     Episode_Reward/lifting_object: -0.0191
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 1.87s
                      Time elapsed: 00:03:10
                               ETA: 01:17:26

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 52636 steps/s (collection: 1.770s, learning 0.098s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0064
                 Mean entropy loss: 43.9526
                       Mean reward: 1.43
               Mean episode length: 243.22
    Episode_Reward/reaching_object: 0.3049
     Episode_Reward/lifting_object: 0.0022
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 1.87s
                      Time elapsed: 00:03:12
                               ETA: 01:17:10

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 50353 steps/s (collection: 1.864s, learning 0.089s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.0577
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 44.0074
                       Mean reward: 1.43
               Mean episode length: 241.52
    Episode_Reward/reaching_object: 0.2975
     Episode_Reward/lifting_object: -0.0196
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 1.95s
                      Time elapsed: 00:03:14
                               ETA: 01:16:57

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 51609 steps/s (collection: 1.791s, learning 0.114s)
             Mean action noise std: 1.32
          Mean value_function loss: 1.0799
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 44.0705
                       Mean reward: 0.77
               Mean episode length: 240.07
    Episode_Reward/reaching_object: 0.2945
     Episode_Reward/lifting_object: -0.1082
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 1.90s
                      Time elapsed: 00:03:16
                               ETA: 01:16:43

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 51623 steps/s (collection: 1.819s, learning 0.086s)
             Mean action noise std: 1.32
          Mean value_function loss: 1.1590
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 44.1187
                       Mean reward: 1.26
               Mean episode length: 236.25
    Episode_Reward/reaching_object: 0.2964
     Episode_Reward/lifting_object: -0.2188
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 1.90s
                      Time elapsed: 00:03:18
                               ETA: 01:16:29

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 51067 steps/s (collection: 1.832s, learning 0.093s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.2238
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 44.1893
                       Mean reward: 1.47
               Mean episode length: 230.62
    Episode_Reward/reaching_object: 0.3027
     Episode_Reward/lifting_object: -0.0532
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 1.92s
                      Time elapsed: 00:03:20
                               ETA: 01:16:16

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 52288 steps/s (collection: 1.792s, learning 0.088s)
             Mean action noise std: 1.33
          Mean value_function loss: 1.1045
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 44.3275
                       Mean reward: 0.66
               Mean episode length: 230.67
    Episode_Reward/reaching_object: 0.3113
     Episode_Reward/lifting_object: -0.0915
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 1.88s
                      Time elapsed: 00:03:22
                               ETA: 01:16:02

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 52097 steps/s (collection: 1.801s, learning 0.086s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.1761
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 44.3700
                       Mean reward: 1.46
               Mean episode length: 224.43
    Episode_Reward/reaching_object: 0.3205
     Episode_Reward/lifting_object: -0.0414
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 1.89s
                      Time elapsed: 00:03:24
                               ETA: 01:15:49

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 51696 steps/s (collection: 1.816s, learning 0.085s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.2474
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 44.4605
                       Mean reward: 1.51
               Mean episode length: 226.61
    Episode_Reward/reaching_object: 0.3312
     Episode_Reward/lifting_object: -0.0323
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.6667
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 1.90s
                      Time elapsed: 00:03:26
                               ETA: 01:15:36

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 51294 steps/s (collection: 1.822s, learning 0.094s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.5519
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 44.4954
                       Mean reward: 1.36
               Mean episode length: 216.26
    Episode_Reward/reaching_object: 0.3331
     Episode_Reward/lifting_object: -0.0829
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.92s
                      Time elapsed: 00:03:28
                               ETA: 01:15:24

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 50560 steps/s (collection: 1.859s, learning 0.085s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.3258
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 44.5351
                       Mean reward: 0.59
               Mean episode length: 228.07
    Episode_Reward/reaching_object: 0.3449
     Episode_Reward/lifting_object: -0.0479
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.8750
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.94s
                      Time elapsed: 00:03:30
                               ETA: 01:15:12

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 51632 steps/s (collection: 1.813s, learning 0.091s)
             Mean action noise std: 1.35
          Mean value_function loss: 1.1620
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 44.6021
                       Mean reward: 1.61
               Mean episode length: 223.58
    Episode_Reward/reaching_object: 0.3530
     Episode_Reward/lifting_object: -0.0686
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 1.90s
                      Time elapsed: 00:03:31
                               ETA: 01:15:00

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 51094 steps/s (collection: 1.831s, learning 0.093s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.1686
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 44.6596
                       Mean reward: 1.28
               Mean episode length: 222.94
    Episode_Reward/reaching_object: 0.3566
     Episode_Reward/lifting_object: -0.0631
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 1.92s
                      Time elapsed: 00:03:33
                               ETA: 01:14:49

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 50681 steps/s (collection: 1.831s, learning 0.109s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.9316
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 44.7443
                       Mean reward: 1.53
               Mean episode length: 227.16
    Episode_Reward/reaching_object: 0.3459
     Episode_Reward/lifting_object: -0.0839
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 1.94s
                      Time elapsed: 00:03:35
                               ETA: 01:14:38

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 51123 steps/s (collection: 1.812s, learning 0.111s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.5762
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 44.7907
                       Mean reward: 0.79
               Mean episode length: 220.66
    Episode_Reward/reaching_object: 0.3382
     Episode_Reward/lifting_object: -0.1093
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 1.92s
                      Time elapsed: 00:03:37
                               ETA: 01:14:27

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 51002 steps/s (collection: 1.811s, learning 0.116s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0261
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 44.8409
                       Mean reward: 1.43
               Mean episode length: 225.58
    Episode_Reward/reaching_object: 0.3181
     Episode_Reward/lifting_object: -0.0148
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 1.93s
                      Time elapsed: 00:03:39
                               ETA: 01:14:16

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 49916 steps/s (collection: 1.879s, learning 0.091s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.6350
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 44.9552
                       Mean reward: 1.64
               Mean episode length: 222.57
    Episode_Reward/reaching_object: 0.3424
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 1.97s
                      Time elapsed: 00:03:41
                               ETA: 01:14:06

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 53013 steps/s (collection: 1.768s, learning 0.086s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0284
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 44.9887
                       Mean reward: 1.65
               Mean episode length: 228.04
    Episode_Reward/reaching_object: 0.3466
     Episode_Reward/lifting_object: -0.0517
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 1.85s
                      Time elapsed: 00:03:43
                               ETA: 01:13:54

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 53033 steps/s (collection: 1.765s, learning 0.088s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.1085
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 45.0766
                       Mean reward: 1.71
               Mean episode length: 226.48
    Episode_Reward/reaching_object: 0.3463
     Episode_Reward/lifting_object: -0.0256
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 1.85s
                      Time elapsed: 00:03:45
                               ETA: 01:13:43

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 52840 steps/s (collection: 1.759s, learning 0.102s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0341
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 45.1125
                       Mean reward: 1.59
               Mean episode length: 227.29
    Episode_Reward/reaching_object: 0.3395
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.86s
                      Time elapsed: 00:03:47
                               ETA: 01:13:32

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 52070 steps/s (collection: 1.803s, learning 0.085s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.1364
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 45.2059
                       Mean reward: 1.52
               Mean episode length: 220.63
    Episode_Reward/reaching_object: 0.3309
     Episode_Reward/lifting_object: -0.0683
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 1.89s
                      Time elapsed: 00:03:49
                               ETA: 01:13:21

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 52210 steps/s (collection: 1.791s, learning 0.092s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.0858
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 45.2435
                       Mean reward: 1.40
               Mean episode length: 218.83
    Episode_Reward/reaching_object: 0.3425
     Episode_Reward/lifting_object: -0.0232
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.88s
                      Time elapsed: 00:03:50
                               ETA: 01:13:10

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 51203 steps/s (collection: 1.831s, learning 0.089s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.1414
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 45.3488
                       Mean reward: 1.54
               Mean episode length: 225.06
    Episode_Reward/reaching_object: 0.3289
     Episode_Reward/lifting_object: 0.0199
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.9167
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.92s
                      Time elapsed: 00:03:52
                               ETA: 01:13:01

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 50364 steps/s (collection: 1.862s, learning 0.090s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0633
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 45.3852
                       Mean reward: 1.25
               Mean episode length: 209.08
    Episode_Reward/reaching_object: 0.3243
     Episode_Reward/lifting_object: -0.0215
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 1.95s
                      Time elapsed: 00:03:54
                               ETA: 01:12:52

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 49656 steps/s (collection: 1.892s, learning 0.088s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.2348
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 45.4649
                       Mean reward: 1.32
               Mean episode length: 197.95
    Episode_Reward/reaching_object: 0.3114
     Episode_Reward/lifting_object: -0.0477
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 20.1667
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 1.98s
                      Time elapsed: 00:03:56
                               ETA: 01:12:44

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 50954 steps/s (collection: 1.838s, learning 0.092s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0781
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 45.4956
                       Mean reward: 1.50
               Mean episode length: 194.02
    Episode_Reward/reaching_object: 0.3135
     Episode_Reward/lifting_object: -0.0105
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 20.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 1.93s
                      Time elapsed: 00:03:58
                               ETA: 01:12:35

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 50950 steps/s (collection: 1.845s, learning 0.084s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.8382
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 45.5746
                       Mean reward: 1.50
               Mean episode length: 188.74
    Episode_Reward/reaching_object: 0.3150
     Episode_Reward/lifting_object: -0.0723
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 21.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 1.93s
                      Time elapsed: 00:04:00
                               ETA: 01:12:26

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 51254 steps/s (collection: 1.824s, learning 0.094s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0776
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 45.6075
                       Mean reward: 1.47
               Mean episode length: 183.73
    Episode_Reward/reaching_object: 0.3107
     Episode_Reward/lifting_object: -0.0503
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 20.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 1.92s
                      Time elapsed: 00:04:02
                               ETA: 01:12:17

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 51538 steps/s (collection: 1.805s, learning 0.102s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0950
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 45.7128
                       Mean reward: 1.39
               Mean episode length: 183.13
    Episode_Reward/reaching_object: 0.3241
     Episode_Reward/lifting_object: -0.0140
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 1.91s
                      Time elapsed: 00:04:04
                               ETA: 01:12:08

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 49716 steps/s (collection: 1.877s, learning 0.101s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.0704
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 45.7850
                       Mean reward: 1.39
               Mean episode length: 180.08
    Episode_Reward/reaching_object: 0.3320
     Episode_Reward/lifting_object: -0.0220
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 1.98s
                      Time elapsed: 00:04:06
                               ETA: 01:12:00

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 49811 steps/s (collection: 1.863s, learning 0.111s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.7682
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 45.8956
                       Mean reward: 0.45
               Mean episode length: 180.97
    Episode_Reward/reaching_object: 0.3386
     Episode_Reward/lifting_object: -0.1232
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 19.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 1.97s
                      Time elapsed: 00:04:08
                               ETA: 01:11:52

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 50170 steps/s (collection: 1.854s, learning 0.106s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.1226
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 45.9298
                       Mean reward: 1.69
               Mean episode length: 180.18
    Episode_Reward/reaching_object: 0.3547
     Episode_Reward/lifting_object: -0.0335
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 1.96s
                      Time elapsed: 00:04:10
                               ETA: 01:11:45

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 50051 steps/s (collection: 1.869s, learning 0.095s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.3846
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 46.0246
                       Mean reward: 1.87
               Mean episode length: 188.34
    Episode_Reward/reaching_object: 0.3704
     Episode_Reward/lifting_object: -0.0447
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 1.96s
                      Time elapsed: 00:04:12
                               ETA: 01:11:37

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 50684 steps/s (collection: 1.849s, learning 0.091s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.0416
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 46.0588
                       Mean reward: 1.94
               Mean episode length: 198.53
    Episode_Reward/reaching_object: 0.4087
     Episode_Reward/lifting_object: -0.0306
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 1.94s
                      Time elapsed: 00:04:14
                               ETA: 01:11:29

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 49413 steps/s (collection: 1.897s, learning 0.092s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.2220
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 46.1464
                       Mean reward: 1.53
               Mean episode length: 208.61
    Episode_Reward/reaching_object: 0.4140
     Episode_Reward/lifting_object: -0.0278
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 1.99s
                      Time elapsed: 00:04:16
                               ETA: 01:11:22

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 50937 steps/s (collection: 1.825s, learning 0.105s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.0581
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 46.2174
                       Mean reward: 2.10
               Mean episode length: 222.70
    Episode_Reward/reaching_object: 0.4314
     Episode_Reward/lifting_object: -0.0177
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 1.93s
                      Time elapsed: 00:04:18
                               ETA: 01:11:14

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 51483 steps/s (collection: 1.819s, learning 0.090s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.0598
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 46.3264
                       Mean reward: 1.97
               Mean episode length: 218.57
    Episode_Reward/reaching_object: 0.4334
     Episode_Reward/lifting_object: -0.0250
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 1.91s
                      Time elapsed: 00:04:20
                               ETA: 01:11:06

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 51378 steps/s (collection: 1.818s, learning 0.096s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.2273
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 46.3880
                       Mean reward: 2.03
               Mean episode length: 214.59
    Episode_Reward/reaching_object: 0.4252
     Episode_Reward/lifting_object: -0.0405
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 1.91s
                      Time elapsed: 00:04:22
                               ETA: 01:10:58

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 51027 steps/s (collection: 1.838s, learning 0.089s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.4252
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 46.4088
                       Mean reward: 1.64
               Mean episode length: 223.41
    Episode_Reward/reaching_object: 0.4394
     Episode_Reward/lifting_object: -0.0224
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 1.93s
                      Time elapsed: 00:04:23
                               ETA: 01:10:51

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 51105 steps/s (collection: 1.836s, learning 0.087s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.3230
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 46.4368
                       Mean reward: 2.01
               Mean episode length: 224.58
    Episode_Reward/reaching_object: 0.4308
     Episode_Reward/lifting_object: -0.1128
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 1.92s
                      Time elapsed: 00:04:25
                               ETA: 01:10:43

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 51727 steps/s (collection: 1.811s, learning 0.090s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.4573
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 46.5179
                       Mean reward: 1.66
               Mean episode length: 219.75
    Episode_Reward/reaching_object: 0.4453
     Episode_Reward/lifting_object: -0.0725
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 1.90s
                      Time elapsed: 00:04:27
                               ETA: 01:10:35

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 51147 steps/s (collection: 1.828s, learning 0.094s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.4219
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.5541
                       Mean reward: 2.07
               Mean episode length: 218.23
    Episode_Reward/reaching_object: 0.4326
     Episode_Reward/lifting_object: -0.0240
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 1.92s
                      Time elapsed: 00:04:29
                               ETA: 01:10:28

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 51875 steps/s (collection: 1.806s, learning 0.089s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.0578
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 46.5928
                       Mean reward: 1.90
               Mean episode length: 209.95
    Episode_Reward/reaching_object: 0.4174
     Episode_Reward/lifting_object: -0.0531
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 1.90s
                      Time elapsed: 00:04:31
                               ETA: 01:10:20

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 50845 steps/s (collection: 1.834s, learning 0.099s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.0532
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 46.7120
                       Mean reward: 2.02
               Mean episode length: 211.84
    Episode_Reward/reaching_object: 0.4095
     Episode_Reward/lifting_object: -0.0139
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 1.93s
                      Time elapsed: 00:04:33
                               ETA: 01:10:13

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 49841 steps/s (collection: 1.861s, learning 0.111s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.1708
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 46.8310
                       Mean reward: 1.51
               Mean episode length: 215.59
    Episode_Reward/reaching_object: 0.4164
     Episode_Reward/lifting_object: -0.0104
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 9.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 1.97s
                      Time elapsed: 00:04:35
                               ETA: 01:10:07

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 50008 steps/s (collection: 1.854s, learning 0.112s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.3252
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.8543
                       Mean reward: 1.43
               Mean episode length: 211.75
    Episode_Reward/reaching_object: 0.4344
     Episode_Reward/lifting_object: -0.0329
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 1.97s
                      Time elapsed: 00:04:37
                               ETA: 01:10:00

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 50379 steps/s (collection: 1.855s, learning 0.096s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.0579
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 46.9459
                       Mean reward: 1.75
               Mean episode length: 221.88
    Episode_Reward/reaching_object: 0.4465
     Episode_Reward/lifting_object: -0.0277
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 1.95s
                      Time elapsed: 00:04:39
                               ETA: 01:09:54

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 50620 steps/s (collection: 1.851s, learning 0.091s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.0460
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 47.0781
                       Mean reward: 2.11
               Mean episode length: 223.05
    Episode_Reward/reaching_object: 0.4613
     Episode_Reward/lifting_object: -0.0381
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 1.94s
                      Time elapsed: 00:04:41
                               ETA: 01:09:47

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 50732 steps/s (collection: 1.853s, learning 0.085s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.7612
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 47.1610
                       Mean reward: 1.32
               Mean episode length: 220.88
    Episode_Reward/reaching_object: 0.4859
     Episode_Reward/lifting_object: -0.1029
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 1.94s
                      Time elapsed: 00:04:43
                               ETA: 01:09:40

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 50959 steps/s (collection: 1.842s, learning 0.087s)
             Mean action noise std: 1.49
          Mean value_function loss: 1.2404
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 47.1793
                       Mean reward: 2.12
               Mean episode length: 225.86
    Episode_Reward/reaching_object: 0.5173
     Episode_Reward/lifting_object: -0.1229
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 1.93s
                      Time elapsed: 00:04:45
                               ETA: 01:09:34

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 50695 steps/s (collection: 1.838s, learning 0.101s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.2050
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.2033
                       Mean reward: 2.16
               Mean episode length: 232.95
    Episode_Reward/reaching_object: 0.5557
     Episode_Reward/lifting_object: -0.0761
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 1.94s
                      Time elapsed: 00:04:47
                               ETA: 01:09:27

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 50607 steps/s (collection: 1.849s, learning 0.093s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.3640
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.2612
                       Mean reward: 2.38
               Mean episode length: 231.18
    Episode_Reward/reaching_object: 0.5573
     Episode_Reward/lifting_object: -0.0242
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 1.94s
                      Time elapsed: 00:04:49
                               ETA: 01:09:21

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 50438 steps/s (collection: 1.848s, learning 0.101s)
             Mean action noise std: 1.49
          Mean value_function loss: 1.5629
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 47.2851
                       Mean reward: 0.05
               Mean episode length: 225.71
    Episode_Reward/reaching_object: 0.5730
     Episode_Reward/lifting_object: -0.1061
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 1.95s
                      Time elapsed: 00:04:51
                               ETA: 01:09:15

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 50946 steps/s (collection: 1.819s, learning 0.111s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.0969
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 47.3110
                       Mean reward: 2.50
               Mean episode length: 234.54
    Episode_Reward/reaching_object: 0.5867
     Episode_Reward/lifting_object: -0.0625
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 1.93s
                      Time elapsed: 00:04:53
                               ETA: 01:09:09

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 49090 steps/s (collection: 1.889s, learning 0.113s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.0275
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 47.3825
                       Mean reward: 3.26
               Mean episode length: 229.83
    Episode_Reward/reaching_object: 0.6018
     Episode_Reward/lifting_object: 0.0021
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 2.00s
                      Time elapsed: 00:04:55
                               ETA: 01:09:03

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 49322 steps/s (collection: 1.899s, learning 0.094s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.1308
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.4690
                       Mean reward: 2.60
               Mean episode length: 228.27
    Episode_Reward/reaching_object: 0.5980
     Episode_Reward/lifting_object: -0.0476
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 1.99s
                      Time elapsed: 00:04:57
                               ETA: 01:08:58

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 50554 steps/s (collection: 1.852s, learning 0.092s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.2503
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.5377
                       Mean reward: 3.00
               Mean episode length: 234.46
    Episode_Reward/reaching_object: 0.6307
     Episode_Reward/lifting_object: -0.0374
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 1.94s
                      Time elapsed: 00:04:58
                               ETA: 01:08:52

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 50145 steps/s (collection: 1.872s, learning 0.089s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.1804
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.5888
                       Mean reward: 2.92
               Mean episode length: 225.63
    Episode_Reward/reaching_object: 0.6508
     Episode_Reward/lifting_object: -0.0124
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 1.96s
                      Time elapsed: 00:05:00
                               ETA: 01:08:46

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 50535 steps/s (collection: 1.853s, learning 0.093s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.4236
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 47.6673
                       Mean reward: 2.85
               Mean episode length: 228.18
    Episode_Reward/reaching_object: 0.6661
     Episode_Reward/lifting_object: -0.0271
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 1.95s
                      Time elapsed: 00:05:02
                               ETA: 01:08:40

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 49757 steps/s (collection: 1.885s, learning 0.091s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.0900
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 47.6878
                       Mean reward: 3.03
               Mean episode length: 233.96
    Episode_Reward/reaching_object: 0.6572
     Episode_Reward/lifting_object: -0.0072
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 1.98s
                      Time elapsed: 00:05:04
                               ETA: 01:08:35

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 50449 steps/s (collection: 1.860s, learning 0.089s)
             Mean action noise std: 1.52
          Mean value_function loss: 1.6289
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.7274
                       Mean reward: 2.47
               Mean episode length: 231.30
    Episode_Reward/reaching_object: 0.6854
     Episode_Reward/lifting_object: -0.1211
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 1.95s
                      Time elapsed: 00:05:06
                               ETA: 01:08:29

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 50178 steps/s (collection: 1.847s, learning 0.112s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.1252
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.7488
                       Mean reward: 3.31
               Mean episode length: 234.37
    Episode_Reward/reaching_object: 0.7031
     Episode_Reward/lifting_object: -0.0961
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 1.96s
                      Time elapsed: 00:05:08
                               ETA: 01:08:24

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 50037 steps/s (collection: 1.880s, learning 0.085s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.4226
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.7968
                       Mean reward: 3.61
               Mean episode length: 237.05
    Episode_Reward/reaching_object: 0.7291
     Episode_Reward/lifting_object: -0.0249
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 1.96s
                      Time elapsed: 00:05:10
                               ETA: 01:08:18

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 48809 steps/s (collection: 1.924s, learning 0.090s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.3577
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 47.8214
                       Mean reward: 3.31
               Mean episode length: 235.42
    Episode_Reward/reaching_object: 0.7381
     Episode_Reward/lifting_object: -0.0100
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 2.01s
                      Time elapsed: 00:05:12
                               ETA: 01:08:14

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 49379 steps/s (collection: 1.903s, learning 0.088s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.4069
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 47.8720
                       Mean reward: 2.95
               Mean episode length: 233.69
    Episode_Reward/reaching_object: 0.7520
     Episode_Reward/lifting_object: -0.0960
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 1.99s
                      Time elapsed: 00:05:14
                               ETA: 01:08:09

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 48524 steps/s (collection: 1.927s, learning 0.099s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.3233
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.8972
                       Mean reward: 3.85
               Mean episode length: 233.36
    Episode_Reward/reaching_object: 0.7571
     Episode_Reward/lifting_object: -0.0663
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 2.03s
                      Time elapsed: 00:05:16
                               ETA: 01:08:04

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 49258 steps/s (collection: 1.894s, learning 0.102s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.3132
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 47.9534
                       Mean reward: 3.76
               Mean episode length: 238.62
    Episode_Reward/reaching_object: 0.7861
     Episode_Reward/lifting_object: -0.0768
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 2.00s
                      Time elapsed: 00:05:18
                               ETA: 01:08:00

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 49802 steps/s (collection: 1.850s, learning 0.124s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.5256
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 47.9765
                       Mean reward: 3.22
               Mean episode length: 232.28
    Episode_Reward/reaching_object: 0.7800
     Episode_Reward/lifting_object: -0.0550
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 1.97s
                      Time elapsed: 00:05:20
                               ETA: 01:07:54

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 49571 steps/s (collection: 1.894s, learning 0.089s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.4729
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.0166
                       Mean reward: 3.51
               Mean episode length: 237.19
    Episode_Reward/reaching_object: 0.8143
     Episode_Reward/lifting_object: -0.1337
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 1.98s
                      Time elapsed: 00:05:22
                               ETA: 01:07:50

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 49899 steps/s (collection: 1.877s, learning 0.093s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.1875
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 48.0916
                       Mean reward: 2.98
               Mean episode length: 229.63
    Episode_Reward/reaching_object: 0.7908
     Episode_Reward/lifting_object: -0.0037
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 1.97s
                      Time elapsed: 00:05:24
                               ETA: 01:07:45

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 49286 steps/s (collection: 1.894s, learning 0.101s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.3893
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.1464
                       Mean reward: 3.62
               Mean episode length: 229.96
    Episode_Reward/reaching_object: 0.8156
     Episode_Reward/lifting_object: -0.0587
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 1.99s
                      Time elapsed: 00:05:26
                               ETA: 01:07:40

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 49676 steps/s (collection: 1.875s, learning 0.104s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.6820
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.1821
                       Mean reward: 3.43
               Mean episode length: 233.09
    Episode_Reward/reaching_object: 0.8118
     Episode_Reward/lifting_object: -0.1203
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 1.98s
                      Time elapsed: 00:05:28
                               ETA: 01:07:35

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 50684 steps/s (collection: 1.845s, learning 0.095s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.9407
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.2113
                       Mean reward: 3.84
               Mean episode length: 224.88
    Episode_Reward/reaching_object: 0.8131
     Episode_Reward/lifting_object: -0.1420
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 1.94s
                      Time elapsed: 00:05:30
                               ETA: 01:07:30

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 47859 steps/s (collection: 1.966s, learning 0.088s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.2835
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 48.2707
                       Mean reward: 4.35
               Mean episode length: 240.48
    Episode_Reward/reaching_object: 0.8344
     Episode_Reward/lifting_object: 0.0242
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 2.05s
                      Time elapsed: 00:05:32
                               ETA: 01:07:26

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 51123 steps/s (collection: 1.838s, learning 0.085s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.1788
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.3211
                       Mean reward: 4.23
               Mean episode length: 237.50
    Episode_Reward/reaching_object: 0.8364
     Episode_Reward/lifting_object: -0.0648
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 1.92s
                      Time elapsed: 00:05:34
                               ETA: 01:07:21

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 50583 steps/s (collection: 1.845s, learning 0.098s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.0693
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 48.4006
                       Mean reward: 3.58
               Mean episode length: 235.71
    Episode_Reward/reaching_object: 0.8377
     Episode_Reward/lifting_object: -0.0288
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 1.94s
                      Time elapsed: 00:05:36
                               ETA: 01:07:15

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 50268 steps/s (collection: 1.844s, learning 0.111s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.3027
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.4886
                       Mean reward: 3.63
               Mean episode length: 236.37
    Episode_Reward/reaching_object: 0.8168
     Episode_Reward/lifting_object: -0.0841
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 1.96s
                      Time elapsed: 00:05:38
                               ETA: 01:07:11

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 49561 steps/s (collection: 1.877s, learning 0.107s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.4495
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.5223
                       Mean reward: 4.13
               Mean episode length: 226.33
    Episode_Reward/reaching_object: 0.7975
     Episode_Reward/lifting_object: -0.0565
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 1.98s
                      Time elapsed: 00:05:40
                               ETA: 01:07:06

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 49953 steps/s (collection: 1.883s, learning 0.085s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.3976
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.5451
                       Mean reward: 3.63
               Mean episode length: 222.12
    Episode_Reward/reaching_object: 0.8175
     Episode_Reward/lifting_object: -0.0476
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 1.97s
                      Time elapsed: 00:05:42
                               ETA: 01:07:01

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 49725 steps/s (collection: 1.888s, learning 0.089s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.2198
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 48.5967
                       Mean reward: 4.53
               Mean episode length: 235.72
    Episode_Reward/reaching_object: 0.8159
     Episode_Reward/lifting_object: 0.0242
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 1.98s
                      Time elapsed: 00:05:44
                               ETA: 01:06:57

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 47103 steps/s (collection: 1.954s, learning 0.133s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.2621
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 48.6757
                       Mean reward: 3.85
               Mean episode length: 237.41
    Episode_Reward/reaching_object: 0.8438
     Episode_Reward/lifting_object: -0.0325
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 2.09s
                      Time elapsed: 00:05:46
                               ETA: 01:06:53

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 47011 steps/s (collection: 2.003s, learning 0.089s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.1908
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.7189
                       Mean reward: 4.35
               Mean episode length: 234.03
    Episode_Reward/reaching_object: 0.8363
     Episode_Reward/lifting_object: 0.0052
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 2.09s
                      Time elapsed: 00:05:48
                               ETA: 01:06:50

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 48367 steps/s (collection: 1.944s, learning 0.089s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.1684
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.7654
                       Mean reward: 4.19
               Mean episode length: 246.10
    Episode_Reward/reaching_object: 0.8517
     Episode_Reward/lifting_object: 0.0135
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 2.03s
                      Time elapsed: 00:05:50
                               ETA: 01:06:46

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 46840 steps/s (collection: 1.984s, learning 0.115s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.4556
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.8043
                       Mean reward: 4.44
               Mean episode length: 237.83
    Episode_Reward/reaching_object: 0.8889
     Episode_Reward/lifting_object: -0.0060
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 2.10s
                      Time elapsed: 00:05:52
                               ETA: 01:06:43

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 47900 steps/s (collection: 1.937s, learning 0.116s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.3147
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 48.8440
                       Mean reward: 4.54
               Mean episode length: 234.42
    Episode_Reward/reaching_object: 0.8405
     Episode_Reward/lifting_object: 0.0467
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 2.05s
                      Time elapsed: 00:05:54
                               ETA: 01:06:40

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 45867 steps/s (collection: 1.968s, learning 0.175s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.8347
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.8954
                       Mean reward: 3.33
               Mean episode length: 235.35
    Episode_Reward/reaching_object: 0.8639
     Episode_Reward/lifting_object: -0.1324
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 2.14s
                      Time elapsed: 00:05:56
                               ETA: 01:06:37

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 46423 steps/s (collection: 2.011s, learning 0.107s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.3919
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 48.9144
                       Mean reward: 4.02
               Mean episode length: 241.64
    Episode_Reward/reaching_object: 0.9091
     Episode_Reward/lifting_object: -0.0316
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 2.12s
                      Time elapsed: 00:05:59
                               ETA: 01:06:34

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 48549 steps/s (collection: 1.937s, learning 0.088s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.4730
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 48.9731
                       Mean reward: 4.31
               Mean episode length: 242.82
    Episode_Reward/reaching_object: 0.9191
     Episode_Reward/lifting_object: -0.0696
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 2.02s
                      Time elapsed: 00:06:01
                               ETA: 01:06:31

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 48308 steps/s (collection: 1.946s, learning 0.089s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.3356
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 49.0231
                       Mean reward: 4.10
               Mean episode length: 232.74
    Episode_Reward/reaching_object: 0.9024
     Episode_Reward/lifting_object: -0.0277
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 2.03s
                      Time elapsed: 00:06:03
                               ETA: 01:06:27

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 48467 steps/s (collection: 1.934s, learning 0.094s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.4419
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 49.1120
                       Mean reward: 2.78
               Mean episode length: 223.91
    Episode_Reward/reaching_object: 0.8513
     Episode_Reward/lifting_object: -0.1036
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 2.03s
                      Time elapsed: 00:06:05
                               ETA: 01:06:23

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 44941 steps/s (collection: 2.072s, learning 0.115s)
             Mean action noise std: 1.61
          Mean value_function loss: 2.1008
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 49.1708
                       Mean reward: 3.70
               Mean episode length: 230.26
    Episode_Reward/reaching_object: 0.8578
     Episode_Reward/lifting_object: -0.0260
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 2.19s
                      Time elapsed: 00:06:07
                               ETA: 01:06:21

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 46346 steps/s (collection: 2.018s, learning 0.103s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.2104
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 49.1900
                       Mean reward: 4.65
               Mean episode length: 238.14
    Episode_Reward/reaching_object: 0.9332
     Episode_Reward/lifting_object: -0.0010
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 2.12s
                      Time elapsed: 00:06:09
                               ETA: 01:06:18

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 46856 steps/s (collection: 2.004s, learning 0.094s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.1907
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 49.2486
                       Mean reward: 4.54
               Mean episode length: 238.93
    Episode_Reward/reaching_object: 0.8883
     Episode_Reward/lifting_object: -0.0060
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 2.10s
                      Time elapsed: 00:06:11
                               ETA: 01:06:15

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 45020 steps/s (collection: 2.089s, learning 0.095s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.2435
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 49.3064
                       Mean reward: 5.11
               Mean episode length: 234.75
    Episode_Reward/reaching_object: 0.8987
     Episode_Reward/lifting_object: 0.0242
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 2.18s
                      Time elapsed: 00:06:13
                               ETA: 01:06:13

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 47512 steps/s (collection: 1.944s, learning 0.125s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.3481
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.3363
                       Mean reward: 4.71
               Mean episode length: 232.30
    Episode_Reward/reaching_object: 0.8928
     Episode_Reward/lifting_object: -0.0036
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 2.07s
                      Time elapsed: 00:06:15
                               ETA: 01:06:10

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 45064 steps/s (collection: 2.013s, learning 0.169s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.3404
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 49.3680
                       Mean reward: 3.22
               Mean episode length: 235.43
    Episode_Reward/reaching_object: 0.9018
     Episode_Reward/lifting_object: -0.0711
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 2.18s
                      Time elapsed: 00:06:17
                               ETA: 01:06:08

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 48241 steps/s (collection: 1.896s, learning 0.142s)
             Mean action noise std: 1.62
          Mean value_function loss: 1.1717
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.4100
                       Mean reward: 4.07
               Mean episode length: 238.87
    Episode_Reward/reaching_object: 0.9121
     Episode_Reward/lifting_object: -0.0941
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 2.04s
                      Time elapsed: 00:06:19
                               ETA: 01:06:04

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 49258 steps/s (collection: 1.869s, learning 0.127s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.3891
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 49.4250
                       Mean reward: 4.45
               Mean episode length: 245.61
    Episode_Reward/reaching_object: 0.8804
     Episode_Reward/lifting_object: -0.0065
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 2.00s
                      Time elapsed: 00:06:21
                               ETA: 01:06:00

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 48399 steps/s (collection: 1.871s, learning 0.160s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.1356
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 49.4694
                       Mean reward: 3.72
               Mean episode length: 237.06
    Episode_Reward/reaching_object: 0.8917
     Episode_Reward/lifting_object: -0.0586
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 2.03s
                      Time elapsed: 00:06:24
                               ETA: 01:05:57

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 50412 steps/s (collection: 1.853s, learning 0.097s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.1280
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 49.5429
                       Mean reward: 3.81
               Mean episode length: 240.45
    Episode_Reward/reaching_object: 0.9095
     Episode_Reward/lifting_object: 0.0268
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 1.95s
                      Time elapsed: 00:06:25
                               ETA: 01:05:52

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 44402 steps/s (collection: 2.058s, learning 0.156s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.5246
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 49.5860
                       Mean reward: 5.37
               Mean episode length: 239.91
    Episode_Reward/reaching_object: 0.9385
     Episode_Reward/lifting_object: 0.0339
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 2.21s
                      Time elapsed: 00:06:28
                               ETA: 01:05:51

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 47944 steps/s (collection: 1.936s, learning 0.115s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.1080
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 49.6431
                       Mean reward: 4.61
               Mean episode length: 234.54
    Episode_Reward/reaching_object: 0.9133
     Episode_Reward/lifting_object: -0.0379
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 2.05s
                      Time elapsed: 00:06:30
                               ETA: 01:05:47

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 47027 steps/s (collection: 1.960s, learning 0.131s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.5099
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 49.7064
                       Mean reward: 4.61
               Mean episode length: 236.62
    Episode_Reward/reaching_object: 0.9203
     Episode_Reward/lifting_object: -0.0188
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 2.09s
                      Time elapsed: 00:06:32
                               ETA: 01:05:44

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 45401 steps/s (collection: 2.079s, learning 0.087s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.2103
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 49.7620
                       Mean reward: 3.74
               Mean episode length: 234.02
    Episode_Reward/reaching_object: 0.9306
     Episode_Reward/lifting_object: -0.0690
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 2.17s
                      Time elapsed: 00:06:34
                               ETA: 01:05:42

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 48351 steps/s (collection: 1.942s, learning 0.092s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.2771
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 49.8117
                       Mean reward: 4.04
               Mean episode length: 238.65
    Episode_Reward/reaching_object: 0.9098
     Episode_Reward/lifting_object: -0.0272
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 2.03s
                      Time elapsed: 00:06:36
                               ETA: 01:05:39

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 44916 steps/s (collection: 2.098s, learning 0.091s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.4482
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 49.8698
                       Mean reward: 3.56
               Mean episode length: 235.19
    Episode_Reward/reaching_object: 0.9095
     Episode_Reward/lifting_object: -0.1158
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 2.19s
                      Time elapsed: 00:06:38
                               ETA: 01:05:37

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 48967 steps/s (collection: 1.920s, learning 0.088s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.4351
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 49.9016
                       Mean reward: 4.51
               Mean episode length: 234.73
    Episode_Reward/reaching_object: 0.9258
     Episode_Reward/lifting_object: -0.0587
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 2.01s
                      Time elapsed: 00:06:40
                               ETA: 01:05:33

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 45193 steps/s (collection: 2.036s, learning 0.139s)
             Mean action noise std: 1.65
          Mean value_function loss: 1.2498
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 49.9507
                       Mean reward: 4.38
               Mean episode length: 234.84
    Episode_Reward/reaching_object: 0.9415
     Episode_Reward/lifting_object: -0.0172
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 2.18s
                      Time elapsed: 00:06:42
                               ETA: 01:05:31

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 48443 steps/s (collection: 1.937s, learning 0.092s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.4716
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.9677
                       Mean reward: 4.63
               Mean episode length: 226.63
    Episode_Reward/reaching_object: 0.9439
     Episode_Reward/lifting_object: 0.0278
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 2.03s
                      Time elapsed: 00:06:44
                               ETA: 01:05:27

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 50631 steps/s (collection: 1.852s, learning 0.089s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.2744
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 50.0069
                       Mean reward: 4.04
               Mean episode length: 241.96
    Episode_Reward/reaching_object: 0.9464
     Episode_Reward/lifting_object: -0.0849
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 1.94s
                      Time elapsed: 00:06:46
                               ETA: 01:05:23

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 48927 steps/s (collection: 1.905s, learning 0.105s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.3942
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 50.0671
                       Mean reward: 4.74
               Mean episode length: 236.72
    Episode_Reward/reaching_object: 0.9476
     Episode_Reward/lifting_object: 0.0087
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 2.01s
                      Time elapsed: 00:06:48
                               ETA: 01:05:19

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 49912 steps/s (collection: 1.864s, learning 0.105s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.1855
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.1213
                       Mean reward: 4.41
               Mean episode length: 240.38
    Episode_Reward/reaching_object: 0.9909
     Episode_Reward/lifting_object: -0.0222
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 1.97s
                      Time elapsed: 00:06:50
                               ETA: 01:05:15

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 49700 steps/s (collection: 1.891s, learning 0.087s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.7804
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 50.1815
                       Mean reward: 5.13
               Mean episode length: 235.88
    Episode_Reward/reaching_object: 0.9720
     Episode_Reward/lifting_object: 0.0532
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 1.98s
                      Time elapsed: 00:06:52
                               ETA: 01:05:11

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 49565 steps/s (collection: 1.897s, learning 0.086s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.3821
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.2003
                       Mean reward: 5.00
               Mean episode length: 234.96
    Episode_Reward/reaching_object: 0.9388
     Episode_Reward/lifting_object: 0.0730
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 1.98s
                      Time elapsed: 00:06:54
                               ETA: 01:05:08

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 49477 steps/s (collection: 1.894s, learning 0.093s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.3207
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 50.2526
                       Mean reward: 4.18
               Mean episode length: 236.96
    Episode_Reward/reaching_object: 0.9191
     Episode_Reward/lifting_object: 0.0135
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 1.99s
                      Time elapsed: 00:06:56
                               ETA: 01:05:04

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 48152 steps/s (collection: 1.953s, learning 0.089s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.6802
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 50.3034
                       Mean reward: 4.35
               Mean episode length: 232.13
    Episode_Reward/reaching_object: 0.9318
     Episode_Reward/lifting_object: 0.0667
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 2.04s
                      Time elapsed: 00:06:58
                               ETA: 01:05:01

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 46284 steps/s (collection: 1.988s, learning 0.136s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.2916
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 50.3486
                       Mean reward: 4.87
               Mean episode length: 234.20
    Episode_Reward/reaching_object: 0.9061
     Episode_Reward/lifting_object: 0.0470
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 2.12s
                      Time elapsed: 00:07:00
                               ETA: 01:04:58

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 46965 steps/s (collection: 1.973s, learning 0.121s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.2132
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.4182
                       Mean reward: 4.97
               Mean episode length: 224.04
    Episode_Reward/reaching_object: 0.8838
     Episode_Reward/lifting_object: 0.0579
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 2.09s
                      Time elapsed: 00:07:03
                               ETA: 01:04:55

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 46922 steps/s (collection: 1.948s, learning 0.147s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.8334
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.4489
                       Mean reward: 3.81
               Mean episode length: 219.98
    Episode_Reward/reaching_object: 0.8691
     Episode_Reward/lifting_object: 0.0656
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 2.10s
                      Time elapsed: 00:07:05
                               ETA: 01:04:53

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 47167 steps/s (collection: 1.972s, learning 0.113s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.2643
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.4651
                       Mean reward: 4.00
               Mean episode length: 222.77
    Episode_Reward/reaching_object: 0.8708
     Episode_Reward/lifting_object: -0.0632
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 2.08s
                      Time elapsed: 00:07:07
                               ETA: 01:04:50

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 49150 steps/s (collection: 1.912s, learning 0.088s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.2412
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 50.5113
                       Mean reward: 4.66
               Mean episode length: 222.25
    Episode_Reward/reaching_object: 0.8938
     Episode_Reward/lifting_object: 0.0122
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 2.00s
                      Time elapsed: 00:07:09
                               ETA: 01:04:46

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 48141 steps/s (collection: 1.936s, learning 0.106s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.2220
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 50.5678
                       Mean reward: 4.31
               Mean episode length: 230.05
    Episode_Reward/reaching_object: 0.8618
     Episode_Reward/lifting_object: 0.0800
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 2.04s
                      Time elapsed: 00:07:11
                               ETA: 01:04:43

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 47489 steps/s (collection: 1.978s, learning 0.092s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.9993
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 50.6187
                       Mean reward: 4.52
               Mean episode length: 225.16
    Episode_Reward/reaching_object: 0.8749
     Episode_Reward/lifting_object: 0.0622
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 2.07s
                      Time elapsed: 00:07:13
                               ETA: 01:04:40

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 46382 steps/s (collection: 2.030s, learning 0.089s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.6561
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.6555
                       Mean reward: 4.65
               Mean episode length: 225.75
    Episode_Reward/reaching_object: 0.8447
     Episode_Reward/lifting_object: 0.0746
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 2.12s
                      Time elapsed: 00:07:15
                               ETA: 01:04:38

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 47894 steps/s (collection: 1.950s, learning 0.103s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.5740
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 50.6819
                       Mean reward: 4.74
               Mean episode length: 235.92
    Episode_Reward/reaching_object: 0.8858
     Episode_Reward/lifting_object: 0.0050
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 2.05s
                      Time elapsed: 00:07:17
                               ETA: 01:04:35

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 46315 steps/s (collection: 2.024s, learning 0.098s)
             Mean action noise std: 1.70
          Mean value_function loss: 1.1717
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 50.7266
                       Mean reward: 4.05
               Mean episode length: 231.76
    Episode_Reward/reaching_object: 0.8710
     Episode_Reward/lifting_object: 0.0156
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 2.12s
                      Time elapsed: 00:07:19
                               ETA: 01:04:32

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 48653 steps/s (collection: 1.935s, learning 0.085s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.3108
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.7426
                       Mean reward: 4.59
               Mean episode length: 238.74
    Episode_Reward/reaching_object: 0.9215
     Episode_Reward/lifting_object: 0.1186
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 2.02s
                      Time elapsed: 00:07:21
                               ETA: 01:04:29

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 49598 steps/s (collection: 1.880s, learning 0.102s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.3117
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 50.7849
                       Mean reward: 4.37
               Mean episode length: 240.26
    Episode_Reward/reaching_object: 0.9158
     Episode_Reward/lifting_object: 0.0965
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 1.98s
                      Time elapsed: 00:07:23
                               ETA: 01:04:25

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 48374 steps/s (collection: 1.923s, learning 0.110s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.9357
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 50.8180
                       Mean reward: 3.86
               Mean episode length: 232.13
    Episode_Reward/reaching_object: 0.8869
     Episode_Reward/lifting_object: 0.0632
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 2.03s
                      Time elapsed: 00:07:25
                               ETA: 01:04:22

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 47929 steps/s (collection: 1.942s, learning 0.109s)
             Mean action noise std: 1.71
          Mean value_function loss: 1.0540
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.8281
                       Mean reward: 2.23
               Mean episode length: 244.41
    Episode_Reward/reaching_object: 0.8615
     Episode_Reward/lifting_object: -0.1244
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 2.05s
                      Time elapsed: 00:07:27
                               ETA: 01:04:19

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 46453 steps/s (collection: 2.017s, learning 0.100s)
             Mean action noise std: 1.71
          Mean value_function loss: 1.4157
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 50.8462
                       Mean reward: 6.02
               Mean episode length: 231.20
    Episode_Reward/reaching_object: 0.8541
     Episode_Reward/lifting_object: 0.1020
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 2.12s
                      Time elapsed: 00:07:29
                               ETA: 01:04:16

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 48036 steps/s (collection: 1.946s, learning 0.101s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.9039
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 50.8957
                       Mean reward: 4.55
               Mean episode length: 239.65
    Episode_Reward/reaching_object: 0.8645
     Episode_Reward/lifting_object: 0.0324
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 2.05s
                      Time elapsed: 00:07:31
                               ETA: 01:04:13

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 48727 steps/s (collection: 1.924s, learning 0.093s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.9994
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 50.9408
                       Mean reward: 5.07
               Mean episode length: 225.12
    Episode_Reward/reaching_object: 0.8275
     Episode_Reward/lifting_object: 0.1751
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 2.02s
                      Time elapsed: 00:07:33
                               ETA: 01:04:10

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 48746 steps/s (collection: 1.923s, learning 0.094s)
             Mean action noise std: 1.72
          Mean value_function loss: 1.0361
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 50.9743
                       Mean reward: 3.41
               Mean episode length: 228.01
    Episode_Reward/reaching_object: 0.7965
     Episode_Reward/lifting_object: -0.0829
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 2.02s
                      Time elapsed: 00:07:35
                               ETA: 01:04:07

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 48726 steps/s (collection: 1.923s, learning 0.095s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.9837
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 51.0006
                       Mean reward: 2.78
               Mean episode length: 233.95
    Episode_Reward/reaching_object: 0.8116
     Episode_Reward/lifting_object: 0.0068
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 2.02s
                      Time elapsed: 00:07:37
                               ETA: 01:04:03

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 47631 steps/s (collection: 1.955s, learning 0.109s)
             Mean action noise std: 1.73
          Mean value_function loss: 1.0738
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 51.0271
                       Mean reward: 4.60
               Mean episode length: 227.88
    Episode_Reward/reaching_object: 0.8128
     Episode_Reward/lifting_object: 0.0469
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 2.06s
                      Time elapsed: 00:07:39
                               ETA: 01:04:01

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 48881 steps/s (collection: 1.899s, learning 0.112s)
             Mean action noise std: 1.73
          Mean value_function loss: 4.1862
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 51.0600
                       Mean reward: 1.90
               Mean episode length: 237.12
    Episode_Reward/reaching_object: 0.8503
     Episode_Reward/lifting_object: -0.1076
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 2.01s
                      Time elapsed: 00:07:42
                               ETA: 01:03:57

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 49676 steps/s (collection: 1.890s, learning 0.089s)
             Mean action noise std: 1.73
          Mean value_function loss: 1.3370
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.0732
                       Mean reward: 2.12
               Mean episode length: 229.53
    Episode_Reward/reaching_object: 0.7730
     Episode_Reward/lifting_object: -0.0404
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0299
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 1.98s
                      Time elapsed: 00:07:43
                               ETA: 01:03:54

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 50246 steps/s (collection: 1.866s, learning 0.090s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.3921
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 51.1053
                       Mean reward: 4.02
               Mean episode length: 234.01
    Episode_Reward/reaching_object: 0.7913
     Episode_Reward/lifting_object: 0.0741
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 1.96s
                      Time elapsed: 00:07:45
                               ETA: 01:03:50

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 49545 steps/s (collection: 1.897s, learning 0.088s)
             Mean action noise std: 1.73
          Mean value_function loss: 1.2472
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.1436
                       Mean reward: 3.16
               Mean episode length: 235.97
    Episode_Reward/reaching_object: 0.7663
     Episode_Reward/lifting_object: 0.0759
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 1.98s
                      Time elapsed: 00:07:47
                               ETA: 01:03:47

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 48904 steps/s (collection: 1.917s, learning 0.093s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.7125
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.1540
                       Mean reward: 4.33
               Mean episode length: 233.03
    Episode_Reward/reaching_object: 0.7985
     Episode_Reward/lifting_object: 0.0488
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 2.01s
                      Time elapsed: 00:07:49
                               ETA: 01:03:43

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 50142 steps/s (collection: 1.869s, learning 0.092s)
             Mean action noise std: 1.73
          Mean value_function loss: 1.3625
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 51.1777
                       Mean reward: 2.44
               Mean episode length: 239.27
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 0.0609
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 1.96s
                      Time elapsed: 00:07:51
                               ETA: 01:03:40

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 49674 steps/s (collection: 1.877s, learning 0.102s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.3932
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.1907
                       Mean reward: 2.80
               Mean episode length: 237.14
    Episode_Reward/reaching_object: 0.7612
     Episode_Reward/lifting_object: 0.0637
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 1.98s
                      Time elapsed: 00:07:53
                               ETA: 01:03:36

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 49579 steps/s (collection: 1.897s, learning 0.086s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.3878
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 51.2286
                       Mean reward: 4.06
               Mean episode length: 237.05
    Episode_Reward/reaching_object: 0.7392
     Episode_Reward/lifting_object: 0.1424
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 1.98s
                      Time elapsed: 00:07:55
                               ETA: 01:03:33

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 50219 steps/s (collection: 1.862s, learning 0.095s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.5905
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 51.2828
                       Mean reward: 4.09
               Mean episode length: 234.08
    Episode_Reward/reaching_object: 0.7606
     Episode_Reward/lifting_object: 0.1017
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 1.96s
                      Time elapsed: 00:07:57
                               ETA: 01:03:29

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 50028 steps/s (collection: 1.871s, learning 0.094s)
             Mean action noise std: 1.74
          Mean value_function loss: 3.9782
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 51.3129
                       Mean reward: 0.25
               Mean episode length: 239.89
    Episode_Reward/reaching_object: 0.7244
     Episode_Reward/lifting_object: -0.0153
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 1.96s
                      Time elapsed: 00:07:59
                               ETA: 01:03:26

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 50075 steps/s (collection: 1.875s, learning 0.088s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.4372
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 51.3239
                       Mean reward: 3.40
               Mean episode length: 238.08
    Episode_Reward/reaching_object: 0.7431
     Episode_Reward/lifting_object: 0.0701
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 1.96s
                      Time elapsed: 00:08:01
                               ETA: 01:03:22

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 50256 steps/s (collection: 1.863s, learning 0.093s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.3653
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 51.3515
                       Mean reward: 4.48
               Mean episode length: 235.54
    Episode_Reward/reaching_object: 0.7341
     Episode_Reward/lifting_object: 0.0610
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 1.96s
                      Time elapsed: 00:08:03
                               ETA: 01:03:18

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 49398 steps/s (collection: 1.896s, learning 0.094s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.2810
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 51.4034
                       Mean reward: 3.81
               Mean episode length: 239.14
    Episode_Reward/reaching_object: 0.7242
     Episode_Reward/lifting_object: 0.1460
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 1.99s
                      Time elapsed: 00:08:05
                               ETA: 01:03:15

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 49464 steps/s (collection: 1.886s, learning 0.102s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.8543
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 51.4571
                       Mean reward: 3.90
               Mean episode length: 234.22
    Episode_Reward/reaching_object: 0.7370
     Episode_Reward/lifting_object: 0.2467
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 1.99s
                      Time elapsed: 00:08:07
                               ETA: 01:03:12

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 49397 steps/s (collection: 1.884s, learning 0.106s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.4218
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 51.4734
                       Mean reward: 3.77
               Mean episode length: 240.92
    Episode_Reward/reaching_object: 0.7402
     Episode_Reward/lifting_object: 0.0027
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 1.99s
                      Time elapsed: 00:08:09
                               ETA: 01:03:08

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 49685 steps/s (collection: 1.880s, learning 0.099s)
             Mean action noise std: 1.76
          Mean value_function loss: 1.0399
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 51.5224
                       Mean reward: 3.65
               Mean episode length: 237.74
    Episode_Reward/reaching_object: 0.7333
     Episode_Reward/lifting_object: 0.1424
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 1.98s
                      Time elapsed: 00:08:11
                               ETA: 01:03:05

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 48864 steps/s (collection: 1.925s, learning 0.087s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.8395
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 51.5910
                       Mean reward: 3.95
               Mean episode length: 237.36
    Episode_Reward/reaching_object: 0.7237
     Episode_Reward/lifting_object: 0.0642
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 2.01s
                      Time elapsed: 00:08:13
                               ETA: 01:03:02

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 49598 steps/s (collection: 1.893s, learning 0.089s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.4774
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 51.6198
                       Mean reward: 4.54
               Mean episode length: 237.76
    Episode_Reward/reaching_object: 0.7585
     Episode_Reward/lifting_object: 0.1124
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 1.98s
                      Time elapsed: 00:08:15
                               ETA: 01:02:59

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 49548 steps/s (collection: 1.892s, learning 0.092s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.4704
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 51.6672
                       Mean reward: 4.05
               Mean episode length: 240.43
    Episode_Reward/reaching_object: 0.7548
     Episode_Reward/lifting_object: 0.0578
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 1.98s
                      Time elapsed: 00:08:17
                               ETA: 01:02:55

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 49808 steps/s (collection: 1.875s, learning 0.099s)
             Mean action noise std: 1.77
          Mean value_function loss: 1.2949
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.6959
                       Mean reward: 4.51
               Mean episode length: 238.88
    Episode_Reward/reaching_object: 0.7856
     Episode_Reward/lifting_object: 0.1266
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 1.97s
                      Time elapsed: 00:08:19
                               ETA: 01:02:52

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 48643 steps/s (collection: 1.934s, learning 0.086s)
             Mean action noise std: 1.77
          Mean value_function loss: 1.0728
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 51.7209
                       Mean reward: 4.57
               Mean episode length: 236.15
    Episode_Reward/reaching_object: 0.7659
     Episode_Reward/lifting_object: 0.1282
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 2.02s
                      Time elapsed: 00:08:21
                               ETA: 01:02:49

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 49239 steps/s (collection: 1.894s, learning 0.102s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.9388
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 51.7713
                       Mean reward: 4.73
               Mean episode length: 238.10
    Episode_Reward/reaching_object: 0.7761
     Episode_Reward/lifting_object: 0.0862
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 2.00s
                      Time elapsed: 00:08:23
                               ETA: 01:02:46

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 49218 steps/s (collection: 1.899s, learning 0.098s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.7231
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 51.8103
                       Mean reward: 4.54
               Mean episode length: 234.68
    Episode_Reward/reaching_object: 0.7976
     Episode_Reward/lifting_object: 0.1057
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 2.00s
                      Time elapsed: 00:08:25
                               ETA: 01:02:43

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 49154 steps/s (collection: 1.907s, learning 0.093s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.8400
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 51.8367
                       Mean reward: 5.39
               Mean episode length: 234.02
    Episode_Reward/reaching_object: 0.7825
     Episode_Reward/lifting_object: 0.1258
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 2.00s
                      Time elapsed: 00:08:27
                               ETA: 01:02:40

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 48457 steps/s (collection: 1.929s, learning 0.100s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.6223
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 51.8895
                       Mean reward: 5.01
               Mean episode length: 230.14
    Episode_Reward/reaching_object: 0.7904
     Episode_Reward/lifting_object: 0.1449
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 2.03s
                      Time elapsed: 00:08:29
                               ETA: 01:02:37

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 49151 steps/s (collection: 1.900s, learning 0.100s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.7097
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 51.9307
                       Mean reward: 4.41
               Mean episode length: 236.11
    Episode_Reward/reaching_object: 0.7768
     Episode_Reward/lifting_object: -0.0525
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 2.00s
                      Time elapsed: 00:08:31
                               ETA: 01:02:34

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 47836 steps/s (collection: 1.969s, learning 0.086s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.6855
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 51.9757
                       Mean reward: 4.10
               Mean episode length: 233.59
    Episode_Reward/reaching_object: 0.8292
     Episode_Reward/lifting_object: 0.2281
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 2.06s
                      Time elapsed: 00:08:33
                               ETA: 01:02:31

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 48484 steps/s (collection: 1.937s, learning 0.091s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.5289
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 52.0172
                       Mean reward: 5.50
               Mean episode length: 229.85
    Episode_Reward/reaching_object: 0.7916
     Episode_Reward/lifting_object: 0.2175
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 2.03s
                      Time elapsed: 00:08:35
                               ETA: 01:02:28

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 47593 steps/s (collection: 1.969s, learning 0.096s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.7139
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 52.0771
                       Mean reward: 5.03
               Mean episode length: 241.56
    Episode_Reward/reaching_object: 0.8224
     Episode_Reward/lifting_object: 0.2997
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 2.07s
                      Time elapsed: 00:08:37
                               ETA: 01:02:25

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 46802 steps/s (collection: 2.008s, learning 0.093s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.5664
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 52.1224
                       Mean reward: 5.23
               Mean episode length: 232.93
    Episode_Reward/reaching_object: 0.8300
     Episode_Reward/lifting_object: 0.3119
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 2.10s
                      Time elapsed: 00:08:39
                               ETA: 01:02:23

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 48281 steps/s (collection: 1.928s, learning 0.108s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.4431
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 52.1699
                       Mean reward: 4.92
               Mean episode length: 236.90
    Episode_Reward/reaching_object: 0.8131
     Episode_Reward/lifting_object: 0.1576
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 2.04s
                      Time elapsed: 00:08:41
                               ETA: 01:02:20

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 49347 steps/s (collection: 1.904s, learning 0.088s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.6141
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 52.2332
                       Mean reward: 5.36
               Mean episode length: 236.40
    Episode_Reward/reaching_object: 0.8014
     Episode_Reward/lifting_object: 0.2216
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 1.99s
                      Time elapsed: 00:08:43
                               ETA: 01:02:17

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 48580 steps/s (collection: 1.920s, learning 0.103s)
             Mean action noise std: 1.81
          Mean value_function loss: 1.9794
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 52.2639
                       Mean reward: 5.50
               Mean episode length: 243.02
    Episode_Reward/reaching_object: 0.8405
     Episode_Reward/lifting_object: 0.2021
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 2.02s
                      Time elapsed: 00:08:45
                               ETA: 01:02:14

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 49587 steps/s (collection: 1.889s, learning 0.093s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.4222
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 52.3006
                       Mean reward: 4.29
               Mean episode length: 234.98
    Episode_Reward/reaching_object: 0.8632
     Episode_Reward/lifting_object: 0.0843
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 1.98s
                      Time elapsed: 00:08:47
                               ETA: 01:02:11

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 48760 steps/s (collection: 1.924s, learning 0.092s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.8769
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.3493
                       Mean reward: 5.22
               Mean episode length: 234.87
    Episode_Reward/reaching_object: 0.8504
     Episode_Reward/lifting_object: 0.2225
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 2.02s
                      Time elapsed: 00:08:49
                               ETA: 01:02:08

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 48684 steps/s (collection: 1.923s, learning 0.097s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.4596
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 52.3823
                       Mean reward: 4.93
               Mean episode length: 233.99
    Episode_Reward/reaching_object: 0.8553
     Episode_Reward/lifting_object: 0.2572
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 2.02s
                      Time elapsed: 00:08:51
                               ETA: 01:02:05

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 49055 steps/s (collection: 1.908s, learning 0.096s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.3801
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 52.4350
                       Mean reward: 4.57
               Mean episode length: 243.34
    Episode_Reward/reaching_object: 0.8553
     Episode_Reward/lifting_object: 0.1760
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 2.00s
                      Time elapsed: 00:08:53
                               ETA: 01:02:02

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 48102 steps/s (collection: 1.954s, learning 0.090s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.8275
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 52.4866
                       Mean reward: 4.74
               Mean episode length: 235.34
    Episode_Reward/reaching_object: 0.8881
     Episode_Reward/lifting_object: 0.2202
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 2.04s
                      Time elapsed: 00:08:55
                               ETA: 01:02:00

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 49221 steps/s (collection: 1.909s, learning 0.088s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.7312
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 52.5358
                       Mean reward: 4.07
               Mean episode length: 233.04
    Episode_Reward/reaching_object: 0.8594
     Episode_Reward/lifting_object: 0.2037
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 2.00s
                      Time elapsed: 00:08:57
                               ETA: 01:01:57

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 47436 steps/s (collection: 1.977s, learning 0.096s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.8514
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 52.5787
                       Mean reward: 5.50
               Mean episode length: 239.83
    Episode_Reward/reaching_object: 0.8608
     Episode_Reward/lifting_object: 0.1879
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 2.07s
                      Time elapsed: 00:09:00
                               ETA: 01:01:54

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 45344 steps/s (collection: 2.064s, learning 0.104s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.9804
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 52.6155
                       Mean reward: 5.03
               Mean episode length: 230.44
    Episode_Reward/reaching_object: 0.8364
     Episode_Reward/lifting_object: 0.1783
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 2.17s
                      Time elapsed: 00:09:02
                               ETA: 01:01:52

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 47527 steps/s (collection: 1.973s, learning 0.096s)
             Mean action noise std: 1.84
          Mean value_function loss: 0.7451
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 52.6668
                       Mean reward: 3.03
               Mean episode length: 231.18
    Episode_Reward/reaching_object: 0.8187
     Episode_Reward/lifting_object: 0.0637
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 2.07s
                      Time elapsed: 00:09:04
                               ETA: 01:01:50

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 47364 steps/s (collection: 1.975s, learning 0.101s)
             Mean action noise std: 1.84
          Mean value_function loss: 0.5414
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 52.7280
                       Mean reward: 5.70
               Mean episode length: 240.70
    Episode_Reward/reaching_object: 0.8488
     Episode_Reward/lifting_object: 0.2053
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 2.08s
                      Time elapsed: 00:09:06
                               ETA: 01:01:47

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 48025 steps/s (collection: 1.925s, learning 0.122s)
             Mean action noise std: 1.84
          Mean value_function loss: 1.8510
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.7697
                       Mean reward: 5.82
               Mean episode length: 236.57
    Episode_Reward/reaching_object: 0.8319
     Episode_Reward/lifting_object: 0.3705
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 2.05s
                      Time elapsed: 00:09:08
                               ETA: 01:01:45

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 46535 steps/s (collection: 1.993s, learning 0.119s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.5428
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 52.8091
                       Mean reward: 5.00
               Mean episode length: 229.90
    Episode_Reward/reaching_object: 0.8335
     Episode_Reward/lifting_object: 0.0873
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 2.11s
                      Time elapsed: 00:09:10
                               ETA: 01:01:42

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 47011 steps/s (collection: 1.985s, learning 0.107s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.6154
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 52.8734
                       Mean reward: 5.12
               Mean episode length: 236.62
    Episode_Reward/reaching_object: 0.8452
     Episode_Reward/lifting_object: 0.2691
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 2.09s
                      Time elapsed: 00:09:12
                               ETA: 01:01:40

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 43522 steps/s (collection: 2.142s, learning 0.117s)
             Mean action noise std: 1.85
          Mean value_function loss: 1.3941
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 52.9009
                       Mean reward: 3.09
               Mean episode length: 233.26
    Episode_Reward/reaching_object: 0.8256
     Episode_Reward/lifting_object: 0.2816
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 2.26s
                      Time elapsed: 00:09:14
                               ETA: 01:01:39

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 45627 steps/s (collection: 2.059s, learning 0.095s)
             Mean action noise std: 1.86
          Mean value_function loss: 1.1148
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 52.9633
                       Mean reward: 5.00
               Mean episode length: 234.77
    Episode_Reward/reaching_object: 0.8391
     Episode_Reward/lifting_object: 0.2384
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 2.15s
                      Time elapsed: 00:09:17
                               ETA: 01:01:37

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 48329 steps/s (collection: 1.940s, learning 0.095s)
             Mean action noise std: 1.86
          Mean value_function loss: 1.0456
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 53.0345
                       Mean reward: 5.93
               Mean episode length: 233.72
    Episode_Reward/reaching_object: 0.8115
     Episode_Reward/lifting_object: 0.3180
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.03s
                      Time elapsed: 00:09:19
                               ETA: 01:01:34

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 48624 steps/s (collection: 1.929s, learning 0.093s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.6389
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 53.0826
                       Mean reward: 4.74
               Mean episode length: 241.21
    Episode_Reward/reaching_object: 0.8220
     Episode_Reward/lifting_object: 0.3112
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 2.02s
                      Time elapsed: 00:09:21
                               ETA: 01:01:31

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 46930 steps/s (collection: 2.004s, learning 0.091s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.7753
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 53.1460
                       Mean reward: 5.77
               Mean episode length: 229.40
    Episode_Reward/reaching_object: 0.8001
     Episode_Reward/lifting_object: 0.3995
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 2.09s
                      Time elapsed: 00:09:23
                               ETA: 01:01:29

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 46666 steps/s (collection: 2.010s, learning 0.096s)
             Mean action noise std: 1.88
          Mean value_function loss: 0.9144
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 53.2029
                       Mean reward: 5.31
               Mean episode length: 228.56
    Episode_Reward/reaching_object: 0.7764
     Episode_Reward/lifting_object: 0.2235
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 2.11s
                      Time elapsed: 00:09:25
                               ETA: 01:01:27

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 46754 steps/s (collection: 2.006s, learning 0.096s)
             Mean action noise std: 1.88
          Mean value_function loss: 0.6829
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 53.2631
                       Mean reward: 5.58
               Mean episode length: 220.74
    Episode_Reward/reaching_object: 0.7717
     Episode_Reward/lifting_object: 0.3656
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 2.10s
                      Time elapsed: 00:09:27
                               ETA: 01:01:24

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 48846 steps/s (collection: 1.923s, learning 0.090s)
             Mean action noise std: 1.88
          Mean value_function loss: 4.8152
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.3031
                       Mean reward: 5.80
               Mean episode length: 223.31
    Episode_Reward/reaching_object: 0.7705
     Episode_Reward/lifting_object: 0.3801
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 2.01s
                      Time elapsed: 00:09:29
                               ETA: 01:01:22

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 47989 steps/s (collection: 1.945s, learning 0.103s)
             Mean action noise std: 1.88
          Mean value_function loss: 1.7346
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 53.3168
                       Mean reward: 6.42
               Mean episode length: 228.04
    Episode_Reward/reaching_object: 0.7510
     Episode_Reward/lifting_object: 0.3642
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 2.05s
                      Time elapsed: 00:09:31
                               ETA: 01:01:19

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 46883 steps/s (collection: 1.995s, learning 0.102s)
             Mean action noise std: 1.89
          Mean value_function loss: 1.1654
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 53.3609
                       Mean reward: 5.28
               Mean episode length: 226.95
    Episode_Reward/reaching_object: 0.7584
     Episode_Reward/lifting_object: 0.3930
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 2.10s
                      Time elapsed: 00:09:33
                               ETA: 01:01:17

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 47077 steps/s (collection: 1.992s, learning 0.096s)
             Mean action noise std: 1.89
          Mean value_function loss: 1.2398
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 53.3953
                       Mean reward: 5.75
               Mean episode length: 227.24
    Episode_Reward/reaching_object: 0.7757
     Episode_Reward/lifting_object: 0.4476
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 2.09s
                      Time elapsed: 00:09:35
                               ETA: 01:01:14

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 36863 steps/s (collection: 2.513s, learning 0.154s)
             Mean action noise std: 1.89
          Mean value_function loss: 1.1933
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 53.4471
                       Mean reward: 6.88
               Mean episode length: 228.38
    Episode_Reward/reaching_object: 0.8064
     Episode_Reward/lifting_object: 0.4638
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 2.67s
                      Time elapsed: 00:09:38
                               ETA: 01:01:16

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 36544 steps/s (collection: 2.500s, learning 0.190s)
             Mean action noise std: 1.90
          Mean value_function loss: 1.0959
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 53.5032
                       Mean reward: 5.19
               Mean episode length: 222.00
    Episode_Reward/reaching_object: 0.7521
     Episode_Reward/lifting_object: 0.3826
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 2.69s
                      Time elapsed: 00:09:41
                               ETA: 01:01:17

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 40449 steps/s (collection: 2.243s, learning 0.188s)
             Mean action noise std: 1.90
          Mean value_function loss: 0.8780
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 53.5692
                       Mean reward: 5.15
               Mean episode length: 217.23
    Episode_Reward/reaching_object: 0.7163
     Episode_Reward/lifting_object: 0.4957
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 2.43s
                      Time elapsed: 00:09:43
                               ETA: 01:01:17

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 42328 steps/s (collection: 2.182s, learning 0.141s)
             Mean action noise std: 1.91
          Mean value_function loss: 0.8767
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 53.6394
                       Mean reward: 2.98
               Mean episode length: 208.68
    Episode_Reward/reaching_object: 0.7317
     Episode_Reward/lifting_object: 0.3889
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 2.32s
                      Time elapsed: 00:09:45
                               ETA: 01:01:16

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 42571 steps/s (collection: 2.214s, learning 0.096s)
             Mean action noise std: 1.91
          Mean value_function loss: 1.4908
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 53.6925
                       Mean reward: 6.75
               Mean episode length: 220.04
    Episode_Reward/reaching_object: 0.7361
     Episode_Reward/lifting_object: 0.4168
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 2.31s
                      Time elapsed: 00:09:48
                               ETA: 01:01:15

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 48886 steps/s (collection: 1.921s, learning 0.090s)
             Mean action noise std: 1.92
          Mean value_function loss: 1.6567
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 53.7649
                       Mean reward: 4.85
               Mean episode length: 212.15
    Episode_Reward/reaching_object: 0.7374
     Episode_Reward/lifting_object: 0.5490
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 2.01s
                      Time elapsed: 00:09:50
                               ETA: 01:01:12

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 48910 steps/s (collection: 1.922s, learning 0.088s)
             Mean action noise std: 1.92
          Mean value_function loss: 1.7004
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 53.8071
                       Mean reward: 5.66
               Mean episode length: 219.59
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 0.5307
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 2.01s
                      Time elapsed: 00:09:52
                               ETA: 01:01:09

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 48748 steps/s (collection: 1.926s, learning 0.090s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.9130
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 53.8728
                       Mean reward: 5.04
               Mean episode length: 221.04
    Episode_Reward/reaching_object: 0.7149
     Episode_Reward/lifting_object: 0.4332
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 2.02s
                      Time elapsed: 00:09:54
                               ETA: 01:01:06

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 49570 steps/s (collection: 1.891s, learning 0.093s)
             Mean action noise std: 1.93
          Mean value_function loss: 1.4971
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 53.9338
                       Mean reward: 5.00
               Mean episode length: 223.13
    Episode_Reward/reaching_object: 0.7170
     Episode_Reward/lifting_object: 0.4887
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 1.98s
                      Time elapsed: 00:09:56
                               ETA: 01:01:03

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 49310 steps/s (collection: 1.893s, learning 0.101s)
             Mean action noise std: 1.93
          Mean value_function loss: 1.3856
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 53.9799
                       Mean reward: 5.79
               Mean episode length: 227.69
    Episode_Reward/reaching_object: 0.6800
     Episode_Reward/lifting_object: 0.4758
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 1.99s
                      Time elapsed: 00:09:58
                               ETA: 01:01:00

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 49609 steps/s (collection: 1.893s, learning 0.089s)
             Mean action noise std: 1.94
          Mean value_function loss: 2.7123
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 54.0307
                       Mean reward: 7.24
               Mean episode length: 219.68
    Episode_Reward/reaching_object: 0.7109
     Episode_Reward/lifting_object: 0.6342
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 1.98s
                      Time elapsed: 00:10:00
                               ETA: 01:00:57

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 49234 steps/s (collection: 1.901s, learning 0.096s)
             Mean action noise std: 1.94
          Mean value_function loss: 1.9685
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 54.0968
                       Mean reward: 4.85
               Mean episode length: 215.30
    Episode_Reward/reaching_object: 0.6920
     Episode_Reward/lifting_object: 0.3051
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 2.00s
                      Time elapsed: 00:10:02
                               ETA: 01:00:54

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 49182 steps/s (collection: 1.903s, learning 0.096s)
             Mean action noise std: 1.95
          Mean value_function loss: 1.4642
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 54.1445
                       Mean reward: 4.42
               Mean episode length: 218.52
    Episode_Reward/reaching_object: 0.7131
     Episode_Reward/lifting_object: 0.3523
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 2.00s
                      Time elapsed: 00:10:04
                               ETA: 01:00:52

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 49004 steps/s (collection: 1.915s, learning 0.091s)
             Mean action noise std: 1.95
          Mean value_function loss: 3.7270
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 54.1893
                       Mean reward: 6.60
               Mean episode length: 217.93
    Episode_Reward/reaching_object: 0.7082
     Episode_Reward/lifting_object: 0.5010
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 2.01s
                      Time elapsed: 00:10:06
                               ETA: 01:00:49

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 49044 steps/s (collection: 1.914s, learning 0.090s)
             Mean action noise std: 1.95
          Mean value_function loss: 1.6537
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 54.2457
                       Mean reward: 6.65
               Mean episode length: 217.75
    Episode_Reward/reaching_object: 0.7101
     Episode_Reward/lifting_object: 0.5300
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 2.00s
                      Time elapsed: 00:10:08
                               ETA: 01:00:46

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 49056 steps/s (collection: 1.904s, learning 0.100s)
             Mean action noise std: 1.96
          Mean value_function loss: 1.4992
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 54.3010
                       Mean reward: 7.41
               Mean episode length: 218.25
    Episode_Reward/reaching_object: 0.7231
     Episode_Reward/lifting_object: 0.8349
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 2.00s
                      Time elapsed: 00:10:10
                               ETA: 01:00:43

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 48433 steps/s (collection: 1.929s, learning 0.101s)
             Mean action noise std: 1.96
          Mean value_function loss: 2.7733
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 54.3449
                       Mean reward: 6.24
               Mean episode length: 226.75
    Episode_Reward/reaching_object: 0.7180
     Episode_Reward/lifting_object: 0.6488
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 2.03s
                      Time elapsed: 00:10:12
                               ETA: 01:00:40

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 48481 steps/s (collection: 1.939s, learning 0.089s)
             Mean action noise std: 1.96
          Mean value_function loss: 1.8084
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 54.3845
                       Mean reward: 7.67
               Mean episode length: 219.48
    Episode_Reward/reaching_object: 0.7232
     Episode_Reward/lifting_object: 0.7171
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 2.03s
                      Time elapsed: 00:10:14
                               ETA: 01:00:38

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 48605 steps/s (collection: 1.935s, learning 0.087s)
             Mean action noise std: 1.97
          Mean value_function loss: 2.4895
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 54.4325
                       Mean reward: 7.38
               Mean episode length: 203.61
    Episode_Reward/reaching_object: 0.6876
     Episode_Reward/lifting_object: 0.7557
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 2.02s
                      Time elapsed: 00:10:16
                               ETA: 01:00:35

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 47565 steps/s (collection: 1.974s, learning 0.093s)
             Mean action noise std: 1.97
          Mean value_function loss: 1.9185
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 54.4883
                       Mean reward: 7.09
               Mean episode length: 209.07
    Episode_Reward/reaching_object: 0.6706
     Episode_Reward/lifting_object: 0.7390
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 2.07s
                      Time elapsed: 00:10:18
                               ETA: 01:00:32

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 49360 steps/s (collection: 1.903s, learning 0.089s)
             Mean action noise std: 1.97
          Mean value_function loss: 3.3684
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 54.5294
                       Mean reward: 7.76
               Mean episode length: 208.32
    Episode_Reward/reaching_object: 0.6757
     Episode_Reward/lifting_object: 0.4696
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 1.99s
                      Time elapsed: 00:10:20
                               ETA: 01:00:29

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 48268 steps/s (collection: 1.942s, learning 0.095s)
             Mean action noise std: 1.98
          Mean value_function loss: 5.8696
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.5622
                       Mean reward: 6.48
               Mean episode length: 204.35
    Episode_Reward/reaching_object: 0.6425
     Episode_Reward/lifting_object: 0.7921
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 2.04s
                      Time elapsed: 00:10:22
                               ETA: 01:00:27

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 47937 steps/s (collection: 1.945s, learning 0.106s)
             Mean action noise std: 1.98
          Mean value_function loss: 4.8256
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.6006
                       Mean reward: 6.26
               Mean episode length: 208.70
    Episode_Reward/reaching_object: 0.6302
     Episode_Reward/lifting_object: 0.6710
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 2.05s
                      Time elapsed: 00:10:24
                               ETA: 01:00:24

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 46226 steps/s (collection: 2.026s, learning 0.101s)
             Mean action noise std: 1.98
          Mean value_function loss: 5.5333
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 54.6255
                       Mean reward: 6.63
               Mean episode length: 197.37
    Episode_Reward/reaching_object: 0.5893
     Episode_Reward/lifting_object: 0.7299
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 11.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 2.13s
                      Time elapsed: 00:10:26
                               ETA: 01:00:22

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 47374 steps/s (collection: 1.975s, learning 0.100s)
             Mean action noise std: 1.99
          Mean value_function loss: 5.6407
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 54.6756
                       Mean reward: 6.02
               Mean episode length: 186.24
    Episode_Reward/reaching_object: 0.6055
     Episode_Reward/lifting_object: 0.6018
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 2.08s
                      Time elapsed: 00:10:28
                               ETA: 01:00:20

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 47439 steps/s (collection: 1.974s, learning 0.098s)
             Mean action noise std: 1.99
          Mean value_function loss: 3.0051
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 54.7195
                       Mean reward: 6.51
               Mean episode length: 185.01
    Episode_Reward/reaching_object: 0.5981
     Episode_Reward/lifting_object: 0.8399
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 9.5000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 2.07s
                      Time elapsed: 00:10:30
                               ETA: 01:00:17

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 48357 steps/s (collection: 1.946s, learning 0.087s)
             Mean action noise std: 1.99
          Mean value_function loss: 2.3483
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 54.7687
                       Mean reward: 7.31
               Mean episode length: 184.53
    Episode_Reward/reaching_object: 0.5701
     Episode_Reward/lifting_object: 0.8014
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 2.03s
                      Time elapsed: 00:10:32
                               ETA: 01:00:15

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 48342 steps/s (collection: 1.944s, learning 0.089s)
             Mean action noise std: 2.00
          Mean value_function loss: 2.6879
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 54.8399
                       Mean reward: 8.52
               Mean episode length: 199.73
    Episode_Reward/reaching_object: 0.6017
     Episode_Reward/lifting_object: 0.8284
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 2.03s
                      Time elapsed: 00:10:34
                               ETA: 01:00:12

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 48141 steps/s (collection: 1.950s, learning 0.092s)
             Mean action noise std: 2.00
          Mean value_function loss: 2.8498
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 54.9134
                       Mean reward: 6.52
               Mean episode length: 183.88
    Episode_Reward/reaching_object: 0.5719
     Episode_Reward/lifting_object: 0.9281
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 2.04s
                      Time elapsed: 00:10:36
                               ETA: 01:00:09

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 47343 steps/s (collection: 1.975s, learning 0.101s)
             Mean action noise std: 2.01
          Mean value_function loss: 2.7708
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 54.9497
                       Mean reward: 8.76
               Mean episode length: 181.96
    Episode_Reward/reaching_object: 0.5646
     Episode_Reward/lifting_object: 0.7049
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 2.08s
                      Time elapsed: 00:10:38
                               ETA: 01:00:07

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 47466 steps/s (collection: 1.969s, learning 0.102s)
             Mean action noise std: 2.01
          Mean value_function loss: 2.5952
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 54.9769
                       Mean reward: 5.37
               Mean episode length: 187.08
    Episode_Reward/reaching_object: 0.5650
     Episode_Reward/lifting_object: 0.8741
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 2.07s
                      Time elapsed: 00:10:40
                               ETA: 01:00:05

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 47280 steps/s (collection: 1.978s, learning 0.102s)
             Mean action noise std: 2.01
          Mean value_function loss: 2.6383
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 54.9997
                       Mean reward: 7.31
               Mean episode length: 184.53
    Episode_Reward/reaching_object: 0.5529
     Episode_Reward/lifting_object: 0.9745
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 2.08s
                      Time elapsed: 00:10:42
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 47670 steps/s (collection: 1.971s, learning 0.091s)
             Mean action noise std: 2.01
          Mean value_function loss: 2.6319
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 55.0302
                       Mean reward: 5.85
               Mean episode length: 177.24
    Episode_Reward/reaching_object: 0.5781
     Episode_Reward/lifting_object: 0.7497
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 2.06s
                      Time elapsed: 00:10:44
                               ETA: 01:00:00

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 48186 steps/s (collection: 1.939s, learning 0.101s)
             Mean action noise std: 2.01
          Mean value_function loss: 3.6159
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.0596
                       Mean reward: 6.96
               Mean episode length: 178.42
    Episode_Reward/reaching_object: 0.5463
     Episode_Reward/lifting_object: 0.9390
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 12.8333
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 2.04s
                      Time elapsed: 00:10:47
                               ETA: 00:59:57

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 47915 steps/s (collection: 1.955s, learning 0.097s)
             Mean action noise std: 2.02
          Mean value_function loss: 2.7433
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 55.0935
                       Mean reward: 8.79
               Mean episode length: 189.05
    Episode_Reward/reaching_object: 0.5686
     Episode_Reward/lifting_object: 1.0592
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 2.05s
                      Time elapsed: 00:10:49
                               ETA: 00:59:55

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 46498 steps/s (collection: 2.007s, learning 0.107s)
             Mean action noise std: 2.02
          Mean value_function loss: 4.5110
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.1320
                       Mean reward: 8.28
               Mean episode length: 186.56
    Episode_Reward/reaching_object: 0.5671
     Episode_Reward/lifting_object: 1.0047
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 2.11s
                      Time elapsed: 00:10:51
                               ETA: 00:59:53

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 46772 steps/s (collection: 1.983s, learning 0.119s)
             Mean action noise std: 2.02
          Mean value_function loss: 2.8749
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 55.1699
                       Mean reward: 7.52
               Mean episode length: 175.58
    Episode_Reward/reaching_object: 0.5306
     Episode_Reward/lifting_object: 1.0229
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 2.10s
                      Time elapsed: 00:10:53
                               ETA: 00:59:50

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 47684 steps/s (collection: 1.958s, learning 0.103s)
             Mean action noise std: 2.03
          Mean value_function loss: 4.6025
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 55.2115
                       Mean reward: 8.82
               Mean episode length: 182.31
    Episode_Reward/reaching_object: 0.5479
     Episode_Reward/lifting_object: 1.3004
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 2.06s
                      Time elapsed: 00:10:55
                               ETA: 00:59:48

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 46915 steps/s (collection: 1.997s, learning 0.098s)
             Mean action noise std: 2.03
          Mean value_function loss: 5.0116
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 55.2682
                       Mean reward: 5.08
               Mean episode length: 167.96
    Episode_Reward/reaching_object: 0.5280
     Episode_Reward/lifting_object: 1.0426
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 15.1250
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 2.10s
                      Time elapsed: 00:10:57
                               ETA: 00:59:46

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 46952 steps/s (collection: 1.990s, learning 0.104s)
             Mean action noise std: 2.03
          Mean value_function loss: 6.1249
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.3080
                       Mean reward: 9.24
               Mean episode length: 175.67
    Episode_Reward/reaching_object: 0.5176
     Episode_Reward/lifting_object: 1.0991
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 2.09s
                      Time elapsed: 00:10:59
                               ETA: 00:59:43

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 47371 steps/s (collection: 1.979s, learning 0.096s)
             Mean action noise std: 2.04
          Mean value_function loss: 10.3261
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.3340
                       Mean reward: 9.02
               Mean episode length: 177.93
    Episode_Reward/reaching_object: 0.5140
     Episode_Reward/lifting_object: 0.8991
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 15.2917
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 2.08s
                      Time elapsed: 00:11:01
                               ETA: 00:59:41

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 47359 steps/s (collection: 1.981s, learning 0.095s)
             Mean action noise std: 2.04
          Mean value_function loss: 6.0007
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 55.3673
                       Mean reward: 6.03
               Mean episode length: 170.54
    Episode_Reward/reaching_object: 0.5028
     Episode_Reward/lifting_object: 0.8255
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.08s
                      Time elapsed: 00:11:03
                               ETA: 00:59:39

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 47806 steps/s (collection: 1.967s, learning 0.090s)
             Mean action noise std: 2.04
          Mean value_function loss: 3.5514
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 55.3963
                       Mean reward: 7.27
               Mean episode length: 190.18
    Episode_Reward/reaching_object: 0.5129
     Episode_Reward/lifting_object: 1.1462
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 2.06s
                      Time elapsed: 00:11:05
                               ETA: 00:59:36

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 47492 steps/s (collection: 1.977s, learning 0.093s)
             Mean action noise std: 2.04
          Mean value_function loss: 8.9206
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.4236
                       Mean reward: 6.64
               Mean episode length: 179.48
    Episode_Reward/reaching_object: 0.5063
     Episode_Reward/lifting_object: 0.8973
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 16.1250
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 2.07s
                      Time elapsed: 00:11:07
                               ETA: 00:59:34

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 47869 steps/s (collection: 1.957s, learning 0.097s)
             Mean action noise std: 2.05
          Mean value_function loss: 8.3496
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 55.4554
                       Mean reward: 7.03
               Mean episode length: 164.28
    Episode_Reward/reaching_object: 0.5082
     Episode_Reward/lifting_object: 1.2471
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 17.2500
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 2.05s
                      Time elapsed: 00:11:09
                               ETA: 00:59:31

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 45633 steps/s (collection: 2.027s, learning 0.128s)
             Mean action noise std: 2.05
          Mean value_function loss: 10.9735
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 55.5030
                       Mean reward: 8.12
               Mean episode length: 157.71
    Episode_Reward/reaching_object: 0.4823
     Episode_Reward/lifting_object: 1.1065
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 16.6667
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.15s
                      Time elapsed: 00:11:12
                               ETA: 00:59:29

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 46092 steps/s (collection: 2.036s, learning 0.096s)
             Mean action noise std: 2.06
          Mean value_function loss: 5.7068
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 55.5664
                       Mean reward: 4.28
               Mean episode length: 175.06
    Episode_Reward/reaching_object: 0.5060
     Episode_Reward/lifting_object: 0.9290
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 17.6667
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 2.13s
                      Time elapsed: 00:11:14
                               ETA: 00:59:27

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 47681 steps/s (collection: 1.971s, learning 0.091s)
             Mean action noise std: 2.06
          Mean value_function loss: 4.5971
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 55.6198
                       Mean reward: 6.45
               Mean episode length: 165.87
    Episode_Reward/reaching_object: 0.4939
     Episode_Reward/lifting_object: 1.0357
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 16.7917
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.06s
                      Time elapsed: 00:11:16
                               ETA: 00:59:25

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 45562 steps/s (collection: 2.034s, learning 0.124s)
             Mean action noise std: 2.06
          Mean value_function loss: 7.4705
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.6552
                       Mean reward: 5.88
               Mean episode length: 163.33
    Episode_Reward/reaching_object: 0.4675
     Episode_Reward/lifting_object: 0.9835
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 2.16s
                      Time elapsed: 00:11:18
                               ETA: 00:59:23

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 46507 steps/s (collection: 1.997s, learning 0.117s)
             Mean action noise std: 2.07
          Mean value_function loss: 5.8379
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 55.7056
                       Mean reward: 7.29
               Mean episode length: 152.34
    Episode_Reward/reaching_object: 0.4517
     Episode_Reward/lifting_object: 0.9352
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 21.7917
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.11s
                      Time elapsed: 00:11:20
                               ETA: 00:59:21

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 47559 steps/s (collection: 1.960s, learning 0.107s)
             Mean action noise std: 2.07
          Mean value_function loss: 4.5088
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 55.7576
                       Mean reward: 7.13
               Mean episode length: 158.38
    Episode_Reward/reaching_object: 0.4628
     Episode_Reward/lifting_object: 1.0551
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 19.5833
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 2.07s
                      Time elapsed: 00:11:22
                               ETA: 00:59:18

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 46110 steps/s (collection: 2.036s, learning 0.096s)
             Mean action noise std: 2.07
          Mean value_function loss: 6.5315
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 55.7909
                       Mean reward: 7.57
               Mean episode length: 140.75
    Episode_Reward/reaching_object: 0.4541
     Episode_Reward/lifting_object: 1.2157
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 18.9167
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.13s
                      Time elapsed: 00:11:24
                               ETA: 00:59:16

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 47332 steps/s (collection: 1.976s, learning 0.101s)
             Mean action noise std: 2.07
          Mean value_function loss: 5.7252
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 55.8051
                       Mean reward: 3.82
               Mean episode length: 156.33
    Episode_Reward/reaching_object: 0.4459
     Episode_Reward/lifting_object: 1.1402
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 19.7083
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 2.08s
                      Time elapsed: 00:11:26
                               ETA: 00:59:14

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 46456 steps/s (collection: 2.016s, learning 0.100s)
             Mean action noise std: 2.08
          Mean value_function loss: 5.9865
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 55.8294
                       Mean reward: 9.41
               Mean episode length: 162.99
    Episode_Reward/reaching_object: 0.4574
     Episode_Reward/lifting_object: 1.2767
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 16.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.12s
                      Time elapsed: 00:11:28
                               ETA: 00:59:12

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 47340 steps/s (collection: 1.976s, learning 0.100s)
             Mean action noise std: 2.08
          Mean value_function loss: 10.6665
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 55.8785
                       Mean reward: 9.68
               Mean episode length: 164.10
    Episode_Reward/reaching_object: 0.4699
     Episode_Reward/lifting_object: 1.5118
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 16.7917
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.08s
                      Time elapsed: 00:11:30
                               ETA: 00:59:10

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 47179 steps/s (collection: 1.984s, learning 0.100s)
             Mean action noise std: 2.08
          Mean value_function loss: 7.3937
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 55.9267
                       Mean reward: 8.74
               Mean episode length: 173.09
    Episode_Reward/reaching_object: 0.4825
     Episode_Reward/lifting_object: 1.4637
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.08s
                      Time elapsed: 00:11:33
                               ETA: 00:59:07

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 47032 steps/s (collection: 1.999s, learning 0.092s)
             Mean action noise std: 2.09
          Mean value_function loss: 8.2677
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 55.9698
                       Mean reward: 6.14
               Mean episode length: 176.44
    Episode_Reward/reaching_object: 0.4655
     Episode_Reward/lifting_object: 1.3367
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 15.1667
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 2.09s
                      Time elapsed: 00:11:35
                               ETA: 00:59:05

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 47892 steps/s (collection: 1.963s, learning 0.090s)
             Mean action noise std: 2.09
          Mean value_function loss: 6.8415
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.0129
                       Mean reward: 9.12
               Mean episode length: 162.41
    Episode_Reward/reaching_object: 0.4866
     Episode_Reward/lifting_object: 1.4457
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 15.5000
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 2.05s
                      Time elapsed: 00:11:37
                               ETA: 00:59:03

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 46484 steps/s (collection: 1.997s, learning 0.118s)
             Mean action noise std: 2.09
          Mean value_function loss: 8.0586
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 56.0469
                       Mean reward: 8.01
               Mean episode length: 173.76
    Episode_Reward/reaching_object: 0.4813
     Episode_Reward/lifting_object: 1.1026
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 2.11s
                      Time elapsed: 00:11:39
                               ETA: 00:59:00

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 47639 steps/s (collection: 1.954s, learning 0.110s)
             Mean action noise std: 2.09
          Mean value_function loss: 9.2438
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.0602
                       Mean reward: 9.34
               Mean episode length: 176.42
    Episode_Reward/reaching_object: 0.4887
     Episode_Reward/lifting_object: 1.3443
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.06s
                      Time elapsed: 00:11:41
                               ETA: 00:58:58

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 47203 steps/s (collection: 1.975s, learning 0.108s)
             Mean action noise std: 2.10
          Mean value_function loss: 8.8555
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.0855
                       Mean reward: 10.54
               Mean episode length: 178.45
    Episode_Reward/reaching_object: 0.4895
     Episode_Reward/lifting_object: 1.3621
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 2.08s
                      Time elapsed: 00:11:43
                               ETA: 00:58:56

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 47406 steps/s (collection: 1.977s, learning 0.097s)
             Mean action noise std: 2.10
          Mean value_function loss: 10.6435
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.1044
                       Mean reward: 8.27
               Mean episode length: 173.00
    Episode_Reward/reaching_object: 0.4789
     Episode_Reward/lifting_object: 1.2939
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 7.6667
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 15.7500
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 2.07s
                      Time elapsed: 00:11:45
                               ETA: 00:58:53

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 19863 steps/s (collection: 4.834s, learning 0.115s)
             Mean action noise std: 2.10
          Mean value_function loss: 5.6298
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.1128
                       Mean reward: 10.76
               Mean episode length: 171.39
    Episode_Reward/reaching_object: 0.4677
     Episode_Reward/lifting_object: 1.4452
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 4.95s
                      Time elapsed: 00:11:50
                               ETA: 00:59:05

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 14292 steps/s (collection: 6.762s, learning 0.116s)
             Mean action noise std: 2.10
          Mean value_function loss: 15.1270
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.1299
                       Mean reward: 10.65
               Mean episode length: 177.50
    Episode_Reward/reaching_object: 0.4753
     Episode_Reward/lifting_object: 1.4866
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 14.5000
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 6.88s
                      Time elapsed: 00:11:57
                               ETA: 00:59:27

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 14373 steps/s (collection: 6.716s, learning 0.123s)
             Mean action noise std: 2.10
          Mean value_function loss: 10.9295
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.1497
                       Mean reward: 4.95
               Mean episode length: 182.16
    Episode_Reward/reaching_object: 0.4691
     Episode_Reward/lifting_object: 1.2460
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 14.9583
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 6.84s
                      Time elapsed: 00:12:04
                               ETA: 00:59:48

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 14837 steps/s (collection: 6.504s, learning 0.121s)
             Mean action noise std: 2.10
          Mean value_function loss: 6.8443
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.1682
                       Mean reward: 10.99
               Mean episode length: 172.85
    Episode_Reward/reaching_object: 0.4648
     Episode_Reward/lifting_object: 1.4146
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 6.63s
                      Time elapsed: 00:12:10
                               ETA: 01:00:08

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 14253 steps/s (collection: 6.772s, learning 0.125s)
             Mean action noise std: 2.10
          Mean value_function loss: 9.3017
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 56.1925
                       Mean reward: 7.14
               Mean episode length: 177.52
    Episode_Reward/reaching_object: 0.4471
     Episode_Reward/lifting_object: 1.2588
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 6.90s
                      Time elapsed: 00:12:17
                               ETA: 01:00:29

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 14116 steps/s (collection: 6.840s, learning 0.124s)
             Mean action noise std: 2.11
          Mean value_function loss: 15.4452
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 56.2242
                       Mean reward: 8.98
               Mean episode length: 174.03
    Episode_Reward/reaching_object: 0.4543
     Episode_Reward/lifting_object: 1.2786
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 6.0000
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 17.6667
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 6.96s
                      Time elapsed: 00:12:24
                               ETA: 01:00:50

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 14107 steps/s (collection: 6.827s, learning 0.142s)
             Mean action noise std: 2.11
          Mean value_function loss: 9.1929
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 56.2654
                       Mean reward: 4.58
               Mean episode length: 173.43
    Episode_Reward/reaching_object: 0.4245
     Episode_Reward/lifting_object: 0.8745
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 6.97s
                      Time elapsed: 00:12:31
                               ETA: 01:01:11

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 13777 steps/s (collection: 6.991s, learning 0.145s)
             Mean action noise std: 2.11
          Mean value_function loss: 12.6156
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 56.2869
                       Mean reward: 9.88
               Mean episode length: 176.66
    Episode_Reward/reaching_object: 0.4451
     Episode_Reward/lifting_object: 1.6937
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 7.14s
                      Time elapsed: 00:12:38
                               ETA: 01:01:33

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 13597 steps/s (collection: 7.122s, learning 0.108s)
             Mean action noise std: 2.12
          Mean value_function loss: 12.9902
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 56.3256
                       Mean reward: 8.98
               Mean episode length: 170.32
    Episode_Reward/reaching_object: 0.4511
     Episode_Reward/lifting_object: 1.0311
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 17.4583
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 7.23s
                      Time elapsed: 00:12:45
                               ETA: 01:01:55

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 49149 steps/s (collection: 1.913s, learning 0.087s)
             Mean action noise std: 2.12
          Mean value_function loss: 10.5239
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 56.3670
                       Mean reward: 8.27
               Mean episode length: 178.78
    Episode_Reward/reaching_object: 0.4401
     Episode_Reward/lifting_object: 1.4360
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 15.3333
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 2.00s
                      Time elapsed: 00:12:47
                               ETA: 01:01:52

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 47845 steps/s (collection: 1.962s, learning 0.093s)
             Mean action noise std: 2.12
          Mean value_function loss: 20.7040
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 56.4125
                       Mean reward: 10.74
               Mean episode length: 172.87
    Episode_Reward/reaching_object: 0.4468
     Episode_Reward/lifting_object: 1.4198
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 15.2500
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 2.05s
                      Time elapsed: 00:12:50
                               ETA: 01:01:49

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 48948 steps/s (collection: 1.921s, learning 0.087s)
             Mean action noise std: 2.13
          Mean value_function loss: 6.7671
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 56.4649
                       Mean reward: 6.81
               Mean episode length: 165.59
    Episode_Reward/reaching_object: 0.4510
     Episode_Reward/lifting_object: 1.5644
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 2.01s
                      Time elapsed: 00:12:52
                               ETA: 01:01:45

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 48057 steps/s (collection: 1.933s, learning 0.113s)
             Mean action noise std: 2.13
          Mean value_function loss: 12.4495
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 56.4828
                       Mean reward: 10.18
               Mean episode length: 180.79
    Episode_Reward/reaching_object: 0.4673
     Episode_Reward/lifting_object: 1.5512
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 14.7917
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 2.05s
                      Time elapsed: 00:12:54
                               ETA: 01:01:42

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 49273 steps/s (collection: 1.893s, learning 0.103s)
             Mean action noise std: 2.13
          Mean value_function loss: 10.0903
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 56.4861
                       Mean reward: 12.92
               Mean episode length: 190.49
    Episode_Reward/reaching_object: 0.4668
     Episode_Reward/lifting_object: 1.7027
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 2.00s
                      Time elapsed: 00:12:56
                               ETA: 01:01:39

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 47167 steps/s (collection: 1.971s, learning 0.114s)
             Mean action noise std: 2.13
          Mean value_function loss: 13.1907
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 56.4894
                       Mean reward: 11.43
               Mean episode length: 181.47
    Episode_Reward/reaching_object: 0.4575
     Episode_Reward/lifting_object: 1.2422
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 2.08s
                      Time elapsed: 00:12:58
                               ETA: 01:01:36

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 47966 steps/s (collection: 1.959s, learning 0.091s)
             Mean action noise std: 2.13
          Mean value_function loss: 22.4684
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 56.4981
                       Mean reward: 11.07
               Mean episode length: 170.92
    Episode_Reward/reaching_object: 0.4406
     Episode_Reward/lifting_object: 1.2177
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 2.05s
                      Time elapsed: 00:13:00
                               ETA: 01:01:33

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 47808 steps/s (collection: 1.960s, learning 0.096s)
             Mean action noise std: 2.13
          Mean value_function loss: 13.5635
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 56.5030
                       Mean reward: 10.44
               Mean episode length: 177.93
    Episode_Reward/reaching_object: 0.4477
     Episode_Reward/lifting_object: 1.3131
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 19.1667
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 2.06s
                      Time elapsed: 00:13:02
                               ETA: 01:01:30

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 38198 steps/s (collection: 2.355s, learning 0.218s)
             Mean action noise std: 2.13
          Mean value_function loss: 15.3431
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 56.5206
                       Mean reward: 9.17
               Mean episode length: 155.92
    Episode_Reward/reaching_object: 0.4236
     Episode_Reward/lifting_object: 1.3409
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 2.57s
                      Time elapsed: 00:13:04
                               ETA: 01:01:29

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 45334 steps/s (collection: 2.050s, learning 0.118s)
             Mean action noise std: 2.13
          Mean value_function loss: 12.2766
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 56.5505
                       Mean reward: 10.47
               Mean episode length: 179.25
    Episode_Reward/reaching_object: 0.4271
     Episode_Reward/lifting_object: 1.2023
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 16.8750
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 2.17s
                      Time elapsed: 00:13:07
                               ETA: 01:01:26

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 48269 steps/s (collection: 1.945s, learning 0.092s)
             Mean action noise std: 2.14
          Mean value_function loss: 7.9862
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 56.5837
                       Mean reward: 8.62
               Mean episode length: 174.03
    Episode_Reward/reaching_object: 0.4367
     Episode_Reward/lifting_object: 1.5573
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 19.3333
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 2.04s
                      Time elapsed: 00:13:09
                               ETA: 01:01:23

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 48424 steps/s (collection: 1.935s, learning 0.095s)
             Mean action noise std: 2.14
          Mean value_function loss: 14.5379
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 56.6162
                       Mean reward: 8.85
               Mean episode length: 162.51
    Episode_Reward/reaching_object: 0.4195
     Episode_Reward/lifting_object: 1.1081
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 17.6250
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 2.03s
                      Time elapsed: 00:13:11
                               ETA: 01:01:20

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 49140 steps/s (collection: 1.910s, learning 0.090s)
             Mean action noise std: 2.14
          Mean value_function loss: 24.8063
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 56.6386
                       Mean reward: 7.77
               Mean episode length: 165.17
    Episode_Reward/reaching_object: 0.4196
     Episode_Reward/lifting_object: 1.4296
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 18.5000
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 2.00s
                      Time elapsed: 00:13:13
                               ETA: 01:01:17

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 48962 steps/s (collection: 1.914s, learning 0.094s)
             Mean action noise std: 2.14
          Mean value_function loss: 10.1515
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 56.6477
                       Mean reward: 11.55
               Mean episode length: 154.17
    Episode_Reward/reaching_object: 0.4258
     Episode_Reward/lifting_object: 1.1538
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 18.5000
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 2.01s
                      Time elapsed: 00:13:15
                               ETA: 01:01:13

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 47897 steps/s (collection: 1.964s, learning 0.088s)
             Mean action noise std: 2.14
          Mean value_function loss: 16.8391
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 56.6683
                       Mean reward: 9.54
               Mean episode length: 139.20
    Episode_Reward/reaching_object: 0.4197
     Episode_Reward/lifting_object: 1.5233
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 17.7500
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 2.05s
                      Time elapsed: 00:13:17
                               ETA: 01:01:10

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 49450 steps/s (collection: 1.900s, learning 0.088s)
             Mean action noise std: 2.15
          Mean value_function loss: 23.3253
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.6966
                       Mean reward: 11.14
               Mean episode length: 162.73
    Episode_Reward/reaching_object: 0.4241
     Episode_Reward/lifting_object: 1.4303
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 19.5833
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 1.99s
                      Time elapsed: 00:13:19
                               ETA: 01:01:07

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 49991 steps/s (collection: 1.877s, learning 0.089s)
             Mean action noise std: 2.15
          Mean value_function loss: 8.8841
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 56.7281
                       Mean reward: 12.34
               Mean episode length: 163.16
    Episode_Reward/reaching_object: 0.4378
     Episode_Reward/lifting_object: 1.9512
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 5.0000
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 17.9167
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 1.97s
                      Time elapsed: 00:13:21
                               ETA: 01:01:04

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 50357 steps/s (collection: 1.859s, learning 0.093s)
             Mean action noise std: 2.15
          Mean value_function loss: 17.0792
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 56.7641
                       Mean reward: 10.49
               Mean episode length: 160.57
    Episode_Reward/reaching_object: 0.4537
     Episode_Reward/lifting_object: 1.6993
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 1.95s
                      Time elapsed: 00:13:23
                               ETA: 01:01:00

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 50460 steps/s (collection: 1.854s, learning 0.095s)
             Mean action noise std: 2.16
          Mean value_function loss: 8.5667
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 56.8040
                       Mean reward: 12.37
               Mean episode length: 152.66
    Episode_Reward/reaching_object: 0.4280
     Episode_Reward/lifting_object: 1.0639
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 6.8750
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 17.4583
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 1.95s
                      Time elapsed: 00:13:25
                               ETA: 01:00:57

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 48320 steps/s (collection: 1.921s, learning 0.113s)
             Mean action noise std: 2.16
          Mean value_function loss: 13.9082
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.8318
                       Mean reward: 10.79
               Mean episode length: 145.52
    Episode_Reward/reaching_object: 0.4387
     Episode_Reward/lifting_object: 1.9722
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 17.4583
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 2.03s
                      Time elapsed: 00:13:27
                               ETA: 01:00:53

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 50764 steps/s (collection: 1.843s, learning 0.093s)
             Mean action noise std: 2.16
          Mean value_function loss: 11.6646
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 56.8614
                       Mean reward: 8.97
               Mean episode length: 156.93
    Episode_Reward/reaching_object: 0.4327
     Episode_Reward/lifting_object: 1.6259
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 18.3333
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 1.94s
                      Time elapsed: 00:13:28
                               ETA: 01:00:50

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 49385 steps/s (collection: 1.894s, learning 0.097s)
             Mean action noise std: 2.16
          Mean value_function loss: 15.3039
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 56.8939
                       Mean reward: 9.69
               Mean episode length: 170.93
    Episode_Reward/reaching_object: 0.4537
     Episode_Reward/lifting_object: 2.1258
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 1.99s
                      Time elapsed: 00:13:30
                               ETA: 01:00:47

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 49363 steps/s (collection: 1.899s, learning 0.092s)
             Mean action noise std: 2.17
          Mean value_function loss: 8.7620
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 56.9268
                       Mean reward: 6.49
               Mean episode length: 159.30
    Episode_Reward/reaching_object: 0.4223
     Episode_Reward/lifting_object: 1.3823
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 1.99s
                      Time elapsed: 00:13:32
                               ETA: 01:00:43

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 48069 steps/s (collection: 1.954s, learning 0.091s)
             Mean action noise std: 2.17
          Mean value_function loss: 12.3059
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 56.9395
                       Mean reward: 12.30
               Mean episode length: 164.12
    Episode_Reward/reaching_object: 0.4199
     Episode_Reward/lifting_object: 1.9615
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 17.2917
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 2.05s
                      Time elapsed: 00:13:35
                               ETA: 01:00:40

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 49745 steps/s (collection: 1.881s, learning 0.095s)
             Mean action noise std: 2.17
          Mean value_function loss: 10.6754
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 56.9464
                       Mean reward: 8.70
               Mean episode length: 154.18
    Episode_Reward/reaching_object: 0.4275
     Episode_Reward/lifting_object: 1.7597
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 1.98s
                      Time elapsed: 00:13:36
                               ETA: 01:00:37

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 50063 steps/s (collection: 1.873s, learning 0.091s)
             Mean action noise std: 2.17
          Mean value_function loss: 20.3775
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 56.9496
                       Mean reward: 12.38
               Mean episode length: 169.57
    Episode_Reward/reaching_object: 0.4212
     Episode_Reward/lifting_object: 1.9340
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 2.7083
     Episode_Termination/robot_out: 20.2083
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 1.96s
                      Time elapsed: 00:13:38
                               ETA: 01:00:34

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 49493 steps/s (collection: 1.882s, learning 0.105s)
             Mean action noise std: 2.17
          Mean value_function loss: 11.4265
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 56.9510
                       Mean reward: 12.09
               Mean episode length: 153.57
    Episode_Reward/reaching_object: 0.4144
     Episode_Reward/lifting_object: 1.8360
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0455
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 2.7500
     Episode_Termination/robot_out: 20.5417
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 1.99s
                      Time elapsed: 00:13:40
                               ETA: 01:00:30

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 48151 steps/s (collection: 1.947s, learning 0.094s)
             Mean action noise std: 2.17
          Mean value_function loss: 7.5834
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 56.9589
                       Mean reward: 10.65
               Mean episode length: 164.69
    Episode_Reward/reaching_object: 0.4226
     Episode_Reward/lifting_object: 1.7489
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 19.7917
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 2.04s
                      Time elapsed: 00:13:42
                               ETA: 01:00:27

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 49859 steps/s (collection: 1.885s, learning 0.087s)
             Mean action noise std: 2.17
          Mean value_function loss: 9.1319
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 56.9763
                       Mean reward: 12.12
               Mean episode length: 157.30
    Episode_Reward/reaching_object: 0.4171
     Episode_Reward/lifting_object: 1.8701
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 5.2500
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 21.2917
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 1.97s
                      Time elapsed: 00:13:44
                               ETA: 01:00:24

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 49768 steps/s (collection: 1.883s, learning 0.093s)
             Mean action noise std: 2.17
          Mean value_function loss: 10.2805
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 56.9968
                       Mean reward: 11.26
               Mean episode length: 141.58
    Episode_Reward/reaching_object: 0.4105
     Episode_Reward/lifting_object: 1.9118
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 21.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 1.98s
                      Time elapsed: 00:13:46
                               ETA: 01:00:21

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 49694 steps/s (collection: 1.887s, learning 0.092s)
             Mean action noise std: 2.17
          Mean value_function loss: 12.2665
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 57.0206
                       Mean reward: 14.11
               Mean episode length: 151.55
    Episode_Reward/reaching_object: 0.4149
     Episode_Reward/lifting_object: 2.1299
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 23.9583
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 1.98s
                      Time elapsed: 00:13:48
                               ETA: 01:00:17

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 48747 steps/s (collection: 1.926s, learning 0.091s)
             Mean action noise std: 2.18
          Mean value_function loss: 21.8312
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.0528
                       Mean reward: 14.55
               Mean episode length: 148.49
    Episode_Reward/reaching_object: 0.4166
     Episode_Reward/lifting_object: 2.1570
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 20.2083
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 2.02s
                      Time elapsed: 00:13:50
                               ETA: 01:00:14

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 48009 steps/s (collection: 1.948s, learning 0.099s)
             Mean action noise std: 2.18
          Mean value_function loss: 12.5237
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.0793
                       Mean reward: 11.40
               Mean episode length: 154.26
    Episode_Reward/reaching_object: 0.4121
     Episode_Reward/lifting_object: 1.5928
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 20.0417
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 2.05s
                      Time elapsed: 00:13:52
                               ETA: 01:00:11

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 48459 steps/s (collection: 1.921s, learning 0.108s)
             Mean action noise std: 2.18
          Mean value_function loss: 14.8964
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 57.0996
                       Mean reward: 9.47
               Mean episode length: 147.07
    Episode_Reward/reaching_object: 0.4144
     Episode_Reward/lifting_object: 2.1589
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 25.5417
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 2.03s
                      Time elapsed: 00:13:54
                               ETA: 01:00:08

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 48421 steps/s (collection: 1.915s, learning 0.116s)
             Mean action noise std: 2.18
          Mean value_function loss: 12.0593
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.1166
                       Mean reward: 9.71
               Mean episode length: 136.72
    Episode_Reward/reaching_object: 0.4018
     Episode_Reward/lifting_object: 1.6405
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 21.6667
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 2.03s
                      Time elapsed: 00:13:57
                               ETA: 01:00:05

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 49408 steps/s (collection: 1.895s, learning 0.095s)
             Mean action noise std: 2.18
          Mean value_function loss: 15.9134
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 57.1382
                       Mean reward: 13.48
               Mean episode length: 148.55
    Episode_Reward/reaching_object: 0.4091
     Episode_Reward/lifting_object: 1.9570
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 22.0833
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 1.99s
                      Time elapsed: 00:13:59
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 49590 steps/s (collection: 1.887s, learning 0.095s)
             Mean action noise std: 2.19
          Mean value_function loss: 11.2583
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 57.1668
                       Mean reward: 13.50
               Mean episode length: 141.29
    Episode_Reward/reaching_object: 0.4119
     Episode_Reward/lifting_object: 2.1412
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 24.6667
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 1.98s
                      Time elapsed: 00:14:00
                               ETA: 00:59:59

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 49587 steps/s (collection: 1.889s, learning 0.094s)
             Mean action noise std: 2.19
          Mean value_function loss: 22.2462
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 57.1994
                       Mean reward: 10.69
               Mean episode length: 145.67
    Episode_Reward/reaching_object: 0.4049
     Episode_Reward/lifting_object: 1.8632
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 25.2083
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 1.98s
                      Time elapsed: 00:14:02
                               ETA: 00:59:55

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 48954 steps/s (collection: 1.914s, learning 0.094s)
             Mean action noise std: 2.19
          Mean value_function loss: 29.3886
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.2275
                       Mean reward: 10.04
               Mean episode length: 130.37
    Episode_Reward/reaching_object: 0.3968
     Episode_Reward/lifting_object: 2.0893
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 26.7500
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 2.01s
                      Time elapsed: 00:14:04
                               ETA: 00:59:52

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 48849 steps/s (collection: 1.913s, learning 0.099s)
             Mean action noise std: 2.19
          Mean value_function loss: 17.8794
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 57.2605
                       Mean reward: 8.15
               Mean episode length: 140.87
    Episode_Reward/reaching_object: 0.3901
     Episode_Reward/lifting_object: 2.0710
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 2.3333
     Episode_Termination/robot_out: 27.0417
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 2.01s
                      Time elapsed: 00:14:06
                               ETA: 00:59:49

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 48748 steps/s (collection: 1.926s, learning 0.091s)
             Mean action noise std: 2.20
          Mean value_function loss: 19.6880
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 57.2888
                       Mean reward: 9.68
               Mean episode length: 144.85
    Episode_Reward/reaching_object: 0.3963
     Episode_Reward/lifting_object: 2.1971
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 23.4583
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 2.02s
                      Time elapsed: 00:14:09
                               ETA: 00:59:46

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 48604 steps/s (collection: 1.919s, learning 0.103s)
             Mean action noise std: 2.20
          Mean value_function loss: 14.6959
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 57.3098
                       Mean reward: 16.88
               Mean episode length: 135.40
    Episode_Reward/reaching_object: 0.3893
     Episode_Reward/lifting_object: 1.9077
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 23.7917
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 2.02s
                      Time elapsed: 00:14:11
                               ETA: 00:59:43

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 48954 steps/s (collection: 1.917s, learning 0.091s)
             Mean action noise std: 2.20
          Mean value_function loss: 17.3273
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.3349
                       Mean reward: 13.27
               Mean episode length: 142.54
    Episode_Reward/reaching_object: 0.3744
     Episode_Reward/lifting_object: 2.3009
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 24.0417
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 2.01s
                      Time elapsed: 00:14:13
                               ETA: 00:59:40

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 49797 steps/s (collection: 1.888s, learning 0.086s)
             Mean action noise std: 2.20
          Mean value_function loss: 18.6488
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.3653
                       Mean reward: 15.26
               Mean episode length: 140.43
    Episode_Reward/reaching_object: 0.3992
     Episode_Reward/lifting_object: 2.2259
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 23.4167
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 1.97s
                      Time elapsed: 00:14:15
                               ETA: 00:59:37

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 49185 steps/s (collection: 1.912s, learning 0.087s)
             Mean action noise std: 2.21
          Mean value_function loss: 18.9767
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 57.3982
                       Mean reward: 11.34
               Mean episode length: 150.33
    Episode_Reward/reaching_object: 0.3981
     Episode_Reward/lifting_object: 2.1282
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 2.00s
                      Time elapsed: 00:14:17
                               ETA: 00:59:34

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 49101 steps/s (collection: 1.905s, learning 0.097s)
             Mean action noise std: 2.21
          Mean value_function loss: 16.7982
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 57.4280
                       Mean reward: 10.60
               Mean episode length: 132.88
    Episode_Reward/reaching_object: 0.3838
     Episode_Reward/lifting_object: 2.2411
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 23.0833
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 2.00s
                      Time elapsed: 00:14:19
                               ETA: 00:59:31

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 48546 steps/s (collection: 1.930s, learning 0.095s)
             Mean action noise std: 2.21
          Mean value_function loss: 23.5316
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 57.4626
                       Mean reward: 15.08
               Mean episode length: 135.09
    Episode_Reward/reaching_object: 0.3979
     Episode_Reward/lifting_object: 2.1677
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 27.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 2.02s
                      Time elapsed: 00:14:21
                               ETA: 00:59:28

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 48697 steps/s (collection: 1.931s, learning 0.088s)
             Mean action noise std: 2.21
          Mean value_function loss: 19.1553
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 57.4948
                       Mean reward: 5.89
               Mean episode length: 154.62
    Episode_Reward/reaching_object: 0.4128
     Episode_Reward/lifting_object: 2.0985
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 25.5417
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 2.02s
                      Time elapsed: 00:14:23
                               ETA: 00:59:25

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 49001 steps/s (collection: 1.907s, learning 0.100s)
             Mean action noise std: 2.21
          Mean value_function loss: 35.1044
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.5157
                       Mean reward: 11.62
               Mean episode length: 127.44
    Episode_Reward/reaching_object: 0.4086
     Episode_Reward/lifting_object: 2.0935
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 27.2500
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 2.01s
                      Time elapsed: 00:14:25
                               ETA: 00:59:22

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 48154 steps/s (collection: 1.942s, learning 0.099s)
             Mean action noise std: 2.22
          Mean value_function loss: 15.2749
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 57.5345
                       Mean reward: 9.16
               Mean episode length: 131.12
    Episode_Reward/reaching_object: 0.4021
     Episode_Reward/lifting_object: 2.1743
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 30.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 2.04s
                      Time elapsed: 00:14:27
                               ETA: 00:59:19

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 48314 steps/s (collection: 1.931s, learning 0.104s)
             Mean action noise std: 2.22
          Mean value_function loss: 19.8590
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.5686
                       Mean reward: 12.18
               Mean episode length: 140.83
    Episode_Reward/reaching_object: 0.3960
     Episode_Reward/lifting_object: 2.3147
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 27.0417
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 2.03s
                      Time elapsed: 00:14:29
                               ETA: 00:59:16

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 48599 steps/s (collection: 1.934s, learning 0.089s)
             Mean action noise std: 2.22
          Mean value_function loss: 23.3570
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 57.6071
                       Mean reward: 16.64
               Mean episode length: 137.96
    Episode_Reward/reaching_object: 0.3823
     Episode_Reward/lifting_object: 2.2249
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 2.02s
                      Time elapsed: 00:14:31
                               ETA: 00:59:13

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 46329 steps/s (collection: 2.027s, learning 0.095s)
             Mean action noise std: 2.23
          Mean value_function loss: 59.4989
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.6350
                       Mean reward: 13.67
               Mean episode length: 127.10
    Episode_Reward/reaching_object: 0.3851
     Episode_Reward/lifting_object: 2.5652
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 28.6250
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 2.12s
                      Time elapsed: 00:14:33
                               ETA: 00:59:10

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 48154 steps/s (collection: 1.955s, learning 0.087s)
             Mean action noise std: 2.23
          Mean value_function loss: 41.6122
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.6642
                       Mean reward: 14.62
               Mean episode length: 135.30
    Episode_Reward/reaching_object: 0.3893
     Episode_Reward/lifting_object: 2.3164
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 2.04s
                      Time elapsed: 00:14:35
                               ETA: 00:59:07

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 48378 steps/s (collection: 1.943s, learning 0.089s)
             Mean action noise std: 2.23
          Mean value_function loss: 40.5268
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 57.6838
                       Mean reward: 13.85
               Mean episode length: 123.37
    Episode_Reward/reaching_object: 0.3844
     Episode_Reward/lifting_object: 1.9017
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 31.6250
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 2.03s
                      Time elapsed: 00:14:37
                               ETA: 00:59:04

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 47289 steps/s (collection: 1.993s, learning 0.086s)
             Mean action noise std: 2.23
          Mean value_function loss: 24.1132
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 57.6996
                       Mean reward: 15.51
               Mean episode length: 143.77
    Episode_Reward/reaching_object: 0.3849
     Episode_Reward/lifting_object: 2.3598
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 2.7083
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 2.08s
                      Time elapsed: 00:14:39
                               ETA: 00:59:02

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 48147 steps/s (collection: 1.947s, learning 0.094s)
             Mean action noise std: 2.23
          Mean value_function loss: 16.4034
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 57.7245
                       Mean reward: 13.84
               Mean episode length: 124.65
    Episode_Reward/reaching_object: 0.3715
     Episode_Reward/lifting_object: 2.4072
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 29.3333
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 2.04s
                      Time elapsed: 00:14:41
                               ETA: 00:58:59

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 46930 steps/s (collection: 2.006s, learning 0.089s)
             Mean action noise std: 2.23
          Mean value_function loss: 24.6548
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 57.7379
                       Mean reward: 12.78
               Mean episode length: 127.91
    Episode_Reward/reaching_object: 0.3806
     Episode_Reward/lifting_object: 2.5474
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 2.7083
     Episode_Termination/robot_out: 30.8750
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 2.09s
                      Time elapsed: 00:14:43
                               ETA: 00:58:56

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 48701 steps/s (collection: 1.931s, learning 0.087s)
             Mean action noise std: 2.23
          Mean value_function loss: 17.1776
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 57.7400
                       Mean reward: 9.91
               Mean episode length: 122.98
    Episode_Reward/reaching_object: 0.3768
     Episode_Reward/lifting_object: 2.1735
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 28.8750
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 2.02s
                      Time elapsed: 00:14:45
                               ETA: 00:58:53

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 47205 steps/s (collection: 1.989s, learning 0.094s)
             Mean action noise std: 2.23
          Mean value_function loss: 33.3780
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 57.7480
                       Mean reward: 11.77
               Mean episode length: 125.31
    Episode_Reward/reaching_object: 0.3773
     Episode_Reward/lifting_object: 2.3551
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 30.2917
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 2.08s
                      Time elapsed: 00:14:47
                               ETA: 00:58:50

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 47404 steps/s (collection: 1.988s, learning 0.086s)
             Mean action noise std: 2.24
          Mean value_function loss: 29.4955
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.7584
                       Mean reward: 13.68
               Mean episode length: 131.65
    Episode_Reward/reaching_object: 0.3547
     Episode_Reward/lifting_object: 2.1796
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 32.1250
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 2.07s
                      Time elapsed: 00:14:49
                               ETA: 00:58:48

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 48741 steps/s (collection: 1.923s, learning 0.094s)
             Mean action noise std: 2.24
          Mean value_function loss: 24.2273
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 57.7714
                       Mean reward: 13.47
               Mean episode length: 121.22
    Episode_Reward/reaching_object: 0.3613
     Episode_Reward/lifting_object: 2.1377
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 3.2917
     Episode_Termination/robot_out: 32.7083
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 2.02s
                      Time elapsed: 00:14:51
                               ETA: 00:58:45

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 47158 steps/s (collection: 1.979s, learning 0.106s)
             Mean action noise std: 2.24
          Mean value_function loss: 26.3542
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 57.7939
                       Mean reward: 12.71
               Mean episode length: 111.45
    Episode_Reward/reaching_object: 0.3491
     Episode_Reward/lifting_object: 2.4791
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 30.6250
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 2.08s
                      Time elapsed: 00:14:53
                               ETA: 00:58:42

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 48654 steps/s (collection: 1.921s, learning 0.099s)
             Mean action noise std: 2.24
          Mean value_function loss: 19.7243
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.8139
                       Mean reward: 11.06
               Mean episode length: 114.61
    Episode_Reward/reaching_object: 0.3477
     Episode_Reward/lifting_object: 1.8335
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 34.2083
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 2.02s
                      Time elapsed: 00:14:55
                               ETA: 00:58:39

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 48637 steps/s (collection: 1.922s, learning 0.100s)
             Mean action noise std: 2.24
          Mean value_function loss: 17.6688
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.8278
                       Mean reward: 8.62
               Mean episode length: 114.71
    Episode_Reward/reaching_object: 0.3502
     Episode_Reward/lifting_object: 2.3055
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 32.2917
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 2.02s
                      Time elapsed: 00:14:57
                               ETA: 00:58:36

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 49077 steps/s (collection: 1.913s, learning 0.090s)
             Mean action noise std: 2.24
          Mean value_function loss: 12.1597
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 57.8417
                       Mean reward: 15.64
               Mean episode length: 115.78
    Episode_Reward/reaching_object: 0.3489
     Episode_Reward/lifting_object: 2.6819
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 2.7917
     Episode_Termination/robot_out: 34.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 2.00s
                      Time elapsed: 00:14:59
                               ETA: 00:58:33

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 48965 steps/s (collection: 1.915s, learning 0.093s)
             Mean action noise std: 2.25
          Mean value_function loss: 31.5114
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.8616
                       Mean reward: 16.25
               Mean episode length: 116.73
    Episode_Reward/reaching_object: 0.3348
     Episode_Reward/lifting_object: 2.6071
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 31.2083
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 2.01s
                      Time elapsed: 00:15:01
                               ETA: 00:58:30

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 48654 steps/s (collection: 1.921s, learning 0.100s)
             Mean action noise std: 2.25
          Mean value_function loss: 39.2960
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 57.8892
                       Mean reward: 12.03
               Mean episode length: 126.52
    Episode_Reward/reaching_object: 0.3436
     Episode_Reward/lifting_object: 2.1274
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 32.6667
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 2.02s
                      Time elapsed: 00:15:03
                               ETA: 00:58:27

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 48058 steps/s (collection: 1.953s, learning 0.093s)
             Mean action noise std: 2.25
          Mean value_function loss: 38.7632
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 57.9180
                       Mean reward: 10.79
               Mean episode length: 114.22
    Episode_Reward/reaching_object: 0.3413
     Episode_Reward/lifting_object: 2.4232
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 2.6250
     Episode_Termination/robot_out: 34.0417
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 2.05s
                      Time elapsed: 00:15:05
                               ETA: 00:58:24

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 47685 steps/s (collection: 1.971s, learning 0.091s)
             Mean action noise std: 2.25
          Mean value_function loss: 26.7728
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 57.9376
                       Mean reward: 13.93
               Mean episode length: 107.07
    Episode_Reward/reaching_object: 0.3301
     Episode_Reward/lifting_object: 2.3222
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 2.5417
     Episode_Termination/robot_out: 35.0833
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 2.06s
                      Time elapsed: 00:15:08
                               ETA: 00:58:22

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 47425 steps/s (collection: 1.968s, learning 0.105s)
             Mean action noise std: 2.25
          Mean value_function loss: 26.0729
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.9457
                       Mean reward: 12.07
               Mean episode length: 111.33
    Episode_Reward/reaching_object: 0.3332
     Episode_Reward/lifting_object: 2.3853
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 2.5833
     Episode_Termination/robot_out: 33.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 2.07s
                      Time elapsed: 00:15:10
                               ETA: 00:58:19

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 47985 steps/s (collection: 1.959s, learning 0.090s)
             Mean action noise std: 2.25
          Mean value_function loss: 29.9712
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 57.9610
                       Mean reward: 11.58
               Mean episode length: 117.95
    Episode_Reward/reaching_object: 0.3410
     Episode_Reward/lifting_object: 2.5749
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 32.5000
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 2.05s
                      Time elapsed: 00:15:12
                               ETA: 00:58:16

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 47146 steps/s (collection: 1.995s, learning 0.091s)
             Mean action noise std: 2.26
          Mean value_function loss: 35.5958
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.9884
                       Mean reward: 17.72
               Mean episode length: 115.35
    Episode_Reward/reaching_object: 0.3412
     Episode_Reward/lifting_object: 2.6941
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 33.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 2.09s
                      Time elapsed: 00:15:14
                               ETA: 00:58:13

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 48267 steps/s (collection: 1.949s, learning 0.088s)
             Mean action noise std: 2.26
          Mean value_function loss: 28.9699
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 58.0111
                       Mean reward: 7.39
               Mean episode length: 96.72
    Episode_Reward/reaching_object: 0.3350
     Episode_Reward/lifting_object: 2.2652
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 2.3333
     Episode_Termination/robot_out: 36.6250
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 2.04s
                      Time elapsed: 00:15:16
                               ETA: 00:58:11

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 48575 steps/s (collection: 1.939s, learning 0.085s)
             Mean action noise std: 2.26
          Mean value_function loss: 25.5247
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.0371
                       Mean reward: 14.44
               Mean episode length: 107.80
    Episode_Reward/reaching_object: 0.3510
     Episode_Reward/lifting_object: 2.7963
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 2.5833
     Episode_Termination/robot_out: 33.2083
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 2.02s
                      Time elapsed: 00:15:18
                               ETA: 00:58:08

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 47820 steps/s (collection: 1.964s, learning 0.092s)
             Mean action noise std: 2.26
          Mean value_function loss: 20.0068
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.0561
                       Mean reward: 16.61
               Mean episode length: 107.46
    Episode_Reward/reaching_object: 0.3290
     Episode_Reward/lifting_object: 2.4731
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 36.9583
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 2.06s
                      Time elapsed: 00:15:20
                               ETA: 00:58:05

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 47144 steps/s (collection: 1.984s, learning 0.102s)
             Mean action noise std: 2.26
          Mean value_function loss: 19.8925
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 58.0677
                       Mean reward: 15.73
               Mean episode length: 112.82
    Episode_Reward/reaching_object: 0.3306
     Episode_Reward/lifting_object: 2.5377
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 35.2917
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 2.09s
                      Time elapsed: 00:15:22
                               ETA: 00:58:02

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 46794 steps/s (collection: 1.997s, learning 0.104s)
             Mean action noise std: 2.26
          Mean value_function loss: 17.8331
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.0835
                       Mean reward: 15.79
               Mean episode length: 102.25
    Episode_Reward/reaching_object: 0.3198
     Episode_Reward/lifting_object: 2.6977
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 34.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 2.10s
                      Time elapsed: 00:15:24
                               ETA: 00:58:00

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 47729 steps/s (collection: 1.962s, learning 0.098s)
             Mean action noise std: 2.27
          Mean value_function loss: 27.4020
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.0999
                       Mean reward: 18.62
               Mean episode length: 121.34
    Episode_Reward/reaching_object: 0.3361
     Episode_Reward/lifting_object: 2.9970
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 32.1250
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 2.06s
                      Time elapsed: 00:15:26
                               ETA: 00:57:57

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 47972 steps/s (collection: 1.947s, learning 0.103s)
             Mean action noise std: 2.27
          Mean value_function loss: 29.2318
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.1180
                       Mean reward: 13.90
               Mean episode length: 120.31
    Episode_Reward/reaching_object: 0.3410
     Episode_Reward/lifting_object: 2.7847
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 33.4167
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 2.05s
                      Time elapsed: 00:15:28
                               ETA: 00:57:54

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 48047 steps/s (collection: 1.949s, learning 0.097s)
             Mean action noise std: 2.27
          Mean value_function loss: 23.3889
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.1404
                       Mean reward: 14.49
               Mean episode length: 114.41
    Episode_Reward/reaching_object: 0.3368
     Episode_Reward/lifting_object: 2.5932
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 2.5000
     Episode_Termination/robot_out: 34.3333
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 2.05s
                      Time elapsed: 00:15:30
                               ETA: 00:57:51

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 47418 steps/s (collection: 1.984s, learning 0.089s)
             Mean action noise std: 2.27
          Mean value_function loss: 24.0491
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 58.1549
                       Mean reward: 13.17
               Mean episode length: 114.86
    Episode_Reward/reaching_object: 0.3424
     Episode_Reward/lifting_object: 2.7428
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 33.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 2.07s
                      Time elapsed: 00:15:32
                               ETA: 00:57:49

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 46084 steps/s (collection: 2.045s, learning 0.089s)
             Mean action noise std: 2.27
          Mean value_function loss: 45.7699
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.1649
                       Mean reward: 13.35
               Mean episode length: 111.66
    Episode_Reward/reaching_object: 0.3459
     Episode_Reward/lifting_object: 2.8213
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 35.6250
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 2.13s
                      Time elapsed: 00:15:34
                               ETA: 00:57:46

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 48631 steps/s (collection: 1.923s, learning 0.099s)
             Mean action noise std: 2.27
          Mean value_function loss: 19.2262
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 58.1841
                       Mean reward: 10.20
               Mean episode length: 106.93
    Episode_Reward/reaching_object: 0.3359
     Episode_Reward/lifting_object: 2.7419
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 32.7500
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 2.02s
                      Time elapsed: 00:15:36
                               ETA: 00:57:43

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 47537 steps/s (collection: 1.976s, learning 0.092s)
             Mean action noise std: 2.27
          Mean value_function loss: 28.7893
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 58.2000
                       Mean reward: 8.92
               Mean episode length: 109.51
    Episode_Reward/reaching_object: 0.3344
     Episode_Reward/lifting_object: 2.3857
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 34.8750
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 2.07s
                      Time elapsed: 00:15:38
                               ETA: 00:57:41

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 48655 steps/s (collection: 1.927s, learning 0.094s)
             Mean action noise std: 2.27
          Mean value_function loss: 31.1216
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 58.2063
                       Mean reward: 15.12
               Mean episode length: 112.27
    Episode_Reward/reaching_object: 0.3298
     Episode_Reward/lifting_object: 2.6457
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 2.4583
     Episode_Termination/robot_out: 34.5000
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 2.02s
                      Time elapsed: 00:15:41
                               ETA: 00:57:38

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 47812 steps/s (collection: 1.942s, learning 0.114s)
             Mean action noise std: 2.28
          Mean value_function loss: 45.9804
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 58.2209
                       Mean reward: 15.15
               Mean episode length: 108.20
    Episode_Reward/reaching_object: 0.3237
     Episode_Reward/lifting_object: 2.7552
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 2.4583
     Episode_Termination/robot_out: 36.0833
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 2.06s
                      Time elapsed: 00:15:43
                               ETA: 00:57:35

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 48563 steps/s (collection: 1.937s, learning 0.087s)
             Mean action noise std: 2.28
          Mean value_function loss: 26.3176
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 58.2388
                       Mean reward: 15.15
               Mean episode length: 105.94
    Episode_Reward/reaching_object: 0.3268
     Episode_Reward/lifting_object: 2.3700
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 2.7917
     Episode_Termination/robot_out: 37.2083
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 2.02s
                      Time elapsed: 00:15:45
                               ETA: 00:57:32

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 47791 steps/s (collection: 1.971s, learning 0.086s)
             Mean action noise std: 2.28
          Mean value_function loss: 20.9466
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.2471
                       Mean reward: 21.87
               Mean episode length: 112.18
    Episode_Reward/reaching_object: 0.3151
     Episode_Reward/lifting_object: 2.9864
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5417
     Episode_Termination/robot_out: 37.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.06s
                      Time elapsed: 00:15:47
                               ETA: 00:57:30

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 48858 steps/s (collection: 1.925s, learning 0.087s)
             Mean action noise std: 2.28
          Mean value_function loss: 36.3502
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 58.2583
                       Mean reward: 14.82
               Mean episode length: 110.68
    Episode_Reward/reaching_object: 0.3192
     Episode_Reward/lifting_object: 3.0313
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 36.0417
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 2.01s
                      Time elapsed: 00:15:49
                               ETA: 00:57:27

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 48444 steps/s (collection: 1.935s, learning 0.094s)
             Mean action noise std: 2.28
          Mean value_function loss: 31.7796
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.2760
                       Mean reward: 13.83
               Mean episode length: 108.80
    Episode_Reward/reaching_object: 0.3200
     Episode_Reward/lifting_object: 2.7275
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.5417
     Episode_Termination/robot_out: 36.5417
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 2.03s
                      Time elapsed: 00:15:51
                               ETA: 00:57:24

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 47223 steps/s (collection: 1.973s, learning 0.109s)
             Mean action noise std: 2.28
          Mean value_function loss: 22.4183
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 58.2932
                       Mean reward: 13.97
               Mean episode length: 104.65
    Episode_Reward/reaching_object: 0.3093
     Episode_Reward/lifting_object: 2.9833
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 2.5000
     Episode_Termination/robot_out: 38.2500
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 2.08s
                      Time elapsed: 00:15:53
                               ETA: 00:57:21

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 47572 steps/s (collection: 1.948s, learning 0.119s)
             Mean action noise std: 2.28
          Mean value_function loss: 20.3353
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.3012
                       Mean reward: 14.34
               Mean episode length: 109.30
    Episode_Reward/reaching_object: 0.3098
     Episode_Reward/lifting_object: 2.9094
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 36.2083
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 2.07s
                      Time elapsed: 00:15:55
                               ETA: 00:57:19

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 48067 steps/s (collection: 1.938s, learning 0.107s)
             Mean action noise std: 2.28
          Mean value_function loss: 40.1380
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 58.3109
                       Mean reward: 8.55
               Mean episode length: 103.69
    Episode_Reward/reaching_object: 0.2997
     Episode_Reward/lifting_object: 2.6482
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 36.7500
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 2.05s
                      Time elapsed: 00:15:57
                               ETA: 00:57:16

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 48446 steps/s (collection: 1.940s, learning 0.089s)
             Mean action noise std: 2.29
          Mean value_function loss: 34.5799
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.3271
                       Mean reward: 14.56
               Mean episode length: 105.09
    Episode_Reward/reaching_object: 0.3045
     Episode_Reward/lifting_object: 2.8761
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 37.8750
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 2.03s
                      Time elapsed: 00:15:59
                               ETA: 00:57:13

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 48148 steps/s (collection: 1.941s, learning 0.101s)
             Mean action noise std: 2.29
          Mean value_function loss: 15.2946
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.3452
                       Mean reward: 18.28
               Mean episode length: 103.96
    Episode_Reward/reaching_object: 0.3019
     Episode_Reward/lifting_object: 3.0528
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.8333
     Episode_Termination/robot_out: 37.6667
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 2.04s
                      Time elapsed: 00:16:01
                               ETA: 00:57:10

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 49051 steps/s (collection: 1.918s, learning 0.086s)
             Mean action noise std: 2.29
          Mean value_function loss: 23.9630
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.3554
                       Mean reward: 13.71
               Mean episode length: 101.14
    Episode_Reward/reaching_object: 0.2963
     Episode_Reward/lifting_object: 2.8147
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 39.2500
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 2.00s
                      Time elapsed: 00:16:03
                               ETA: 00:57:08

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 47455 steps/s (collection: 1.985s, learning 0.086s)
             Mean action noise std: 2.29
          Mean value_function loss: 23.3939
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.3661
                       Mean reward: 18.21
               Mean episode length: 103.09
    Episode_Reward/reaching_object: 0.2909
     Episode_Reward/lifting_object: 2.8291
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 2.6250
     Episode_Termination/robot_out: 37.1667
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 2.07s
                      Time elapsed: 00:16:05
                               ETA: 00:57:05

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 47731 steps/s (collection: 1.969s, learning 0.090s)
             Mean action noise std: 2.29
          Mean value_function loss: 37.5105
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 58.3793
                       Mean reward: 11.32
               Mean episode length: 97.68
    Episode_Reward/reaching_object: 0.2873
     Episode_Reward/lifting_object: 2.7657
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 36.4167
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 2.06s
                      Time elapsed: 00:16:07
                               ETA: 00:57:02

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 48867 steps/s (collection: 1.924s, learning 0.088s)
             Mean action noise std: 2.29
          Mean value_function loss: 30.9550
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.3873
                       Mean reward: 15.00
               Mean episode length: 101.98
    Episode_Reward/reaching_object: 0.3007
     Episode_Reward/lifting_object: 2.7679
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 39.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 2.01s
                      Time elapsed: 00:16:09
                               ETA: 00:56:59

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 47455 steps/s (collection: 1.950s, learning 0.122s)
             Mean action noise std: 2.29
          Mean value_function loss: 28.7291
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.4026
                       Mean reward: 16.80
               Mean episode length: 105.38
    Episode_Reward/reaching_object: 0.3029
     Episode_Reward/lifting_object: 2.9351
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.5000
     Episode_Termination/robot_out: 34.2917
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 2.07s
                      Time elapsed: 00:16:11
                               ETA: 00:56:57

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 45404 steps/s (collection: 2.076s, learning 0.090s)
             Mean action noise std: 2.29
          Mean value_function loss: 33.9527
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.4232
                       Mean reward: 17.20
               Mean episode length: 106.47
    Episode_Reward/reaching_object: 0.2933
     Episode_Reward/lifting_object: 2.9185
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 39.3333
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 2.17s
                      Time elapsed: 00:16:13
                               ETA: 00:56:55

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 48340 steps/s (collection: 1.945s, learning 0.089s)
             Mean action noise std: 2.30
          Mean value_function loss: 27.1862
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.4419
                       Mean reward: 15.94
               Mean episode length: 100.51
    Episode_Reward/reaching_object: 0.2930
     Episode_Reward/lifting_object: 2.8978
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 37.9583
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 2.03s
                      Time elapsed: 00:16:15
                               ETA: 00:56:52

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 47263 steps/s (collection: 1.974s, learning 0.106s)
             Mean action noise std: 2.30
          Mean value_function loss: 23.8025
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 58.4531
                       Mean reward: 15.37
               Mean episode length: 100.21
    Episode_Reward/reaching_object: 0.2960
     Episode_Reward/lifting_object: 3.1014
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 38.3333
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 2.08s
                      Time elapsed: 00:16:17
                               ETA: 00:56:49

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 47894 steps/s (collection: 1.952s, learning 0.101s)
             Mean action noise std: 2.30
          Mean value_function loss: 31.8246
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 58.4606
                       Mean reward: 17.00
               Mean episode length: 107.90
    Episode_Reward/reaching_object: 0.2900
     Episode_Reward/lifting_object: 2.8139
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 39.1250
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 2.05s
                      Time elapsed: 00:16:20
                               ETA: 00:56:46

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 47584 steps/s (collection: 1.968s, learning 0.098s)
             Mean action noise std: 2.30
          Mean value_function loss: 38.6886
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 58.4700
                       Mean reward: 15.78
               Mean episode length: 100.46
    Episode_Reward/reaching_object: 0.2920
     Episode_Reward/lifting_object: 2.9963
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 41.2500
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 2.07s
                      Time elapsed: 00:16:22
                               ETA: 00:56:44

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 46237 steps/s (collection: 2.022s, learning 0.105s)
             Mean action noise std: 2.30
          Mean value_function loss: 46.2694
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 58.4817
                       Mean reward: 21.72
               Mean episode length: 95.20
    Episode_Reward/reaching_object: 0.2897
     Episode_Reward/lifting_object: 2.6432
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4583
     Episode_Termination/robot_out: 40.1667
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 2.13s
                      Time elapsed: 00:16:24
                               ETA: 00:56:41

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 48209 steps/s (collection: 1.932s, learning 0.107s)
             Mean action noise std: 2.30
          Mean value_function loss: 39.6001
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 58.5010
                       Mean reward: 13.83
               Mean episode length: 96.07
    Episode_Reward/reaching_object: 0.2815
     Episode_Reward/lifting_object: 2.5442
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.6250
     Episode_Termination/robot_out: 41.1250
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 2.04s
                      Time elapsed: 00:16:26
                               ETA: 00:56:39

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 48415 steps/s (collection: 1.927s, learning 0.103s)
             Mean action noise std: 2.30
          Mean value_function loss: 33.7059
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 58.5215
                       Mean reward: 17.54
               Mean episode length: 94.33
    Episode_Reward/reaching_object: 0.2971
     Episode_Reward/lifting_object: 3.1282
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.1667
     Episode_Termination/robot_out: 40.1667
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 2.03s
                      Time elapsed: 00:16:28
                               ETA: 00:56:36

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 47944 steps/s (collection: 1.952s, learning 0.098s)
             Mean action noise std: 2.30
          Mean value_function loss: 29.1987
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.5313
                       Mean reward: 13.96
               Mean episode length: 103.82
    Episode_Reward/reaching_object: 0.2828
     Episode_Reward/lifting_object: 2.9716
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.9167
     Episode_Termination/robot_out: 38.1250
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 2.05s
                      Time elapsed: 00:16:30
                               ETA: 00:56:33

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 48095 steps/s (collection: 1.950s, learning 0.094s)
             Mean action noise std: 2.30
          Mean value_function loss: 50.1049
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 58.5411
                       Mean reward: 19.42
               Mean episode length: 94.39
    Episode_Reward/reaching_object: 0.2791
     Episode_Reward/lifting_object: 2.8349
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4583
     Episode_Termination/robot_out: 42.2083
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.04s
                      Time elapsed: 00:16:32
                               ETA: 00:56:31

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 47887 steps/s (collection: 1.946s, learning 0.107s)
             Mean action noise std: 2.31
          Mean value_function loss: 48.5135
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 58.5494
                       Mean reward: 15.33
               Mean episode length: 93.85
    Episode_Reward/reaching_object: 0.2793
     Episode_Reward/lifting_object: 2.5910
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.3333
     Episode_Termination/robot_out: 40.5000
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 2.05s
                      Time elapsed: 00:16:34
                               ETA: 00:56:28

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 47994 steps/s (collection: 1.935s, learning 0.114s)
             Mean action noise std: 2.31
          Mean value_function loss: 45.2828
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 58.5583
                       Mean reward: 18.63
               Mean episode length: 102.20
    Episode_Reward/reaching_object: 0.2843
     Episode_Reward/lifting_object: 2.9885
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 39.8333
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 2.05s
                      Time elapsed: 00:16:36
                               ETA: 00:56:25

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 46050 steps/s (collection: 2.032s, learning 0.103s)
             Mean action noise std: 2.31
          Mean value_function loss: 45.9905
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 58.5665
                       Mean reward: 16.38
               Mean episode length: 96.94
    Episode_Reward/reaching_object: 0.2744
     Episode_Reward/lifting_object: 2.5216
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5417
     Episode_Termination/robot_out: 40.5833
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 2.13s
                      Time elapsed: 00:16:38
                               ETA: 00:56:23

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 47055 steps/s (collection: 1.983s, learning 0.106s)
             Mean action noise std: 2.31
          Mean value_function loss: 61.8390
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 58.5747
                       Mean reward: 15.98
               Mean episode length: 100.25
    Episode_Reward/reaching_object: 0.2858
     Episode_Reward/lifting_object: 2.9616
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 39.8333
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 2.09s
                      Time elapsed: 00:16:40
                               ETA: 00:56:20

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 47021 steps/s (collection: 1.998s, learning 0.093s)
             Mean action noise std: 2.31
          Mean value_function loss: 41.0959
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 58.5924
                       Mean reward: 18.45
               Mean episode length: 89.53
    Episode_Reward/reaching_object: 0.2870
     Episode_Reward/lifting_object: 3.3816
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7083
     Episode_Termination/robot_out: 42.5417
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 2.09s
                      Time elapsed: 00:16:42
                               ETA: 00:56:18

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 46858 steps/s (collection: 2.008s, learning 0.090s)
             Mean action noise std: 2.31
          Mean value_function loss: 55.6459
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.6134
                       Mean reward: 6.18
               Mean episode length: 95.37
    Episode_Reward/reaching_object: 0.2787
     Episode_Reward/lifting_object: 2.5770
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 42.8750
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 2.10s
                      Time elapsed: 00:16:44
                               ETA: 00:56:15

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 47884 steps/s (collection: 1.963s, learning 0.090s)
             Mean action noise std: 2.31
          Mean value_function loss: 57.3943
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.6304
                       Mean reward: 14.48
               Mean episode length: 97.02
    Episode_Reward/reaching_object: 0.2761
     Episode_Reward/lifting_object: 2.9708
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7083
     Episode_Termination/robot_out: 43.7917
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 2.05s
                      Time elapsed: 00:16:46
                               ETA: 00:56:13

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 46676 steps/s (collection: 2.014s, learning 0.093s)
             Mean action noise std: 2.31
          Mean value_function loss: 51.3779
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.6447
                       Mean reward: 15.21
               Mean episode length: 97.11
    Episode_Reward/reaching_object: 0.2692
     Episode_Reward/lifting_object: 2.8477
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 42.1250
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.11s
                      Time elapsed: 00:16:49
                               ETA: 00:56:10

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 46955 steps/s (collection: 2.004s, learning 0.090s)
             Mean action noise std: 2.32
          Mean value_function loss: 30.3126
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.6558
                       Mean reward: 16.95
               Mean episode length: 88.16
    Episode_Reward/reaching_object: 0.2736
     Episode_Reward/lifting_object: 3.0860
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 41.9167
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 2.09s
                      Time elapsed: 00:16:51
                               ETA: 00:56:08

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 47335 steps/s (collection: 1.986s, learning 0.091s)
             Mean action noise std: 2.32
          Mean value_function loss: 45.5171
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 58.6672
                       Mean reward: 12.77
               Mean episode length: 90.76
    Episode_Reward/reaching_object: 0.2640
     Episode_Reward/lifting_object: 2.9748
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 3.0417
     Episode_Termination/robot_out: 42.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 2.08s
                      Time elapsed: 00:16:53
                               ETA: 00:56:05

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 46804 steps/s (collection: 2.007s, learning 0.093s)
             Mean action noise std: 2.32
          Mean value_function loss: 44.2146
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 58.6843
                       Mean reward: 18.41
               Mean episode length: 91.74
    Episode_Reward/reaching_object: 0.2675
     Episode_Reward/lifting_object: 3.2218
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7500
     Episode_Termination/robot_out: 40.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 2.10s
                      Time elapsed: 00:16:55
                               ETA: 00:56:03

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 47227 steps/s (collection: 1.991s, learning 0.091s)
             Mean action noise std: 2.32
          Mean value_function loss: 60.9482
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 58.6951
                       Mean reward: 20.03
               Mean episode length: 89.91
    Episode_Reward/reaching_object: 0.2645
     Episode_Reward/lifting_object: 3.1595
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7500
     Episode_Termination/robot_out: 42.4583
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.08s
                      Time elapsed: 00:16:57
                               ETA: 00:56:00

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 47112 steps/s (collection: 1.995s, learning 0.092s)
             Mean action noise std: 2.32
          Mean value_function loss: 33.3065
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 58.7001
                       Mean reward: 7.69
               Mean episode length: 87.28
    Episode_Reward/reaching_object: 0.2737
     Episode_Reward/lifting_object: 2.8894
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 40.6667
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 2.09s
                      Time elapsed: 00:16:59
                               ETA: 00:55:58

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 47951 steps/s (collection: 1.959s, learning 0.091s)
             Mean action noise std: 2.32
          Mean value_function loss: 33.8506
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 58.7125
                       Mean reward: 17.03
               Mean episode length: 91.85
    Episode_Reward/reaching_object: 0.2740
     Episode_Reward/lifting_object: 3.1494
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.3750
     Episode_Termination/robot_out: 40.4167
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.05s
                      Time elapsed: 00:17:01
                               ETA: 00:55:55

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 47335 steps/s (collection: 1.985s, learning 0.092s)
             Mean action noise std: 2.32
          Mean value_function loss: 47.7563
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.7235
                       Mean reward: 19.98
               Mean episode length: 84.71
    Episode_Reward/reaching_object: 0.2824
     Episode_Reward/lifting_object: 3.4465
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 40.5417
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 2.08s
                      Time elapsed: 00:17:03
                               ETA: 00:55:52

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 46377 steps/s (collection: 2.013s, learning 0.107s)
             Mean action noise std: 2.32
          Mean value_function loss: 45.1493
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 58.7328
                       Mean reward: 24.34
               Mean episode length: 102.14
    Episode_Reward/reaching_object: 0.2782
     Episode_Reward/lifting_object: 3.4417
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5417
     Episode_Termination/robot_out: 43.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 2.12s
                      Time elapsed: 00:17:05
                               ETA: 00:55:50

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 47068 steps/s (collection: 1.981s, learning 0.107s)
             Mean action noise std: 2.32
          Mean value_function loss: 24.3584
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 58.7398
                       Mean reward: 18.55
               Mean episode length: 94.16
    Episode_Reward/reaching_object: 0.2738
     Episode_Reward/lifting_object: 3.3768
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 41.5000
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 2.09s
                      Time elapsed: 00:17:07
                               ETA: 00:55:48

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 46025 steps/s (collection: 2.021s, learning 0.115s)
             Mean action noise std: 2.32
          Mean value_function loss: 34.3030
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 58.7456
                       Mean reward: 16.68
               Mean episode length: 89.26
    Episode_Reward/reaching_object: 0.2739
     Episode_Reward/lifting_object: 3.6249
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 43.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 2.14s
                      Time elapsed: 00:17:09
                               ETA: 00:55:45

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 45417 steps/s (collection: 2.072s, learning 0.093s)
             Mean action noise std: 2.32
          Mean value_function loss: 26.9317
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 58.7545
                       Mean reward: 4.17
               Mean episode length: 90.84
    Episode_Reward/reaching_object: 0.2668
     Episode_Reward/lifting_object: 3.2058
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 3.0417
     Episode_Termination/robot_out: 40.3750
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 2.16s
                      Time elapsed: 00:17:12
                               ETA: 00:55:43

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 47412 steps/s (collection: 1.983s, learning 0.091s)
             Mean action noise std: 2.32
          Mean value_function loss: 26.4667
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.7592
                       Mean reward: 21.04
               Mean episode length: 91.04
    Episode_Reward/reaching_object: 0.2552
     Episode_Reward/lifting_object: 3.4052
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.2500
     Episode_Termination/robot_out: 45.9167
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.07s
                      Time elapsed: 00:17:14
                               ETA: 00:55:40

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 43737 steps/s (collection: 2.057s, learning 0.191s)
             Mean action noise std: 2.32
          Mean value_function loss: 30.4650
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 58.7657
                       Mean reward: 15.24
               Mean episode length: 97.53
    Episode_Reward/reaching_object: 0.2645
     Episode_Reward/lifting_object: 3.5234
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5417
     Episode_Termination/robot_out: 41.3750
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 2.25s
                      Time elapsed: 00:17:16
                               ETA: 00:55:38

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 41872 steps/s (collection: 2.225s, learning 0.123s)
             Mean action noise std: 2.33
          Mean value_function loss: 46.9675
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 58.7713
                       Mean reward: 19.18
               Mean episode length: 99.19
    Episode_Reward/reaching_object: 0.2614
     Episode_Reward/lifting_object: 3.5308
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 41.2500
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 2.35s
                      Time elapsed: 00:17:18
                               ETA: 00:55:37

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 42405 steps/s (collection: 2.216s, learning 0.103s)
             Mean action noise std: 2.33
          Mean value_function loss: 46.0919
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.7794
                       Mean reward: 13.47
               Mean episode length: 93.16
    Episode_Reward/reaching_object: 0.2616
     Episode_Reward/lifting_object: 3.1251
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.8333
     Episode_Termination/robot_out: 42.0833
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 2.32s
                      Time elapsed: 00:17:21
                               ETA: 00:55:35

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 43181 steps/s (collection: 2.187s, learning 0.090s)
             Mean action noise std: 2.33
          Mean value_function loss: 34.3840
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 58.7920
                       Mean reward: 19.63
               Mean episode length: 92.91
    Episode_Reward/reaching_object: 0.2666
     Episode_Reward/lifting_object: 3.7015
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.3750
     Episode_Termination/robot_out: 41.1667
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 2.28s
                      Time elapsed: 00:17:23
                               ETA: 00:55:33

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 41089 steps/s (collection: 2.284s, learning 0.108s)
             Mean action noise std: 2.33
          Mean value_function loss: 33.1853
               Mean surrogate loss: 0.0143
                 Mean entropy loss: 58.8019
                       Mean reward: 19.56
               Mean episode length: 95.76
    Episode_Reward/reaching_object: 0.2692
     Episode_Reward/lifting_object: 3.4089
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 43.8750
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 2.39s
                      Time elapsed: 00:17:25
                               ETA: 00:55:32

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 42685 steps/s (collection: 2.201s, learning 0.102s)
             Mean action noise std: 2.33
          Mean value_function loss: 28.3133
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 58.8028
                       Mean reward: 20.59
               Mean episode length: 94.73
    Episode_Reward/reaching_object: 0.2682
     Episode_Reward/lifting_object: 3.7919
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 44.9583
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.30s
                      Time elapsed: 00:17:28
                               ETA: 00:55:30

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 42058 steps/s (collection: 2.239s, learning 0.098s)
             Mean action noise std: 2.33
          Mean value_function loss: 33.9074
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 58.8047
                       Mean reward: 22.83
               Mean episode length: 83.58
    Episode_Reward/reaching_object: 0.2723
     Episode_Reward/lifting_object: 4.0386
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6250
     Episode_Termination/robot_out: 44.8750
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 2.34s
                      Time elapsed: 00:17:30
                               ETA: 00:55:28

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 40099 steps/s (collection: 2.273s, learning 0.178s)
             Mean action noise std: 2.33
          Mean value_function loss: 44.2621
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 58.8061
                       Mean reward: 13.20
               Mean episode length: 88.20
    Episode_Reward/reaching_object: 0.2631
     Episode_Reward/lifting_object: 3.6809
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7917
     Episode_Termination/robot_out: 47.1667
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 2.45s
                      Time elapsed: 00:17:32
                               ETA: 00:55:27

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 41956 steps/s (collection: 2.239s, learning 0.104s)
             Mean action noise std: 2.33
          Mean value_function loss: 49.7579
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.8117
                       Mean reward: 21.06
               Mean episode length: 85.56
    Episode_Reward/reaching_object: 0.2705
     Episode_Reward/lifting_object: 3.8479
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 43.7500
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 2.34s
                      Time elapsed: 00:17:35
                               ETA: 00:55:25

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 43070 steps/s (collection: 2.150s, learning 0.133s)
             Mean action noise std: 2.33
          Mean value_function loss: 37.0697
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 58.8247
                       Mean reward: 20.78
               Mean episode length: 80.74
    Episode_Reward/reaching_object: 0.2652
     Episode_Reward/lifting_object: 3.9201
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7917
     Episode_Termination/robot_out: 44.3333
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.28s
                      Time elapsed: 00:17:37
                               ETA: 00:55:23

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 42093 steps/s (collection: 2.167s, learning 0.169s)
             Mean action noise std: 2.33
          Mean value_function loss: 54.6512
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.8390
                       Mean reward: 18.28
               Mean episode length: 88.15
    Episode_Reward/reaching_object: 0.2607
     Episode_Reward/lifting_object: 3.3163
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3333
     Episode_Termination/robot_out: 42.7083
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 2.34s
                      Time elapsed: 00:17:39
                               ETA: 00:55:21

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 43316 steps/s (collection: 2.119s, learning 0.150s)
             Mean action noise std: 2.33
          Mean value_function loss: 50.1404
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 58.8590
                       Mean reward: 16.38
               Mean episode length: 90.23
    Episode_Reward/reaching_object: 0.2777
     Episode_Reward/lifting_object: 3.6332
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.9167
     Episode_Termination/robot_out: 44.1667
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.27s
                      Time elapsed: 00:17:42
                               ETA: 00:55:19

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 41958 steps/s (collection: 2.213s, learning 0.130s)
             Mean action noise std: 2.33
          Mean value_function loss: 65.4432
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 58.8708
                       Mean reward: 19.41
               Mean episode length: 83.31
    Episode_Reward/reaching_object: 0.2709
     Episode_Reward/lifting_object: 3.7379
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 51.0833
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 2.34s
                      Time elapsed: 00:17:44
                               ETA: 00:55:18

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 43490 steps/s (collection: 2.088s, learning 0.173s)
             Mean action noise std: 2.34
          Mean value_function loss: 73.7579
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 58.8813
                       Mean reward: 21.43
               Mean episode length: 90.38
    Episode_Reward/reaching_object: 0.2692
     Episode_Reward/lifting_object: 3.9224
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 43.5417
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.26s
                      Time elapsed: 00:17:46
                               ETA: 00:55:16

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 42862 steps/s (collection: 2.150s, learning 0.143s)
             Mean action noise std: 2.34
          Mean value_function loss: 92.4399
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 58.8998
                       Mean reward: 19.00
               Mean episode length: 91.93
    Episode_Reward/reaching_object: 0.2670
     Episode_Reward/lifting_object: 3.6421
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.7083
     Episode_Termination/robot_out: 43.5000
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 2.29s
                      Time elapsed: 00:17:48
                               ETA: 00:55:14

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 43688 steps/s (collection: 2.143s, learning 0.108s)
             Mean action noise std: 2.34
          Mean value_function loss: 75.1094
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.9123
                       Mean reward: 14.53
               Mean episode length: 83.94
    Episode_Reward/reaching_object: 0.2651
     Episode_Reward/lifting_object: 3.6100
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 44.5417
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.25s
                      Time elapsed: 00:17:51
                               ETA: 00:55:12

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 43606 steps/s (collection: 2.107s, learning 0.148s)
             Mean action noise std: 2.34
          Mean value_function loss: 51.3238
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 58.9214
                       Mean reward: 27.44
               Mean episode length: 83.79
    Episode_Reward/reaching_object: 0.2639
     Episode_Reward/lifting_object: 3.7689
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3333
     Episode_Termination/robot_out: 46.8750
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 2.25s
                      Time elapsed: 00:17:53
                               ETA: 00:55:10

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 42233 steps/s (collection: 2.135s, learning 0.193s)
             Mean action noise std: 2.34
          Mean value_function loss: 42.6113
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.9317
                       Mean reward: 13.14
               Mean episode length: 85.16
    Episode_Reward/reaching_object: 0.2625
     Episode_Reward/lifting_object: 3.6239
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 46.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.33s
                      Time elapsed: 00:17:55
                               ETA: 00:55:08

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 43184 steps/s (collection: 2.154s, learning 0.122s)
             Mean action noise std: 2.34
          Mean value_function loss: 66.1237
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.9392
                       Mean reward: 22.91
               Mean episode length: 90.22
    Episode_Reward/reaching_object: 0.2657
     Episode_Reward/lifting_object: 3.7545
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.9167
     Episode_Termination/robot_out: 46.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 2.28s
                      Time elapsed: 00:17:58
                               ETA: 00:55:06

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 41998 steps/s (collection: 2.211s, learning 0.130s)
             Mean action noise std: 2.34
          Mean value_function loss: 40.1982
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 58.9496
                       Mean reward: 19.59
               Mean episode length: 84.88
    Episode_Reward/reaching_object: 0.2600
     Episode_Reward/lifting_object: 3.7321
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.9583
     Episode_Termination/robot_out: 46.0833
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.34s
                      Time elapsed: 00:18:00
                               ETA: 00:55:04

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 41426 steps/s (collection: 2.184s, learning 0.189s)
             Mean action noise std: 2.34
          Mean value_function loss: 45.0915
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 58.9561
                       Mean reward: 18.27
               Mean episode length: 84.35
    Episode_Reward/reaching_object: 0.2552
     Episode_Reward/lifting_object: 3.4985
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.3333
     Episode_Termination/robot_out: 46.9583
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 2.37s
                      Time elapsed: 00:18:02
                               ETA: 00:55:03

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 39884 steps/s (collection: 2.329s, learning 0.135s)
             Mean action noise std: 2.34
          Mean value_function loss: 40.2517
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 58.9647
                       Mean reward: 18.18
               Mean episode length: 74.91
    Episode_Reward/reaching_object: 0.2626
     Episode_Reward/lifting_object: 3.7970
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 46.2917
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.46s
                      Time elapsed: 00:18:05
                               ETA: 00:55:01

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 41295 steps/s (collection: 2.202s, learning 0.178s)
             Mean action noise std: 2.34
          Mean value_function loss: 75.0651
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.9784
                       Mean reward: 23.96
               Mean episode length: 83.87
    Episode_Reward/reaching_object: 0.2647
     Episode_Reward/lifting_object: 4.1838
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 45.7500
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 2.38s
                      Time elapsed: 00:18:07
                               ETA: 00:55:00

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 42157 steps/s (collection: 2.190s, learning 0.142s)
             Mean action noise std: 2.35
          Mean value_function loss: 39.8482
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 58.9872
                       Mean reward: 22.33
               Mean episode length: 87.17
    Episode_Reward/reaching_object: 0.2579
     Episode_Reward/lifting_object: 3.9716
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.9167
     Episode_Termination/robot_out: 45.8750
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.33s
                      Time elapsed: 00:18:09
                               ETA: 00:54:58

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 40613 steps/s (collection: 2.286s, learning 0.134s)
             Mean action noise std: 2.35
          Mean value_function loss: 55.8685
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 58.9970
                       Mean reward: 14.29
               Mean episode length: 82.11
    Episode_Reward/reaching_object: 0.2722
     Episode_Reward/lifting_object: 4.2223
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.2917
     Episode_Termination/robot_out: 45.4167
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 2.42s
                      Time elapsed: 00:18:12
                               ETA: 00:54:56

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 41417 steps/s (collection: 2.226s, learning 0.148s)
             Mean action noise std: 2.35
          Mean value_function loss: 41.6888
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 59.0058
                       Mean reward: 22.90
               Mean episode length: 86.12
    Episode_Reward/reaching_object: 0.2730
     Episode_Reward/lifting_object: 4.4075
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7500
     Episode_Termination/robot_out: 46.2917
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 2.37s
                      Time elapsed: 00:18:14
                               ETA: 00:54:55

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 40875 steps/s (collection: 2.273s, learning 0.132s)
             Mean action noise std: 2.35
          Mean value_function loss: 60.3208
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 59.0102
                       Mean reward: 19.30
               Mean episode length: 82.37
    Episode_Reward/reaching_object: 0.2662
     Episode_Reward/lifting_object: 3.8989
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.8333
     Episode_Termination/robot_out: 46.8333
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 2.40s
                      Time elapsed: 00:18:17
                               ETA: 00:54:53

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 41202 steps/s (collection: 2.239s, learning 0.147s)
             Mean action noise std: 2.35
          Mean value_function loss: 55.6800
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 59.0147
                       Mean reward: 23.15
               Mean episode length: 90.45
    Episode_Reward/reaching_object: 0.2756
     Episode_Reward/lifting_object: 4.4418
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 46.1667
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.39s
                      Time elapsed: 00:18:19
                               ETA: 00:54:52

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 41351 steps/s (collection: 2.252s, learning 0.125s)
             Mean action noise std: 2.35
          Mean value_function loss: 41.7740
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 59.0200
                       Mean reward: 17.62
               Mean episode length: 78.24
    Episode_Reward/reaching_object: 0.2653
     Episode_Reward/lifting_object: 4.1738
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.8750
     Episode_Termination/robot_out: 45.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 2.38s
                      Time elapsed: 00:18:21
                               ETA: 00:54:50

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 40995 steps/s (collection: 2.275s, learning 0.123s)
             Mean action noise std: 2.35
          Mean value_function loss: 36.6229
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 59.0230
                       Mean reward: 26.19
               Mean episode length: 86.30
    Episode_Reward/reaching_object: 0.2775
     Episode_Reward/lifting_object: 4.4844
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3333
     Episode_Termination/robot_out: 45.2500
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.40s
                      Time elapsed: 00:18:24
                               ETA: 00:54:48

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 41207 steps/s (collection: 2.221s, learning 0.165s)
             Mean action noise std: 2.35
          Mean value_function loss: 42.1317
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 59.0248
                       Mean reward: 21.19
               Mean episode length: 87.35
    Episode_Reward/reaching_object: 0.2656
     Episode_Reward/lifting_object: 4.2894
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5833
     Episode_Termination/robot_out: 45.8750
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 2.39s
                      Time elapsed: 00:18:26
                               ETA: 00:54:47

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 41674 steps/s (collection: 2.226s, learning 0.133s)
             Mean action noise std: 2.35
          Mean value_function loss: 40.2138
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 59.0256
                       Mean reward: 26.48
               Mean episode length: 81.91
    Episode_Reward/reaching_object: 0.2664
     Episode_Reward/lifting_object: 4.5518
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 48.2083
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 2.36s
                      Time elapsed: 00:18:29
                               ETA: 00:54:45

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 41776 steps/s (collection: 2.203s, learning 0.150s)
             Mean action noise std: 2.35
          Mean value_function loss: 58.9131
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 59.0259
                       Mean reward: 19.28
               Mean episode length: 85.26
    Episode_Reward/reaching_object: 0.2722
     Episode_Reward/lifting_object: 4.3641
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 47.4583
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 2.35s
                      Time elapsed: 00:18:31
                               ETA: 00:54:43

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 38168 steps/s (collection: 2.376s, learning 0.199s)
             Mean action noise std: 2.35
          Mean value_function loss: 47.6492
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 59.0269
                       Mean reward: 20.43
               Mean episode length: 83.99
    Episode_Reward/reaching_object: 0.2623
     Episode_Reward/lifting_object: 4.0841
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 47.2500
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 2.58s
                      Time elapsed: 00:18:34
                               ETA: 00:54:42

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 41021 steps/s (collection: 2.193s, learning 0.203s)
             Mean action noise std: 2.35
          Mean value_function loss: 39.4233
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 59.0284
                       Mean reward: 24.03
               Mean episode length: 88.72
    Episode_Reward/reaching_object: 0.2588
     Episode_Reward/lifting_object: 4.1124
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 47.8333
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 2.40s
                      Time elapsed: 00:18:36
                               ETA: 00:54:41

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 40863 steps/s (collection: 2.267s, learning 0.139s)
             Mean action noise std: 2.35
          Mean value_function loss: 44.3001
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.0306
                       Mean reward: 15.34
               Mean episode length: 87.10
    Episode_Reward/reaching_object: 0.2514
     Episode_Reward/lifting_object: 4.3227
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7500
     Episode_Termination/robot_out: 45.5417
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.41s
                      Time elapsed: 00:18:38
                               ETA: 00:54:39

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 39750 steps/s (collection: 2.293s, learning 0.180s)
             Mean action noise std: 2.35
          Mean value_function loss: 52.2218
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 59.0377
                       Mean reward: 19.13
               Mean episode length: 90.74
    Episode_Reward/reaching_object: 0.2595
     Episode_Reward/lifting_object: 4.3215
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5417
     Episode_Termination/robot_out: 45.4583
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 2.47s
                      Time elapsed: 00:18:41
                               ETA: 00:54:38

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 38614 steps/s (collection: 2.332s, learning 0.214s)
             Mean action noise std: 2.35
          Mean value_function loss: 43.2855
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.0478
                       Mean reward: 21.15
               Mean episode length: 83.60
    Episode_Reward/reaching_object: 0.2583
     Episode_Reward/lifting_object: 4.1813
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 3.0000
     Episode_Termination/robot_out: 47.6667
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 2.55s
                      Time elapsed: 00:18:43
                               ETA: 00:54:36

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 40558 steps/s (collection: 2.284s, learning 0.140s)
             Mean action noise std: 2.35
          Mean value_function loss: 54.9445
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.0590
                       Mean reward: 23.19
               Mean episode length: 85.23
    Episode_Reward/reaching_object: 0.2606
     Episode_Reward/lifting_object: 4.4064
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6250
     Episode_Termination/robot_out: 46.4167
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 2.42s
                      Time elapsed: 00:18:46
                               ETA: 00:54:35

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 40409 steps/s (collection: 2.226s, learning 0.207s)
             Mean action noise std: 2.35
          Mean value_function loss: 44.9240
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 59.0705
                       Mean reward: 24.40
               Mean episode length: 78.09
    Episode_Reward/reaching_object: 0.2558
     Episode_Reward/lifting_object: 4.5133
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.0417
     Episode_Termination/robot_out: 47.8750
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 2.43s
                      Time elapsed: 00:18:48
                               ETA: 00:54:33

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 39519 steps/s (collection: 2.360s, learning 0.128s)
             Mean action noise std: 2.35
          Mean value_function loss: 44.6995
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 59.0730
                       Mean reward: 19.25
               Mean episode length: 80.94
    Episode_Reward/reaching_object: 0.2603
     Episode_Reward/lifting_object: 4.5906
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.8750
     Episode_Termination/robot_out: 47.2500
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 2.49s
                      Time elapsed: 00:18:51
                               ETA: 00:54:32

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 39582 steps/s (collection: 2.332s, learning 0.152s)
             Mean action noise std: 2.35
          Mean value_function loss: 50.4775
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 59.0742
                       Mean reward: 25.59
               Mean episode length: 78.67
    Episode_Reward/reaching_object: 0.2529
     Episode_Reward/lifting_object: 4.5419
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.8333
     Episode_Termination/robot_out: 46.2083
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.48s
                      Time elapsed: 00:18:53
                               ETA: 00:54:31

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 38928 steps/s (collection: 2.376s, learning 0.150s)
             Mean action noise std: 2.35
          Mean value_function loss: 42.1444
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 59.0744
                       Mean reward: 28.15
               Mean episode length: 85.98
    Episode_Reward/reaching_object: 0.2521
     Episode_Reward/lifting_object: 4.3837
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7083
     Episode_Termination/robot_out: 47.6667
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 2.53s
                      Time elapsed: 00:18:56
                               ETA: 00:54:29

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 40516 steps/s (collection: 2.291s, learning 0.135s)
             Mean action noise std: 2.35
          Mean value_function loss: 36.1391
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 59.0746
                       Mean reward: 23.92
               Mean episode length: 89.52
    Episode_Reward/reaching_object: 0.2576
     Episode_Reward/lifting_object: 4.7176
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 48.9167
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 2.43s
                      Time elapsed: 00:18:58
                               ETA: 00:54:28

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 39875 steps/s (collection: 2.322s, learning 0.143s)
             Mean action noise std: 2.35
          Mean value_function loss: 58.5566
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 59.0747
                       Mean reward: 22.04
               Mean episode length: 81.35
    Episode_Reward/reaching_object: 0.2525
     Episode_Reward/lifting_object: 4.4235
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.8333
     Episode_Termination/robot_out: 48.9583
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 2.47s
                      Time elapsed: 00:19:01
                               ETA: 00:54:26

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 40219 steps/s (collection: 2.293s, learning 0.152s)
             Mean action noise std: 2.35
          Mean value_function loss: 46.2949
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 59.0749
                       Mean reward: 27.75
               Mean episode length: 87.49
    Episode_Reward/reaching_object: 0.2521
     Episode_Reward/lifting_object: 4.5645
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.9167
     Episode_Termination/robot_out: 49.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.44s
                      Time elapsed: 00:19:03
                               ETA: 00:54:25

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 35394 steps/s (collection: 2.585s, learning 0.193s)
             Mean action noise std: 2.35
          Mean value_function loss: 53.9071
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 59.0769
                       Mean reward: 25.31
               Mean episode length: 79.14
    Episode_Reward/reaching_object: 0.2591
     Episode_Reward/lifting_object: 4.6635
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.3750
     Episode_Termination/robot_out: 45.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 2.78s
                      Time elapsed: 00:19:06
                               ETA: 00:54:24

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 39705 steps/s (collection: 2.286s, learning 0.190s)
             Mean action noise std: 2.35
          Mean value_function loss: 85.6753
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 59.0837
                       Mean reward: 14.54
               Mean episode length: 77.53
    Episode_Reward/reaching_object: 0.2573
     Episode_Reward/lifting_object: 4.5973
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.2083
     Episode_Termination/robot_out: 46.3750
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 2.48s
                      Time elapsed: 00:19:08
                               ETA: 00:54:23

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 39041 steps/s (collection: 2.380s, learning 0.138s)
             Mean action noise std: 2.35
          Mean value_function loss: 65.3122
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 59.0894
                       Mean reward: 24.26
               Mean episode length: 89.99
    Episode_Reward/reaching_object: 0.2672
     Episode_Reward/lifting_object: 4.8677
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.9583
     Episode_Termination/robot_out: 45.0833
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 2.52s
                      Time elapsed: 00:19:11
                               ETA: 00:54:22

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 41619 steps/s (collection: 2.225s, learning 0.137s)
             Mean action noise std: 2.36
          Mean value_function loss: 74.1154
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 59.0973
                       Mean reward: 17.61
               Mean episode length: 82.06
    Episode_Reward/reaching_object: 0.2628
     Episode_Reward/lifting_object: 4.7664
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.9167
     Episode_Termination/robot_out: 47.2083
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.36s
                      Time elapsed: 00:19:13
                               ETA: 00:54:20

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 42271 steps/s (collection: 2.208s, learning 0.118s)
             Mean action noise std: 2.36
          Mean value_function loss: 53.8489
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.1099
                       Mean reward: 25.08
               Mean episode length: 87.52
    Episode_Reward/reaching_object: 0.2669
     Episode_Reward/lifting_object: 4.4483
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 45.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 2.33s
                      Time elapsed: 00:19:15
                               ETA: 00:54:18

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 40936 steps/s (collection: 2.261s, learning 0.141s)
             Mean action noise std: 2.36
          Mean value_function loss: 65.6666
               Mean surrogate loss: 0.0149
                 Mean entropy loss: 59.1211
                       Mean reward: 25.44
               Mean episode length: 82.43
    Episode_Reward/reaching_object: 0.2649
     Episode_Reward/lifting_object: 4.5730
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.8750
     Episode_Termination/robot_out: 46.2083
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 2.40s
                      Time elapsed: 00:19:18
                               ETA: 00:54:16

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 39223 steps/s (collection: 2.271s, learning 0.235s)
             Mean action noise std: 2.36
          Mean value_function loss: 42.4310
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 59.1243
                       Mean reward: 23.50
               Mean episode length: 83.65
    Episode_Reward/reaching_object: 0.2674
     Episode_Reward/lifting_object: 4.5803
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5833
     Episode_Termination/robot_out: 44.1250
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 2.51s
                      Time elapsed: 00:19:20
                               ETA: 00:54:15

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 39002 steps/s (collection: 2.351s, learning 0.169s)
             Mean action noise std: 2.36
          Mean value_function loss: 63.4904
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 59.1252
                       Mean reward: 25.77
               Mean episode length: 91.59
    Episode_Reward/reaching_object: 0.2719
     Episode_Reward/lifting_object: 4.9694
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5000
     Episode_Termination/robot_out: 44.0833
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.52s
                      Time elapsed: 00:19:23
                               ETA: 00:54:14

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 41340 steps/s (collection: 2.185s, learning 0.193s)
             Mean action noise std: 2.36
          Mean value_function loss: 51.4516
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.1267
                       Mean reward: 23.35
               Mean episode length: 86.43
    Episode_Reward/reaching_object: 0.2632
     Episode_Reward/lifting_object: 4.5336
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6250
     Episode_Termination/robot_out: 46.8333
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 2.38s
                      Time elapsed: 00:19:25
                               ETA: 00:54:12

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 41135 steps/s (collection: 2.235s, learning 0.155s)
             Mean action noise std: 2.36
          Mean value_function loss: 49.6142
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 59.1309
                       Mean reward: 20.16
               Mean episode length: 87.68
    Episode_Reward/reaching_object: 0.2596
     Episode_Reward/lifting_object: 4.5345
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 46.8333
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 2.39s
                      Time elapsed: 00:19:28
                               ETA: 00:54:10

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 42329 steps/s (collection: 2.185s, learning 0.137s)
             Mean action noise std: 2.36
          Mean value_function loss: 48.4766
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 59.1381
                       Mean reward: 25.41
               Mean episode length: 81.77
    Episode_Reward/reaching_object: 0.2618
     Episode_Reward/lifting_object: 4.6363
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 48.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 2.32s
                      Time elapsed: 00:19:30
                               ETA: 00:54:08

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 41032 steps/s (collection: 2.263s, learning 0.133s)
             Mean action noise std: 2.36
          Mean value_function loss: 93.9207
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.1531
                       Mean reward: 23.99
               Mean episode length: 78.80
    Episode_Reward/reaching_object: 0.2573
     Episode_Reward/lifting_object: 4.6017
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 48.6250
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 2.40s
                      Time elapsed: 00:19:32
                               ETA: 00:54:06

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 41648 steps/s (collection: 2.237s, learning 0.123s)
             Mean action noise std: 2.36
          Mean value_function loss: 94.0025
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 59.1667
                       Mean reward: 18.16
               Mean episode length: 86.45
    Episode_Reward/reaching_object: 0.2585
     Episode_Reward/lifting_object: 4.2021
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5833
     Episode_Termination/robot_out: 48.2083
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 2.36s
                      Time elapsed: 00:19:35
                               ETA: 00:54:05

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 36894 steps/s (collection: 2.498s, learning 0.166s)
             Mean action noise std: 2.36
          Mean value_function loss: 49.8575
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 59.1709
                       Mean reward: 34.83
               Mean episode length: 91.14
    Episode_Reward/reaching_object: 0.2542
     Episode_Reward/lifting_object: 4.3298
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.9583
     Episode_Termination/robot_out: 50.3750
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.66s
                      Time elapsed: 00:19:37
                               ETA: 00:54:04

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 39701 steps/s (collection: 2.372s, learning 0.104s)
             Mean action noise std: 2.36
          Mean value_function loss: 52.7419
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.1752
                       Mean reward: 21.02
               Mean episode length: 85.60
    Episode_Reward/reaching_object: 0.2546
     Episode_Reward/lifting_object: 4.7128
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 48.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 2.48s
                      Time elapsed: 00:19:40
                               ETA: 00:54:02

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 37541 steps/s (collection: 2.427s, learning 0.191s)
             Mean action noise std: 2.36
          Mean value_function loss: 37.2049
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 59.1788
                       Mean reward: 27.66
               Mean episode length: 79.78
    Episode_Reward/reaching_object: 0.2598
     Episode_Reward/lifting_object: 4.5486
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 46.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 2.62s
                      Time elapsed: 00:19:43
                               ETA: 00:54:01

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 34417 steps/s (collection: 2.684s, learning 0.172s)
             Mean action noise std: 2.36
          Mean value_function loss: 49.6692
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 59.1800
                       Mean reward: 25.08
               Mean episode length: 83.25
    Episode_Reward/reaching_object: 0.2603
     Episode_Reward/lifting_object: 4.9241
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 46.5833
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 2.86s
                      Time elapsed: 00:19:45
                               ETA: 00:54:01

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 36451 steps/s (collection: 2.507s, learning 0.190s)
             Mean action noise std: 2.36
          Mean value_function loss: 39.8574
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 59.1846
                       Mean reward: 36.27
               Mean episode length: 84.38
    Episode_Reward/reaching_object: 0.2654
     Episode_Reward/lifting_object: 4.9032
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 44.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.70s
                      Time elapsed: 00:19:48
                               ETA: 00:54:00

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 40391 steps/s (collection: 2.262s, learning 0.172s)
             Mean action noise std: 2.36
          Mean value_function loss: 64.5739
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.1943
                       Mean reward: 26.74
               Mean episode length: 84.18
    Episode_Reward/reaching_object: 0.2708
     Episode_Reward/lifting_object: 5.0465
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5000
     Episode_Termination/robot_out: 47.3750
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 2.43s
                      Time elapsed: 00:19:51
                               ETA: 00:53:58

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 43086 steps/s (collection: 2.177s, learning 0.105s)
             Mean action noise std: 2.37
          Mean value_function loss: 56.3517
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.2035
                       Mean reward: 24.58
               Mean episode length: 87.78
    Episode_Reward/reaching_object: 0.2823
     Episode_Reward/lifting_object: 5.2468
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7500
     Episode_Termination/robot_out: 49.5417
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.28s
                      Time elapsed: 00:19:53
                               ETA: 00:53:56

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 44943 steps/s (collection: 2.084s, learning 0.103s)
             Mean action noise std: 2.37
          Mean value_function loss: 59.7024
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 59.2101
                       Mean reward: 27.83
               Mean episode length: 84.46
    Episode_Reward/reaching_object: 0.2798
     Episode_Reward/lifting_object: 5.3627
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 48.3750
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 2.19s
                      Time elapsed: 00:19:55
                               ETA: 00:53:54

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 43796 steps/s (collection: 2.124s, learning 0.120s)
             Mean action noise std: 2.37
          Mean value_function loss: 55.5434
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 59.2142
                       Mean reward: 26.77
               Mean episode length: 79.57
    Episode_Reward/reaching_object: 0.2760
     Episode_Reward/lifting_object: 5.1842
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 43.7500
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.24s
                      Time elapsed: 00:19:57
                               ETA: 00:53:52

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 42726 steps/s (collection: 2.165s, learning 0.136s)
             Mean action noise std: 2.37
          Mean value_function loss: 46.5536
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 59.2164
                       Mean reward: 27.49
               Mean episode length: 84.42
    Episode_Reward/reaching_object: 0.2790
     Episode_Reward/lifting_object: 5.3041
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 47.7917
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 2.30s
                      Time elapsed: 00:20:00
                               ETA: 00:53:50

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 40898 steps/s (collection: 2.275s, learning 0.129s)
             Mean action noise std: 2.37
          Mean value_function loss: 73.4798
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 59.2224
                       Mean reward: 27.63
               Mean episode length: 80.61
    Episode_Reward/reaching_object: 0.2854
     Episode_Reward/lifting_object: 5.5155
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 47.8333
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.40s
                      Time elapsed: 00:20:02
                               ETA: 00:53:48

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 43017 steps/s (collection: 2.192s, learning 0.093s)
             Mean action noise std: 2.37
          Mean value_function loss: 67.3607
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 59.2337
                       Mean reward: 32.56
               Mean episode length: 89.78
    Episode_Reward/reaching_object: 0.2782
     Episode_Reward/lifting_object: 5.6275
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 48.8750
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 2.29s
                      Time elapsed: 00:20:04
                               ETA: 00:53:46

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 45465 steps/s (collection: 2.074s, learning 0.089s)
             Mean action noise std: 2.37
          Mean value_function loss: 60.2762
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 59.2438
                       Mean reward: 24.18
               Mean episode length: 85.71
    Episode_Reward/reaching_object: 0.2712
     Episode_Reward/lifting_object: 4.9832
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 48.3333
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.16s
                      Time elapsed: 00:20:06
                               ETA: 00:53:44

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 40543 steps/s (collection: 2.305s, learning 0.120s)
             Mean action noise std: 2.37
          Mean value_function loss: 75.2023
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.2496
                       Mean reward: 30.75
               Mean episode length: 86.24
    Episode_Reward/reaching_object: 0.2670
     Episode_Reward/lifting_object: 5.5471
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5000
     Episode_Termination/robot_out: 49.8750
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 2.42s
                      Time elapsed: 00:20:09
                               ETA: 00:53:42

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 42714 steps/s (collection: 2.185s, learning 0.116s)
             Mean action noise std: 2.37
          Mean value_function loss: 98.8558
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 59.2548
                       Mean reward: 17.56
               Mean episode length: 81.30
    Episode_Reward/reaching_object: 0.2732
     Episode_Reward/lifting_object: 5.3221
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4583
     Episode_Termination/robot_out: 46.8750
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.30s
                      Time elapsed: 00:20:11
                               ETA: 00:53:40

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 41558 steps/s (collection: 2.241s, learning 0.125s)
             Mean action noise std: 2.37
          Mean value_function loss: 89.6572
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 59.2621
                       Mean reward: 20.64
               Mean episode length: 88.97
    Episode_Reward/reaching_object: 0.2692
     Episode_Reward/lifting_object: 5.2846
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4583
     Episode_Termination/robot_out: 49.9583
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 2.37s
                      Time elapsed: 00:20:13
                               ETA: 00:53:38

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 39342 steps/s (collection: 2.348s, learning 0.151s)
             Mean action noise std: 2.37
          Mean value_function loss: 82.3214
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 59.2722
                       Mean reward: 24.05
               Mean episode length: 77.42
    Episode_Reward/reaching_object: 0.2687
     Episode_Reward/lifting_object: 4.9930
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5000
     Episode_Termination/robot_out: 50.2500
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 2.50s
                      Time elapsed: 00:20:16
                               ETA: 00:53:37

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 42160 steps/s (collection: 2.211s, learning 0.121s)
             Mean action noise std: 2.37
          Mean value_function loss: 54.4350
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 59.2782
                       Mean reward: 35.91
               Mean episode length: 86.08
    Episode_Reward/reaching_object: 0.2715
     Episode_Reward/lifting_object: 5.3055
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7083
     Episode_Termination/robot_out: 46.8333
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.33s
                      Time elapsed: 00:20:18
                               ETA: 00:53:35

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 43940 steps/s (collection: 2.108s, learning 0.129s)
             Mean action noise std: 2.37
          Mean value_function loss: 54.4037
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 59.2866
                       Mean reward: 27.77
               Mean episode length: 76.96
    Episode_Reward/reaching_object: 0.2717
     Episode_Reward/lifting_object: 5.6816
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4583
     Episode_Termination/robot_out: 48.9583
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.24s
                      Time elapsed: 00:20:21
                               ETA: 00:53:33

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 41229 steps/s (collection: 2.200s, learning 0.184s)
             Mean action noise std: 2.37
          Mean value_function loss: 58.3512
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 59.2901
                       Mean reward: 23.61
               Mean episode length: 82.72
    Episode_Reward/reaching_object: 0.2730
     Episode_Reward/lifting_object: 5.6860
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 50.5417
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 2.38s
                      Time elapsed: 00:20:23
                               ETA: 00:53:31

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 40631 steps/s (collection: 2.293s, learning 0.127s)
             Mean action noise std: 2.37
          Mean value_function loss: 64.8021
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 59.2915
                       Mean reward: 28.64
               Mean episode length: 80.55
    Episode_Reward/reaching_object: 0.2673
     Episode_Reward/lifting_object: 5.4833
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3333
     Episode_Termination/robot_out: 48.1250
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.42s
                      Time elapsed: 00:20:25
                               ETA: 00:53:29

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 42745 steps/s (collection: 2.168s, learning 0.132s)
             Mean action noise std: 2.37
          Mean value_function loss: 68.1446
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 59.2960
                       Mean reward: 36.03
               Mean episode length: 81.48
    Episode_Reward/reaching_object: 0.2691
     Episode_Reward/lifting_object: 5.9552
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 50.6250
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.30s
                      Time elapsed: 00:20:28
                               ETA: 00:53:27

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 43900 steps/s (collection: 2.135s, learning 0.105s)
             Mean action noise std: 2.37
          Mean value_function loss: 72.9640
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 59.3035
                       Mean reward: 37.76
               Mean episode length: 81.94
    Episode_Reward/reaching_object: 0.2732
     Episode_Reward/lifting_object: 5.8774
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 49.6250
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.24s
                      Time elapsed: 00:20:30
                               ETA: 00:53:25

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 43944 steps/s (collection: 2.128s, learning 0.109s)
             Mean action noise std: 2.37
          Mean value_function loss: 66.8911
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 59.3050
                       Mean reward: 28.05
               Mean episode length: 78.01
    Episode_Reward/reaching_object: 0.2703
     Episode_Reward/lifting_object: 5.5921
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 50.0417
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 2.24s
                      Time elapsed: 00:20:32
                               ETA: 00:53:23

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 43848 steps/s (collection: 2.123s, learning 0.119s)
             Mean action noise std: 2.37
          Mean value_function loss: 84.0259
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 59.3060
                       Mean reward: 30.50
               Mean episode length: 81.33
    Episode_Reward/reaching_object: 0.2785
     Episode_Reward/lifting_object: 6.0381
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.8333
     Episode_Termination/robot_out: 48.7500
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 2.24s
                      Time elapsed: 00:20:34
                               ETA: 00:53:21

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 41319 steps/s (collection: 2.264s, learning 0.115s)
             Mean action noise std: 2.37
          Mean value_function loss: 85.2240
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 59.3070
                       Mean reward: 30.11
               Mean episode length: 80.30
    Episode_Reward/reaching_object: 0.2781
     Episode_Reward/lifting_object: 6.0767
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5417
     Episode_Termination/robot_out: 50.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 2.38s
                      Time elapsed: 00:20:37
                               ETA: 00:53:19

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 44647 steps/s (collection: 2.086s, learning 0.116s)
             Mean action noise std: 2.38
          Mean value_function loss: 72.4818
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.3091
                       Mean reward: 34.29
               Mean episode length: 78.33
    Episode_Reward/reaching_object: 0.2776
     Episode_Reward/lifting_object: 6.0423
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4583
     Episode_Termination/robot_out: 48.9167
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 2.20s
                      Time elapsed: 00:20:39
                               ETA: 00:53:17

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 43097 steps/s (collection: 2.176s, learning 0.105s)
             Mean action noise std: 2.38
          Mean value_function loss: 57.5152
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 59.3132
                       Mean reward: 31.95
               Mean episode length: 82.07
    Episode_Reward/reaching_object: 0.2676
     Episode_Reward/lifting_object: 6.2052
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.9167
     Episode_Termination/robot_out: 51.0833
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 2.28s
                      Time elapsed: 00:20:41
                               ETA: 00:53:15

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 44651 steps/s (collection: 2.102s, learning 0.100s)
             Mean action noise std: 2.38
          Mean value_function loss: 57.0851
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 59.3190
                       Mean reward: 32.79
               Mean episode length: 79.76
    Episode_Reward/reaching_object: 0.2717
     Episode_Reward/lifting_object: 6.3536
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 51.5000
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 2.20s
                      Time elapsed: 00:20:43
                               ETA: 00:53:12

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 44652 steps/s (collection: 2.103s, learning 0.099s)
             Mean action noise std: 2.38
          Mean value_function loss: 86.1449
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 59.3230
                       Mean reward: 26.88
               Mean episode length: 73.78
    Episode_Reward/reaching_object: 0.2651
     Episode_Reward/lifting_object: 5.8254
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7917
     Episode_Termination/robot_out: 51.5417
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 2.20s
                      Time elapsed: 00:20:46
                               ETA: 00:53:10

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 43896 steps/s (collection: 2.138s, learning 0.102s)
             Mean action noise std: 2.38
          Mean value_function loss: 72.3376
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 59.3275
                       Mean reward: 24.62
               Mean episode length: 75.30
    Episode_Reward/reaching_object: 0.2587
     Episode_Reward/lifting_object: 5.7642
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7500
     Episode_Termination/robot_out: 50.7500
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 2.24s
                      Time elapsed: 00:20:48
                               ETA: 00:53:08

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 44695 steps/s (collection: 2.101s, learning 0.099s)
             Mean action noise std: 2.38
          Mean value_function loss: 62.3029
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 59.3312
                       Mean reward: 29.41
               Mean episode length: 80.40
    Episode_Reward/reaching_object: 0.2648
     Episode_Reward/lifting_object: 6.2212
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4583
     Episode_Termination/robot_out: 51.9167
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 2.20s
                      Time elapsed: 00:20:50
                               ETA: 00:53:06

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 43804 steps/s (collection: 2.139s, learning 0.105s)
             Mean action noise std: 2.38
          Mean value_function loss: 70.8728
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 59.3327
                       Mean reward: 25.54
               Mean episode length: 70.96
    Episode_Reward/reaching_object: 0.2553
     Episode_Reward/lifting_object: 5.7936
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 50.2917
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 2.24s
                      Time elapsed: 00:20:52
                               ETA: 00:53:04

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 43526 steps/s (collection: 2.118s, learning 0.141s)
             Mean action noise std: 2.38
          Mean value_function loss: 81.3628
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 59.3363
                       Mean reward: 18.61
               Mean episode length: 72.23
    Episode_Reward/reaching_object: 0.2564
     Episode_Reward/lifting_object: 5.5335
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 54.0833
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 2.26s
                      Time elapsed: 00:20:55
                               ETA: 00:53:01

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 40189 steps/s (collection: 2.353s, learning 0.093s)
             Mean action noise std: 2.38
          Mean value_function loss: 88.6215
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 59.3437
                       Mean reward: 32.25
               Mean episode length: 75.81
    Episode_Reward/reaching_object: 0.2602
     Episode_Reward/lifting_object: 5.7206
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 52.4167
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 2.45s
                      Time elapsed: 00:20:57
                               ETA: 00:53:00

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 44557 steps/s (collection: 2.109s, learning 0.097s)
             Mean action noise std: 2.38
          Mean value_function loss: 101.5351
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 59.3494
                       Mean reward: 32.84
               Mean episode length: 75.46
    Episode_Reward/reaching_object: 0.2597
     Episode_Reward/lifting_object: 5.4839
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5417
     Episode_Termination/robot_out: 49.3333
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 2.21s
                      Time elapsed: 00:20:59
                               ETA: 00:52:58

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 42179 steps/s (collection: 2.226s, learning 0.105s)
             Mean action noise std: 2.38
          Mean value_function loss: 62.3069
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 59.3559
                       Mean reward: 29.59
               Mean episode length: 70.86
    Episode_Reward/reaching_object: 0.2688
     Episode_Reward/lifting_object: 6.1502
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 52.0833
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 2.33s
                      Time elapsed: 00:21:02
                               ETA: 00:52:56

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 42378 steps/s (collection: 2.217s, learning 0.103s)
             Mean action noise std: 2.38
          Mean value_function loss: 108.3306
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 59.3596
                       Mean reward: 29.79
               Mean episode length: 78.10
    Episode_Reward/reaching_object: 0.2572
     Episode_Reward/lifting_object: 5.5746
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7083
     Episode_Termination/robot_out: 49.0833
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 2.32s
                      Time elapsed: 00:21:04
                               ETA: 00:52:54

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 43199 steps/s (collection: 2.165s, learning 0.111s)
             Mean action noise std: 2.38
          Mean value_function loss: 95.5895
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 59.3602
                       Mean reward: 32.40
               Mean episode length: 76.14
    Episode_Reward/reaching_object: 0.2714
     Episode_Reward/lifting_object: 5.8327
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 52.6250
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.28s
                      Time elapsed: 00:21:06
                               ETA: 00:52:52

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 44200 steps/s (collection: 2.117s, learning 0.107s)
             Mean action noise std: 2.38
          Mean value_function loss: 68.9814
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 59.3608
                       Mean reward: 33.76
               Mean episode length: 74.98
    Episode_Reward/reaching_object: 0.2715
     Episode_Reward/lifting_object: 6.0900
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 50.5000
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 2.22s
                      Time elapsed: 00:21:08
                               ETA: 00:52:49

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 43244 steps/s (collection: 2.133s, learning 0.140s)
             Mean action noise std: 2.38
          Mean value_function loss: 70.9357
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 59.3622
                       Mean reward: 26.75
               Mean episode length: 76.59
    Episode_Reward/reaching_object: 0.2664
     Episode_Reward/lifting_object: 6.0362
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5833
     Episode_Termination/robot_out: 49.3750
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 2.27s
                      Time elapsed: 00:21:11
                               ETA: 00:52:47

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 43961 steps/s (collection: 2.130s, learning 0.106s)
             Mean action noise std: 2.38
          Mean value_function loss: 60.4471
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 59.3633
                       Mean reward: 32.65
               Mean episode length: 80.96
    Episode_Reward/reaching_object: 0.2714
     Episode_Reward/lifting_object: 6.2639
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4583
     Episode_Termination/robot_out: 52.1250
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 2.24s
                      Time elapsed: 00:21:13
                               ETA: 00:52:45

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 43114 steps/s (collection: 2.175s, learning 0.105s)
             Mean action noise std: 2.38
          Mean value_function loss: 63.2776
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.3668
                       Mean reward: 30.65
               Mean episode length: 79.73
    Episode_Reward/reaching_object: 0.2813
     Episode_Reward/lifting_object: 6.7260
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.0000
     Episode_Termination/robot_out: 49.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.28s
                      Time elapsed: 00:21:15
                               ETA: 00:52:43

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 44911 steps/s (collection: 2.091s, learning 0.098s)
             Mean action noise std: 2.38
          Mean value_function loss: 103.5435
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.3759
                       Mean reward: 26.59
               Mean episode length: 73.57
    Episode_Reward/reaching_object: 0.2608
     Episode_Reward/lifting_object: 5.9896
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 51.3333
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 2.19s
                      Time elapsed: 00:21:17
                               ETA: 00:52:41

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 44431 steps/s (collection: 2.109s, learning 0.103s)
             Mean action noise std: 2.38
          Mean value_function loss: 74.2446
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 59.3849
                       Mean reward: 33.00
               Mean episode length: 82.65
    Episode_Reward/reaching_object: 0.2688
     Episode_Reward/lifting_object: 6.1776
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.9167
     Episode_Termination/robot_out: 49.3750
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.21s
                      Time elapsed: 00:21:20
                               ETA: 00:52:39

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 43471 steps/s (collection: 2.098s, learning 0.163s)
             Mean action noise std: 2.38
          Mean value_function loss: 78.0632
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 59.3943
                       Mean reward: 27.12
               Mean episode length: 75.16
    Episode_Reward/reaching_object: 0.2582
     Episode_Reward/lifting_object: 5.7538
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5000
     Episode_Termination/robot_out: 52.4167
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 2.26s
                      Time elapsed: 00:21:22
                               ETA: 00:52:36

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 44218 steps/s (collection: 2.119s, learning 0.105s)
             Mean action noise std: 2.38
          Mean value_function loss: 69.4661
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 59.4007
                       Mean reward: 31.83
               Mean episode length: 71.10
    Episode_Reward/reaching_object: 0.2570
     Episode_Reward/lifting_object: 5.7105
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 49.8333
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 2.22s
                      Time elapsed: 00:21:24
                               ETA: 00:52:34

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 44862 steps/s (collection: 2.081s, learning 0.110s)
             Mean action noise std: 2.38
          Mean value_function loss: 70.2355
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 59.4053
                       Mean reward: 31.08
               Mean episode length: 79.59
    Episode_Reward/reaching_object: 0.2690
     Episode_Reward/lifting_object: 6.6411
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.8750
     Episode_Termination/robot_out: 47.9167
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.19s
                      Time elapsed: 00:21:26
                               ETA: 00:52:32

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 43684 steps/s (collection: 2.134s, learning 0.116s)
             Mean action noise std: 2.38
          Mean value_function loss: 69.5827
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 59.4098
                       Mean reward: 30.02
               Mean episode length: 78.44
    Episode_Reward/reaching_object: 0.2637
     Episode_Reward/lifting_object: 6.0697
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.8750
     Episode_Termination/robot_out: 51.6250
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 2.25s
                      Time elapsed: 00:21:28
                               ETA: 00:52:30

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 41441 steps/s (collection: 2.205s, learning 0.167s)
             Mean action noise std: 2.39
          Mean value_function loss: 74.3319
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 59.4150
                       Mean reward: 45.95
               Mean episode length: 85.23
    Episode_Reward/reaching_object: 0.2622
     Episode_Reward/lifting_object: 6.3560
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3333
     Episode_Termination/robot_out: 50.0833
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 2.37s
                      Time elapsed: 00:21:31
                               ETA: 00:52:28

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 42981 steps/s (collection: 2.164s, learning 0.123s)
             Mean action noise std: 2.39
          Mean value_function loss: 76.4036
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 59.4211
                       Mean reward: 29.82
               Mean episode length: 78.71
    Episode_Reward/reaching_object: 0.2539
     Episode_Reward/lifting_object: 5.9332
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.0833
     Episode_Termination/robot_out: 50.8333
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 2.29s
                      Time elapsed: 00:21:33
                               ETA: 00:52:26

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 42439 steps/s (collection: 2.207s, learning 0.110s)
             Mean action noise std: 2.39
          Mean value_function loss: 76.0352
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 59.4235
                       Mean reward: 33.71
               Mean episode length: 78.04
    Episode_Reward/reaching_object: 0.2575
     Episode_Reward/lifting_object: 5.9803
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.3750
     Episode_Termination/robot_out: 51.7500
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 2.32s
                      Time elapsed: 00:21:35
                               ETA: 00:52:24

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 45228 steps/s (collection: 2.082s, learning 0.092s)
             Mean action noise std: 2.39
          Mean value_function loss: 70.3928
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 59.4248
                       Mean reward: 30.52
               Mean episode length: 70.76
    Episode_Reward/reaching_object: 0.2542
     Episode_Reward/lifting_object: 6.2283
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7917
     Episode_Termination/robot_out: 55.2500
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.17s
                      Time elapsed: 00:21:38
                               ETA: 00:52:22

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 44557 steps/s (collection: 2.112s, learning 0.095s)
             Mean action noise std: 2.39
          Mean value_function loss: 77.1593
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.4271
                       Mean reward: 27.87
               Mean episode length: 73.34
    Episode_Reward/reaching_object: 0.2500
     Episode_Reward/lifting_object: 6.1879
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.1667
     Episode_Termination/robot_out: 52.3750
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.21s
                      Time elapsed: 00:21:40
                               ETA: 00:52:19

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 44297 steps/s (collection: 2.114s, learning 0.106s)
             Mean action noise std: 2.39
          Mean value_function loss: 80.2862
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 59.4335
                       Mean reward: 36.86
               Mean episode length: 74.28
    Episode_Reward/reaching_object: 0.2544
     Episode_Reward/lifting_object: 6.0859
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 51.1250
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.22s
                      Time elapsed: 00:21:42
                               ETA: 00:52:17

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 42562 steps/s (collection: 2.188s, learning 0.122s)
             Mean action noise std: 2.39
          Mean value_function loss: 81.2722
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 59.4410
                       Mean reward: 30.86
               Mean episode length: 75.71
    Episode_Reward/reaching_object: 0.2591
     Episode_Reward/lifting_object: 6.2303
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5833
     Episode_Termination/robot_out: 51.5000
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 2.31s
                      Time elapsed: 00:21:44
                               ETA: 00:52:15

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 44187 steps/s (collection: 2.128s, learning 0.097s)
             Mean action noise std: 2.39
          Mean value_function loss: 97.5566
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.4478
                       Mean reward: 21.99
               Mean episode length: 78.46
    Episode_Reward/reaching_object: 0.2663
     Episode_Reward/lifting_object: 6.3715
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0389
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 53.0417
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.22s
                      Time elapsed: 00:21:47
                               ETA: 00:52:13

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 43601 steps/s (collection: 2.162s, learning 0.093s)
             Mean action noise std: 2.39
          Mean value_function loss: 91.4263
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.4559
                       Mean reward: 36.89
               Mean episode length: 76.28
    Episode_Reward/reaching_object: 0.2664
     Episode_Reward/lifting_object: 6.1422
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 55.3333
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 2.25s
                      Time elapsed: 00:21:49
                               ETA: 00:52:11

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 42934 steps/s (collection: 2.194s, learning 0.096s)
             Mean action noise std: 2.39
          Mean value_function loss: 93.2507
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 59.4633
                       Mean reward: 37.30
               Mean episode length: 67.19
    Episode_Reward/reaching_object: 0.2634
     Episode_Reward/lifting_object: 6.5576
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 54.5833
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.29s
                      Time elapsed: 00:21:51
                               ETA: 00:52:09

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 41380 steps/s (collection: 2.250s, learning 0.126s)
             Mean action noise std: 2.39
          Mean value_function loss: 113.2884
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 59.4730
                       Mean reward: 36.80
               Mean episode length: 71.68
    Episode_Reward/reaching_object: 0.2540
     Episode_Reward/lifting_object: 6.3253
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3333
     Episode_Termination/robot_out: 59.8333
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.38s
                      Time elapsed: 00:21:54
                               ETA: 00:52:07

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 41510 steps/s (collection: 2.245s, learning 0.123s)
             Mean action noise std: 2.39
          Mean value_function loss: 123.0618
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 59.4821
                       Mean reward: 37.73
               Mean episode length: 69.87
    Episode_Reward/reaching_object: 0.2627
     Episode_Reward/lifting_object: 6.4363
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 54.1667
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.37s
                      Time elapsed: 00:21:56
                               ETA: 00:52:05

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 43076 steps/s (collection: 2.166s, learning 0.116s)
             Mean action noise std: 2.39
          Mean value_function loss: 87.3434
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.4868
                       Mean reward: 30.82
               Mean episode length: 73.01
    Episode_Reward/reaching_object: 0.2674
     Episode_Reward/lifting_object: 6.3082
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 52.9583
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 2.28s
                      Time elapsed: 00:21:58
                               ETA: 00:52:03

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 42269 steps/s (collection: 2.185s, learning 0.141s)
             Mean action noise std: 2.39
          Mean value_function loss: 103.9888
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.4948
                       Mean reward: 39.06
               Mean episode length: 74.76
    Episode_Reward/reaching_object: 0.2628
     Episode_Reward/lifting_object: 6.3961
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 53.5417
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.33s
                      Time elapsed: 00:22:00
                               ETA: 00:52:01

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 38425 steps/s (collection: 2.398s, learning 0.161s)
             Mean action noise std: 2.39
          Mean value_function loss: 89.3169
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 59.5010
                       Mean reward: 30.35
               Mean episode length: 74.87
    Episode_Reward/reaching_object: 0.2779
     Episode_Reward/lifting_object: 7.1086
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 53.9583
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.56s
                      Time elapsed: 00:22:03
                               ETA: 00:52:00

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 41251 steps/s (collection: 2.250s, learning 0.133s)
             Mean action noise std: 2.39
          Mean value_function loss: 73.7719
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.5090
                       Mean reward: 32.75
               Mean episode length: 72.00
    Episode_Reward/reaching_object: 0.2700
     Episode_Reward/lifting_object: 6.7367
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 51.8333
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.38s
                      Time elapsed: 00:22:05
                               ETA: 00:51:58

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 42398 steps/s (collection: 2.215s, learning 0.104s)
             Mean action noise std: 2.40
          Mean value_function loss: 107.5249
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 59.5158
                       Mean reward: 36.77
               Mean episode length: 78.89
    Episode_Reward/reaching_object: 0.2766
     Episode_Reward/lifting_object: 7.0404
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 52.9583
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 2.32s
                      Time elapsed: 00:22:08
                               ETA: 00:51:56

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 41181 steps/s (collection: 2.270s, learning 0.117s)
             Mean action noise std: 2.40
          Mean value_function loss: 107.6211
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.5219
                       Mean reward: 38.39
               Mean episode length: 78.32
    Episode_Reward/reaching_object: 0.2764
     Episode_Reward/lifting_object: 6.8238
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 53.0417
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 2.39s
                      Time elapsed: 00:22:10
                               ETA: 00:51:54

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 42895 steps/s (collection: 2.193s, learning 0.099s)
             Mean action noise std: 2.40
          Mean value_function loss: 95.4192
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 59.5289
                       Mean reward: 21.48
               Mean episode length: 79.20
    Episode_Reward/reaching_object: 0.2793
     Episode_Reward/lifting_object: 6.6886
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 50.4583
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.29s
                      Time elapsed: 00:22:12
                               ETA: 00:51:52

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 37659 steps/s (collection: 2.370s, learning 0.241s)
             Mean action noise std: 2.40
          Mean value_function loss: 99.4286
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 59.5313
                       Mean reward: 37.40
               Mean episode length: 79.58
    Episode_Reward/reaching_object: 0.2836
     Episode_Reward/lifting_object: 7.4449
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5000
     Episode_Termination/robot_out: 50.8333
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.61s
                      Time elapsed: 00:22:15
                               ETA: 00:51:51

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 33000 steps/s (collection: 2.737s, learning 0.242s)
             Mean action noise std: 2.40
          Mean value_function loss: 75.9860
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 59.5343
                       Mean reward: 31.58
               Mean episode length: 70.69
    Episode_Reward/reaching_object: 0.2855
     Episode_Reward/lifting_object: 7.2893
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5833
     Episode_Termination/robot_out: 50.3750
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.98s
                      Time elapsed: 00:22:18
                               ETA: 00:51:50

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 42562 steps/s (collection: 2.203s, learning 0.107s)
             Mean action noise std: 2.40
          Mean value_function loss: 85.5693
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 59.5400
                       Mean reward: 39.58
               Mean episode length: 72.80
    Episode_Reward/reaching_object: 0.2822
     Episode_Reward/lifting_object: 7.0969
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 48.4167
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 2.31s
                      Time elapsed: 00:22:20
                               ETA: 00:51:48

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 43166 steps/s (collection: 2.145s, learning 0.133s)
             Mean action noise std: 2.40
          Mean value_function loss: 93.1016
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 59.5449
                       Mean reward: 36.29
               Mean episode length: 77.07
    Episode_Reward/reaching_object: 0.2891
     Episode_Reward/lifting_object: 7.2937
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 52.7917
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 2.28s
                      Time elapsed: 00:22:23
                               ETA: 00:51:46

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 36785 steps/s (collection: 2.463s, learning 0.210s)
             Mean action noise std: 2.40
          Mean value_function loss: 107.4192
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 59.5514
                       Mean reward: 29.86
               Mean episode length: 69.80
    Episode_Reward/reaching_object: 0.2857
     Episode_Reward/lifting_object: 7.4698
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 50.9167
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 2.67s
                      Time elapsed: 00:22:25
                               ETA: 00:51:45

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 32415 steps/s (collection: 2.836s, learning 0.196s)
             Mean action noise std: 2.40
          Mean value_function loss: 94.0034
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 59.5547
                       Mean reward: 31.45
               Mean episode length: 78.81
    Episode_Reward/reaching_object: 0.2832
     Episode_Reward/lifting_object: 7.1166
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0831
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 52.2917
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 3.03s
                      Time elapsed: 00:22:28
                               ETA: 00:51:44

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 36564 steps/s (collection: 2.585s, learning 0.103s)
             Mean action noise std: 2.40
          Mean value_function loss: 87.6233
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.5565
                       Mean reward: 33.83
               Mean episode length: 68.99
    Episode_Reward/reaching_object: 0.2836
     Episode_Reward/lifting_object: 7.5085
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 52.9167
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.69s
                      Time elapsed: 00:22:31
                               ETA: 00:51:43

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 44329 steps/s (collection: 2.099s, learning 0.119s)
             Mean action noise std: 2.40
          Mean value_function loss: 89.3022
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 59.5620
                       Mean reward: 41.41
               Mean episode length: 76.32
    Episode_Reward/reaching_object: 0.2890
     Episode_Reward/lifting_object: 7.4041
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 53.0417
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.22s
                      Time elapsed: 00:22:33
                               ETA: 00:51:41

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 45069 steps/s (collection: 2.087s, learning 0.095s)
             Mean action noise std: 2.40
          Mean value_function loss: 99.2340
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 59.5652
                       Mean reward: 34.90
               Mean episode length: 77.63
    Episode_Reward/reaching_object: 0.2905
     Episode_Reward/lifting_object: 7.7592
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 49.5417
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.18s
                      Time elapsed: 00:22:35
                               ETA: 00:51:39

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 44939 steps/s (collection: 2.096s, learning 0.091s)
             Mean action noise std: 2.40
          Mean value_function loss: 125.6754
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 59.5677
                       Mean reward: 41.16
               Mean episode length: 78.17
    Episode_Reward/reaching_object: 0.2873
     Episode_Reward/lifting_object: 7.5266
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 49.2917
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 2.19s
                      Time elapsed: 00:22:38
                               ETA: 00:51:36

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 44757 steps/s (collection: 2.102s, learning 0.095s)
             Mean action noise std: 2.40
          Mean value_function loss: 126.2825
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 59.5744
                       Mean reward: 33.43
               Mean episode length: 76.84
    Episode_Reward/reaching_object: 0.2956
     Episode_Reward/lifting_object: 7.7190
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 47.7500
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.20s
                      Time elapsed: 00:22:40
                               ETA: 00:51:34

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 44572 steps/s (collection: 2.106s, learning 0.100s)
             Mean action noise std: 2.40
          Mean value_function loss: 98.6917
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 59.5814
                       Mean reward: 30.98
               Mean episode length: 75.15
    Episode_Reward/reaching_object: 0.3006
     Episode_Reward/lifting_object: 7.8015
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4583
     Episode_Termination/robot_out: 51.1667
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 2.21s
                      Time elapsed: 00:22:42
                               ETA: 00:51:32

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 45076 steps/s (collection: 2.082s, learning 0.099s)
             Mean action noise std: 2.40
          Mean value_function loss: 89.9584
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 59.5850
                       Mean reward: 40.15
               Mean episode length: 78.53
    Episode_Reward/reaching_object: 0.3016
     Episode_Reward/lifting_object: 7.8822
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6250
     Episode_Termination/robot_out: 47.8333
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.18s
                      Time elapsed: 00:22:44
                               ETA: 00:51:29

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 44926 steps/s (collection: 2.082s, learning 0.106s)
             Mean action noise std: 2.40
          Mean value_function loss: 96.3823
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.5900
                       Mean reward: 41.74
               Mean episode length: 79.47
    Episode_Reward/reaching_object: 0.2990
     Episode_Reward/lifting_object: 8.0508
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7500
     Episode_Termination/robot_out: 49.9167
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 2.19s
                      Time elapsed: 00:22:46
                               ETA: 00:51:27

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 44740 steps/s (collection: 2.102s, learning 0.095s)
             Mean action noise std: 2.40
          Mean value_function loss: 85.6625
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.5979
                       Mean reward: 40.42
               Mean episode length: 84.20
    Episode_Reward/reaching_object: 0.2947
     Episode_Reward/lifting_object: 7.7686
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3333
     Episode_Termination/robot_out: 48.3333
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.20s
                      Time elapsed: 00:22:49
                               ETA: 00:51:25

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 44377 steps/s (collection: 2.100s, learning 0.116s)
             Mean action noise std: 2.40
          Mean value_function loss: 96.5116
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 59.6038
                       Mean reward: 36.71
               Mean episode length: 78.19
    Episode_Reward/reaching_object: 0.2909
     Episode_Reward/lifting_object: 7.7954
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 49.2917
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 2.22s
                      Time elapsed: 00:22:51
                               ETA: 00:51:23

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 44165 steps/s (collection: 2.131s, learning 0.095s)
             Mean action noise std: 2.40
          Mean value_function loss: 90.3478
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 59.6061
                       Mean reward: 39.63
               Mean episode length: 75.90
    Episode_Reward/reaching_object: 0.2944
     Episode_Reward/lifting_object: 7.7435
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 51.1250
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.23s
                      Time elapsed: 00:22:53
                               ETA: 00:51:20

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 44358 steps/s (collection: 2.124s, learning 0.093s)
             Mean action noise std: 2.40
          Mean value_function loss: 100.4511
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 59.6075
                       Mean reward: 38.45
               Mean episode length: 71.80
    Episode_Reward/reaching_object: 0.2936
     Episode_Reward/lifting_object: 8.3932
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6250
     Episode_Termination/robot_out: 48.5833
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.22s
                      Time elapsed: 00:22:55
                               ETA: 00:51:18

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 44610 steps/s (collection: 2.092s, learning 0.112s)
             Mean action noise std: 2.40
          Mean value_function loss: 80.7534
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 59.6078
                       Mean reward: 42.41
               Mean episode length: 78.99
    Episode_Reward/reaching_object: 0.2993
     Episode_Reward/lifting_object: 8.4836
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5417
     Episode_Termination/robot_out: 49.3333
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.20s
                      Time elapsed: 00:22:57
                               ETA: 00:51:16

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 44656 steps/s (collection: 2.102s, learning 0.100s)
             Mean action noise std: 2.40
          Mean value_function loss: 85.8627
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 59.6081
                       Mean reward: 36.73
               Mean episode length: 76.75
    Episode_Reward/reaching_object: 0.2899
     Episode_Reward/lifting_object: 8.1360
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 52.1667
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 2.20s
                      Time elapsed: 00:23:00
                               ETA: 00:51:14

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 44098 steps/s (collection: 2.133s, learning 0.096s)
             Mean action noise std: 2.40
          Mean value_function loss: 123.7680
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 59.6083
                       Mean reward: 43.30
               Mean episode length: 68.86
    Episode_Reward/reaching_object: 0.2935
     Episode_Reward/lifting_object: 8.4072
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 49.4583
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 2.23s
                      Time elapsed: 00:23:02
                               ETA: 00:51:11

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 42148 steps/s (collection: 2.193s, learning 0.139s)
             Mean action noise std: 2.40
          Mean value_function loss: 117.0455
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 59.6085
                       Mean reward: 47.40
               Mean episode length: 79.68
    Episode_Reward/reaching_object: 0.2912
     Episode_Reward/lifting_object: 8.1679
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 51.3750
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 2.33s
                      Time elapsed: 00:23:04
                               ETA: 00:51:09

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 42890 steps/s (collection: 2.193s, learning 0.099s)
             Mean action noise std: 2.40
          Mean value_function loss: 83.8755
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 59.6086
                       Mean reward: 41.57
               Mean episode length: 75.06
    Episode_Reward/reaching_object: 0.2843
     Episode_Reward/lifting_object: 7.9771
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 52.4583
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 2.29s
                      Time elapsed: 00:23:06
                               ETA: 00:51:07

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 42851 steps/s (collection: 2.192s, learning 0.103s)
             Mean action noise std: 2.40
          Mean value_function loss: 85.2864
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 59.6088
                       Mean reward: 37.91
               Mean episode length: 73.47
    Episode_Reward/reaching_object: 0.2847
     Episode_Reward/lifting_object: 8.1422
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 51.7917
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 2.29s
                      Time elapsed: 00:23:09
                               ETA: 00:51:05

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 32269 steps/s (collection: 2.936s, learning 0.111s)
             Mean action noise std: 2.40
          Mean value_function loss: 105.2652
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 59.6089
                       Mean reward: 33.85
               Mean episode length: 78.61
    Episode_Reward/reaching_object: 0.2891
     Episode_Reward/lifting_object: 7.9349
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 50.2500
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 3.05s
                      Time elapsed: 00:23:12
                               ETA: 00:51:05

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 40481 steps/s (collection: 2.313s, learning 0.115s)
             Mean action noise std: 2.40
          Mean value_function loss: 105.0814
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 59.6092
                       Mean reward: 43.27
               Mean episode length: 80.65
    Episode_Reward/reaching_object: 0.2897
     Episode_Reward/lifting_object: 8.5528
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 51.2917
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 2.43s
                      Time elapsed: 00:23:14
                               ETA: 00:51:03

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 40799 steps/s (collection: 2.240s, learning 0.169s)
             Mean action noise std: 2.40
          Mean value_function loss: 104.4260
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 59.6114
                       Mean reward: 51.76
               Mean episode length: 77.34
    Episode_Reward/reaching_object: 0.2864
     Episode_Reward/lifting_object: 8.2589
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 51.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.41s
                      Time elapsed: 00:23:17
                               ETA: 00:51:01

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 35691 steps/s (collection: 2.566s, learning 0.189s)
             Mean action noise std: 2.40
          Mean value_function loss: 103.4673
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 59.6162
                       Mean reward: 39.79
               Mean episode length: 74.97
    Episode_Reward/reaching_object: 0.2874
     Episode_Reward/lifting_object: 8.0935
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7917
     Episode_Termination/robot_out: 49.4583
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 2.75s
                      Time elapsed: 00:23:19
                               ETA: 00:51:00

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 37134 steps/s (collection: 2.524s, learning 0.123s)
             Mean action noise std: 2.41
          Mean value_function loss: 110.9344
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.6227
                       Mean reward: 35.29
               Mean episode length: 72.18
    Episode_Reward/reaching_object: 0.2814
     Episode_Reward/lifting_object: 7.9104
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.8333
     Episode_Termination/robot_out: 51.2500
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.65s
                      Time elapsed: 00:23:22
                               ETA: 00:50:59

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 41391 steps/s (collection: 2.257s, learning 0.118s)
             Mean action noise std: 2.41
          Mean value_function loss: 114.5129
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 59.6268
                       Mean reward: 33.49
               Mean episode length: 75.71
    Episode_Reward/reaching_object: 0.2857
     Episode_Reward/lifting_object: 7.9340
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 50.8333
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 2.37s
                      Time elapsed: 00:23:24
                               ETA: 00:50:57

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 41127 steps/s (collection: 2.276s, learning 0.114s)
             Mean action noise std: 2.41
          Mean value_function loss: 89.3080
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 59.6299
                       Mean reward: 35.72
               Mean episode length: 72.97
    Episode_Reward/reaching_object: 0.2800
     Episode_Reward/lifting_object: 8.0389
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.9583
     Episode_Termination/robot_out: 47.9583
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.39s
                      Time elapsed: 00:23:27
                               ETA: 00:50:55

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 36107 steps/s (collection: 2.628s, learning 0.095s)
             Mean action noise std: 2.41
          Mean value_function loss: 97.9381
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 59.6313
                       Mean reward: 46.41
               Mean episode length: 82.22
    Episode_Reward/reaching_object: 0.2837
     Episode_Reward/lifting_object: 7.9880
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.9167
     Episode_Termination/robot_out: 51.2083
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.72s
                      Time elapsed: 00:23:30
                               ETA: 00:50:54

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 38227 steps/s (collection: 2.400s, learning 0.172s)
             Mean action noise std: 2.41
          Mean value_function loss: 103.6814
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 59.6337
                       Mean reward: 44.58
               Mean episode length: 80.53
    Episode_Reward/reaching_object: 0.2828
     Episode_Reward/lifting_object: 8.3709
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4583
     Episode_Termination/robot_out: 50.2917
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.57s
                      Time elapsed: 00:23:32
                               ETA: 00:50:52

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 38563 steps/s (collection: 2.448s, learning 0.101s)
             Mean action noise std: 2.41
          Mean value_function loss: 99.9209
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 59.6384
                       Mean reward: 45.93
               Mean episode length: 78.51
    Episode_Reward/reaching_object: 0.2873
     Episode_Reward/lifting_object: 8.5410
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 50.7083
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 2.55s
                      Time elapsed: 00:23:35
                               ETA: 00:50:51

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 39822 steps/s (collection: 2.313s, learning 0.156s)
             Mean action noise std: 2.41
          Mean value_function loss: 122.3552
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 59.6409
                       Mean reward: 38.70
               Mean episode length: 76.42
    Episode_Reward/reaching_object: 0.2791
     Episode_Reward/lifting_object: 8.3832
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5833
     Episode_Termination/robot_out: 51.2917
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.47s
                      Time elapsed: 00:23:37
                               ETA: 00:50:49

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 38878 steps/s (collection: 2.419s, learning 0.109s)
             Mean action noise std: 2.41
          Mean value_function loss: 144.1141
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 59.6432
                       Mean reward: 36.62
               Mean episode length: 75.22
    Episode_Reward/reaching_object: 0.2804
     Episode_Reward/lifting_object: 8.3216
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 50.7083
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 2.53s
                      Time elapsed: 00:23:40
                               ETA: 00:50:47

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 37901 steps/s (collection: 2.473s, learning 0.121s)
             Mean action noise std: 2.41
          Mean value_function loss: 111.7348
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 59.6450
                       Mean reward: 44.18
               Mean episode length: 80.59
    Episode_Reward/reaching_object: 0.2806
     Episode_Reward/lifting_object: 8.0908
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.1250
     Episode_Termination/robot_out: 48.7917
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.59s
                      Time elapsed: 00:23:42
                               ETA: 00:50:46

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 43803 steps/s (collection: 2.155s, learning 0.090s)
             Mean action noise std: 2.41
          Mean value_function loss: 92.8186
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 59.6470
                       Mean reward: 49.48
               Mean episode length: 77.27
    Episode_Reward/reaching_object: 0.2919
     Episode_Reward/lifting_object: 9.2209
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5000
     Episode_Termination/robot_out: 50.3333
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.24s
                      Time elapsed: 00:23:44
                               ETA: 00:50:44

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 44880 steps/s (collection: 2.096s, learning 0.094s)
             Mean action noise std: 2.41
          Mean value_function loss: 95.8784
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 59.6491
                       Mean reward: 38.44
               Mean episode length: 83.09
    Episode_Reward/reaching_object: 0.2848
     Episode_Reward/lifting_object: 8.5373
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.1667
     Episode_Termination/robot_out: 49.8333
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.19s
                      Time elapsed: 00:23:47
                               ETA: 00:50:41

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 45182 steps/s (collection: 2.083s, learning 0.093s)
             Mean action noise std: 2.41
          Mean value_function loss: 105.2018
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 59.6519
                       Mean reward: 46.22
               Mean episode length: 79.52
    Episode_Reward/reaching_object: 0.2898
     Episode_Reward/lifting_object: 9.0699
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7917
     Episode_Termination/robot_out: 50.3750
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 2.18s
                      Time elapsed: 00:23:49
                               ETA: 00:50:39

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 44212 steps/s (collection: 2.129s, learning 0.095s)
             Mean action noise std: 2.41
          Mean value_function loss: 114.2585
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 59.6542
                       Mean reward: 43.27
               Mean episode length: 76.86
    Episode_Reward/reaching_object: 0.2749
     Episode_Reward/lifting_object: 8.4994
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.0417
     Episode_Termination/robot_out: 51.0417
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.22s
                      Time elapsed: 00:23:51
                               ETA: 00:50:37

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 45020 steps/s (collection: 2.091s, learning 0.093s)
             Mean action noise std: 2.41
          Mean value_function loss: 117.7516
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 59.6557
                       Mean reward: 43.07
               Mean episode length: 78.64
    Episode_Reward/reaching_object: 0.2754
     Episode_Reward/lifting_object: 8.6863
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7083
     Episode_Termination/robot_out: 51.1250
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.18s
                      Time elapsed: 00:23:53
                               ETA: 00:50:34

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 45105 steps/s (collection: 2.089s, learning 0.091s)
             Mean action noise std: 2.41
          Mean value_function loss: 132.0333
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 59.6563
                       Mean reward: 43.85
               Mean episode length: 76.93
    Episode_Reward/reaching_object: 0.2832
     Episode_Reward/lifting_object: 8.7912
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 53.0417
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.18s
                      Time elapsed: 00:23:55
                               ETA: 00:50:32

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 44588 steps/s (collection: 2.110s, learning 0.095s)
             Mean action noise std: 2.41
          Mean value_function loss: 127.8086
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 59.6577
                       Mean reward: 36.97
               Mean episode length: 72.01
    Episode_Reward/reaching_object: 0.2726
     Episode_Reward/lifting_object: 8.4273
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6250
     Episode_Termination/robot_out: 51.0417
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 2.20s
                      Time elapsed: 00:23:58
                               ETA: 00:50:30

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 44529 steps/s (collection: 2.101s, learning 0.107s)
             Mean action noise std: 2.41
          Mean value_function loss: 119.9103
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 59.6610
                       Mean reward: 43.57
               Mean episode length: 77.59
    Episode_Reward/reaching_object: 0.2787
     Episode_Reward/lifting_object: 8.5301
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 50.0833
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.21s
                      Time elapsed: 00:24:00
                               ETA: 00:50:28

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 41497 steps/s (collection: 2.261s, learning 0.108s)
             Mean action noise std: 2.41
          Mean value_function loss: 116.7638
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 59.6661
                       Mean reward: 48.14
               Mean episode length: 74.25
    Episode_Reward/reaching_object: 0.2806
     Episode_Reward/lifting_object: 8.7843
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3333
     Episode_Termination/robot_out: 53.6250
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.37s
                      Time elapsed: 00:24:02
                               ETA: 00:50:26

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 45194 steps/s (collection: 2.070s, learning 0.105s)
             Mean action noise std: 2.41
          Mean value_function loss: 106.7316
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 59.6713
                       Mean reward: 49.12
               Mean episode length: 75.96
    Episode_Reward/reaching_object: 0.2780
     Episode_Reward/lifting_object: 8.4239
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 51.7083
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.18s
                      Time elapsed: 00:24:04
                               ETA: 00:50:23

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 45904 steps/s (collection: 2.050s, learning 0.092s)
             Mean action noise std: 2.41
          Mean value_function loss: 127.6777
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.6768
                       Mean reward: 51.77
               Mean episode length: 78.68
    Episode_Reward/reaching_object: 0.2914
     Episode_Reward/lifting_object: 9.6097
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3333
     Episode_Termination/robot_out: 50.2083
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 2.14s
                      Time elapsed: 00:24:07
                               ETA: 00:50:21

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 45715 steps/s (collection: 2.037s, learning 0.113s)
             Mean action noise std: 2.41
          Mean value_function loss: 104.8614
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 59.6828
                       Mean reward: 44.02
               Mean episode length: 76.30
    Episode_Reward/reaching_object: 0.2847
     Episode_Reward/lifting_object: 9.1328
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.0833
     Episode_Termination/robot_out: 50.3333
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.15s
                      Time elapsed: 00:24:09
                               ETA: 00:50:18

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 46126 steps/s (collection: 2.038s, learning 0.093s)
             Mean action noise std: 2.41
          Mean value_function loss: 121.3877
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 59.6854
                       Mean reward: 40.46
               Mean episode length: 80.32
    Episode_Reward/reaching_object: 0.2852
     Episode_Reward/lifting_object: 9.2516
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.1250
     Episode_Termination/robot_out: 50.5833
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 2.13s
                      Time elapsed: 00:24:11
                               ETA: 00:50:16

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 46318 steps/s (collection: 2.033s, learning 0.089s)
             Mean action noise std: 2.41
          Mean value_function loss: 131.4771
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.6897
                       Mean reward: 46.44
               Mean episode length: 82.97
    Episode_Reward/reaching_object: 0.2931
     Episode_Reward/lifting_object: 9.3874
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 49.5000
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.12s
                      Time elapsed: 00:24:13
                               ETA: 00:50:14

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 45746 steps/s (collection: 2.059s, learning 0.090s)
             Mean action noise std: 2.41
          Mean value_function loss: 106.3543
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 59.6957
                       Mean reward: 51.51
               Mean episode length: 74.60
    Episode_Reward/reaching_object: 0.2925
     Episode_Reward/lifting_object: 9.9484
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.2083
     Episode_Termination/robot_out: 51.8333
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.15s
                      Time elapsed: 00:24:15
                               ETA: 00:50:11

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 45890 steps/s (collection: 2.053s, learning 0.089s)
             Mean action noise std: 2.41
          Mean value_function loss: 101.1292
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 59.7011
                       Mean reward: 48.45
               Mean episode length: 75.64
    Episode_Reward/reaching_object: 0.2790
     Episode_Reward/lifting_object: 9.2802
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5000
     Episode_Termination/robot_out: 49.5000
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.14s
                      Time elapsed: 00:24:17
                               ETA: 00:50:09

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 45955 steps/s (collection: 2.050s, learning 0.090s)
             Mean action noise std: 2.41
          Mean value_function loss: 112.5684
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 59.7054
                       Mean reward: 44.37
               Mean episode length: 76.85
    Episode_Reward/reaching_object: 0.2751
     Episode_Reward/lifting_object: 8.7692
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5833
     Episode_Termination/robot_out: 52.0417
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 2.14s
                      Time elapsed: 00:24:19
                               ETA: 00:50:06

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 46729 steps/s (collection: 2.009s, learning 0.095s)
             Mean action noise std: 2.41
          Mean value_function loss: 113.8692
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 59.7078
                       Mean reward: 44.68
               Mean episode length: 75.60
    Episode_Reward/reaching_object: 0.2802
     Episode_Reward/lifting_object: 9.1950
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 53.5000
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.10s
                      Time elapsed: 00:24:21
                               ETA: 00:50:04

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 46774 steps/s (collection: 2.014s, learning 0.088s)
             Mean action noise std: 2.41
          Mean value_function loss: 109.6453
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 59.7111
                       Mean reward: 57.51
               Mean episode length: 77.88
    Episode_Reward/reaching_object: 0.2809
     Episode_Reward/lifting_object: 9.6790
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5417
     Episode_Termination/robot_out: 50.8333
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 2.10s
                      Time elapsed: 00:24:24
                               ETA: 00:50:01

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 46556 steps/s (collection: 2.010s, learning 0.102s)
             Mean action noise std: 2.41
          Mean value_function loss: 132.1149
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 59.7141
                       Mean reward: 45.81
               Mean episode length: 78.11
    Episode_Reward/reaching_object: 0.2729
     Episode_Reward/lifting_object: 9.1131
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.8333
     Episode_Termination/robot_out: 54.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 2.11s
                      Time elapsed: 00:24:26
                               ETA: 00:49:59

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 45303 steps/s (collection: 2.066s, learning 0.104s)
             Mean action noise std: 2.41
          Mean value_function loss: 119.9746
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 59.7166
                       Mean reward: 54.09
               Mean episode length: 75.85
    Episode_Reward/reaching_object: 0.2741
     Episode_Reward/lifting_object: 9.2158
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 53.8333
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 2.17s
                      Time elapsed: 00:24:28
                               ETA: 00:49:56

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 44858 steps/s (collection: 2.102s, learning 0.090s)
             Mean action noise std: 2.41
          Mean value_function loss: 111.1473
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 59.7178
                       Mean reward: 40.57
               Mean episode length: 67.72
    Episode_Reward/reaching_object: 0.2751
     Episode_Reward/lifting_object: 9.1831
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5000
     Episode_Termination/robot_out: 52.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 2.19s
                      Time elapsed: 00:24:30
                               ETA: 00:49:54

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 46282 steps/s (collection: 2.030s, learning 0.094s)
             Mean action noise std: 2.41
          Mean value_function loss: 98.3776
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 59.7194
                       Mean reward: 48.94
               Mean episode length: 76.80
    Episode_Reward/reaching_object: 0.2713
     Episode_Reward/lifting_object: 9.1750
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5417
     Episode_Termination/robot_out: 52.3333
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 2.12s
                      Time elapsed: 00:24:32
                               ETA: 00:49:52

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 45863 steps/s (collection: 2.035s, learning 0.108s)
             Mean action noise std: 2.42
          Mean value_function loss: 126.8443
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 59.7231
                       Mean reward: 46.60
               Mean episode length: 72.12
    Episode_Reward/reaching_object: 0.2743
     Episode_Reward/lifting_object: 9.4426
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5000
     Episode_Termination/robot_out: 51.3333
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 2.14s
                      Time elapsed: 00:24:34
                               ETA: 00:49:49

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 46256 steps/s (collection: 2.029s, learning 0.097s)
             Mean action noise std: 2.42
          Mean value_function loss: 148.1914
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 59.7278
                       Mean reward: 53.38
               Mean episode length: 82.08
    Episode_Reward/reaching_object: 0.2757
     Episode_Reward/lifting_object: 9.2178
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.6250
     Episode_Termination/robot_out: 52.5417
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 2.13s
                      Time elapsed: 00:24:36
                               ETA: 00:49:47

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 46772 steps/s (collection: 2.009s, learning 0.093s)
             Mean action noise std: 2.42
          Mean value_function loss: 107.5283
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 59.7324
                       Mean reward: 57.04
               Mean episode length: 79.93
    Episode_Reward/reaching_object: 0.2717
     Episode_Reward/lifting_object: 8.9766
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0434
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7917
     Episode_Termination/robot_out: 51.7917
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 2.10s
                      Time elapsed: 00:24:39
                               ETA: 00:49:44

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 46607 steps/s (collection: 2.018s, learning 0.092s)
             Mean action noise std: 2.42
          Mean value_function loss: 142.7374
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.7361
                       Mean reward: 50.81
               Mean episode length: 80.44
    Episode_Reward/reaching_object: 0.2765
     Episode_Reward/lifting_object: 9.0604
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 50.0417
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 2.11s
                      Time elapsed: 00:24:41
                               ETA: 00:49:42

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 46164 steps/s (collection: 2.032s, learning 0.098s)
             Mean action noise std: 2.42
          Mean value_function loss: 124.4511
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.7407
                       Mean reward: 52.38
               Mean episode length: 82.60
    Episode_Reward/reaching_object: 0.2831
     Episode_Reward/lifting_object: 9.2148
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 50.9167
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 2.13s
                      Time elapsed: 00:24:43
                               ETA: 00:49:39

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 46199 steps/s (collection: 2.034s, learning 0.094s)
             Mean action noise std: 2.42
          Mean value_function loss: 140.5230
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 59.7466
                       Mean reward: 54.64
               Mean episode length: 77.05
    Episode_Reward/reaching_object: 0.2817
     Episode_Reward/lifting_object: 9.2104
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 50.5000
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 2.13s
                      Time elapsed: 00:24:45
                               ETA: 00:49:37

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 26951 steps/s (collection: 3.545s, learning 0.102s)
             Mean action noise std: 2.42
          Mean value_function loss: 111.0776
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 59.7536
                       Mean reward: 37.86
               Mean episode length: 75.83
    Episode_Reward/reaching_object: 0.2860
     Episode_Reward/lifting_object: 9.4288
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6250
     Episode_Termination/robot_out: 48.2083
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 3.65s
                      Time elapsed: 00:24:49
                               ETA: 00:49:38

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 13965 steps/s (collection: 6.918s, learning 0.121s)
             Mean action noise std: 2.42
          Mean value_function loss: 130.8948
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 59.7581
                       Mean reward: 60.79
               Mean episode length: 82.19
    Episode_Reward/reaching_object: 0.2849
     Episode_Reward/lifting_object: 9.6315
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 50.4167
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 7.04s
                      Time elapsed: 00:24:56
                               ETA: 00:49:45

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 14577 steps/s (collection: 6.626s, learning 0.118s)
             Mean action noise std: 2.42
          Mean value_function loss: 129.6303
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 59.7595
                       Mean reward: 44.60
               Mean episode length: 78.36
    Episode_Reward/reaching_object: 0.2819
     Episode_Reward/lifting_object: 9.0125
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3333
     Episode_Termination/robot_out: 53.9583
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 6.74s
                      Time elapsed: 00:25:02
                               ETA: 00:49:52

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 14179 steps/s (collection: 6.801s, learning 0.132s)
             Mean action noise std: 2.42
          Mean value_function loss: 100.9662
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 59.7606
                       Mean reward: 37.15
               Mean episode length: 70.29
    Episode_Reward/reaching_object: 0.2792
     Episode_Reward/lifting_object: 9.2032
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.9583
     Episode_Termination/robot_out: 47.3750
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 6.93s
                      Time elapsed: 00:25:09
                               ETA: 00:49:59

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 14540 steps/s (collection: 6.650s, learning 0.111s)
             Mean action noise std: 2.42
          Mean value_function loss: 108.0251
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 59.7631
                       Mean reward: 57.15
               Mean episode length: 80.24
    Episode_Reward/reaching_object: 0.2892
     Episode_Reward/lifting_object: 10.2598
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5417
     Episode_Termination/robot_out: 51.7083
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 6.76s
                      Time elapsed: 00:25:16
                               ETA: 00:50:05

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 14131 steps/s (collection: 6.813s, learning 0.143s)
             Mean action noise std: 2.42
          Mean value_function loss: 139.8993
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.7667
                       Mean reward: 40.82
               Mean episode length: 78.40
    Episode_Reward/reaching_object: 0.2835
     Episode_Reward/lifting_object: 9.6687
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 53.1250
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 6.96s
                      Time elapsed: 00:25:23
                               ETA: 00:50:12

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 13886 steps/s (collection: 6.952s, learning 0.127s)
             Mean action noise std: 2.42
          Mean value_function loss: 160.9967
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 59.7702
                       Mean reward: 58.04
               Mean episode length: 75.67
    Episode_Reward/reaching_object: 0.2826
     Episode_Reward/lifting_object: 9.3548
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 52.8750
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 7.08s
                      Time elapsed: 00:25:30
                               ETA: 00:50:20

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 14513 steps/s (collection: 6.659s, learning 0.115s)
             Mean action noise std: 2.42
          Mean value_function loss: 149.4178
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 59.7747
                       Mean reward: 50.93
               Mean episode length: 73.04
    Episode_Reward/reaching_object: 0.2929
     Episode_Reward/lifting_object: 10.5481
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 51.4167
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 6.77s
                      Time elapsed: 00:25:37
                               ETA: 00:50:26

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 14137 steps/s (collection: 6.839s, learning 0.115s)
             Mean action noise std: 2.42
          Mean value_function loss: 126.8055
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 59.7795
                       Mean reward: 53.57
               Mean episode length: 78.59
    Episode_Reward/reaching_object: 0.2899
     Episode_Reward/lifting_object: 10.1370
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0418
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5417
     Episode_Termination/robot_out: 51.4167
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 6.95s
                      Time elapsed: 00:25:44
                               ETA: 00:50:33

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 23599 steps/s (collection: 4.061s, learning 0.105s)
             Mean action noise std: 2.42
          Mean value_function loss: 153.0533
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 59.7845
                       Mean reward: 57.14
               Mean episode length: 80.75
    Episode_Reward/reaching_object: 0.3001
     Episode_Reward/lifting_object: 10.6509
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.0000
     Episode_Termination/robot_out: 50.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 4.17s
                      Time elapsed: 00:25:48
                               ETA: 00:50:35

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 46754 steps/s (collection: 2.003s, learning 0.100s)
             Mean action noise std: 2.42
          Mean value_function loss: 158.8197
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 59.7881
                       Mean reward: 57.63
               Mean episode length: 70.11
    Episode_Reward/reaching_object: 0.2975
     Episode_Reward/lifting_object: 10.3602
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 52.3750
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 2.10s
                      Time elapsed: 00:25:50
                               ETA: 00:50:32

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 46571 steps/s (collection: 2.016s, learning 0.095s)
             Mean action noise std: 2.42
          Mean value_function loss: 124.8942
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 59.7919
                       Mean reward: 47.56
               Mean episode length: 73.88
    Episode_Reward/reaching_object: 0.2989
     Episode_Reward/lifting_object: 10.6060
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 49.6667
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 2.11s
                      Time elapsed: 00:25:52
                               ETA: 00:50:29

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 43843 steps/s (collection: 2.153s, learning 0.090s)
             Mean action noise std: 2.42
          Mean value_function loss: 129.0957
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 59.7945
                       Mean reward: 51.17
               Mean episode length: 71.56
    Episode_Reward/reaching_object: 0.3041
     Episode_Reward/lifting_object: 11.0362
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.8333
     Episode_Termination/robot_out: 49.1250
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 2.24s
                      Time elapsed: 00:25:54
                               ETA: 00:50:27

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 46184 steps/s (collection: 2.033s, learning 0.096s)
             Mean action noise std: 2.42
          Mean value_function loss: 125.2290
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 59.7969
                       Mean reward: 67.59
               Mean episode length: 77.91
    Episode_Reward/reaching_object: 0.3053
     Episode_Reward/lifting_object: 11.2598
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7500
     Episode_Termination/robot_out: 50.7500
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 2.13s
                      Time elapsed: 00:25:57
                               ETA: 00:50:24

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 46817 steps/s (collection: 2.008s, learning 0.092s)
             Mean action noise std: 2.42
          Mean value_function loss: 132.1882
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 59.7991
                       Mean reward: 55.87
               Mean episode length: 77.84
    Episode_Reward/reaching_object: 0.2993
     Episode_Reward/lifting_object: 11.0089
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5833
     Episode_Termination/robot_out: 48.1250
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 2.10s
                      Time elapsed: 00:25:59
                               ETA: 00:50:22

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 47067 steps/s (collection: 1.998s, learning 0.091s)
             Mean action noise std: 2.42
          Mean value_function loss: 128.3465
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 59.8035
                       Mean reward: 56.18
               Mean episode length: 79.04
    Episode_Reward/reaching_object: 0.3003
     Episode_Reward/lifting_object: 10.6871
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.9167
     Episode_Termination/robot_out: 48.8333
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 2.09s
                      Time elapsed: 00:26:01
                               ETA: 00:50:19

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 45975 steps/s (collection: 2.046s, learning 0.092s)
             Mean action noise std: 2.42
          Mean value_function loss: 115.9856
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 59.8101
                       Mean reward: 50.75
               Mean episode length: 75.69
    Episode_Reward/reaching_object: 0.3012
     Episode_Reward/lifting_object: 11.0149
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 50.5833
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 2.14s
                      Time elapsed: 00:26:03
                               ETA: 00:50:16

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 46227 steps/s (collection: 2.041s, learning 0.086s)
             Mean action noise std: 2.42
          Mean value_function loss: 138.3400
               Mean surrogate loss: 0.0120
                 Mean entropy loss: 59.8123
                       Mean reward: 52.17
               Mean episode length: 76.77
    Episode_Reward/reaching_object: 0.2893
     Episode_Reward/lifting_object: 10.5874
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7500
     Episode_Termination/robot_out: 51.5833
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 2.13s
                      Time elapsed: 00:26:05
                               ETA: 00:50:14

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 45891 steps/s (collection: 2.034s, learning 0.109s)
             Mean action noise std: 2.42
          Mean value_function loss: 140.7830
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 59.8129
                       Mean reward: 58.25
               Mean episode length: 80.52
    Episode_Reward/reaching_object: 0.2991
     Episode_Reward/lifting_object: 10.9142
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 48.6250
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 2.14s
                      Time elapsed: 00:26:07
                               ETA: 00:50:11

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 46516 steps/s (collection: 2.027s, learning 0.087s)
             Mean action noise std: 2.42
          Mean value_function loss: 154.6231
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 59.8158
                       Mean reward: 54.38
               Mean episode length: 76.55
    Episode_Reward/reaching_object: 0.2994
     Episode_Reward/lifting_object: 10.9344
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 50.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 2.11s
                      Time elapsed: 00:26:09
                               ETA: 00:50:09

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 46123 steps/s (collection: 2.046s, learning 0.085s)
             Mean action noise std: 2.43
          Mean value_function loss: 148.1891
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 59.8216
                       Mean reward: 55.34
               Mean episode length: 75.48
    Episode_Reward/reaching_object: 0.2931
     Episode_Reward/lifting_object: 10.1936
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7917
     Episode_Termination/robot_out: 51.4583
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 2.13s
                      Time elapsed: 00:26:11
                               ETA: 00:50:06

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 46710 steps/s (collection: 2.017s, learning 0.088s)
             Mean action noise std: 2.43
          Mean value_function loss: 141.1880
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 59.8250
                       Mean reward: 46.01
               Mean episode length: 78.96
    Episode_Reward/reaching_object: 0.3031
     Episode_Reward/lifting_object: 11.2227
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4583
     Episode_Termination/robot_out: 50.5417
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 2.10s
                      Time elapsed: 00:26:13
                               ETA: 00:50:03

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 46153 steps/s (collection: 2.022s, learning 0.108s)
             Mean action noise std: 2.43
          Mean value_function loss: 137.7979
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 59.8262
                       Mean reward: 59.59
               Mean episode length: 78.65
    Episode_Reward/reaching_object: 0.3038
     Episode_Reward/lifting_object: 11.5421
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.8333
     Episode_Termination/robot_out: 51.6250
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 2.13s
                      Time elapsed: 00:26:16
                               ETA: 00:50:01

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 46120 steps/s (collection: 2.018s, learning 0.113s)
             Mean action noise std: 2.43
          Mean value_function loss: 137.9761
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 59.8271
                       Mean reward: 58.14
               Mean episode length: 77.21
    Episode_Reward/reaching_object: 0.3017
     Episode_Reward/lifting_object: 11.6055
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 52.6250
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 2.13s
                      Time elapsed: 00:26:18
                               ETA: 00:49:58

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 45908 steps/s (collection: 2.031s, learning 0.110s)
             Mean action noise std: 2.43
          Mean value_function loss: 146.4286
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 59.8287
                       Mean reward: 60.56
               Mean episode length: 76.56
    Episode_Reward/reaching_object: 0.3006
     Episode_Reward/lifting_object: 10.7991
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 47.2917
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 2.14s
                      Time elapsed: 00:26:20
                               ETA: 00:49:56

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 46415 steps/s (collection: 2.023s, learning 0.095s)
             Mean action noise std: 2.43
          Mean value_function loss: 135.0295
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.8301
                       Mean reward: 66.89
               Mean episode length: 81.62
    Episode_Reward/reaching_object: 0.3108
     Episode_Reward/lifting_object: 11.5928
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 50.9167
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 2.12s
                      Time elapsed: 00:26:22
                               ETA: 00:49:53

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 47043 steps/s (collection: 1.995s, learning 0.095s)
             Mean action noise std: 2.43
          Mean value_function loss: 156.8362
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 59.8331
                       Mean reward: 59.11
               Mean episode length: 75.88
    Episode_Reward/reaching_object: 0.3063
     Episode_Reward/lifting_object: 11.6580
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 49.2083
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 2.09s
                      Time elapsed: 00:26:24
                               ETA: 00:49:50

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 45789 steps/s (collection: 2.037s, learning 0.110s)
             Mean action noise std: 2.43
          Mean value_function loss: 149.8526
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 59.8353
                       Mean reward: 45.83
               Mean episode length: 80.22
    Episode_Reward/reaching_object: 0.3152
     Episode_Reward/lifting_object: 11.4819
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 51.0417
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 2.15s
                      Time elapsed: 00:26:26
                               ETA: 00:49:48

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 45924 steps/s (collection: 2.052s, learning 0.089s)
             Mean action noise std: 2.43
          Mean value_function loss: 182.0174
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 59.8399
                       Mean reward: 52.70
               Mean episode length: 78.64
    Episode_Reward/reaching_object: 0.3052
     Episode_Reward/lifting_object: 11.1364
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 48.6250
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 2.14s
                      Time elapsed: 00:26:28
                               ETA: 00:49:45

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 46757 steps/s (collection: 2.016s, learning 0.087s)
             Mean action noise std: 2.43
          Mean value_function loss: 179.9357
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 59.8448
                       Mean reward: 64.01
               Mean episode length: 81.25
    Episode_Reward/reaching_object: 0.3065
     Episode_Reward/lifting_object: 11.4154
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 52.0833
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 2.10s
                      Time elapsed: 00:26:30
                               ETA: 00:49:43

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 46139 steps/s (collection: 2.042s, learning 0.089s)
             Mean action noise std: 2.43
          Mean value_function loss: 141.4383
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 59.8477
                       Mean reward: 63.27
               Mean episode length: 80.65
    Episode_Reward/reaching_object: 0.3179
     Episode_Reward/lifting_object: 12.4542
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 48.9167
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 2.13s
                      Time elapsed: 00:26:33
                               ETA: 00:49:40

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 46706 steps/s (collection: 2.015s, learning 0.090s)
             Mean action noise std: 2.43
          Mean value_function loss: 137.9187
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 59.8503
                       Mean reward: 60.86
               Mean episode length: 83.70
    Episode_Reward/reaching_object: 0.3120
     Episode_Reward/lifting_object: 11.9621
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 48.1667
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 2.10s
                      Time elapsed: 00:26:35
                               ETA: 00:49:37

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 46141 steps/s (collection: 2.045s, learning 0.086s)
             Mean action noise std: 2.43
          Mean value_function loss: 135.5419
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 59.8517
                       Mean reward: 48.91
               Mean episode length: 76.71
    Episode_Reward/reaching_object: 0.3198
     Episode_Reward/lifting_object: 12.6020
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.9583
     Episode_Termination/robot_out: 46.1667
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 2.13s
                      Time elapsed: 00:26:37
                               ETA: 00:49:35

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 47214 steps/s (collection: 1.997s, learning 0.085s)
             Mean action noise std: 2.43
          Mean value_function loss: 124.7227
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 59.8523
                       Mean reward: 68.08
               Mean episode length: 83.63
    Episode_Reward/reaching_object: 0.3155
     Episode_Reward/lifting_object: 12.4314
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 48.2083
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 2.08s
                      Time elapsed: 00:26:39
                               ETA: 00:49:32

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 45950 steps/s (collection: 2.038s, learning 0.102s)
             Mean action noise std: 2.43
          Mean value_function loss: 144.3622
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 59.8526
                       Mean reward: 52.62
               Mean episode length: 79.63
    Episode_Reward/reaching_object: 0.3092
     Episode_Reward/lifting_object: 11.7466
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 48.7917
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 2.14s
                      Time elapsed: 00:26:41
                               ETA: 00:49:30

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 45501 steps/s (collection: 2.062s, learning 0.099s)
             Mean action noise std: 2.43
          Mean value_function loss: 151.6221
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 59.8531
                       Mean reward: 57.03
               Mean episode length: 77.72
    Episode_Reward/reaching_object: 0.3119
     Episode_Reward/lifting_object: 12.0678
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 49.6250
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 2.16s
                      Time elapsed: 00:26:43
                               ETA: 00:49:27

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 46272 steps/s (collection: 2.017s, learning 0.108s)
             Mean action noise std: 2.43
          Mean value_function loss: 171.1004
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 59.8540
                       Mean reward: 66.40
               Mean episode length: 87.60
    Episode_Reward/reaching_object: 0.3149
     Episode_Reward/lifting_object: 12.0648
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5833
     Episode_Termination/robot_out: 44.3750
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 2.12s
                      Time elapsed: 00:26:45
                               ETA: 00:49:25

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 46280 steps/s (collection: 2.012s, learning 0.112s)
             Mean action noise std: 2.43
          Mean value_function loss: 138.2388
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 59.8546
                       Mean reward: 62.25
               Mean episode length: 82.39
    Episode_Reward/reaching_object: 0.3138
     Episode_Reward/lifting_object: 11.7813
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 48.0417
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 2.12s
                      Time elapsed: 00:26:47
                               ETA: 00:49:22

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 46391 steps/s (collection: 2.013s, learning 0.106s)
             Mean action noise std: 2.43
          Mean value_function loss: 162.7969
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.8562
                       Mean reward: 54.36
               Mean episode length: 78.59
    Episode_Reward/reaching_object: 0.3142
     Episode_Reward/lifting_object: 12.0150
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.0833
     Episode_Termination/robot_out: 44.5417
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 2.12s
                      Time elapsed: 00:26:50
                               ETA: 00:49:19

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 46412 steps/s (collection: 2.022s, learning 0.096s)
             Mean action noise std: 2.43
          Mean value_function loss: 133.8455
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 59.8590
                       Mean reward: 65.32
               Mean episode length: 84.90
    Episode_Reward/reaching_object: 0.3118
     Episode_Reward/lifting_object: 11.7780
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6250
     Episode_Termination/robot_out: 44.6667
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 2.12s
                      Time elapsed: 00:26:52
                               ETA: 00:49:17

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 46154 steps/s (collection: 2.039s, learning 0.091s)
             Mean action noise std: 2.43
          Mean value_function loss: 156.0862
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 59.8614
                       Mean reward: 59.66
               Mean episode length: 87.50
    Episode_Reward/reaching_object: 0.3077
     Episode_Reward/lifting_object: 11.8237
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.3750
     Episode_Termination/robot_out: 45.0000
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 2.13s
                      Time elapsed: 00:26:54
                               ETA: 00:49:14

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 46743 steps/s (collection: 2.018s, learning 0.086s)
             Mean action noise std: 2.43
          Mean value_function loss: 142.3737
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.8670
                       Mean reward: 49.99
               Mean episode length: 84.89
    Episode_Reward/reaching_object: 0.3198
     Episode_Reward/lifting_object: 11.7651
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6250
     Episode_Termination/robot_out: 47.9583
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 2.10s
                      Time elapsed: 00:26:56
                               ETA: 00:49:12

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 46770 steps/s (collection: 2.015s, learning 0.087s)
             Mean action noise std: 2.43
          Mean value_function loss: 138.1782
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 59.8714
                       Mean reward: 64.54
               Mean episode length: 91.67
    Episode_Reward/reaching_object: 0.3104
     Episode_Reward/lifting_object: 11.5023
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.9167
     Episode_Termination/robot_out: 49.8750
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 2.10s
                      Time elapsed: 00:26:58
                               ETA: 00:49:09

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 46188 steps/s (collection: 2.037s, learning 0.092s)
             Mean action noise std: 2.43
          Mean value_function loss: 170.4850
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 59.8720
                       Mean reward: 52.76
               Mean episode length: 75.93
    Episode_Reward/reaching_object: 0.2983
     Episode_Reward/lifting_object: 11.0318
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 50.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 2.13s
                      Time elapsed: 00:27:00
                               ETA: 00:49:06

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 45344 steps/s (collection: 2.075s, learning 0.093s)
             Mean action noise std: 2.43
          Mean value_function loss: 159.4539
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 59.8723
                       Mean reward: 65.82
               Mean episode length: 82.11
    Episode_Reward/reaching_object: 0.3076
     Episode_Reward/lifting_object: 11.6024
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7083
     Episode_Termination/robot_out: 48.6667
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 2.17s
                      Time elapsed: 00:27:02
                               ETA: 00:49:04

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 46221 steps/s (collection: 2.038s, learning 0.089s)
             Mean action noise std: 2.43
          Mean value_function loss: 172.4151
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 59.8727
                       Mean reward: 65.36
               Mean episode length: 82.53
    Episode_Reward/reaching_object: 0.3018
     Episode_Reward/lifting_object: 11.3460
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5000
     Episode_Termination/robot_out: 52.9167
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 2.13s
                      Time elapsed: 00:27:04
                               ETA: 00:49:01

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 45892 steps/s (collection: 2.045s, learning 0.098s)
             Mean action noise std: 2.43
          Mean value_function loss: 162.4146
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 59.8734
                       Mean reward: 50.61
               Mean episode length: 73.67
    Episode_Reward/reaching_object: 0.3024
     Episode_Reward/lifting_object: 11.5737
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 50.1667
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 2.14s
                      Time elapsed: 00:27:07
                               ETA: 00:48:59

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 45702 steps/s (collection: 2.067s, learning 0.084s)
             Mean action noise std: 2.43
          Mean value_function loss: 162.8484
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 59.8747
                       Mean reward: 55.00
               Mean episode length: 77.34
    Episode_Reward/reaching_object: 0.3109
     Episode_Reward/lifting_object: 12.1758
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 48.2917
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 2.15s
                      Time elapsed: 00:27:09
                               ETA: 00:48:56

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 46239 steps/s (collection: 2.037s, learning 0.089s)
             Mean action noise std: 2.43
          Mean value_function loss: 141.2890
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 59.8766
                       Mean reward: 66.91
               Mean episode length: 76.90
    Episode_Reward/reaching_object: 0.3198
     Episode_Reward/lifting_object: 13.0139
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 48.4583
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 2.13s
                      Time elapsed: 00:27:11
                               ETA: 00:48:54

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 45394 steps/s (collection: 2.073s, learning 0.092s)
             Mean action noise std: 2.43
          Mean value_function loss: 156.6927
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 59.8783
                       Mean reward: 70.62
               Mean episode length: 84.17
    Episode_Reward/reaching_object: 0.3234
     Episode_Reward/lifting_object: 13.1615
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7083
     Episode_Termination/robot_out: 49.3333
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 2.17s
                      Time elapsed: 00:27:13
                               ETA: 00:48:51

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 45581 steps/s (collection: 2.063s, learning 0.094s)
             Mean action noise std: 2.43
          Mean value_function loss: 164.5483
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 59.8830
                       Mean reward: 59.99
               Mean episode length: 77.26
    Episode_Reward/reaching_object: 0.3167
     Episode_Reward/lifting_object: 12.4753
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3333
     Episode_Termination/robot_out: 48.2083
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 2.16s
                      Time elapsed: 00:27:15
                               ETA: 00:48:49

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 44520 steps/s (collection: 2.100s, learning 0.109s)
             Mean action noise std: 2.43
          Mean value_function loss: 166.5910
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.8916
                       Mean reward: 77.20
               Mean episode length: 87.03
    Episode_Reward/reaching_object: 0.3143
     Episode_Reward/lifting_object: 12.5163
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 48.5833
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 2.21s
                      Time elapsed: 00:27:17
                               ETA: 00:48:46

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 45593 steps/s (collection: 2.061s, learning 0.095s)
             Mean action noise std: 2.43
          Mean value_function loss: 180.4488
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 59.8971
                       Mean reward: 51.83
               Mean episode length: 75.16
    Episode_Reward/reaching_object: 0.3285
     Episode_Reward/lifting_object: 13.1683
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 49.1250
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 2.16s
                      Time elapsed: 00:27:20
                               ETA: 00:48:44

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 46325 steps/s (collection: 2.034s, learning 0.088s)
             Mean action noise std: 2.43
          Mean value_function loss: 179.7348
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 59.8992
                       Mean reward: 68.92
               Mean episode length: 77.85
    Episode_Reward/reaching_object: 0.3172
     Episode_Reward/lifting_object: 12.6875
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.9167
     Episode_Termination/robot_out: 50.8750
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 2.12s
                      Time elapsed: 00:27:22
                               ETA: 00:48:41

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 46478 steps/s (collection: 2.027s, learning 0.088s)
             Mean action noise std: 2.43
          Mean value_function loss: 170.5449
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 59.9007
                       Mean reward: 68.64
               Mean episode length: 81.86
    Episode_Reward/reaching_object: 0.3188
     Episode_Reward/lifting_object: 13.0421
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 49.2917
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 2.12s
                      Time elapsed: 00:27:24
                               ETA: 00:48:39

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 45775 steps/s (collection: 2.054s, learning 0.094s)
             Mean action noise std: 2.43
          Mean value_function loss: 201.1013
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 59.9024
                       Mean reward: 57.37
               Mean episode length: 81.57
    Episode_Reward/reaching_object: 0.3084
     Episode_Reward/lifting_object: 12.1052
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 47.7500
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 2.15s
                      Time elapsed: 00:27:26
                               ETA: 00:48:36

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 46129 steps/s (collection: 2.042s, learning 0.089s)
             Mean action noise std: 2.43
          Mean value_function loss: 172.7077
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 59.9043
                       Mean reward: 70.24
               Mean episode length: 82.86
    Episode_Reward/reaching_object: 0.3213
     Episode_Reward/lifting_object: 13.4516
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5833
     Episode_Termination/robot_out: 48.2917
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 2.13s
                      Time elapsed: 00:27:28
                               ETA: 00:48:34

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 45460 steps/s (collection: 2.072s, learning 0.090s)
             Mean action noise std: 2.43
          Mean value_function loss: 200.8724
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 59.9048
                       Mean reward: 66.51
               Mean episode length: 84.22
    Episode_Reward/reaching_object: 0.3208
     Episode_Reward/lifting_object: 13.4160
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 48.2917
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 2.16s
                      Time elapsed: 00:27:30
                               ETA: 00:48:31

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 45993 steps/s (collection: 2.042s, learning 0.095s)
             Mean action noise std: 2.43
          Mean value_function loss: 194.1283
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 59.9055
                       Mean reward: 59.14
               Mean episode length: 79.52
    Episode_Reward/reaching_object: 0.3216
     Episode_Reward/lifting_object: 12.5918
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 46.4167
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 2.14s
                      Time elapsed: 00:27:32
                               ETA: 00:48:29

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 45759 steps/s (collection: 2.060s, learning 0.089s)
             Mean action noise std: 2.43
          Mean value_function loss: 163.9167
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 59.9063
                       Mean reward: 68.91
               Mean episode length: 77.60
    Episode_Reward/reaching_object: 0.3306
     Episode_Reward/lifting_object: 14.0400
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3333
     Episode_Termination/robot_out: 47.9167
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 2.15s
                      Time elapsed: 00:27:35
                               ETA: 00:48:26

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 44731 steps/s (collection: 2.097s, learning 0.101s)
             Mean action noise std: 2.43
          Mean value_function loss: 206.2561
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 59.9075
                       Mean reward: 59.85
               Mean episode length: 78.47
    Episode_Reward/reaching_object: 0.3280
     Episode_Reward/lifting_object: 13.6199
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 46.6250
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 2.20s
                      Time elapsed: 00:27:37
                               ETA: 00:48:24

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 45991 steps/s (collection: 2.050s, learning 0.088s)
             Mean action noise std: 2.43
          Mean value_function loss: 166.0775
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 59.9090
                       Mean reward: 68.65
               Mean episode length: 83.36
    Episode_Reward/reaching_object: 0.3310
     Episode_Reward/lifting_object: 13.9444
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 47.7917
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 2.14s
                      Time elapsed: 00:27:39
                               ETA: 00:48:21

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 45905 steps/s (collection: 2.046s, learning 0.095s)
             Mean action noise std: 2.43
          Mean value_function loss: 188.5316
               Mean surrogate loss: 0.0101
                 Mean entropy loss: 59.9099
                       Mean reward: 78.75
               Mean episode length: 85.24
    Episode_Reward/reaching_object: 0.3308
     Episode_Reward/lifting_object: 13.9392
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5417
     Episode_Termination/robot_out: 46.9583
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 2.14s
                      Time elapsed: 00:27:41
                               ETA: 00:48:19

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 46227 steps/s (collection: 2.035s, learning 0.092s)
             Mean action noise std: 2.43
          Mean value_function loss: 191.5632
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 59.9101
                       Mean reward: 69.94
               Mean episode length: 85.41
    Episode_Reward/reaching_object: 0.3298
     Episode_Reward/lifting_object: 13.6615
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 47.9583
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 2.13s
                      Time elapsed: 00:27:43
                               ETA: 00:48:16

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 45185 steps/s (collection: 2.076s, learning 0.100s)
             Mean action noise std: 2.43
          Mean value_function loss: 208.2104
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 59.9102
                       Mean reward: 73.14
               Mean episode length: 83.05
    Episode_Reward/reaching_object: 0.3332
     Episode_Reward/lifting_object: 14.4436
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.6250
     Episode_Termination/robot_out: 47.3333
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 2.18s
                      Time elapsed: 00:27:45
                               ETA: 00:48:14

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 45982 steps/s (collection: 2.031s, learning 0.107s)
             Mean action noise std: 2.43
          Mean value_function loss: 197.9624
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 59.9106
                       Mean reward: 73.92
               Mean episode length: 81.12
    Episode_Reward/reaching_object: 0.3303
     Episode_Reward/lifting_object: 14.2545
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 47.3333
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 2.14s
                      Time elapsed: 00:27:47
                               ETA: 00:48:11

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 45638 steps/s (collection: 2.049s, learning 0.105s)
             Mean action noise std: 2.43
          Mean value_function loss: 191.7469
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 59.9110
                       Mean reward: 70.45
               Mean episode length: 82.94
    Episode_Reward/reaching_object: 0.3203
     Episode_Reward/lifting_object: 13.7939
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4583
     Episode_Termination/robot_out: 47.7500
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 2.15s
                      Time elapsed: 00:27:50
                               ETA: 00:48:09

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 46031 steps/s (collection: 2.046s, learning 0.090s)
             Mean action noise std: 2.43
          Mean value_function loss: 175.6819
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.9123
                       Mean reward: 64.49
               Mean episode length: 83.29
    Episode_Reward/reaching_object: 0.3246
     Episode_Reward/lifting_object: 14.0312
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6250
     Episode_Termination/robot_out: 48.8333
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 2.14s
                      Time elapsed: 00:27:52
                               ETA: 00:48:06

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 45356 steps/s (collection: 2.074s, learning 0.093s)
             Mean action noise std: 2.43
          Mean value_function loss: 205.1477
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 59.9139
                       Mean reward: 76.02
               Mean episode length: 83.02
    Episode_Reward/reaching_object: 0.3284
     Episode_Reward/lifting_object: 14.2104
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 48.1250
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 2.17s
                      Time elapsed: 00:27:54
                               ETA: 00:48:04

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 45418 steps/s (collection: 2.067s, learning 0.097s)
             Mean action noise std: 2.43
          Mean value_function loss: 196.3872
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 59.9150
                       Mean reward: 70.68
               Mean episode length: 85.82
    Episode_Reward/reaching_object: 0.3330
     Episode_Reward/lifting_object: 14.6199
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 47.0417
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 2.16s
                      Time elapsed: 00:27:56
                               ETA: 00:48:01

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 45201 steps/s (collection: 2.080s, learning 0.095s)
             Mean action noise std: 2.43
          Mean value_function loss: 189.4644
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 59.9162
                       Mean reward: 72.70
               Mean episode length: 74.29
    Episode_Reward/reaching_object: 0.3284
     Episode_Reward/lifting_object: 14.4534
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 45.4583
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 2.17s
                      Time elapsed: 00:27:58
                               ETA: 00:47:59

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 45618 steps/s (collection: 2.061s, learning 0.094s)
             Mean action noise std: 2.43
          Mean value_function loss: 201.4211
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 59.9178
                       Mean reward: 73.21
               Mean episode length: 84.58
    Episode_Reward/reaching_object: 0.3335
     Episode_Reward/lifting_object: 15.0107
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 45.8333
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 2.15s
                      Time elapsed: 00:28:00
                               ETA: 00:47:56

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 44905 steps/s (collection: 2.098s, learning 0.091s)
             Mean action noise std: 2.44
          Mean value_function loss: 172.3300
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 59.9187
                       Mean reward: 70.21
               Mean episode length: 77.42
    Episode_Reward/reaching_object: 0.3267
     Episode_Reward/lifting_object: 14.3913
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.7500
     Episode_Termination/robot_out: 49.5833
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 2.19s
                      Time elapsed: 00:28:03
                               ETA: 00:47:54

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 46077 steps/s (collection: 2.038s, learning 0.096s)
             Mean action noise std: 2.44
          Mean value_function loss: 192.8938
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 59.9195
                       Mean reward: 79.50
               Mean episode length: 84.14
    Episode_Reward/reaching_object: 0.3266
     Episode_Reward/lifting_object: 14.7518
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 51.1667
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 2.13s
                      Time elapsed: 00:28:05
                               ETA: 00:47:51

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 45558 steps/s (collection: 2.066s, learning 0.092s)
             Mean action noise std: 2.44
          Mean value_function loss: 185.3812
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 59.9204
                       Mean reward: 71.75
               Mean episode length: 79.94
    Episode_Reward/reaching_object: 0.3062
     Episode_Reward/lifting_object: 13.4489
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 48.7917
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 2.16s
                      Time elapsed: 00:28:07
                               ETA: 00:47:49

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 45938 steps/s (collection: 2.052s, learning 0.088s)
             Mean action noise std: 2.44
          Mean value_function loss: 256.6563
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 59.9241
                       Mean reward: 78.05
               Mean episode length: 90.21
    Episode_Reward/reaching_object: 0.3099
     Episode_Reward/lifting_object: 12.9993
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 47.7083
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 2.14s
                      Time elapsed: 00:28:09
                               ETA: 00:47:46

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 45883 steps/s (collection: 2.053s, learning 0.090s)
             Mean action noise std: 2.44
          Mean value_function loss: 217.0594
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 59.9299
                       Mean reward: 71.12
               Mean episode length: 82.43
    Episode_Reward/reaching_object: 0.3058
     Episode_Reward/lifting_object: 12.6236
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 52.6667
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 2.14s
                      Time elapsed: 00:28:11
                               ETA: 00:47:44

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 45951 steps/s (collection: 2.053s, learning 0.087s)
             Mean action noise std: 2.44
          Mean value_function loss: 217.4678
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 59.9334
                       Mean reward: 67.70
               Mean episode length: 82.00
    Episode_Reward/reaching_object: 0.2988
     Episode_Reward/lifting_object: 12.2078
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 51.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 2.14s
                      Time elapsed: 00:28:13
                               ETA: 00:47:41

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 45419 steps/s (collection: 2.051s, learning 0.114s)
             Mean action noise std: 2.44
          Mean value_function loss: 218.4160
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 59.9377
                       Mean reward: 70.98
               Mean episode length: 82.24
    Episode_Reward/reaching_object: 0.3158
     Episode_Reward/lifting_object: 13.7956
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5833
     Episode_Termination/robot_out: 44.4167
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 2.16s
                      Time elapsed: 00:28:15
                               ETA: 00:47:39

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 46003 steps/s (collection: 2.041s, learning 0.096s)
             Mean action noise std: 2.44
          Mean value_function loss: 163.9906
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 59.9410
                       Mean reward: 74.57
               Mean episode length: 83.31
    Episode_Reward/reaching_object: 0.3178
     Episode_Reward/lifting_object: 13.6940
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.1250
     Episode_Termination/robot_out: 46.0833
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 2.14s
                      Time elapsed: 00:28:18
                               ETA: 00:47:36

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 46263 steps/s (collection: 2.036s, learning 0.089s)
             Mean action noise std: 2.44
          Mean value_function loss: 155.8776
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 59.9451
                       Mean reward: 68.29
               Mean episode length: 84.95
    Episode_Reward/reaching_object: 0.3152
     Episode_Reward/lifting_object: 13.6655
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.6250
     Episode_Termination/robot_out: 47.2917
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 2.12s
                      Time elapsed: 00:28:20
                               ETA: 00:47:34

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 45712 steps/s (collection: 2.054s, learning 0.096s)
             Mean action noise std: 2.44
          Mean value_function loss: 182.2060
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 59.9488
                       Mean reward: 56.87
               Mean episode length: 77.44
    Episode_Reward/reaching_object: 0.2973
     Episode_Reward/lifting_object: 12.7651
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.9167
     Episode_Termination/robot_out: 45.3333
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 2.15s
                      Time elapsed: 00:28:22
                               ETA: 00:47:31

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 46350 steps/s (collection: 2.031s, learning 0.090s)
             Mean action noise std: 2.44
          Mean value_function loss: 191.3160
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 59.9521
                       Mean reward: 62.44
               Mean episode length: 85.06
    Episode_Reward/reaching_object: 0.3009
     Episode_Reward/lifting_object: 12.7331
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.5000
     Episode_Termination/robot_out: 46.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 2.12s
                      Time elapsed: 00:28:24
                               ETA: 00:47:29

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 46037 steps/s (collection: 2.045s, learning 0.090s)
             Mean action noise std: 2.44
          Mean value_function loss: 192.0026
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 59.9545
                       Mean reward: 73.39
               Mean episode length: 91.62
    Episode_Reward/reaching_object: 0.3030
     Episode_Reward/lifting_object: 13.1192
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.2500
     Episode_Termination/robot_out: 46.0000
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 2.14s
                      Time elapsed: 00:28:26
                               ETA: 00:47:26

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 46216 steps/s (collection: 2.039s, learning 0.088s)
             Mean action noise std: 2.44
          Mean value_function loss: 204.3728
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 59.9584
                       Mean reward: 58.14
               Mean episode length: 76.52
    Episode_Reward/reaching_object: 0.2919
     Episode_Reward/lifting_object: 12.1793
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 3.8750
     Episode_Termination/robot_out: 45.8333
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 2.13s
                      Time elapsed: 00:28:28
                               ETA: 00:47:24

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 45601 steps/s (collection: 2.068s, learning 0.088s)
             Mean action noise std: 2.44
          Mean value_function loss: 204.1166
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 59.9633
                       Mean reward: 63.45
               Mean episode length: 78.78
    Episode_Reward/reaching_object: 0.2994
     Episode_Reward/lifting_object: 12.9130
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 47.0833
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 2.16s
                      Time elapsed: 00:28:30
                               ETA: 00:47:21

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 45614 steps/s (collection: 2.063s, learning 0.092s)
             Mean action noise std: 2.44
          Mean value_function loss: 194.9791
               Mean surrogate loss: 0.0143
                 Mean entropy loss: 59.9663
                       Mean reward: 65.95
               Mean episode length: 86.45
    Episode_Reward/reaching_object: 0.3101
     Episode_Reward/lifting_object: 13.7882
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 48.7083
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 2.16s
                      Time elapsed: 00:28:33
                               ETA: 00:47:19

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 45292 steps/s (collection: 2.081s, learning 0.090s)
             Mean action noise std: 2.44
          Mean value_function loss: 262.9679
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.9685
                       Mean reward: 62.73
               Mean episode length: 76.15
    Episode_Reward/reaching_object: 0.2981
     Episode_Reward/lifting_object: 13.2818
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 54.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 2.17s
                      Time elapsed: 00:28:35
                               ETA: 00:47:16

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 45292 steps/s (collection: 2.081s, learning 0.090s)
             Mean action noise std: 2.44
          Mean value_function loss: 230.0934
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 59.9727
                       Mean reward: 79.39
               Mean episode length: 83.19
    Episode_Reward/reaching_object: 0.3066
     Episode_Reward/lifting_object: 14.2279
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 49.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 2.17s
                      Time elapsed: 00:28:37
                               ETA: 00:47:14

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 44723 steps/s (collection: 2.103s, learning 0.095s)
             Mean action noise std: 2.44
          Mean value_function loss: 236.8475
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 59.9750
                       Mean reward: 73.42
               Mean episode length: 78.83
    Episode_Reward/reaching_object: 0.3076
     Episode_Reward/lifting_object: 14.6971
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 51.3750
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 2.20s
                      Time elapsed: 00:28:39
                               ETA: 00:47:11

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 45269 steps/s (collection: 2.080s, learning 0.092s)
             Mean action noise std: 2.44
          Mean value_function loss: 239.3631
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 59.9766
                       Mean reward: 88.62
               Mean episode length: 82.77
    Episode_Reward/reaching_object: 0.3167
     Episode_Reward/lifting_object: 15.2055
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 46.2083
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 2.17s
                      Time elapsed: 00:28:41
                               ETA: 00:47:09

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 45671 steps/s (collection: 2.070s, learning 0.083s)
             Mean action noise std: 2.44
          Mean value_function loss: 246.7177
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 59.9781
                       Mean reward: 83.92
               Mean episode length: 82.40
    Episode_Reward/reaching_object: 0.3335
     Episode_Reward/lifting_object: 16.1193
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 50.5000
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 2.15s
                      Time elapsed: 00:28:43
                               ETA: 00:47:06

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 45406 steps/s (collection: 2.066s, learning 0.099s)
             Mean action noise std: 2.44
          Mean value_function loss: 296.8833
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 59.9804
                       Mean reward: 68.58
               Mean episode length: 75.96
    Episode_Reward/reaching_object: 0.3138
     Episode_Reward/lifting_object: 14.8556
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 50.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 2.16s
                      Time elapsed: 00:28:46
                               ETA: 00:47:04

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 44312 steps/s (collection: 2.084s, learning 0.134s)
             Mean action noise std: 2.44
          Mean value_function loss: 275.7704
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 59.9811
                       Mean reward: 89.74
               Mean episode length: 77.91
    Episode_Reward/reaching_object: 0.3367
     Episode_Reward/lifting_object: 16.8476
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 47.6250
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 2.22s
                      Time elapsed: 00:28:48
                               ETA: 00:47:02

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 43718 steps/s (collection: 2.155s, learning 0.094s)
             Mean action noise std: 2.44
          Mean value_function loss: 280.8222
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 59.9817
                       Mean reward: 75.26
               Mean episode length: 77.13
    Episode_Reward/reaching_object: 0.3216
     Episode_Reward/lifting_object: 15.3642
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4583
     Episode_Termination/robot_out: 50.5833
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 2.25s
                      Time elapsed: 00:28:50
                               ETA: 00:46:59

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 45258 steps/s (collection: 2.082s, learning 0.091s)
             Mean action noise std: 2.44
          Mean value_function loss: 246.1977
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 59.9826
                       Mean reward: 85.71
               Mean episode length: 82.13
    Episode_Reward/reaching_object: 0.3345
     Episode_Reward/lifting_object: 16.8724
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 48.4167
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 2.17s
                      Time elapsed: 00:28:52
                               ETA: 00:46:57

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 44777 steps/s (collection: 2.091s, learning 0.104s)
             Mean action noise std: 2.44
          Mean value_function loss: 245.8594
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 59.9845
                       Mean reward: 79.88
               Mean episode length: 79.54
    Episode_Reward/reaching_object: 0.3389
     Episode_Reward/lifting_object: 17.4886
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4583
     Episode_Termination/robot_out: 45.7083
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 2.20s
                      Time elapsed: 00:28:54
                               ETA: 00:46:55

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 44575 steps/s (collection: 2.093s, learning 0.112s)
             Mean action noise std: 2.44
          Mean value_function loss: 264.8229
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 59.9854
                       Mean reward: 104.73
               Mean episode length: 83.53
    Episode_Reward/reaching_object: 0.3591
     Episode_Reward/lifting_object: 18.7421
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 45.2917
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 2.21s
                      Time elapsed: 00:28:57
                               ETA: 00:46:52

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 44270 steps/s (collection: 2.110s, learning 0.111s)
             Mean action noise std: 2.44
          Mean value_function loss: 264.8319
               Mean surrogate loss: 0.0099
                 Mean entropy loss: 59.9862
                       Mean reward: 86.66
               Mean episode length: 84.41
    Episode_Reward/reaching_object: 0.3507
     Episode_Reward/lifting_object: 18.0971
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 47.8333
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 2.22s
                      Time elapsed: 00:28:59
                               ETA: 00:46:50

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 44385 steps/s (collection: 2.121s, learning 0.094s)
             Mean action noise std: 2.44
          Mean value_function loss: 281.4478
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 59.9865
                       Mean reward: 73.72
               Mean episode length: 82.75
    Episode_Reward/reaching_object: 0.3512
     Episode_Reward/lifting_object: 18.0543
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 46.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 2.21s
                      Time elapsed: 00:29:01
                               ETA: 00:46:47

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 45677 steps/s (collection: 2.066s, learning 0.086s)
             Mean action noise std: 2.44
          Mean value_function loss: 237.9538
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 59.9873
                       Mean reward: 98.65
               Mean episode length: 90.57
    Episode_Reward/reaching_object: 0.3560
     Episode_Reward/lifting_object: 18.4747
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 45.2917
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 2.15s
                      Time elapsed: 00:29:03
                               ETA: 00:46:45

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 45131 steps/s (collection: 2.090s, learning 0.089s)
             Mean action noise std: 2.44
          Mean value_function loss: 241.1023
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.9881
                       Mean reward: 87.33
               Mean episode length: 83.43
    Episode_Reward/reaching_object: 0.3584
     Episode_Reward/lifting_object: 19.0530
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 44.1250
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 2.18s
                      Time elapsed: 00:29:05
                               ETA: 00:46:43

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 44293 steps/s (collection: 2.128s, learning 0.092s)
             Mean action noise std: 2.44
          Mean value_function loss: 282.1306
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 59.9890
                       Mean reward: 108.02
               Mean episode length: 87.70
    Episode_Reward/reaching_object: 0.3616
     Episode_Reward/lifting_object: 19.2120
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 43.5000
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 2.22s
                      Time elapsed: 00:29:08
                               ETA: 00:46:40

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 45936 steps/s (collection: 2.047s, learning 0.093s)
             Mean action noise std: 2.44
          Mean value_function loss: 268.7269
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 59.9902
                       Mean reward: 86.57
               Mean episode length: 80.66
    Episode_Reward/reaching_object: 0.3648
     Episode_Reward/lifting_object: 19.7778
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 46.6667
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 2.14s
                      Time elapsed: 00:29:10
                               ETA: 00:46:38

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 43865 steps/s (collection: 2.124s, learning 0.117s)
             Mean action noise std: 2.44
          Mean value_function loss: 285.0473
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 59.9919
                       Mean reward: 91.85
               Mean episode length: 84.29
    Episode_Reward/reaching_object: 0.3579
     Episode_Reward/lifting_object: 18.8817
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0480
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 43.8750
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 2.24s
                      Time elapsed: 00:29:12
                               ETA: 00:46:35

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 45127 steps/s (collection: 2.077s, learning 0.102s)
             Mean action noise std: 2.44
          Mean value_function loss: 295.9405
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.9941
                       Mean reward: 100.88
               Mean episode length: 86.36
    Episode_Reward/reaching_object: 0.3747
     Episode_Reward/lifting_object: 20.6233
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 41.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 2.18s
                      Time elapsed: 00:29:14
                               ETA: 00:46:33

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 45617 steps/s (collection: 2.059s, learning 0.096s)
             Mean action noise std: 2.44
          Mean value_function loss: 286.6498
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 59.9969
                       Mean reward: 95.30
               Mean episode length: 87.04
    Episode_Reward/reaching_object: 0.3821
     Episode_Reward/lifting_object: 20.5934
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 39.4583
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 2.15s
                      Time elapsed: 00:29:16
                               ETA: 00:46:30

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 45918 steps/s (collection: 2.032s, learning 0.109s)
             Mean action noise std: 2.44
          Mean value_function loss: 272.2864
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 59.9993
                       Mean reward: 114.44
               Mean episode length: 91.88
    Episode_Reward/reaching_object: 0.3893
     Episode_Reward/lifting_object: 21.7756
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 41.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 2.14s
                      Time elapsed: 00:29:18
                               ETA: 00:46:28

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 45584 steps/s (collection: 2.051s, learning 0.106s)
             Mean action noise std: 2.44
          Mean value_function loss: 290.1265
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 60.0014
                       Mean reward: 105.30
               Mean episode length: 93.45
    Episode_Reward/reaching_object: 0.3851
     Episode_Reward/lifting_object: 22.0021
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 42.2083
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 2.16s
                      Time elapsed: 00:29:21
                               ETA: 00:46:26

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 46252 steps/s (collection: 2.040s, learning 0.086s)
             Mean action noise std: 2.44
          Mean value_function loss: 294.3981
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 60.0049
                       Mean reward: 137.02
               Mean episode length: 92.70
    Episode_Reward/reaching_object: 0.3894
     Episode_Reward/lifting_object: 22.8782
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 41.2917
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 2.13s
                      Time elapsed: 00:29:23
                               ETA: 00:46:23

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 45819 steps/s (collection: 2.045s, learning 0.100s)
             Mean action noise std: 2.44
          Mean value_function loss: 285.0690
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 60.0065
                       Mean reward: 106.69
               Mean episode length: 93.51
    Episode_Reward/reaching_object: 0.3753
     Episode_Reward/lifting_object: 20.6461
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 40.1667
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 2.15s
                      Time elapsed: 00:29:25
                               ETA: 00:46:21

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 45959 steps/s (collection: 2.049s, learning 0.090s)
             Mean action noise std: 2.44
          Mean value_function loss: 327.1416
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 60.0073
                       Mean reward: 114.76
               Mean episode length: 94.37
    Episode_Reward/reaching_object: 0.4057
     Episode_Reward/lifting_object: 24.7945
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 36.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 2.14s
                      Time elapsed: 00:29:27
                               ETA: 00:46:18

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 45337 steps/s (collection: 2.057s, learning 0.112s)
             Mean action noise std: 2.44
          Mean value_function loss: 300.3953
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 60.0076
                       Mean reward: 131.47
               Mean episode length: 97.81
    Episode_Reward/reaching_object: 0.4035
     Episode_Reward/lifting_object: 25.1903
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 41.5417
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 2.17s
                      Time elapsed: 00:29:29
                               ETA: 00:46:16

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 45995 steps/s (collection: 2.042s, learning 0.096s)
             Mean action noise std: 2.44
          Mean value_function loss: 317.0391
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 60.0084
                       Mean reward: 134.60
               Mean episode length: 101.98
    Episode_Reward/reaching_object: 0.4170
     Episode_Reward/lifting_object: 26.0826
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 41.1667
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 2.14s
                      Time elapsed: 00:29:31
                               ETA: 00:46:13

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 45243 steps/s (collection: 2.084s, learning 0.088s)
             Mean action noise std: 2.44
          Mean value_function loss: 324.8258
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 60.0093
                       Mean reward: 140.73
               Mean episode length: 101.83
    Episode_Reward/reaching_object: 0.3979
     Episode_Reward/lifting_object: 24.0204
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 43.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 2.17s
                      Time elapsed: 00:29:34
                               ETA: 00:46:11

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 44133 steps/s (collection: 2.138s, learning 0.089s)
             Mean action noise std: 2.44
          Mean value_function loss: 321.2044
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 60.0103
                       Mean reward: 118.84
               Mean episode length: 93.03
    Episode_Reward/reaching_object: 0.3920
     Episode_Reward/lifting_object: 23.8577
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 37.9167
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 2.23s
                      Time elapsed: 00:29:36
                               ETA: 00:46:08

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 45449 steps/s (collection: 2.074s, learning 0.089s)
             Mean action noise std: 2.44
          Mean value_function loss: 334.2509
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 60.0106
                       Mean reward: 102.08
               Mean episode length: 93.73
    Episode_Reward/reaching_object: 0.3888
     Episode_Reward/lifting_object: 23.2320
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 40.1250
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 2.16s
                      Time elapsed: 00:29:38
                               ETA: 00:46:06

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 45873 steps/s (collection: 2.056s, learning 0.087s)
             Mean action noise std: 2.44
          Mean value_function loss: 329.1680
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 60.0104
                       Mean reward: 128.84
               Mean episode length: 93.92
    Episode_Reward/reaching_object: 0.3900
     Episode_Reward/lifting_object: 24.1203
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 40.6667
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 2.14s
                      Time elapsed: 00:29:40
                               ETA: 00:46:03

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 45821 steps/s (collection: 2.057s, learning 0.089s)
             Mean action noise std: 2.44
          Mean value_function loss: 300.4778
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 60.0105
                       Mean reward: 130.76
               Mean episode length: 102.84
    Episode_Reward/reaching_object: 0.4060
     Episode_Reward/lifting_object: 24.7009
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 38.3333
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 2.15s
                      Time elapsed: 00:29:42
                               ETA: 00:46:01

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 44480 steps/s (collection: 2.119s, learning 0.091s)
             Mean action noise std: 2.44
          Mean value_function loss: 316.7623
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 60.0106
                       Mean reward: 139.89
               Mean episode length: 98.50
    Episode_Reward/reaching_object: 0.4122
     Episode_Reward/lifting_object: 26.0847
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 41.2917
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 2.21s
                      Time elapsed: 00:29:44
                               ETA: 00:45:59

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 44886 steps/s (collection: 2.086s, learning 0.104s)
             Mean action noise std: 2.44
          Mean value_function loss: 333.3047
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 60.0108
                       Mean reward: 120.90
               Mean episode length: 90.52
    Episode_Reward/reaching_object: 0.4061
     Episode_Reward/lifting_object: 25.2878
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 41.4167
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 2.19s
                      Time elapsed: 00:29:47
                               ETA: 00:45:56

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 45489 steps/s (collection: 2.053s, learning 0.108s)
             Mean action noise std: 2.45
          Mean value_function loss: 340.5247
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 60.0117
                       Mean reward: 117.90
               Mean episode length: 97.67
    Episode_Reward/reaching_object: 0.4128
     Episode_Reward/lifting_object: 25.7931
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 40.4583
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 2.16s
                      Time elapsed: 00:29:49
                               ETA: 00:45:54

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 45383 steps/s (collection: 2.057s, learning 0.109s)
             Mean action noise std: 2.45
          Mean value_function loss: 360.8623
               Mean surrogate loss: 0.0148
                 Mean entropy loss: 60.0125
                       Mean reward: 128.19
               Mean episode length: 91.18
    Episode_Reward/reaching_object: 0.4177
     Episode_Reward/lifting_object: 26.9023
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 37.7083
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 2.17s
                      Time elapsed: 00:29:51
                               ETA: 00:45:51

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 44900 steps/s (collection: 2.095s, learning 0.094s)
             Mean action noise std: 2.45
          Mean value_function loss: 358.4354
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 60.0125
                       Mean reward: 116.87
               Mean episode length: 95.13
    Episode_Reward/reaching_object: 0.4069
     Episode_Reward/lifting_object: 25.8545
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 38.5000
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 2.19s
                      Time elapsed: 00:29:53
                               ETA: 00:45:49

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 43912 steps/s (collection: 2.133s, learning 0.106s)
             Mean action noise std: 2.45
          Mean value_function loss: 360.1298
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 60.0126
                       Mean reward: 127.23
               Mean episode length: 94.59
    Episode_Reward/reaching_object: 0.4382
     Episode_Reward/lifting_object: 29.4668
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 38.7083
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 2.24s
                      Time elapsed: 00:29:55
                               ETA: 00:45:47

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 44714 steps/s (collection: 2.110s, learning 0.089s)
             Mean action noise std: 2.45
          Mean value_function loss: 403.8892
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 60.0138
                       Mean reward: 139.99
               Mean episode length: 93.00
    Episode_Reward/reaching_object: 0.4338
     Episode_Reward/lifting_object: 29.2881
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 36.7500
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 2.20s
                      Time elapsed: 00:29:58
                               ETA: 00:45:44

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 45631 steps/s (collection: 2.059s, learning 0.095s)
             Mean action noise std: 2.45
          Mean value_function loss: 429.3360
               Mean surrogate loss: 0.0177
                 Mean entropy loss: 60.0155
                       Mean reward: 159.06
               Mean episode length: 103.87
    Episode_Reward/reaching_object: 0.4377
     Episode_Reward/lifting_object: 29.6121
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 40.3333
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 2.15s
                      Time elapsed: 00:30:00
                               ETA: 00:45:42

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 45531 steps/s (collection: 2.073s, learning 0.087s)
             Mean action noise std: 2.45
          Mean value_function loss: 369.8056
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 60.0156
                       Mean reward: 192.39
               Mean episode length: 116.04
    Episode_Reward/reaching_object: 0.4669
     Episode_Reward/lifting_object: 33.1614
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 38.5417
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 2.16s
                      Time elapsed: 00:30:02
                               ETA: 00:45:39

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 44484 steps/s (collection: 2.107s, learning 0.103s)
             Mean action noise std: 2.45
          Mean value_function loss: 425.9277
               Mean surrogate loss: 0.0126
                 Mean entropy loss: 60.0158
                       Mean reward: 154.68
               Mean episode length: 98.47
    Episode_Reward/reaching_object: 0.4547
     Episode_Reward/lifting_object: 32.3371
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 39.4167
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 2.21s
                      Time elapsed: 00:30:04
                               ETA: 00:45:37

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 43796 steps/s (collection: 2.148s, learning 0.097s)
             Mean action noise std: 2.45
          Mean value_function loss: 412.7397
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 60.0163
                       Mean reward: 192.76
               Mean episode length: 108.29
    Episode_Reward/reaching_object: 0.4732
     Episode_Reward/lifting_object: 33.9434
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 32.1667
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 2.24s
                      Time elapsed: 00:30:06
                               ETA: 00:45:35

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 44627 steps/s (collection: 2.116s, learning 0.087s)
             Mean action noise std: 2.45
          Mean value_function loss: 432.7085
               Mean surrogate loss: 0.0199
                 Mean entropy loss: 60.0164
                       Mean reward: 182.20
               Mean episode length: 101.81
    Episode_Reward/reaching_object: 0.4907
     Episode_Reward/lifting_object: 36.3531
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 36.1250
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 2.20s
                      Time elapsed: 00:30:09
                               ETA: 00:45:32

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 45586 steps/s (collection: 2.068s, learning 0.088s)
             Mean action noise std: 2.45
          Mean value_function loss: 387.8079
               Mean surrogate loss: 0.0200
                 Mean entropy loss: 60.0163
                       Mean reward: 238.27
               Mean episode length: 113.23
    Episode_Reward/reaching_object: 0.5194
     Episode_Reward/lifting_object: 39.8377
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 36.1667
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 2.16s
                      Time elapsed: 00:30:11
                               ETA: 00:45:30

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 44911 steps/s (collection: 2.101s, learning 0.088s)
             Mean action noise std: 2.45
          Mean value_function loss: 402.0916
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 60.0163
                       Mean reward: 195.35
               Mean episode length: 104.37
    Episode_Reward/reaching_object: 0.4967
     Episode_Reward/lifting_object: 38.0386
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0491
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 36.2083
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 2.19s
                      Time elapsed: 00:30:13
                               ETA: 00:45:28

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 43412 steps/s (collection: 2.146s, learning 0.118s)
             Mean action noise std: 2.45
          Mean value_function loss: 391.3226
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 60.0163
                       Mean reward: 196.42
               Mean episode length: 99.31
    Episode_Reward/reaching_object: 0.5067
     Episode_Reward/lifting_object: 39.4395
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 36.5417
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 2.26s
                      Time elapsed: 00:30:15
                               ETA: 00:45:25

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 44421 steps/s (collection: 2.103s, learning 0.110s)
             Mean action noise std: 2.45
          Mean value_function loss: 422.8296
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 60.0155
                       Mean reward: 241.33
               Mean episode length: 112.02
    Episode_Reward/reaching_object: 0.5083
     Episode_Reward/lifting_object: 40.0963
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 39.3750
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 2.21s
                      Time elapsed: 00:30:17
                               ETA: 00:45:23

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 40495 steps/s (collection: 2.323s, learning 0.105s)
             Mean action noise std: 2.45
          Mean value_function loss: 428.1320
               Mean surrogate loss: 0.0116
                 Mean entropy loss: 60.0143
                       Mean reward: 209.88
               Mean episode length: 109.46
    Episode_Reward/reaching_object: 0.5158
     Episode_Reward/lifting_object: 41.0715
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 36.6250
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 2.43s
                      Time elapsed: 00:30:20
                               ETA: 00:45:21

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 42029 steps/s (collection: 2.241s, learning 0.098s)
             Mean action noise std: 2.45
          Mean value_function loss: 396.9458
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 60.0149
                       Mean reward: 187.53
               Mean episode length: 100.98
    Episode_Reward/reaching_object: 0.5148
     Episode_Reward/lifting_object: 40.1371
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 32.8750
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 2.34s
                      Time elapsed: 00:30:22
                               ETA: 00:45:19

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 40653 steps/s (collection: 2.297s, learning 0.121s)
             Mean action noise std: 2.45
          Mean value_function loss: 432.6998
               Mean surrogate loss: 0.0108
                 Mean entropy loss: 60.0159
                       Mean reward: 211.67
               Mean episode length: 106.60
    Episode_Reward/reaching_object: 0.4752
     Episode_Reward/lifting_object: 35.4725
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 35.2500
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 2.42s
                      Time elapsed: 00:30:25
                               ETA: 00:45:17

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 35702 steps/s (collection: 2.625s, learning 0.129s)
             Mean action noise std: 2.45
          Mean value_function loss: 402.1115
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 60.0168
                       Mean reward: 230.75
               Mean episode length: 113.70
    Episode_Reward/reaching_object: 0.5183
     Episode_Reward/lifting_object: 39.9491
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 30.9167
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 2.75s
                      Time elapsed: 00:30:27
                               ETA: 00:45:15

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 41884 steps/s (collection: 2.248s, learning 0.100s)
             Mean action noise std: 2.45
          Mean value_function loss: 404.4836
               Mean surrogate loss: 0.0152
                 Mean entropy loss: 60.0175
                       Mean reward: 187.90
               Mean episode length: 106.09
    Episode_Reward/reaching_object: 0.5233
     Episode_Reward/lifting_object: 40.1657
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 29.7500
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 2.35s
                      Time elapsed: 00:30:30
                               ETA: 00:45:13

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 36506 steps/s (collection: 2.371s, learning 0.322s)
             Mean action noise std: 2.45
          Mean value_function loss: 415.2994
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 60.0175
                       Mean reward: 199.99
               Mean episode length: 106.50
    Episode_Reward/reaching_object: 0.5305
     Episode_Reward/lifting_object: 40.5473
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 30.7500
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 2.69s
                      Time elapsed: 00:30:32
                               ETA: 00:45:11

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 35978 steps/s (collection: 2.572s, learning 0.161s)
             Mean action noise std: 2.45
          Mean value_function loss: 403.9868
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 60.0177
                       Mean reward: 200.66
               Mean episode length: 111.63
    Episode_Reward/reaching_object: 0.5665
     Episode_Reward/lifting_object: 45.7563
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 33.2083
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 2.73s
                      Time elapsed: 00:30:35
                               ETA: 00:45:10

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 37345 steps/s (collection: 2.480s, learning 0.153s)
             Mean action noise std: 2.45
          Mean value_function loss: 396.0406
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 60.0180
                       Mean reward: 150.86
               Mean episode length: 105.86
    Episode_Reward/reaching_object: 0.5305
     Episode_Reward/lifting_object: 42.3451
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 33.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 2.63s
                      Time elapsed: 00:30:38
                               ETA: 00:45:08

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 35633 steps/s (collection: 2.525s, learning 0.234s)
             Mean action noise std: 2.45
          Mean value_function loss: 374.5323
               Mean surrogate loss: 0.0129
                 Mean entropy loss: 60.0183
                       Mean reward: 212.26
               Mean episode length: 110.64
    Episode_Reward/reaching_object: 0.5415
     Episode_Reward/lifting_object: 43.0205
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 31.1250
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 2.76s
                      Time elapsed: 00:30:40
                               ETA: 00:45:06

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 40522 steps/s (collection: 2.322s, learning 0.104s)
             Mean action noise std: 2.45
          Mean value_function loss: 411.8432
               Mean surrogate loss: 0.0086
                 Mean entropy loss: 60.0185
                       Mean reward: 251.52
               Mean episode length: 118.88
    Episode_Reward/reaching_object: 0.5723
     Episode_Reward/lifting_object: 46.3614
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 31.8750
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 2.43s
                      Time elapsed: 00:30:43
                               ETA: 00:45:04

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 44239 steps/s (collection: 2.104s, learning 0.118s)
             Mean action noise std: 2.45
          Mean value_function loss: 402.6083
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 60.0189
                       Mean reward: 237.01
               Mean episode length: 111.13
    Episode_Reward/reaching_object: 0.5716
     Episode_Reward/lifting_object: 46.7279
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 33.5000
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 2.22s
                      Time elapsed: 00:30:45
                               ETA: 00:45:02

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 44822 steps/s (collection: 2.092s, learning 0.101s)
             Mean action noise std: 2.45
          Mean value_function loss: 443.3122
               Mean surrogate loss: 0.0710
                 Mean entropy loss: 60.0202
                       Mean reward: 163.23
               Mean episode length: 100.55
    Episode_Reward/reaching_object: 0.5162
     Episode_Reward/lifting_object: 39.5948
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 32.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 2.19s
                      Time elapsed: 00:30:47
                               ETA: 00:45:00

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 42673 steps/s (collection: 2.194s, learning 0.110s)
             Mean action noise std: 2.45
          Mean value_function loss: 419.4620
               Mean surrogate loss: 0.0093
                 Mean entropy loss: 60.0205
                       Mean reward: 204.17
               Mean episode length: 109.77
    Episode_Reward/reaching_object: 0.5397
     Episode_Reward/lifting_object: 42.1942
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 29.8333
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 2.30s
                      Time elapsed: 00:30:50
                               ETA: 00:44:57

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 44375 steps/s (collection: 2.127s, learning 0.089s)
             Mean action noise std: 2.45
          Mean value_function loss: 395.0545
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 60.0207
                       Mean reward: 220.40
               Mean episode length: 111.96
    Episode_Reward/reaching_object: 0.5380
     Episode_Reward/lifting_object: 42.4345
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 29.7500
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 2.22s
                      Time elapsed: 00:30:52
                               ETA: 00:44:55

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 43843 steps/s (collection: 2.144s, learning 0.098s)
             Mean action noise std: 2.45
          Mean value_function loss: 478.2016
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 60.0211
                       Mean reward: 214.32
               Mean episode length: 106.57
    Episode_Reward/reaching_object: 0.5686
     Episode_Reward/lifting_object: 47.0747
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 33.7500
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 2.24s
                      Time elapsed: 00:30:54
                               ETA: 00:44:53

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 43012 steps/s (collection: 2.164s, learning 0.121s)
             Mean action noise std: 2.45
          Mean value_function loss: 470.3865
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 60.0219
                       Mean reward: 253.46
               Mean episode length: 117.82
    Episode_Reward/reaching_object: 0.5790
     Episode_Reward/lifting_object: 48.6355
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 31.1667
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 2.29s
                      Time elapsed: 00:30:56
                               ETA: 00:44:50

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 39954 steps/s (collection: 2.331s, learning 0.130s)
             Mean action noise std: 2.45
          Mean value_function loss: 492.9272
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 60.0222
                       Mean reward: 226.82
               Mean episode length: 114.48
    Episode_Reward/reaching_object: 0.5488
     Episode_Reward/lifting_object: 44.6032
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 32.0833
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 2.46s
                      Time elapsed: 00:30:59
                               ETA: 00:44:48

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 34320 steps/s (collection: 2.676s, learning 0.188s)
             Mean action noise std: 2.45
          Mean value_function loss: 439.4307
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 60.0222
                       Mean reward: 238.39
               Mean episode length: 115.73
    Episode_Reward/reaching_object: 0.5860
     Episode_Reward/lifting_object: 49.8339
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 31.2500
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 2.86s
                      Time elapsed: 00:31:02
                               ETA: 00:44:47

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 41000 steps/s (collection: 2.284s, learning 0.114s)
             Mean action noise std: 2.45
          Mean value_function loss: 389.9060
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 60.0220
                       Mean reward: 260.29
               Mean episode length: 125.74
    Episode_Reward/reaching_object: 0.5544
     Episode_Reward/lifting_object: 45.6257
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 33.5833
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 2.40s
                      Time elapsed: 00:31:04
                               ETA: 00:44:45

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 37559 steps/s (collection: 2.453s, learning 0.164s)
             Mean action noise std: 2.45
          Mean value_function loss: 412.6344
               Mean surrogate loss: 0.0133
                 Mean entropy loss: 60.0221
                       Mean reward: 222.22
               Mean episode length: 112.80
    Episode_Reward/reaching_object: 0.5462
     Episode_Reward/lifting_object: 45.1442
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 32.8333
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 2.62s
                      Time elapsed: 00:31:07
                               ETA: 00:44:43

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 43349 steps/s (collection: 2.156s, learning 0.112s)
             Mean action noise std: 2.45
          Mean value_function loss: 392.6807
               Mean surrogate loss: 0.0177
                 Mean entropy loss: 60.0222
                       Mean reward: 213.92
               Mean episode length: 113.62
    Episode_Reward/reaching_object: 0.5416
     Episode_Reward/lifting_object: 44.4204
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 34.2917
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 2.27s
                      Time elapsed: 00:31:09
                               ETA: 00:44:41

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 44251 steps/s (collection: 2.129s, learning 0.092s)
             Mean action noise std: 2.45
          Mean value_function loss: 389.4896
               Mean surrogate loss: 0.0108
                 Mean entropy loss: 60.0222
                       Mean reward: 208.66
               Mean episode length: 113.94
    Episode_Reward/reaching_object: 0.5226
     Episode_Reward/lifting_object: 41.7047
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 38.6667
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 2.22s
                      Time elapsed: 00:31:11
                               ETA: 00:44:39

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 43883 steps/s (collection: 2.152s, learning 0.088s)
             Mean action noise std: 2.45
          Mean value_function loss: 405.9485
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 60.0223
                       Mean reward: 182.01
               Mean episode length: 113.44
    Episode_Reward/reaching_object: 0.4911
     Episode_Reward/lifting_object: 37.2588
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 35.7917
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 2.24s
                      Time elapsed: 00:31:13
                               ETA: 00:44:36

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 44818 steps/s (collection: 2.108s, learning 0.085s)
             Mean action noise std: 2.45
          Mean value_function loss: 394.7965
               Mean surrogate loss: 0.0134
                 Mean entropy loss: 60.0224
                       Mean reward: 194.99
               Mean episode length: 112.68
    Episode_Reward/reaching_object: 0.4886
     Episode_Reward/lifting_object: 37.7878
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 33.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 2.19s
                      Time elapsed: 00:31:16
                               ETA: 00:44:34

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 39632 steps/s (collection: 2.372s, learning 0.108s)
             Mean action noise std: 2.45
          Mean value_function loss: 420.6830
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 60.0225
                       Mean reward: 232.11
               Mean episode length: 112.90
    Episode_Reward/reaching_object: 0.4942
     Episode_Reward/lifting_object: 38.6615
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 30.9167
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 2.48s
                      Time elapsed: 00:31:18
                               ETA: 00:44:32

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 39058 steps/s (collection: 2.360s, learning 0.156s)
             Mean action noise std: 2.45
          Mean value_function loss: 420.4963
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 60.0225
                       Mean reward: 200.10
               Mean episode length: 102.17
    Episode_Reward/reaching_object: 0.5229
     Episode_Reward/lifting_object: 41.5437
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 33.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 2.52s
                      Time elapsed: 00:31:21
                               ETA: 00:44:30

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 42823 steps/s (collection: 2.207s, learning 0.089s)
             Mean action noise std: 2.45
          Mean value_function loss: 419.4634
               Mean surrogate loss: 0.0138
                 Mean entropy loss: 60.0226
                       Mean reward: 212.71
               Mean episode length: 110.91
    Episode_Reward/reaching_object: 0.5409
     Episode_Reward/lifting_object: 44.3671
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 34.1667
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 2.30s
                      Time elapsed: 00:31:23
                               ETA: 00:44:28

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 43673 steps/s (collection: 2.120s, learning 0.131s)
             Mean action noise std: 2.45
          Mean value_function loss: 398.1993
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 60.0226
                       Mean reward: 200.40
               Mean episode length: 112.32
    Episode_Reward/reaching_object: 0.5298
     Episode_Reward/lifting_object: 42.3852
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 36.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 2.25s
                      Time elapsed: 00:31:25
                               ETA: 00:44:25

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 38659 steps/s (collection: 2.457s, learning 0.086s)
             Mean action noise std: 2.45
          Mean value_function loss: 383.9363
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 60.0226
                       Mean reward: 221.20
               Mean episode length: 117.46
    Episode_Reward/reaching_object: 0.5356
     Episode_Reward/lifting_object: 43.3242
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 38.9583
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 2.54s
                      Time elapsed: 00:31:28
                               ETA: 00:44:23

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 43152 steps/s (collection: 2.187s, learning 0.092s)
             Mean action noise std: 2.45
          Mean value_function loss: 394.6634
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 60.0226
                       Mean reward: 224.62
               Mean episode length: 112.21
    Episode_Reward/reaching_object: 0.5008
     Episode_Reward/lifting_object: 40.0399
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 39.2917
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 2.28s
                      Time elapsed: 00:31:30
                               ETA: 00:44:21

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 44448 steps/s (collection: 2.101s, learning 0.111s)
             Mean action noise std: 2.45
          Mean value_function loss: 402.7167
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 60.0226
                       Mean reward: 164.22
               Mean episode length: 104.15
    Episode_Reward/reaching_object: 0.4712
     Episode_Reward/lifting_object: 36.3699
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 36.8333
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 2.21s
                      Time elapsed: 00:31:32
                               ETA: 00:44:19

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 45257 steps/s (collection: 2.079s, learning 0.093s)
             Mean action noise std: 2.45
          Mean value_function loss: 391.0889
               Mean surrogate loss: 0.0127
                 Mean entropy loss: 60.0226
                       Mean reward: 169.80
               Mean episode length: 97.22
    Episode_Reward/reaching_object: 0.4857
     Episode_Reward/lifting_object: 38.2833
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 33.0833
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 2.17s
                      Time elapsed: 00:31:34
                               ETA: 00:44:16

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 45114 steps/s (collection: 2.077s, learning 0.102s)
             Mean action noise std: 2.45
          Mean value_function loss: 403.9316
               Mean surrogate loss: 0.0124
                 Mean entropy loss: 60.0226
                       Mean reward: 149.20
               Mean episode length: 93.13
    Episode_Reward/reaching_object: 0.4644
     Episode_Reward/lifting_object: 35.0163
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 35.2500
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 2.18s
                      Time elapsed: 00:31:37
                               ETA: 00:44:14

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 45732 steps/s (collection: 2.061s, learning 0.089s)
             Mean action noise std: 2.45
          Mean value_function loss: 445.9949
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 60.0232
                       Mean reward: 155.76
               Mean episode length: 92.20
    Episode_Reward/reaching_object: 0.4849
     Episode_Reward/lifting_object: 37.4164
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 36.0833
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 2.15s
                      Time elapsed: 00:31:39
                               ETA: 00:44:12

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 45377 steps/s (collection: 2.078s, learning 0.089s)
             Mean action noise std: 2.45
          Mean value_function loss: 408.8474
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 60.0249
                       Mean reward: 178.74
               Mean episode length: 95.54
    Episode_Reward/reaching_object: 0.4656
     Episode_Reward/lifting_object: 36.1942
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0477
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 33.7083
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 2.17s
                      Time elapsed: 00:31:41
                               ETA: 00:44:09

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 45438 steps/s (collection: 2.072s, learning 0.091s)
             Mean action noise std: 2.45
          Mean value_function loss: 418.3163
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 60.0263
                       Mean reward: 215.26
               Mean episode length: 107.64
    Episode_Reward/reaching_object: 0.5181
     Episode_Reward/lifting_object: 40.8025
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 31.8750
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 2.16s
                      Time elapsed: 00:31:43
                               ETA: 00:44:07

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 44707 steps/s (collection: 2.101s, learning 0.098s)
             Mean action noise std: 2.45
          Mean value_function loss: 421.4459
               Mean surrogate loss: 0.0100
                 Mean entropy loss: 60.0268
                       Mean reward: 266.55
               Mean episode length: 116.67
    Episode_Reward/reaching_object: 0.5007
     Episode_Reward/lifting_object: 40.2380
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 32.3333
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 2.20s
                      Time elapsed: 00:31:45
                               ETA: 00:44:04

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 45332 steps/s (collection: 2.069s, learning 0.099s)
             Mean action noise std: 2.45
          Mean value_function loss: 427.8485
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 60.0269
                       Mean reward: 257.18
               Mean episode length: 114.83
    Episode_Reward/reaching_object: 0.5528
     Episode_Reward/lifting_object: 46.1155
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 32.5000
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 2.17s
                      Time elapsed: 00:31:47
                               ETA: 00:44:02

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 45995 steps/s (collection: 2.049s, learning 0.089s)
             Mean action noise std: 2.45
          Mean value_function loss: 437.4403
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 60.0277
                       Mean reward: 268.85
               Mean episode length: 118.77
    Episode_Reward/reaching_object: 0.5768
     Episode_Reward/lifting_object: 49.2714
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 2.14s
                      Time elapsed: 00:31:50
                               ETA: 00:43:59

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 45708 steps/s (collection: 2.050s, learning 0.101s)
             Mean action noise std: 2.45
          Mean value_function loss: 477.9889
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 60.0295
                       Mean reward: 280.81
               Mean episode length: 123.63
    Episode_Reward/reaching_object: 0.6135
     Episode_Reward/lifting_object: 53.5115
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 29.5417
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 2.15s
                      Time elapsed: 00:31:52
                               ETA: 00:43:57

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 45760 steps/s (collection: 2.061s, learning 0.087s)
             Mean action noise std: 2.45
          Mean value_function loss: 475.5725
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 60.0307
                       Mean reward: 283.57
               Mean episode length: 123.11
    Episode_Reward/reaching_object: 0.6389
     Episode_Reward/lifting_object: 57.0285
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 29.7917
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 2.15s
                      Time elapsed: 00:31:54
                               ETA: 00:43:55

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 45297 steps/s (collection: 2.078s, learning 0.092s)
             Mean action noise std: 2.45
          Mean value_function loss: 475.3490
               Mean surrogate loss: 0.0131
                 Mean entropy loss: 60.0314
                       Mean reward: 275.55
               Mean episode length: 116.48
    Episode_Reward/reaching_object: 0.6156
     Episode_Reward/lifting_object: 54.1973
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 26.7083
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 2.17s
                      Time elapsed: 00:31:56
                               ETA: 00:43:52

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 44552 steps/s (collection: 2.112s, learning 0.095s)
             Mean action noise std: 2.45
          Mean value_function loss: 480.6602
               Mean surrogate loss: 0.0119
                 Mean entropy loss: 60.0317
                       Mean reward: 290.56
               Mean episode length: 125.18
    Episode_Reward/reaching_object: 0.6042
     Episode_Reward/lifting_object: 53.4257
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 30.1250
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 2.21s
                      Time elapsed: 00:31:58
                               ETA: 00:43:50

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 45449 steps/s (collection: 2.077s, learning 0.086s)
             Mean action noise std: 2.45
          Mean value_function loss: 485.6048
               Mean surrogate loss: 0.0092
                 Mean entropy loss: 60.0318
                       Mean reward: 219.01
               Mean episode length: 105.18
    Episode_Reward/reaching_object: 0.6186
     Episode_Reward/lifting_object: 56.0331
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 29.0833
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 2.16s
                      Time elapsed: 00:32:00
                               ETA: 00:43:47

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 44097 steps/s (collection: 2.131s, learning 0.098s)
             Mean action noise std: 2.45
          Mean value_function loss: 455.1147
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 60.0319
                       Mean reward: 241.28
               Mean episode length: 111.10
    Episode_Reward/reaching_object: 0.6026
     Episode_Reward/lifting_object: 53.1500
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 32.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 2.23s
                      Time elapsed: 00:32:03
                               ETA: 00:43:45

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 44366 steps/s (collection: 2.115s, learning 0.101s)
             Mean action noise std: 2.45
          Mean value_function loss: 459.2198
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 60.0320
                       Mean reward: 256.85
               Mean episode length: 113.39
    Episode_Reward/reaching_object: 0.5842
     Episode_Reward/lifting_object: 51.9942
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 31.9167
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 2.22s
                      Time elapsed: 00:32:05
                               ETA: 00:43:43

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 44853 steps/s (collection: 2.097s, learning 0.095s)
             Mean action noise std: 2.45
          Mean value_function loss: 479.1186
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 60.0323
                       Mean reward: 273.19
               Mean episode length: 115.31
    Episode_Reward/reaching_object: 0.6167
     Episode_Reward/lifting_object: 55.8609
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 27.6250
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 2.19s
                      Time elapsed: 00:32:07
                               ETA: 00:43:40

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 45215 steps/s (collection: 2.087s, learning 0.087s)
             Mean action noise std: 2.45
          Mean value_function loss: 471.6131
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 60.0325
                       Mean reward: 280.51
               Mean episode length: 126.62
    Episode_Reward/reaching_object: 0.6014
     Episode_Reward/lifting_object: 53.1766
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 28.5417
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 2.17s
                      Time elapsed: 00:32:09
                               ETA: 00:43:38

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 46128 steps/s (collection: 2.045s, learning 0.086s)
             Mean action noise std: 2.45
          Mean value_function loss: 497.7288
               Mean surrogate loss: 0.0204
                 Mean entropy loss: 60.0328
                       Mean reward: 252.71
               Mean episode length: 116.24
    Episode_Reward/reaching_object: 0.6128
     Episode_Reward/lifting_object: 54.0220
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 3.9167
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 29.6667
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 2.13s
                      Time elapsed: 00:32:11
                               ETA: 00:43:35

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 44516 steps/s (collection: 2.108s, learning 0.101s)
             Mean action noise std: 2.45
          Mean value_function loss: 459.6978
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 60.0329
                       Mean reward: 289.54
               Mean episode length: 129.60
    Episode_Reward/reaching_object: 0.6286
     Episode_Reward/lifting_object: 56.1643
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 25.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 2.21s
                      Time elapsed: 00:32:14
                               ETA: 00:43:33

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 43983 steps/s (collection: 2.135s, learning 0.100s)
             Mean action noise std: 2.45
          Mean value_function loss: 450.0893
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 60.0329
                       Mean reward: 265.69
               Mean episode length: 116.22
    Episode_Reward/reaching_object: 0.6479
     Episode_Reward/lifting_object: 58.7341
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 28.2083
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 2.24s
                      Time elapsed: 00:32:16
                               ETA: 00:43:31

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 44329 steps/s (collection: 2.098s, learning 0.120s)
             Mean action noise std: 2.45
          Mean value_function loss: 527.1604
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 60.0335
                       Mean reward: 286.90
               Mean episode length: 125.46
    Episode_Reward/reaching_object: 0.6241
     Episode_Reward/lifting_object: 56.6285
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 29.9167
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 2.22s
                      Time elapsed: 00:32:18
                               ETA: 00:43:28

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 39451 steps/s (collection: 2.365s, learning 0.127s)
             Mean action noise std: 2.45
          Mean value_function loss: 522.2237
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 60.0360
                       Mean reward: 284.70
               Mean episode length: 119.83
    Episode_Reward/reaching_object: 0.6343
     Episode_Reward/lifting_object: 57.8252
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 28.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 2.49s
                      Time elapsed: 00:32:20
                               ETA: 00:43:26

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 41034 steps/s (collection: 2.300s, learning 0.096s)
             Mean action noise std: 2.45
          Mean value_function loss: 501.5255
               Mean surrogate loss: 0.0167
                 Mean entropy loss: 60.0377
                       Mean reward: 290.93
               Mean episode length: 119.18
    Episode_Reward/reaching_object: 0.6439
     Episode_Reward/lifting_object: 58.9009
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 30.1250
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 2.40s
                      Time elapsed: 00:32:23
                               ETA: 00:43:24

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 41541 steps/s (collection: 2.278s, learning 0.088s)
             Mean action noise std: 2.45
          Mean value_function loss: 533.9783
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 60.0381
                       Mean reward: 272.48
               Mean episode length: 117.68
    Episode_Reward/reaching_object: 0.6503
     Episode_Reward/lifting_object: 59.8770
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 29.8750
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 2.37s
                      Time elapsed: 00:32:25
                               ETA: 00:43:22

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 38792 steps/s (collection: 2.326s, learning 0.209s)
             Mean action noise std: 2.45
          Mean value_function loss: 512.6027
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 60.0396
                       Mean reward: 284.48
               Mean episode length: 122.97
    Episode_Reward/reaching_object: 0.6234
     Episode_Reward/lifting_object: 56.0875
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 31.0833
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.53s
                      Time elapsed: 00:32:28
                               ETA: 00:43:20

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 37395 steps/s (collection: 2.469s, learning 0.160s)
             Mean action noise std: 2.45
          Mean value_function loss: 521.5242
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 60.0404
                       Mean reward: 240.36
               Mean episode length: 113.92
    Episode_Reward/reaching_object: 0.5883
     Episode_Reward/lifting_object: 51.0818
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 32.3750
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 2.63s
                      Time elapsed: 00:32:30
                               ETA: 00:43:18

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 38982 steps/s (collection: 2.352s, learning 0.170s)
             Mean action noise std: 2.45
          Mean value_function loss: 476.5933
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 60.0407
                       Mean reward: 296.58
               Mean episode length: 123.39
    Episode_Reward/reaching_object: 0.5873
     Episode_Reward/lifting_object: 51.8591
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 28.2917
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 2.52s
                      Time elapsed: 00:32:33
                               ETA: 00:43:16

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 42347 steps/s (collection: 2.231s, learning 0.090s)
             Mean action noise std: 2.45
          Mean value_function loss: 499.1424
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 60.0412
                       Mean reward: 273.56
               Mean episode length: 115.28
    Episode_Reward/reaching_object: 0.5631
     Episode_Reward/lifting_object: 49.0307
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 30.9583
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 2.32s
                      Time elapsed: 00:32:35
                               ETA: 00:43:14

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 43628 steps/s (collection: 2.152s, learning 0.102s)
             Mean action noise std: 2.45
          Mean value_function loss: 501.4803
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 60.0419
                       Mean reward: 307.08
               Mean episode length: 126.87
    Episode_Reward/reaching_object: 0.6160
     Episode_Reward/lifting_object: 54.3592
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 26.4583
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 2.25s
                      Time elapsed: 00:32:37
                               ETA: 00:43:12

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 42837 steps/s (collection: 2.162s, learning 0.133s)
             Mean action noise std: 2.45
          Mean value_function loss: 508.7362
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 60.0450
                       Mean reward: 255.02
               Mean episode length: 114.84
    Episode_Reward/reaching_object: 0.6302
     Episode_Reward/lifting_object: 55.9096
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 25.4167
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 2.29s
                      Time elapsed: 00:32:40
                               ETA: 00:43:10

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 44145 steps/s (collection: 2.131s, learning 0.096s)
             Mean action noise std: 2.45
          Mean value_function loss: 480.5334
               Mean surrogate loss: 0.0111
                 Mean entropy loss: 60.0480
                       Mean reward: 264.70
               Mean episode length: 119.29
    Episode_Reward/reaching_object: 0.6555
     Episode_Reward/lifting_object: 59.3218
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 2.23s
                      Time elapsed: 00:32:42
                               ETA: 00:43:07

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 43397 steps/s (collection: 2.169s, learning 0.097s)
             Mean action noise std: 2.45
          Mean value_function loss: 523.3893
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 60.0488
                       Mean reward: 298.29
               Mean episode length: 123.89
    Episode_Reward/reaching_object: 0.6956
     Episode_Reward/lifting_object: 65.2134
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 23.8750
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 2.27s
                      Time elapsed: 00:32:44
                               ETA: 00:43:05

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 42786 steps/s (collection: 2.168s, learning 0.129s)
             Mean action noise std: 2.45
          Mean value_function loss: 514.0232
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 60.0497
                       Mean reward: 345.93
               Mean episode length: 139.20
    Episode_Reward/reaching_object: 0.6823
     Episode_Reward/lifting_object: 63.9022
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 26.8333
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 2.30s
                      Time elapsed: 00:32:47
                               ETA: 00:43:03

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 41157 steps/s (collection: 2.294s, learning 0.095s)
             Mean action noise std: 2.45
          Mean value_function loss: 542.1815
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 60.0505
                       Mean reward: 337.00
               Mean episode length: 133.54
    Episode_Reward/reaching_object: 0.6691
     Episode_Reward/lifting_object: 62.7099
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 24.4583
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 2.39s
                      Time elapsed: 00:32:49
                               ETA: 00:43:01

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 41102 steps/s (collection: 2.301s, learning 0.091s)
             Mean action noise std: 2.45
          Mean value_function loss: 520.7772
               Mean surrogate loss: 0.0138
                 Mean entropy loss: 60.0503
                       Mean reward: 328.01
               Mean episode length: 131.28
    Episode_Reward/reaching_object: 0.6971
     Episode_Reward/lifting_object: 66.0180
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 28.2917
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 2.39s
                      Time elapsed: 00:32:51
                               ETA: 00:42:59

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 37302 steps/s (collection: 2.539s, learning 0.097s)
             Mean action noise std: 2.45
          Mean value_function loss: 506.6026
               Mean surrogate loss: 0.0110
                 Mean entropy loss: 60.0507
                       Mean reward: 299.00
               Mean episode length: 127.86
    Episode_Reward/reaching_object: 0.6244
     Episode_Reward/lifting_object: 56.7230
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 33.5833
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 2.64s
                      Time elapsed: 00:32:54
                               ETA: 00:42:57

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 44232 steps/s (collection: 2.131s, learning 0.092s)
             Mean action noise std: 2.45
          Mean value_function loss: 508.0458
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 60.0509
                       Mean reward: 231.02
               Mean episode length: 116.02
    Episode_Reward/reaching_object: 0.6086
     Episode_Reward/lifting_object: 54.6409
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 4.0833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 29.7083
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 2.22s
                      Time elapsed: 00:32:56
                               ETA: 00:42:54

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 41627 steps/s (collection: 2.266s, learning 0.095s)
             Mean action noise std: 2.45
          Mean value_function loss: 549.5919
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 60.0513
                       Mean reward: 253.52
               Mean episode length: 113.26
    Episode_Reward/reaching_object: 0.6025
     Episode_Reward/lifting_object: 53.5054
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 4.0417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 31.9583
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 2.36s
                      Time elapsed: 00:32:59
                               ETA: 00:42:52

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 43533 steps/s (collection: 2.166s, learning 0.092s)
             Mean action noise std: 2.45
          Mean value_function loss: 557.9093
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 60.0519
                       Mean reward: 245.20
               Mean episode length: 111.60
    Episode_Reward/reaching_object: 0.5672
     Episode_Reward/lifting_object: 49.2173
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 28.6250
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 2.26s
                      Time elapsed: 00:33:01
                               ETA: 00:42:50

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 43437 steps/s (collection: 2.162s, learning 0.101s)
             Mean action noise std: 2.45
          Mean value_function loss: 540.4755
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 60.0526
                       Mean reward: 253.90
               Mean episode length: 111.08
    Episode_Reward/reaching_object: 0.5992
     Episode_Reward/lifting_object: 51.5662
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 29.1667
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 2.26s
                      Time elapsed: 00:33:03
                               ETA: 00:42:48

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 44388 steps/s (collection: 2.117s, learning 0.098s)
             Mean action noise std: 2.45
          Mean value_function loss: 584.1990
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 60.0538
                       Mean reward: 301.53
               Mean episode length: 125.81
    Episode_Reward/reaching_object: 0.6204
     Episode_Reward/lifting_object: 54.5251
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 27.6250
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 2.21s
                      Time elapsed: 00:33:05
                               ETA: 00:42:45

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 43955 steps/s (collection: 2.140s, learning 0.097s)
             Mean action noise std: 2.45
          Mean value_function loss: 493.4908
               Mean surrogate loss: 0.0130
                 Mean entropy loss: 60.0553
                       Mean reward: 333.13
               Mean episode length: 130.81
    Episode_Reward/reaching_object: 0.6288
     Episode_Reward/lifting_object: 55.8714
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 25.4167
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 2.24s
                      Time elapsed: 00:33:08
                               ETA: 00:42:43

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 43056 steps/s (collection: 2.187s, learning 0.096s)
             Mean action noise std: 2.45
          Mean value_function loss: 481.0421
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 60.0558
                       Mean reward: 334.83
               Mean episode length: 131.58
    Episode_Reward/reaching_object: 0.6452
     Episode_Reward/lifting_object: 57.9763
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 24.8333
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 2.28s
                      Time elapsed: 00:33:10
                               ETA: 00:42:41

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 43922 steps/s (collection: 2.140s, learning 0.098s)
             Mean action noise std: 2.45
          Mean value_function loss: 533.5167
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 60.0560
                       Mean reward: 318.91
               Mean episode length: 125.89
    Episode_Reward/reaching_object: 0.6702
     Episode_Reward/lifting_object: 62.3493
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 4.8333
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 24.6250
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 2.24s
                      Time elapsed: 00:33:12
                               ETA: 00:42:38

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 43513 steps/s (collection: 2.164s, learning 0.095s)
             Mean action noise std: 2.45
          Mean value_function loss: 524.6045
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 60.0565
                       Mean reward: 356.67
               Mean episode length: 138.22
    Episode_Reward/reaching_object: 0.6735
     Episode_Reward/lifting_object: 62.6813
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 27.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 2.26s
                      Time elapsed: 00:33:14
                               ETA: 00:42:36

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 44801 steps/s (collection: 2.105s, learning 0.090s)
             Mean action noise std: 2.45
          Mean value_function loss: 495.3981
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 60.0568
                       Mean reward: 290.72
               Mean episode length: 119.21
    Episode_Reward/reaching_object: 0.6856
     Episode_Reward/lifting_object: 64.5954
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 28.2917
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 2.19s
                      Time elapsed: 00:33:17
                               ETA: 00:42:34

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 43900 steps/s (collection: 2.121s, learning 0.119s)
             Mean action noise std: 2.45
          Mean value_function loss: 507.5936
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 60.0570
                       Mean reward: 361.73
               Mean episode length: 136.66
    Episode_Reward/reaching_object: 0.7120
     Episode_Reward/lifting_object: 67.1921
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 5.3750
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 27.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 2.24s
                      Time elapsed: 00:33:19
                               ETA: 00:42:31

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 43983 steps/s (collection: 2.120s, learning 0.115s)
             Mean action noise std: 2.45
          Mean value_function loss: 527.7509
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 60.0572
                       Mean reward: 356.55
               Mean episode length: 134.77
    Episode_Reward/reaching_object: 0.6767
     Episode_Reward/lifting_object: 63.3271
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 5.1250
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 29.6667
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 2.24s
                      Time elapsed: 00:33:21
                               ETA: 00:42:29

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 44477 steps/s (collection: 2.118s, learning 0.092s)
             Mean action noise std: 2.45
          Mean value_function loss: 521.3462
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 60.0574
                       Mean reward: 345.23
               Mean episode length: 130.00
    Episode_Reward/reaching_object: 0.6530
     Episode_Reward/lifting_object: 59.6451
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 27.1250
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 2.21s
                      Time elapsed: 00:33:23
                               ETA: 00:42:27

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 44469 steps/s (collection: 2.112s, learning 0.099s)
             Mean action noise std: 2.45
          Mean value_function loss: 531.9683
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 60.0579
                       Mean reward: 245.75
               Mean episode length: 113.51
    Episode_Reward/reaching_object: 0.6309
     Episode_Reward/lifting_object: 56.8634
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 28.2083
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 2.21s
                      Time elapsed: 00:33:25
                               ETA: 00:42:24

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 43971 steps/s (collection: 2.126s, learning 0.110s)
             Mean action noise std: 2.45
          Mean value_function loss: 520.2002
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 60.0592
                       Mean reward: 288.49
               Mean episode length: 116.33
    Episode_Reward/reaching_object: 0.6695
     Episode_Reward/lifting_object: 61.1693
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 4.6667
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 23.5000
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 2.24s
                      Time elapsed: 00:33:28
                               ETA: 00:42:22

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 43588 steps/s (collection: 2.152s, learning 0.104s)
             Mean action noise std: 2.45
          Mean value_function loss: 660.2070
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 60.0606
                       Mean reward: 299.72
               Mean episode length: 121.63
    Episode_Reward/reaching_object: 0.6410
     Episode_Reward/lifting_object: 58.4079
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 4.2500
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 27.6250
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 2.26s
                      Time elapsed: 00:33:30
                               ETA: 00:42:20

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 44549 steps/s (collection: 2.112s, learning 0.095s)
             Mean action noise std: 2.45
          Mean value_function loss: 554.4803
               Mean surrogate loss: 0.0145
                 Mean entropy loss: 60.0637
                       Mean reward: 328.48
               Mean episode length: 135.14
    Episode_Reward/reaching_object: 0.6918
     Episode_Reward/lifting_object: 62.6038
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 25.9583
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 2.21s
                      Time elapsed: 00:33:32
                               ETA: 00:42:17

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 43922 steps/s (collection: 2.132s, learning 0.107s)
             Mean action noise std: 2.45
          Mean value_function loss: 501.2748
               Mean surrogate loss: 0.0085
                 Mean entropy loss: 60.0643
                       Mean reward: 275.77
               Mean episode length: 122.29
    Episode_Reward/reaching_object: 0.6765
     Episode_Reward/lifting_object: 61.8537
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 5.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 25.7500
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 2.24s
                      Time elapsed: 00:33:34
                               ETA: 00:42:15

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 44278 steps/s (collection: 2.128s, learning 0.093s)
             Mean action noise std: 2.45
          Mean value_function loss: 582.3840
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 60.0645
                       Mean reward: 326.80
               Mean episode length: 125.98
    Episode_Reward/reaching_object: 0.6857
     Episode_Reward/lifting_object: 64.2581
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 4.5833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 30.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 2.22s
                      Time elapsed: 00:33:37
                               ETA: 00:42:13

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 44145 steps/s (collection: 2.122s, learning 0.105s)
             Mean action noise std: 2.45
          Mean value_function loss: 610.2329
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 60.0655
                       Mean reward: 327.28
               Mean episode length: 125.88
    Episode_Reward/reaching_object: 0.6808
     Episode_Reward/lifting_object: 63.2783
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 2.23s
                      Time elapsed: 00:33:39
                               ETA: 00:42:10

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 44010 steps/s (collection: 2.131s, learning 0.103s)
             Mean action noise std: 2.45
          Mean value_function loss: 566.0563
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 60.0665
                       Mean reward: 320.27
               Mean episode length: 124.89
    Episode_Reward/reaching_object: 0.6943
     Episode_Reward/lifting_object: 63.9263
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 28.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 2.23s
                      Time elapsed: 00:33:41
                               ETA: 00:42:08

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 43889 steps/s (collection: 2.145s, learning 0.095s)
             Mean action noise std: 2.45
          Mean value_function loss: 534.8851
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 60.0668
                       Mean reward: 277.90
               Mean episode length: 118.30
    Episode_Reward/reaching_object: 0.6877
     Episode_Reward/lifting_object: 63.5083
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 26.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 2.24s
                      Time elapsed: 00:33:43
                               ETA: 00:42:06

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 44195 steps/s (collection: 2.129s, learning 0.095s)
             Mean action noise std: 2.45
          Mean value_function loss: 539.5459
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 60.0688
                       Mean reward: 361.34
               Mean episode length: 132.99
    Episode_Reward/reaching_object: 0.6840
     Episode_Reward/lifting_object: 62.6380
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 25.8333
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 2.22s
                      Time elapsed: 00:33:45
                               ETA: 00:42:03

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 44414 steps/s (collection: 2.114s, learning 0.099s)
             Mean action noise std: 2.45
          Mean value_function loss: 535.5203
               Mean surrogate loss: 0.0143
                 Mean entropy loss: 60.0722
                       Mean reward: 293.91
               Mean episode length: 128.20
    Episode_Reward/reaching_object: 0.6849
     Episode_Reward/lifting_object: 62.4509
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 25.4167
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 2.21s
                      Time elapsed: 00:33:48
                               ETA: 00:42:01

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 43945 steps/s (collection: 2.130s, learning 0.107s)
             Mean action noise std: 2.45
          Mean value_function loss: 480.2058
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 60.0730
                       Mean reward: 249.59
               Mean episode length: 115.71
    Episode_Reward/reaching_object: 0.6393
     Episode_Reward/lifting_object: 56.2125
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 24.9167
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 2.24s
                      Time elapsed: 00:33:50
                               ETA: 00:41:59

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 44039 steps/s (collection: 2.122s, learning 0.110s)
             Mean action noise std: 2.45
          Mean value_function loss: 500.5716
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 60.0740
                       Mean reward: 288.32
               Mean episode length: 120.24
    Episode_Reward/reaching_object: 0.6501
     Episode_Reward/lifting_object: 58.3205
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 24.4167
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 2.23s
                      Time elapsed: 00:33:52
                               ETA: 00:41:56

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 43898 steps/s (collection: 2.132s, learning 0.107s)
             Mean action noise std: 2.45
          Mean value_function loss: 500.0925
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 60.0752
                       Mean reward: 377.66
               Mean episode length: 143.83
    Episode_Reward/reaching_object: 0.7355
     Episode_Reward/lifting_object: 68.6711
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 21.2500
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 2.24s
                      Time elapsed: 00:33:54
                               ETA: 00:41:54

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 44551 steps/s (collection: 2.110s, learning 0.097s)
             Mean action noise std: 2.45
          Mean value_function loss: 498.5735
               Mean surrogate loss: 0.0111
                 Mean entropy loss: 60.0755
                       Mean reward: 398.56
               Mean episode length: 145.96
    Episode_Reward/reaching_object: 0.7189
     Episode_Reward/lifting_object: 67.6255
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 23.5833
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 2.21s
                      Time elapsed: 00:33:57
                               ETA: 00:41:52

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 43656 steps/s (collection: 2.150s, learning 0.102s)
             Mean action noise std: 2.45
          Mean value_function loss: 514.3236
               Mean surrogate loss: 0.0130
                 Mean entropy loss: 60.0757
                       Mean reward: 397.49
               Mean episode length: 146.54
    Episode_Reward/reaching_object: 0.7595
     Episode_Reward/lifting_object: 71.8696
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 24.7500
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 2.25s
                      Time elapsed: 00:33:59
                               ETA: 00:41:49

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 43887 steps/s (collection: 2.143s, learning 0.097s)
             Mean action noise std: 2.45
          Mean value_function loss: 486.2741
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 60.0759
                       Mean reward: 292.89
               Mean episode length: 124.28
    Episode_Reward/reaching_object: 0.7185
     Episode_Reward/lifting_object: 66.4444
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 22.8750
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.24s
                      Time elapsed: 00:34:01
                               ETA: 00:41:47

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 44141 steps/s (collection: 2.131s, learning 0.096s)
             Mean action noise std: 2.45
          Mean value_function loss: 489.2442
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 60.0762
                       Mean reward: 369.40
               Mean episode length: 140.84
    Episode_Reward/reaching_object: 0.7735
     Episode_Reward/lifting_object: 74.4709
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 22.7500
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 2.23s
                      Time elapsed: 00:34:03
                               ETA: 00:41:45

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 42473 steps/s (collection: 2.211s, learning 0.104s)
             Mean action noise std: 2.45
          Mean value_function loss: 528.6190
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 60.0761
                       Mean reward: 308.64
               Mean episode length: 127.33
    Episode_Reward/reaching_object: 0.7715
     Episode_Reward/lifting_object: 73.7892
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 21.5833
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 2.31s
                      Time elapsed: 00:34:06
                               ETA: 00:41:43

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 43360 steps/s (collection: 2.169s, learning 0.098s)
             Mean action noise std: 2.45
          Mean value_function loss: 569.6731
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 60.0754
                       Mean reward: 336.69
               Mean episode length: 127.01
    Episode_Reward/reaching_object: 0.7149
     Episode_Reward/lifting_object: 67.6761
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 24.7917
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 2.27s
                      Time elapsed: 00:34:08
                               ETA: 00:41:40

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 43238 steps/s (collection: 2.182s, learning 0.091s)
             Mean action noise std: 2.45
          Mean value_function loss: 529.3999
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 60.0758
                       Mean reward: 347.38
               Mean episode length: 130.86
    Episode_Reward/reaching_object: 0.7188
     Episode_Reward/lifting_object: 67.7536
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 2.27s
                      Time elapsed: 00:34:10
                               ETA: 00:41:38

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 44019 steps/s (collection: 2.140s, learning 0.093s)
             Mean action noise std: 2.45
          Mean value_function loss: 534.8999
               Mean surrogate loss: 0.0133
                 Mean entropy loss: 60.0759
                       Mean reward: 293.61
               Mean episode length: 124.87
    Episode_Reward/reaching_object: 0.7534
     Episode_Reward/lifting_object: 72.5549
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 23.7500
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 2.23s
                      Time elapsed: 00:34:12
                               ETA: 00:41:36

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 43648 steps/s (collection: 2.156s, learning 0.096s)
             Mean action noise std: 2.45
          Mean value_function loss: 526.3992
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 60.0759
                       Mean reward: 441.84
               Mean episode length: 152.33
    Episode_Reward/reaching_object: 0.7656
     Episode_Reward/lifting_object: 74.5919
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 23.7500
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 2.25s
                      Time elapsed: 00:34:15
                               ETA: 00:41:33

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 43778 steps/s (collection: 2.139s, learning 0.107s)
             Mean action noise std: 2.45
          Mean value_function loss: 542.0866
               Mean surrogate loss: 0.0131
                 Mean entropy loss: 60.0762
                       Mean reward: 369.76
               Mean episode length: 130.62
    Episode_Reward/reaching_object: 0.7112
     Episode_Reward/lifting_object: 67.7210
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 27.0417
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 2.25s
                      Time elapsed: 00:34:17
                               ETA: 00:41:31

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 42389 steps/s (collection: 2.207s, learning 0.113s)
             Mean action noise std: 2.45
          Mean value_function loss: 567.4778
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 60.0764
                       Mean reward: 324.83
               Mean episode length: 127.71
    Episode_Reward/reaching_object: 0.7089
     Episode_Reward/lifting_object: 67.6298
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 26.3750
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.32s
                      Time elapsed: 00:34:19
                               ETA: 00:41:29

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 43541 steps/s (collection: 2.149s, learning 0.109s)
             Mean action noise std: 2.45
          Mean value_function loss: 492.6298
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 60.0767
                       Mean reward: 339.36
               Mean episode length: 131.32
    Episode_Reward/reaching_object: 0.7003
     Episode_Reward/lifting_object: 65.0154
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 22.7917
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 2.26s
                      Time elapsed: 00:34:21
                               ETA: 00:41:27

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 43870 steps/s (collection: 2.147s, learning 0.094s)
             Mean action noise std: 2.45
          Mean value_function loss: 549.8985
               Mean surrogate loss: 0.0153
                 Mean entropy loss: 60.0767
                       Mean reward: 345.89
               Mean episode length: 135.10
    Episode_Reward/reaching_object: 0.7378
     Episode_Reward/lifting_object: 70.6205
      Episode_Reward/object_height: 0.0057
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 5.9583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 25.2083
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.24s
                      Time elapsed: 00:34:24
                               ETA: 00:41:24

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 42312 steps/s (collection: 2.207s, learning 0.116s)
             Mean action noise std: 2.45
          Mean value_function loss: 477.3111
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 60.0767
                       Mean reward: 381.92
               Mean episode length: 137.62
    Episode_Reward/reaching_object: 0.7699
     Episode_Reward/lifting_object: 74.7153
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 20.5833
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.32s
                      Time elapsed: 00:34:26
                               ETA: 00:41:22

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 43668 steps/s (collection: 2.158s, learning 0.094s)
             Mean action noise std: 2.45
          Mean value_function loss: 507.2781
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 60.0768
                       Mean reward: 365.92
               Mean episode length: 140.27
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 75.0029
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 5.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 21.4167
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.25s
                      Time elapsed: 00:34:28
                               ETA: 00:41:20

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 43773 steps/s (collection: 2.148s, learning 0.098s)
             Mean action noise std: 2.45
          Mean value_function loss: 523.2269
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 60.0772
                       Mean reward: 341.95
               Mean episode length: 132.60
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 73.0360
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 24.6250
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.25s
                      Time elapsed: 00:34:31
                               ETA: 00:41:17

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 41228 steps/s (collection: 2.290s, learning 0.095s)
             Mean action noise std: 2.46
          Mean value_function loss: 529.6824
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 60.0793
                       Mean reward: 352.25
               Mean episode length: 126.17
    Episode_Reward/reaching_object: 0.7213
     Episode_Reward/lifting_object: 69.5962
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 22.2917
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 2.38s
                      Time elapsed: 00:34:33
                               ETA: 00:41:15

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 41094 steps/s (collection: 2.297s, learning 0.095s)
             Mean action noise std: 2.46
          Mean value_function loss: 516.3786
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 60.0811
                       Mean reward: 314.97
               Mean episode length: 123.73
    Episode_Reward/reaching_object: 0.7423
     Episode_Reward/lifting_object: 71.5737
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 22.7500
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 2.39s
                      Time elapsed: 00:34:35
                               ETA: 00:41:13

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 43834 steps/s (collection: 2.149s, learning 0.094s)
             Mean action noise std: 2.46
          Mean value_function loss: 570.9199
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 60.0824
                       Mean reward: 432.97
               Mean episode length: 153.40
    Episode_Reward/reaching_object: 0.8311
     Episode_Reward/lifting_object: 82.1265
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 20.5000
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 2.24s
                      Time elapsed: 00:34:38
                               ETA: 00:41:11

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 44144 steps/s (collection: 2.134s, learning 0.093s)
             Mean action noise std: 2.46
          Mean value_function loss: 496.4298
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 60.0840
                       Mean reward: 477.11
               Mean episode length: 159.10
    Episode_Reward/reaching_object: 0.8195
     Episode_Reward/lifting_object: 80.6094
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 21.0000
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 2.23s
                      Time elapsed: 00:34:40
                               ETA: 00:41:09

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 43172 steps/s (collection: 2.167s, learning 0.110s)
             Mean action noise std: 2.46
          Mean value_function loss: 520.5377
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.0847
                       Mean reward: 348.72
               Mean episode length: 132.15
    Episode_Reward/reaching_object: 0.7771
     Episode_Reward/lifting_object: 76.1775
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 23.6250
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.28s
                      Time elapsed: 00:34:42
                               ETA: 00:41:06

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 43588 steps/s (collection: 2.163s, learning 0.092s)
             Mean action noise std: 2.46
          Mean value_function loss: 563.5146
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 60.0852
                       Mean reward: 370.75
               Mean episode length: 140.06
    Episode_Reward/reaching_object: 0.7789
     Episode_Reward/lifting_object: 76.2640
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 24.4167
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.26s
                      Time elapsed: 00:34:44
                               ETA: 00:41:04

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 42147 steps/s (collection: 2.225s, learning 0.107s)
             Mean action noise std: 2.46
          Mean value_function loss: 552.8245
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 60.0874
                       Mean reward: 460.36
               Mean episode length: 152.93
    Episode_Reward/reaching_object: 0.7878
     Episode_Reward/lifting_object: 77.7992
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 22.0833
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.33s
                      Time elapsed: 00:34:47
                               ETA: 00:41:02

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 43654 steps/s (collection: 2.151s, learning 0.101s)
             Mean action noise std: 2.46
          Mean value_function loss: 544.3714
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 60.0914
                       Mean reward: 322.77
               Mean episode length: 120.21
    Episode_Reward/reaching_object: 0.7024
     Episode_Reward/lifting_object: 67.0981
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 23.7500
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.25s
                      Time elapsed: 00:34:49
                               ETA: 00:41:00

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 40968 steps/s (collection: 2.267s, learning 0.133s)
             Mean action noise std: 2.46
          Mean value_function loss: 529.9005
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 60.0927
                       Mean reward: 394.80
               Mean episode length: 139.67
    Episode_Reward/reaching_object: 0.7694
     Episode_Reward/lifting_object: 76.3443
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 6.0833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 21.9167
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.40s
                      Time elapsed: 00:34:51
                               ETA: 00:40:57

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 41858 steps/s (collection: 2.252s, learning 0.097s)
             Mean action noise std: 2.46
          Mean value_function loss: 527.6058
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 60.0947
                       Mean reward: 387.88
               Mean episode length: 141.05
    Episode_Reward/reaching_object: 0.7753
     Episode_Reward/lifting_object: 76.0087
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 20.2917
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 2.35s
                      Time elapsed: 00:34:54
                               ETA: 00:40:55

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 39948 steps/s (collection: 2.342s, learning 0.119s)
             Mean action noise std: 2.46
          Mean value_function loss: 561.1658
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 60.0975
                       Mean reward: 330.20
               Mean episode length: 133.03
    Episode_Reward/reaching_object: 0.8033
     Episode_Reward/lifting_object: 79.4938
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 25.0833
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 2.46s
                      Time elapsed: 00:34:56
                               ETA: 00:40:53

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 40513 steps/s (collection: 2.326s, learning 0.100s)
             Mean action noise std: 2.46
          Mean value_function loss: 573.6029
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 60.0999
                       Mean reward: 397.33
               Mean episode length: 136.56
    Episode_Reward/reaching_object: 0.7504
     Episode_Reward/lifting_object: 73.3932
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 23.6250
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 2.43s
                      Time elapsed: 00:34:59
                               ETA: 00:40:51

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 40310 steps/s (collection: 2.305s, learning 0.134s)
             Mean action noise std: 2.46
          Mean value_function loss: 660.0735
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 60.1043
                       Mean reward: 368.89
               Mean episode length: 134.64
    Episode_Reward/reaching_object: 0.7562
     Episode_Reward/lifting_object: 75.0844
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 25.2917
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.44s
                      Time elapsed: 00:35:01
                               ETA: 00:40:49

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 41502 steps/s (collection: 2.246s, learning 0.123s)
             Mean action noise std: 2.46
          Mean value_function loss: 583.9983
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 60.1055
                       Mean reward: 372.48
               Mean episode length: 136.43
    Episode_Reward/reaching_object: 0.7388
     Episode_Reward/lifting_object: 71.6187
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 23.9167
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.37s
                      Time elapsed: 00:35:03
                               ETA: 00:40:47

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 41229 steps/s (collection: 2.262s, learning 0.122s)
             Mean action noise std: 2.46
          Mean value_function loss: 569.2420
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 60.1061
                       Mean reward: 378.86
               Mean episode length: 132.85
    Episode_Reward/reaching_object: 0.7351
     Episode_Reward/lifting_object: 71.1242
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 23.5417
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.38s
                      Time elapsed: 00:35:06
                               ETA: 00:40:45

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 40742 steps/s (collection: 2.277s, learning 0.136s)
             Mean action noise std: 2.46
          Mean value_function loss: 546.8667
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 60.1076
                       Mean reward: 411.39
               Mean episode length: 141.77
    Episode_Reward/reaching_object: 0.8096
     Episode_Reward/lifting_object: 80.5431
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 21.7083
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 2.41s
                      Time elapsed: 00:35:08
                               ETA: 00:40:43

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 41908 steps/s (collection: 2.248s, learning 0.098s)
             Mean action noise std: 2.46
          Mean value_function loss: 519.4860
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 60.1116
                       Mean reward: 404.50
               Mean episode length: 139.12
    Episode_Reward/reaching_object: 0.8140
     Episode_Reward/lifting_object: 81.5379
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 18.5000
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 2.35s
                      Time elapsed: 00:35:11
                               ETA: 00:40:40

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 43397 steps/s (collection: 2.171s, learning 0.094s)
             Mean action noise std: 2.46
          Mean value_function loss: 511.4561
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 60.1131
                       Mean reward: 394.81
               Mean episode length: 138.79
    Episode_Reward/reaching_object: 0.7875
     Episode_Reward/lifting_object: 78.2724
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 19.8750
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.27s
                      Time elapsed: 00:35:13
                               ETA: 00:40:38

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 40810 steps/s (collection: 2.254s, learning 0.155s)
             Mean action noise std: 2.46
          Mean value_function loss: 526.8661
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 60.1133
                       Mean reward: 411.37
               Mean episode length: 146.88
    Episode_Reward/reaching_object: 0.7909
     Episode_Reward/lifting_object: 78.4120
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 20.5417
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 2.41s
                      Time elapsed: 00:35:15
                               ETA: 00:40:36

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 42189 steps/s (collection: 2.224s, learning 0.106s)
             Mean action noise std: 2.46
          Mean value_function loss: 484.3661
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 60.1136
                       Mean reward: 412.70
               Mean episode length: 144.90
    Episode_Reward/reaching_object: 0.8154
     Episode_Reward/lifting_object: 80.9114
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 5.8333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 19.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 2.33s
                      Time elapsed: 00:35:18
                               ETA: 00:40:34

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 43328 steps/s (collection: 2.166s, learning 0.103s)
             Mean action noise std: 2.46
          Mean value_function loss: 510.3437
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 60.1144
                       Mean reward: 429.25
               Mean episode length: 149.00
    Episode_Reward/reaching_object: 0.8548
     Episode_Reward/lifting_object: 86.1992
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 18.7500
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.27s
                      Time elapsed: 00:35:20
                               ETA: 00:40:31

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 43388 steps/s (collection: 2.163s, learning 0.103s)
             Mean action noise std: 2.46
          Mean value_function loss: 599.0567
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 60.1157
                       Mean reward: 403.98
               Mean episode length: 142.25
    Episode_Reward/reaching_object: 0.8456
     Episode_Reward/lifting_object: 84.8042
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 23.2917
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.27s
                      Time elapsed: 00:35:22
                               ETA: 00:40:29

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 43351 steps/s (collection: 2.177s, learning 0.091s)
             Mean action noise std: 2.46
          Mean value_function loss: 560.0110
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.1181
                       Mean reward: 458.09
               Mean episode length: 153.50
    Episode_Reward/reaching_object: 0.8536
     Episode_Reward/lifting_object: 85.5770
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 21.8750
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 2.27s
                      Time elapsed: 00:35:24
                               ETA: 00:40:27

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 43078 steps/s (collection: 2.187s, learning 0.095s)
             Mean action noise std: 2.46
          Mean value_function loss: 613.3738
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 60.1210
                       Mean reward: 380.48
               Mean episode length: 135.88
    Episode_Reward/reaching_object: 0.8136
     Episode_Reward/lifting_object: 80.7104
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 22.8333
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.28s
                      Time elapsed: 00:35:27
                               ETA: 00:40:25

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 43194 steps/s (collection: 2.180s, learning 0.096s)
             Mean action noise std: 2.46
          Mean value_function loss: 617.4159
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 60.1234
                       Mean reward: 368.76
               Mean episode length: 131.52
    Episode_Reward/reaching_object: 0.8178
     Episode_Reward/lifting_object: 81.3877
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 23.4583
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 2.28s
                      Time elapsed: 00:35:29
                               ETA: 00:40:22

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 43303 steps/s (collection: 2.175s, learning 0.096s)
             Mean action noise std: 2.46
          Mean value_function loss: 559.7068
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 60.1276
                       Mean reward: 394.38
               Mean episode length: 144.40
    Episode_Reward/reaching_object: 0.7780
     Episode_Reward/lifting_object: 75.9315
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 20.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 2.27s
                      Time elapsed: 00:35:31
                               ETA: 00:40:20

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 43267 steps/s (collection: 2.173s, learning 0.099s)
             Mean action noise std: 2.46
          Mean value_function loss: 597.1822
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 60.1308
                       Mean reward: 345.06
               Mean episode length: 131.27
    Episode_Reward/reaching_object: 0.7644
     Episode_Reward/lifting_object: 73.7223
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 21.7500
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.27s
                      Time elapsed: 00:35:33
                               ETA: 00:40:18

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 43749 steps/s (collection: 2.149s, learning 0.098s)
             Mean action noise std: 2.46
          Mean value_function loss: 535.8480
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 60.1322
                       Mean reward: 288.93
               Mean episode length: 125.75
    Episode_Reward/reaching_object: 0.7251
     Episode_Reward/lifting_object: 68.3809
      Episode_Reward/object_height: 0.0060
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 19.8750
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.25s
                      Time elapsed: 00:35:36
                               ETA: 00:40:15

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 43892 steps/s (collection: 2.147s, learning 0.093s)
             Mean action noise std: 2.46
          Mean value_function loss: 503.2059
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 60.1362
                       Mean reward: 370.38
               Mean episode length: 139.38
    Episode_Reward/reaching_object: 0.8041
     Episode_Reward/lifting_object: 78.4792
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 7.2083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 18.0417
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.24s
                      Time elapsed: 00:35:38
                               ETA: 00:40:13

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 43905 steps/s (collection: 2.146s, learning 0.093s)
             Mean action noise std: 2.46
          Mean value_function loss: 489.1475
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 60.1379
                       Mean reward: 461.16
               Mean episode length: 159.90
    Episode_Reward/reaching_object: 0.8070
     Episode_Reward/lifting_object: 79.3854
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 17.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.24s
                      Time elapsed: 00:35:40
                               ETA: 00:40:11

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 43841 steps/s (collection: 2.152s, learning 0.091s)
             Mean action noise std: 2.46
          Mean value_function loss: 498.7131
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 60.1383
                       Mean reward: 457.16
               Mean episode length: 151.72
    Episode_Reward/reaching_object: 0.8303
     Episode_Reward/lifting_object: 83.4842
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 18.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.24s
                      Time elapsed: 00:35:42
                               ETA: 00:40:09

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 44220 steps/s (collection: 2.131s, learning 0.092s)
             Mean action noise std: 2.46
          Mean value_function loss: 540.8524
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 60.1388
                       Mean reward: 432.73
               Mean episode length: 149.13
    Episode_Reward/reaching_object: 0.8496
     Episode_Reward/lifting_object: 85.5787
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 18.3750
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.22s
                      Time elapsed: 00:35:45
                               ETA: 00:40:06

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 44241 steps/s (collection: 2.125s, learning 0.097s)
             Mean action noise std: 2.46
          Mean value_function loss: 579.2115
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 60.1404
                       Mean reward: 471.85
               Mean episode length: 158.18
    Episode_Reward/reaching_object: 0.8844
     Episode_Reward/lifting_object: 89.5343
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 20.0000
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.22s
                      Time elapsed: 00:35:47
                               ETA: 00:40:04

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 43381 steps/s (collection: 2.167s, learning 0.099s)
             Mean action noise std: 2.46
          Mean value_function loss: 563.2311
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 60.1438
                       Mean reward: 456.13
               Mean episode length: 162.67
    Episode_Reward/reaching_object: 0.8534
     Episode_Reward/lifting_object: 85.9265
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 18.9167
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.27s
                      Time elapsed: 00:35:49
                               ETA: 00:40:02

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 42897 steps/s (collection: 2.178s, learning 0.114s)
             Mean action noise std: 2.46
          Mean value_function loss: 579.8548
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 60.1450
                       Mean reward: 457.93
               Mean episode length: 160.19
    Episode_Reward/reaching_object: 0.9077
     Episode_Reward/lifting_object: 92.0897
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 20.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.29s
                      Time elapsed: 00:35:51
                               ETA: 00:39:59

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 43338 steps/s (collection: 2.158s, learning 0.110s)
             Mean action noise std: 2.46
          Mean value_function loss: 615.4789
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 60.1465
                       Mean reward: 401.55
               Mean episode length: 142.84
    Episode_Reward/reaching_object: 0.8315
     Episode_Reward/lifting_object: 83.2636
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 8.5000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 20.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.27s
                      Time elapsed: 00:35:54
                               ETA: 00:39:57

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 43992 steps/s (collection: 2.138s, learning 0.097s)
             Mean action noise std: 2.46
          Mean value_function loss: 590.1506
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 60.1484
                       Mean reward: 435.14
               Mean episode length: 147.75
    Episode_Reward/reaching_object: 0.7686
     Episode_Reward/lifting_object: 74.9687
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 20.8750
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 2.23s
                      Time elapsed: 00:35:56
                               ETA: 00:39:55

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 42467 steps/s (collection: 2.210s, learning 0.105s)
             Mean action noise std: 2.46
          Mean value_function loss: 561.5997
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 60.1489
                       Mean reward: 411.99
               Mean episode length: 153.30
    Episode_Reward/reaching_object: 0.8465
     Episode_Reward/lifting_object: 83.8549
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 8.2500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.31s
                      Time elapsed: 00:35:58
                               ETA: 00:39:52

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 42973 steps/s (collection: 2.180s, learning 0.108s)
             Mean action noise std: 2.46
          Mean value_function loss: 534.9796
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 60.1499
                       Mean reward: 339.69
               Mean episode length: 127.56
    Episode_Reward/reaching_object: 0.7845
     Episode_Reward/lifting_object: 78.0963
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 6.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 19.5000
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.29s
                      Time elapsed: 00:36:00
                               ETA: 00:39:50

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 43387 steps/s (collection: 2.171s, learning 0.095s)
             Mean action noise std: 2.46
          Mean value_function loss: 558.4993
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 60.1514
                       Mean reward: 396.78
               Mean episode length: 139.81
    Episode_Reward/reaching_object: 0.8279
     Episode_Reward/lifting_object: 81.7475
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 18.7917
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.27s
                      Time elapsed: 00:36:03
                               ETA: 00:39:48

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 43673 steps/s (collection: 2.156s, learning 0.095s)
             Mean action noise std: 2.47
          Mean value_function loss: 516.8055
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 60.1527
                       Mean reward: 423.09
               Mean episode length: 148.67
    Episode_Reward/reaching_object: 0.8570
     Episode_Reward/lifting_object: 85.7909
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 17.0833
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.25s
                      Time elapsed: 00:36:05
                               ETA: 00:39:46

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 43744 steps/s (collection: 2.144s, learning 0.104s)
             Mean action noise std: 2.47
          Mean value_function loss: 520.1543
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 60.1533
                       Mean reward: 456.53
               Mean episode length: 155.61
    Episode_Reward/reaching_object: 0.9197
     Episode_Reward/lifting_object: 94.1563
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.25s
                      Time elapsed: 00:36:07
                               ETA: 00:39:43

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 43043 steps/s (collection: 2.179s, learning 0.105s)
             Mean action noise std: 2.47
          Mean value_function loss: 576.5065
               Mean surrogate loss: 0.0091
                 Mean entropy loss: 60.1536
                       Mean reward: 460.91
               Mean episode length: 157.44
    Episode_Reward/reaching_object: 0.9131
     Episode_Reward/lifting_object: 93.5926
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 7.6250
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 19.4167
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.28s
                      Time elapsed: 00:36:10
                               ETA: 00:39:41

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 43684 steps/s (collection: 2.153s, learning 0.098s)
             Mean action noise std: 2.47
          Mean value_function loss: 518.6696
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 60.1544
                       Mean reward: 412.20
               Mean episode length: 148.91
    Episode_Reward/reaching_object: 0.8489
     Episode_Reward/lifting_object: 86.0346
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 18.2500
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.25s
                      Time elapsed: 00:36:12
                               ETA: 00:39:39

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 44145 steps/s (collection: 2.132s, learning 0.095s)
             Mean action noise std: 2.47
          Mean value_function loss: 554.0787
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 60.1567
                       Mean reward: 479.90
               Mean episode length: 161.75
    Episode_Reward/reaching_object: 0.8351
     Episode_Reward/lifting_object: 84.8935
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 19.5417
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.23s
                      Time elapsed: 00:36:14
                               ETA: 00:39:36

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 43975 steps/s (collection: 2.140s, learning 0.096s)
             Mean action noise std: 2.47
          Mean value_function loss: 548.8856
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 60.1597
                       Mean reward: 380.58
               Mean episode length: 135.55
    Episode_Reward/reaching_object: 0.8335
     Episode_Reward/lifting_object: 84.3364
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 17.6250
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.24s
                      Time elapsed: 00:36:16
                               ETA: 00:39:34

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 43698 steps/s (collection: 2.125s, learning 0.125s)
             Mean action noise std: 2.47
          Mean value_function loss: 515.7735
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 60.1616
                       Mean reward: 397.52
               Mean episode length: 143.49
    Episode_Reward/reaching_object: 0.8579
     Episode_Reward/lifting_object: 87.5644
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 18.2083
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.25s
                      Time elapsed: 00:36:18
                               ETA: 00:39:32

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 42998 steps/s (collection: 2.170s, learning 0.117s)
             Mean action noise std: 2.47
          Mean value_function loss: 508.0293
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 60.1633
                       Mean reward: 446.37
               Mean episode length: 149.48
    Episode_Reward/reaching_object: 0.8623
     Episode_Reward/lifting_object: 89.1505
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 17.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.29s
                      Time elapsed: 00:36:21
                               ETA: 00:39:30

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 44409 steps/s (collection: 2.118s, learning 0.096s)
             Mean action noise std: 2.47
          Mean value_function loss: 493.6874
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 60.1643
                       Mean reward: 497.21
               Mean episode length: 161.96
    Episode_Reward/reaching_object: 0.9022
     Episode_Reward/lifting_object: 93.5268
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 17.3750
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.21s
                      Time elapsed: 00:36:23
                               ETA: 00:39:27

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 43714 steps/s (collection: 2.148s, learning 0.101s)
             Mean action noise std: 2.47
          Mean value_function loss: 519.4754
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 60.1652
                       Mean reward: 393.07
               Mean episode length: 141.70
    Episode_Reward/reaching_object: 0.8557
     Episode_Reward/lifting_object: 86.3385
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 17.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 2.25s
                      Time elapsed: 00:36:25
                               ETA: 00:39:25

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 43904 steps/s (collection: 2.132s, learning 0.107s)
             Mean action noise std: 2.47
          Mean value_function loss: 477.0119
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 60.1661
                       Mean reward: 377.85
               Mean episode length: 137.06
    Episode_Reward/reaching_object: 0.8609
     Episode_Reward/lifting_object: 87.9814
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 8.7500
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.24s
                      Time elapsed: 00:36:27
                               ETA: 00:39:23

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 43999 steps/s (collection: 2.140s, learning 0.095s)
             Mean action noise std: 2.47
          Mean value_function loss: 569.0832
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 60.1667
                       Mean reward: 422.99
               Mean episode length: 152.88
    Episode_Reward/reaching_object: 0.9150
     Episode_Reward/lifting_object: 94.5457
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 17.9167
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.23s
                      Time elapsed: 00:36:30
                               ETA: 00:39:20

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 43943 steps/s (collection: 2.143s, learning 0.095s)
             Mean action noise std: 2.47
          Mean value_function loss: 572.3285
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 60.1678
                       Mean reward: 453.27
               Mean episode length: 154.60
    Episode_Reward/reaching_object: 0.8076
     Episode_Reward/lifting_object: 82.1356
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 20.8750
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.24s
                      Time elapsed: 00:36:32
                               ETA: 00:39:18

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 43349 steps/s (collection: 2.158s, learning 0.110s)
             Mean action noise std: 2.47
          Mean value_function loss: 569.2871
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 60.1695
                       Mean reward: 452.76
               Mean episode length: 150.73
    Episode_Reward/reaching_object: 0.8229
     Episode_Reward/lifting_object: 83.6951
      Episode_Reward/object_height: 0.0073
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 19.3333
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.27s
                      Time elapsed: 00:36:34
                               ETA: 00:39:16

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 43755 steps/s (collection: 2.150s, learning 0.097s)
             Mean action noise std: 2.47
          Mean value_function loss: 579.6744
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 60.1703
                       Mean reward: 399.95
               Mean episode length: 141.52
    Episode_Reward/reaching_object: 0.8561
     Episode_Reward/lifting_object: 88.2799
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 19.6667
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.25s
                      Time elapsed: 00:36:36
                               ETA: 00:39:13

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 44196 steps/s (collection: 2.131s, learning 0.094s)
             Mean action noise std: 2.47
          Mean value_function loss: 547.7010
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 60.1707
                       Mean reward: 406.53
               Mean episode length: 141.73
    Episode_Reward/reaching_object: 0.8499
     Episode_Reward/lifting_object: 87.4296
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 17.5000
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.22s
                      Time elapsed: 00:36:39
                               ETA: 00:39:11

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 44242 steps/s (collection: 2.126s, learning 0.096s)
             Mean action noise std: 2.47
          Mean value_function loss: 507.3369
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 60.1698
                       Mean reward: 475.77
               Mean episode length: 157.41
    Episode_Reward/reaching_object: 0.8824
     Episode_Reward/lifting_object: 91.6826
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 16.5833
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.22s
                      Time elapsed: 00:36:41
                               ETA: 00:39:09

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 44398 steps/s (collection: 2.112s, learning 0.102s)
             Mean action noise std: 2.47
          Mean value_function loss: 595.6359
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 60.1703
                       Mean reward: 397.86
               Mean episode length: 137.45
    Episode_Reward/reaching_object: 0.8260
     Episode_Reward/lifting_object: 84.0755
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 19.2917
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.21s
                      Time elapsed: 00:36:43
                               ETA: 00:39:06

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 43912 steps/s (collection: 2.136s, learning 0.103s)
             Mean action noise std: 2.47
          Mean value_function loss: 591.4275
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 60.1711
                       Mean reward: 441.74
               Mean episode length: 148.19
    Episode_Reward/reaching_object: 0.8026
     Episode_Reward/lifting_object: 81.5462
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 21.2917
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.24s
                      Time elapsed: 00:36:45
                               ETA: 00:39:04

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 43330 steps/s (collection: 2.149s, learning 0.120s)
             Mean action noise std: 2.47
          Mean value_function loss: 646.4005
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 60.1725
                       Mean reward: 395.78
               Mean episode length: 131.78
    Episode_Reward/reaching_object: 0.7421
     Episode_Reward/lifting_object: 75.2365
      Episode_Reward/object_height: 0.0061
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 23.5833
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.27s
                      Time elapsed: 00:36:48
                               ETA: 00:39:02

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 43598 steps/s (collection: 2.161s, learning 0.094s)
             Mean action noise std: 2.47
          Mean value_function loss: 765.2313
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 60.1757
                       Mean reward: 381.35
               Mean episode length: 130.76
    Episode_Reward/reaching_object: 0.7375
     Episode_Reward/lifting_object: 74.7922
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0493
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.5000
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.25s
                      Time elapsed: 00:36:50
                               ETA: 00:39:00

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 43424 steps/s (collection: 2.164s, learning 0.100s)
             Mean action noise std: 2.47
          Mean value_function loss: 657.6492
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 60.1787
                       Mean reward: 478.87
               Mean episode length: 152.52
    Episode_Reward/reaching_object: 0.7843
     Episode_Reward/lifting_object: 81.2225
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 21.1250
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.26s
                      Time elapsed: 00:36:52
                               ETA: 00:38:57

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 43990 steps/s (collection: 2.144s, learning 0.091s)
             Mean action noise std: 2.47
          Mean value_function loss: 622.0360
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 60.1820
                       Mean reward: 453.02
               Mean episode length: 150.21
    Episode_Reward/reaching_object: 0.8420
     Episode_Reward/lifting_object: 87.9862
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.3750
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.23s
                      Time elapsed: 00:36:54
                               ETA: 00:38:55

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 43227 steps/s (collection: 2.156s, learning 0.119s)
             Mean action noise std: 2.47
          Mean value_function loss: 608.8873
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 60.1854
                       Mean reward: 398.12
               Mean episode length: 135.41
    Episode_Reward/reaching_object: 0.8347
     Episode_Reward/lifting_object: 85.9179
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 19.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.27s
                      Time elapsed: 00:36:57
                               ETA: 00:38:53

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 43421 steps/s (collection: 2.156s, learning 0.108s)
             Mean action noise std: 2.47
          Mean value_function loss: 538.1064
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 60.1869
                       Mean reward: 516.15
               Mean episode length: 163.14
    Episode_Reward/reaching_object: 0.8624
     Episode_Reward/lifting_object: 88.6627
      Episode_Reward/object_height: 0.0068
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 2.26s
                      Time elapsed: 00:36:59
                               ETA: 00:38:50

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 43278 steps/s (collection: 2.171s, learning 0.101s)
             Mean action noise std: 2.47
          Mean value_function loss: 518.8873
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 60.1880
                       Mean reward: 400.65
               Mean episode length: 139.98
    Episode_Reward/reaching_object: 0.8781
     Episode_Reward/lifting_object: 91.1987
      Episode_Reward/object_height: 0.0069
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.4583
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.27s
                      Time elapsed: 00:37:01
                               ETA: 00:38:48

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 43893 steps/s (collection: 2.147s, learning 0.093s)
             Mean action noise std: 2.47
          Mean value_function loss: 524.7145
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 60.1881
                       Mean reward: 398.84
               Mean episode length: 140.10
    Episode_Reward/reaching_object: 0.8042
     Episode_Reward/lifting_object: 80.5396
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 18.7083
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.24s
                      Time elapsed: 00:37:03
                               ETA: 00:38:46

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 43079 steps/s (collection: 2.167s, learning 0.115s)
             Mean action noise std: 2.47
          Mean value_function loss: 512.3284
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 60.1881
                       Mean reward: 403.84
               Mean episode length: 139.79
    Episode_Reward/reaching_object: 0.8198
     Episode_Reward/lifting_object: 82.9894
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 17.1250
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.28s
                      Time elapsed: 00:37:06
                               ETA: 00:38:43

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 42460 steps/s (collection: 2.198s, learning 0.118s)
             Mean action noise std: 2.47
          Mean value_function loss: 538.0431
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 60.1880
                       Mean reward: 432.23
               Mean episode length: 150.66
    Episode_Reward/reaching_object: 0.8566
     Episode_Reward/lifting_object: 87.6265
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 18.9583
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.32s
                      Time elapsed: 00:37:08
                               ETA: 00:38:41

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 43241 steps/s (collection: 2.179s, learning 0.095s)
             Mean action noise std: 2.47
          Mean value_function loss: 512.4294
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 60.1886
                       Mean reward: 474.37
               Mean episode length: 156.42
    Episode_Reward/reaching_object: 0.8906
     Episode_Reward/lifting_object: 92.2764
      Episode_Reward/object_height: 0.0076
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 8.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.4167
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 2.27s
                      Time elapsed: 00:37:10
                               ETA: 00:38:39

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 43715 steps/s (collection: 2.154s, learning 0.095s)
             Mean action noise std: 2.47
          Mean value_function loss: 508.8877
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 60.1906
                       Mean reward: 473.14
               Mean episode length: 160.08
    Episode_Reward/reaching_object: 0.9323
     Episode_Reward/lifting_object: 96.6123
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.7500
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.25s
                      Time elapsed: 00:37:13
                               ETA: 00:38:37

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 44106 steps/s (collection: 2.133s, learning 0.096s)
             Mean action noise std: 2.47
          Mean value_function loss: 526.0025
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 60.1941
                       Mean reward: 500.84
               Mean episode length: 165.38
    Episode_Reward/reaching_object: 0.9144
     Episode_Reward/lifting_object: 95.5917
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 16.3750
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.23s
                      Time elapsed: 00:37:15
                               ETA: 00:38:34

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 43307 steps/s (collection: 2.173s, learning 0.097s)
             Mean action noise std: 2.47
          Mean value_function loss: 513.9951
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 60.1974
                       Mean reward: 479.60
               Mean episode length: 159.21
    Episode_Reward/reaching_object: 0.8865
     Episode_Reward/lifting_object: 91.4001
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.2500
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 2.27s
                      Time elapsed: 00:37:17
                               ETA: 00:38:32

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 41287 steps/s (collection: 2.270s, learning 0.111s)
             Mean action noise std: 2.47
          Mean value_function loss: 503.1972
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 60.1992
                       Mean reward: 436.88
               Mean episode length: 148.52
    Episode_Reward/reaching_object: 0.9129
     Episode_Reward/lifting_object: 93.7954
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 16.3750
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 2.38s
                      Time elapsed: 00:37:19
                               ETA: 00:38:30

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 40505 steps/s (collection: 2.296s, learning 0.131s)
             Mean action noise std: 2.47
          Mean value_function loss: 517.5500
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.2012
                       Mean reward: 508.52
               Mean episode length: 164.20
    Episode_Reward/reaching_object: 0.9226
     Episode_Reward/lifting_object: 95.6400
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.8750
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 2.43s
                      Time elapsed: 00:37:22
                               ETA: 00:38:28

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 31551 steps/s (collection: 2.991s, learning 0.125s)
             Mean action noise std: 2.47
          Mean value_function loss: 511.7570
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 60.2066
                       Mean reward: 525.18
               Mean episode length: 171.13
    Episode_Reward/reaching_object: 0.9317
     Episode_Reward/lifting_object: 95.7883
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.0833
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 3.12s
                      Time elapsed: 00:37:25
                               ETA: 00:38:26

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 35055 steps/s (collection: 2.625s, learning 0.180s)
             Mean action noise std: 2.47
          Mean value_function loss: 554.1012
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 60.2114
                       Mean reward: 499.40
               Mean episode length: 159.82
    Episode_Reward/reaching_object: 0.9454
     Episode_Reward/lifting_object: 98.4650
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 16.1250
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.80s
                      Time elapsed: 00:37:28
                               ETA: 00:38:25

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 30595 steps/s (collection: 2.906s, learning 0.307s)
             Mean action noise std: 2.47
          Mean value_function loss: 565.1213
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 60.2130
                       Mean reward: 477.63
               Mean episode length: 155.70
    Episode_Reward/reaching_object: 0.9496
     Episode_Reward/lifting_object: 97.8334
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 3.21s
                      Time elapsed: 00:37:31
                               ETA: 00:38:23

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 35046 steps/s (collection: 2.615s, learning 0.190s)
             Mean action noise std: 2.47
          Mean value_function loss: 543.4188
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 60.2144
                       Mean reward: 519.24
               Mean episode length: 164.50
    Episode_Reward/reaching_object: 0.9724
     Episode_Reward/lifting_object: 100.7597
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.80s
                      Time elapsed: 00:37:34
                               ETA: 00:38:22

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 40555 steps/s (collection: 2.324s, learning 0.100s)
             Mean action noise std: 2.47
          Mean value_function loss: 541.6275
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 60.2162
                       Mean reward: 475.49
               Mean episode length: 155.67
    Episode_Reward/reaching_object: 0.9677
     Episode_Reward/lifting_object: 99.7707
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.5833
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.42s
                      Time elapsed: 00:37:36
                               ETA: 00:38:19

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 42810 steps/s (collection: 2.194s, learning 0.103s)
             Mean action noise std: 2.48
          Mean value_function loss: 504.0340
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 60.2183
                       Mean reward: 486.83
               Mean episode length: 162.10
    Episode_Reward/reaching_object: 0.9476
     Episode_Reward/lifting_object: 96.9484
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.30s
                      Time elapsed: 00:37:39
                               ETA: 00:38:17

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 42514 steps/s (collection: 2.208s, learning 0.104s)
             Mean action noise std: 2.48
          Mean value_function loss: 546.7988
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 60.2219
                       Mean reward: 467.90
               Mean episode length: 152.55
    Episode_Reward/reaching_object: 0.9516
     Episode_Reward/lifting_object: 98.7407
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 16.5833
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.31s
                      Time elapsed: 00:37:41
                               ETA: 00:38:15

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 43012 steps/s (collection: 2.186s, learning 0.099s)
             Mean action noise std: 2.48
          Mean value_function loss: 540.7251
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 60.2238
                       Mean reward: 478.91
               Mean episode length: 158.56
    Episode_Reward/reaching_object: 0.9437
     Episode_Reward/lifting_object: 97.3786
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 15.9583
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.29s
                      Time elapsed: 00:37:43
                               ETA: 00:38:13

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 42222 steps/s (collection: 2.234s, learning 0.094s)
             Mean action noise std: 2.48
          Mean value_function loss: 520.6377
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 60.2243
                       Mean reward: 486.78
               Mean episode length: 160.32
    Episode_Reward/reaching_object: 0.9602
     Episode_Reward/lifting_object: 99.0172
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.33s
                      Time elapsed: 00:37:45
                               ETA: 00:38:10

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 43336 steps/s (collection: 2.165s, learning 0.103s)
             Mean action noise std: 2.48
          Mean value_function loss: 512.9571
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 60.2246
                       Mean reward: 463.82
               Mean episode length: 156.48
    Episode_Reward/reaching_object: 0.9581
     Episode_Reward/lifting_object: 99.5484
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.1250
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.27s
                      Time elapsed: 00:37:48
                               ETA: 00:38:08

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 42273 steps/s (collection: 2.211s, learning 0.115s)
             Mean action noise std: 2.48
          Mean value_function loss: 519.1875
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 60.2247
                       Mean reward: 493.12
               Mean episode length: 162.22
    Episode_Reward/reaching_object: 0.9875
     Episode_Reward/lifting_object: 102.4469
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.33s
                      Time elapsed: 00:37:50
                               ETA: 00:38:06

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 43954 steps/s (collection: 2.133s, learning 0.103s)
             Mean action noise std: 2.48
          Mean value_function loss: 526.5740
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 60.2247
                       Mean reward: 540.62
               Mean episode length: 168.03
    Episode_Reward/reaching_object: 0.9714
     Episode_Reward/lifting_object: 101.2058
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 16.2917
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.24s
                      Time elapsed: 00:37:52
                               ETA: 00:38:04

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 44580 steps/s (collection: 2.108s, learning 0.097s)
             Mean action noise std: 2.48
          Mean value_function loss: 529.3778
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 60.2254
                       Mean reward: 522.67
               Mean episode length: 165.21
    Episode_Reward/reaching_object: 0.9643
     Episode_Reward/lifting_object: 100.8933
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.6250
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 2.21s
                      Time elapsed: 00:37:54
                               ETA: 00:38:01

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 44393 steps/s (collection: 2.115s, learning 0.100s)
             Mean action noise std: 2.48
          Mean value_function loss: 511.3313
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 60.2287
                       Mean reward: 470.79
               Mean episode length: 160.76
    Episode_Reward/reaching_object: 0.9296
     Episode_Reward/lifting_object: 95.5808
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.3333
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 2.21s
                      Time elapsed: 00:37:57
                               ETA: 00:37:59

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 14184 steps/s (collection: 6.814s, learning 0.117s)
             Mean action noise std: 2.48
          Mean value_function loss: 532.5769
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 60.2348
                       Mean reward: 457.90
               Mean episode length: 153.47
    Episode_Reward/reaching_object: 0.9854
     Episode_Reward/lifting_object: 102.9408
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.3333
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 6.93s
                      Time elapsed: 00:38:04
                               ETA: 00:38:01

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 14189 steps/s (collection: 6.803s, learning 0.126s)
             Mean action noise std: 2.48
          Mean value_function loss: 519.2199
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 60.2372
                       Mean reward: 517.76
               Mean episode length: 166.50
    Episode_Reward/reaching_object: 0.9425
     Episode_Reward/lifting_object: 96.8488
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 6.93s
                      Time elapsed: 00:38:11
                               ETA: 00:38:04

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 14332 steps/s (collection: 6.746s, learning 0.113s)
             Mean action noise std: 2.48
          Mean value_function loss: 520.9463
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 60.2398
                       Mean reward: 507.21
               Mean episode length: 165.25
    Episode_Reward/reaching_object: 0.9784
     Episode_Reward/lifting_object: 102.7321
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 9.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.5417
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 6.86s
                      Time elapsed: 00:38:17
                               ETA: 00:38:06

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 14386 steps/s (collection: 6.723s, learning 0.110s)
             Mean action noise std: 2.48
          Mean value_function loss: 565.5539
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 60.2427
                       Mean reward: 558.75
               Mean episode length: 168.61
    Episode_Reward/reaching_object: 0.9850
     Episode_Reward/lifting_object: 103.5931
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 6.83s
                      Time elapsed: 00:38:24
                               ETA: 00:38:08

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 13995 steps/s (collection: 6.915s, learning 0.109s)
             Mean action noise std: 2.48
          Mean value_function loss: 529.7797
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 60.2459
                       Mean reward: 529.89
               Mean episode length: 166.14
    Episode_Reward/reaching_object: 0.9872
     Episode_Reward/lifting_object: 103.5203
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 14.5417
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 7.02s
                      Time elapsed: 00:38:31
                               ETA: 00:38:11

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 14075 steps/s (collection: 6.868s, learning 0.116s)
             Mean action noise std: 2.48
          Mean value_function loss: 519.7713
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 60.2501
                       Mean reward: 576.82
               Mean episode length: 177.28
    Episode_Reward/reaching_object: 1.0336
     Episode_Reward/lifting_object: 109.1110
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 6.98s
                      Time elapsed: 00:38:38
                               ETA: 00:38:13

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 13966 steps/s (collection: 6.912s, learning 0.126s)
             Mean action noise std: 2.48
          Mean value_function loss: 477.5899
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 60.2540
                       Mean reward: 512.55
               Mean episode length: 165.21
    Episode_Reward/reaching_object: 1.0284
     Episode_Reward/lifting_object: 108.1673
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 7.04s
                      Time elapsed: 00:38:45
                               ETA: 00:38:15

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 14000 steps/s (collection: 6.907s, learning 0.114s)
             Mean action noise std: 2.48
          Mean value_function loss: 541.2707
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 60.2551
                       Mean reward: 566.27
               Mean episode length: 177.00
    Episode_Reward/reaching_object: 1.0002
     Episode_Reward/lifting_object: 105.5012
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 8.9583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 15.7917
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 7.02s
                      Time elapsed: 00:38:52
                               ETA: 00:38:18

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 17567 steps/s (collection: 5.496s, learning 0.100s)
             Mean action noise std: 2.48
          Mean value_function loss: 493.1982
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 60.2562
                       Mean reward: 513.93
               Mean episode length: 162.35
    Episode_Reward/reaching_object: 0.9837
     Episode_Reward/lifting_object: 103.7512
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 15.6250
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 5.60s
                      Time elapsed: 00:38:58
                               ETA: 00:38:19

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 44678 steps/s (collection: 2.103s, learning 0.097s)
             Mean action noise std: 2.48
          Mean value_function loss: 513.0008
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 60.2614
                       Mean reward: 526.83
               Mean episode length: 167.85
    Episode_Reward/reaching_object: 1.0161
     Episode_Reward/lifting_object: 107.9512
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 15.0000
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 2.20s
                      Time elapsed: 00:39:00
                               ETA: 00:38:16

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 45423 steps/s (collection: 2.078s, learning 0.087s)
             Mean action noise std: 2.48
          Mean value_function loss: 504.3993
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 60.2676
                       Mean reward: 520.45
               Mean episode length: 169.20
    Episode_Reward/reaching_object: 1.0191
     Episode_Reward/lifting_object: 107.7243
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.6667
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 2.16s
                      Time elapsed: 00:39:02
                               ETA: 00:38:14

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 46139 steps/s (collection: 2.043s, learning 0.088s)
             Mean action noise std: 2.48
          Mean value_function loss: 517.1444
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 60.2731
                       Mean reward: 507.89
               Mean episode length: 162.28
    Episode_Reward/reaching_object: 0.9414
     Episode_Reward/lifting_object: 98.8161
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 8.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.6667
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 2.13s
                      Time elapsed: 00:39:04
                               ETA: 00:38:11

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 47443 steps/s (collection: 1.985s, learning 0.087s)
             Mean action noise std: 2.48
          Mean value_function loss: 518.6531
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 60.2760
                       Mean reward: 443.68
               Mean episode length: 157.60
    Episode_Reward/reaching_object: 1.0205
     Episode_Reward/lifting_object: 108.4202
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.4167
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 2.07s
                      Time elapsed: 00:39:06
                               ETA: 00:38:09

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 46936 steps/s (collection: 2.005s, learning 0.089s)
             Mean action noise std: 2.48
          Mean value_function loss: 532.4948
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.2780
                       Mean reward: 539.43
               Mean episode length: 166.61
    Episode_Reward/reaching_object: 1.0152
     Episode_Reward/lifting_object: 107.5090
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 2.09s
                      Time elapsed: 00:39:09
                               ETA: 00:38:06

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 46338 steps/s (collection: 2.024s, learning 0.097s)
             Mean action noise std: 2.48
          Mean value_function loss: 560.5829
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.2824
                       Mean reward: 526.16
               Mean episode length: 168.56
    Episode_Reward/reaching_object: 0.9632
     Episode_Reward/lifting_object: 101.3649
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.9583
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 2.12s
                      Time elapsed: 00:39:11
                               ETA: 00:38:04

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 45903 steps/s (collection: 2.051s, learning 0.091s)
             Mean action noise std: 2.48
          Mean value_function loss: 551.2148
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 60.2867
                       Mean reward: 526.15
               Mean episode length: 166.71
    Episode_Reward/reaching_object: 0.9562
     Episode_Reward/lifting_object: 100.1370
      Episode_Reward/object_height: 0.0093
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 15.4583
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 2.14s
                      Time elapsed: 00:39:13
                               ETA: 00:38:01

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 47443 steps/s (collection: 1.985s, learning 0.087s)
             Mean action noise std: 2.48
          Mean value_function loss: 541.3230
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 60.2875
                       Mean reward: 567.28
               Mean episode length: 175.42
    Episode_Reward/reaching_object: 0.9381
     Episode_Reward/lifting_object: 97.8707
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 8.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 16.0833
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 2.07s
                      Time elapsed: 00:39:15
                               ETA: 00:37:58

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 46920 steps/s (collection: 2.006s, learning 0.090s)
             Mean action noise std: 2.48
          Mean value_function loss: 491.9025
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 60.2887
                       Mean reward: 503.17
               Mean episode length: 159.90
    Episode_Reward/reaching_object: 0.9676
     Episode_Reward/lifting_object: 102.1439
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 9.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 14.8333
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 2.10s
                      Time elapsed: 00:39:17
                               ETA: 00:37:56

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 47248 steps/s (collection: 1.996s, learning 0.085s)
             Mean action noise std: 2.48
          Mean value_function loss: 478.3499
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 60.2891
                       Mean reward: 565.19
               Mean episode length: 173.11
    Episode_Reward/reaching_object: 1.0193
     Episode_Reward/lifting_object: 108.3169
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 2.08s
                      Time elapsed: 00:39:19
                               ETA: 00:37:53

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 45166 steps/s (collection: 2.086s, learning 0.091s)
             Mean action noise std: 2.48
          Mean value_function loss: 485.6687
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 60.2895
                       Mean reward: 604.77
               Mean episode length: 182.42
    Episode_Reward/reaching_object: 1.0531
     Episode_Reward/lifting_object: 111.8456
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 2.18s
                      Time elapsed: 00:39:21
                               ETA: 00:37:51

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 46201 steps/s (collection: 2.039s, learning 0.089s)
             Mean action noise std: 2.48
          Mean value_function loss: 477.1158
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 60.2905
                       Mean reward: 592.47
               Mean episode length: 184.45
    Episode_Reward/reaching_object: 1.0201
     Episode_Reward/lifting_object: 108.1136
      Episode_Reward/object_height: 0.0092
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 2.13s
                      Time elapsed: 00:39:23
                               ETA: 00:37:48

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 46597 steps/s (collection: 1.993s, learning 0.117s)
             Mean action noise std: 2.48
          Mean value_function loss: 461.9721
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 60.2909
                       Mean reward: 483.61
               Mean episode length: 161.22
    Episode_Reward/reaching_object: 0.9827
     Episode_Reward/lifting_object: 101.9139
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 2.11s
                      Time elapsed: 00:39:25
                               ETA: 00:37:46

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 46535 steps/s (collection: 2.011s, learning 0.101s)
             Mean action noise std: 2.48
          Mean value_function loss: 514.4746
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 60.2914
                       Mean reward: 514.32
               Mean episode length: 167.00
    Episode_Reward/reaching_object: 1.0048
     Episode_Reward/lifting_object: 105.4967
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.9167
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 2.11s
                      Time elapsed: 00:39:28
                               ETA: 00:37:43

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 46823 steps/s (collection: 2.011s, learning 0.089s)
             Mean action noise std: 2.48
          Mean value_function loss: 558.6768
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 60.2924
                       Mean reward: 535.11
               Mean episode length: 166.95
    Episode_Reward/reaching_object: 1.0223
     Episode_Reward/lifting_object: 107.6939
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 2.10s
                      Time elapsed: 00:39:30
                               ETA: 00:37:41

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 46216 steps/s (collection: 2.032s, learning 0.096s)
             Mean action noise std: 2.49
          Mean value_function loss: 509.1179
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 60.2951
                       Mean reward: 491.33
               Mean episode length: 161.51
    Episode_Reward/reaching_object: 0.9878
     Episode_Reward/lifting_object: 103.3301
      Episode_Reward/object_height: 0.0088
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 2.13s
                      Time elapsed: 00:39:32
                               ETA: 00:37:38

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 46543 steps/s (collection: 2.022s, learning 0.090s)
             Mean action noise std: 2.49
          Mean value_function loss: 489.0023
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 60.2976
                       Mean reward: 545.75
               Mean episode length: 167.22
    Episode_Reward/reaching_object: 1.0506
     Episode_Reward/lifting_object: 111.9425
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 2.11s
                      Time elapsed: 00:39:34
                               ETA: 00:37:36

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 45441 steps/s (collection: 2.071s, learning 0.093s)
             Mean action noise std: 2.49
          Mean value_function loss: 536.6297
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 60.3024
                       Mean reward: 455.70
               Mean episode length: 154.50
    Episode_Reward/reaching_object: 1.0102
     Episode_Reward/lifting_object: 106.3349
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.6250
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 2.16s
                      Time elapsed: 00:39:36
                               ETA: 00:37:33

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 45642 steps/s (collection: 2.038s, learning 0.116s)
             Mean action noise std: 2.49
          Mean value_function loss: 588.2559
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 60.3067
                       Mean reward: 464.01
               Mean episode length: 146.48
    Episode_Reward/reaching_object: 0.9490
     Episode_Reward/lifting_object: 99.1297
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 16.5833
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 2.15s
                      Time elapsed: 00:39:38
                               ETA: 00:37:31

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 47116 steps/s (collection: 1.990s, learning 0.096s)
             Mean action noise std: 2.49
          Mean value_function loss: 614.5486
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.3094
                       Mean reward: 501.00
               Mean episode length: 154.60
    Episode_Reward/reaching_object: 0.9343
     Episode_Reward/lifting_object: 97.8199
      Episode_Reward/object_height: 0.0082
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.2917
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 2.09s
                      Time elapsed: 00:39:40
                               ETA: 00:37:28

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 44551 steps/s (collection: 2.097s, learning 0.110s)
             Mean action noise std: 2.49
          Mean value_function loss: 586.1633
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 60.3139
                       Mean reward: 444.67
               Mean episode length: 144.55
    Episode_Reward/reaching_object: 0.8847
     Episode_Reward/lifting_object: 91.3473
      Episode_Reward/object_height: 0.0075
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 17.2083
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 2.21s
                      Time elapsed: 00:39:43
                               ETA: 00:37:26

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 46473 steps/s (collection: 2.025s, learning 0.090s)
             Mean action noise std: 2.49
          Mean value_function loss: 560.5001
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 60.3253
                       Mean reward: 530.28
               Mean episode length: 164.87
    Episode_Reward/reaching_object: 0.9437
     Episode_Reward/lifting_object: 98.7700
      Episode_Reward/object_height: 0.0080
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 16.4167
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 2.12s
                      Time elapsed: 00:39:45
                               ETA: 00:37:24

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 46409 steps/s (collection: 2.029s, learning 0.090s)
             Mean action noise std: 2.49
          Mean value_function loss: 575.1593
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 60.3345
                       Mean reward: 496.22
               Mean episode length: 161.33
    Episode_Reward/reaching_object: 0.9595
     Episode_Reward/lifting_object: 99.9905
      Episode_Reward/object_height: 0.0079
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 15.3750
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 2.12s
                      Time elapsed: 00:39:47
                               ETA: 00:37:21

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 47257 steps/s (collection: 1.989s, learning 0.091s)
             Mean action noise std: 2.49
          Mean value_function loss: 528.6033
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 60.3416
                       Mean reward: 482.50
               Mean episode length: 155.29
    Episode_Reward/reaching_object: 0.9909
     Episode_Reward/lifting_object: 103.4344
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 9.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 2.08s
                      Time elapsed: 00:39:49
                               ETA: 00:37:19

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 46706 steps/s (collection: 2.018s, learning 0.087s)
             Mean action noise std: 2.49
          Mean value_function loss: 506.7204
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 60.3516
                       Mean reward: 544.06
               Mean episode length: 170.84
    Episode_Reward/reaching_object: 1.0536
     Episode_Reward/lifting_object: 111.3884
      Episode_Reward/object_height: 0.0087
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 2.10s
                      Time elapsed: 00:39:51
                               ETA: 00:37:16

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 47014 steps/s (collection: 2.005s, learning 0.086s)
             Mean action noise std: 2.49
          Mean value_function loss: 445.9123
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 60.3589
                       Mean reward: 568.29
               Mean episode length: 172.01
    Episode_Reward/reaching_object: 1.0423
     Episode_Reward/lifting_object: 110.2292
      Episode_Reward/object_height: 0.0085
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 2.09s
                      Time elapsed: 00:39:53
                               ETA: 00:37:13

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 46797 steps/s (collection: 2.007s, learning 0.094s)
             Mean action noise std: 2.49
          Mean value_function loss: 471.3177
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 60.3633
                       Mean reward: 497.03
               Mean episode length: 157.04
    Episode_Reward/reaching_object: 1.0117
     Episode_Reward/lifting_object: 106.5148
      Episode_Reward/object_height: 0.0083
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 2.10s
                      Time elapsed: 00:39:55
                               ETA: 00:37:11

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 46922 steps/s (collection: 2.004s, learning 0.091s)
             Mean action noise std: 2.49
          Mean value_function loss: 502.3964
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 60.3684
                       Mean reward: 485.83
               Mean episode length: 160.80
    Episode_Reward/reaching_object: 1.0610
     Episode_Reward/lifting_object: 112.8579
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 2.10s
                      Time elapsed: 00:39:57
                               ETA: 00:37:08

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 46520 steps/s (collection: 2.019s, learning 0.094s)
             Mean action noise std: 2.50
          Mean value_function loss: 491.3271
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 60.3757
                       Mean reward: 559.57
               Mean episode length: 171.29
    Episode_Reward/reaching_object: 1.0142
     Episode_Reward/lifting_object: 107.1563
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 2.11s
                      Time elapsed: 00:39:59
                               ETA: 00:37:06

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 46563 steps/s (collection: 2.012s, learning 0.099s)
             Mean action noise std: 2.50
          Mean value_function loss: 490.7915
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.3810
                       Mean reward: 503.12
               Mean episode length: 163.85
    Episode_Reward/reaching_object: 1.0121
     Episode_Reward/lifting_object: 106.4915
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 2.11s
                      Time elapsed: 00:40:01
                               ETA: 00:37:03

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 45697 steps/s (collection: 2.053s, learning 0.098s)
             Mean action noise std: 2.50
          Mean value_function loss: 528.0425
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 60.3881
                       Mean reward: 592.25
               Mean episode length: 180.03
    Episode_Reward/reaching_object: 1.1158
     Episode_Reward/lifting_object: 119.9557
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 2.15s
                      Time elapsed: 00:40:04
                               ETA: 00:37:01

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 47099 steps/s (collection: 2.002s, learning 0.086s)
             Mean action noise std: 2.50
          Mean value_function loss: 524.1015
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 60.3955
                       Mean reward: 567.14
               Mean episode length: 172.99
    Episode_Reward/reaching_object: 1.0522
     Episode_Reward/lifting_object: 112.2797
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 2.09s
                      Time elapsed: 00:40:06
                               ETA: 00:36:58

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 47062 steps/s (collection: 1.998s, learning 0.091s)
             Mean action noise std: 2.50
          Mean value_function loss: 495.3242
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 60.4021
                       Mean reward: 514.69
               Mean episode length: 163.18
    Episode_Reward/reaching_object: 1.0413
     Episode_Reward/lifting_object: 110.9983
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.7083
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 2.09s
                      Time elapsed: 00:40:08
                               ETA: 00:36:56

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 47295 steps/s (collection: 1.990s, learning 0.088s)
             Mean action noise std: 2.50
          Mean value_function loss: 477.3075
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.4108
                       Mean reward: 560.67
               Mean episode length: 175.31
    Episode_Reward/reaching_object: 1.0617
     Episode_Reward/lifting_object: 113.9066
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 2.08s
                      Time elapsed: 00:40:10
                               ETA: 00:36:53

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 45570 steps/s (collection: 2.063s, learning 0.094s)
             Mean action noise std: 2.50
          Mean value_function loss: 498.0188
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 60.4179
                       Mean reward: 591.91
               Mean episode length: 177.10
    Episode_Reward/reaching_object: 1.0424
     Episode_Reward/lifting_object: 110.9925
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.5000
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 2.16s
                      Time elapsed: 00:40:12
                               ETA: 00:36:51

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 46228 steps/s (collection: 2.037s, learning 0.090s)
             Mean action noise std: 2.50
          Mean value_function loss: 472.3327
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 60.4184
                       Mean reward: 583.46
               Mean episode length: 179.24
    Episode_Reward/reaching_object: 1.0911
     Episode_Reward/lifting_object: 117.8083
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 2.13s
                      Time elapsed: 00:40:14
                               ETA: 00:36:49

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 46520 steps/s (collection: 2.023s, learning 0.090s)
             Mean action noise std: 2.50
          Mean value_function loss: 505.9670
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 60.4196
                       Mean reward: 508.24
               Mean episode length: 160.34
    Episode_Reward/reaching_object: 1.0184
     Episode_Reward/lifting_object: 108.1423
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 2.11s
                      Time elapsed: 00:40:16
                               ETA: 00:36:46

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 47164 steps/s (collection: 1.999s, learning 0.085s)
             Mean action noise std: 2.50
          Mean value_function loss: 496.8698
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 60.4228
                       Mean reward: 488.13
               Mean episode length: 154.08
    Episode_Reward/reaching_object: 1.0423
     Episode_Reward/lifting_object: 111.5644
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.6667
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 2.08s
                      Time elapsed: 00:40:18
                               ETA: 00:36:44

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 46298 steps/s (collection: 2.026s, learning 0.098s)
             Mean action noise std: 2.50
          Mean value_function loss: 503.8922
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 60.4251
                       Mean reward: 603.18
               Mean episode length: 181.56
    Episode_Reward/reaching_object: 1.0546
     Episode_Reward/lifting_object: 112.4030
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 2.12s
                      Time elapsed: 00:40:20
                               ETA: 00:36:41

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 46583 steps/s (collection: 2.021s, learning 0.090s)
             Mean action noise std: 2.50
          Mean value_function loss: 504.1008
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 60.4284
                       Mean reward: 545.45
               Mean episode length: 170.04
    Episode_Reward/reaching_object: 1.0445
     Episode_Reward/lifting_object: 111.1651
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 2.11s
                      Time elapsed: 00:40:23
                               ETA: 00:36:39

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 46235 steps/s (collection: 2.028s, learning 0.098s)
             Mean action noise std: 2.50
          Mean value_function loss: 525.7671
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 60.4340
                       Mean reward: 555.52
               Mean episode length: 170.20
    Episode_Reward/reaching_object: 1.0902
     Episode_Reward/lifting_object: 118.1046
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.6250
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 2.13s
                      Time elapsed: 00:40:25
                               ETA: 00:36:36

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 46909 steps/s (collection: 2.009s, learning 0.087s)
             Mean action noise std: 2.50
          Mean value_function loss: 563.0731
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 60.4381
                       Mean reward: 575.30
               Mean episode length: 175.18
    Episode_Reward/reaching_object: 1.0583
     Episode_Reward/lifting_object: 113.3092
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 2.10s
                      Time elapsed: 00:40:27
                               ETA: 00:36:34

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 46208 steps/s (collection: 2.039s, learning 0.088s)
             Mean action noise std: 2.50
          Mean value_function loss: 543.0409
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 60.4413
                       Mean reward: 547.10
               Mean episode length: 168.11
    Episode_Reward/reaching_object: 1.0311
     Episode_Reward/lifting_object: 109.4403
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 14.3750
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 2.13s
                      Time elapsed: 00:40:29
                               ETA: 00:36:31

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 46794 steps/s (collection: 2.001s, learning 0.100s)
             Mean action noise std: 2.51
          Mean value_function loss: 517.4187
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 60.4498
                       Mean reward: 565.32
               Mean episode length: 172.14
    Episode_Reward/reaching_object: 1.0622
     Episode_Reward/lifting_object: 113.6930
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 2.10s
                      Time elapsed: 00:40:31
                               ETA: 00:36:29

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 46465 steps/s (collection: 2.009s, learning 0.107s)
             Mean action noise std: 2.51
          Mean value_function loss: 523.1660
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 60.4603
                       Mean reward: 595.79
               Mean episode length: 182.92
    Episode_Reward/reaching_object: 1.0547
     Episode_Reward/lifting_object: 111.8786
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 9.7917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 2.12s
                      Time elapsed: 00:40:33
                               ETA: 00:36:26

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 46861 steps/s (collection: 2.009s, learning 0.089s)
             Mean action noise std: 2.51
          Mean value_function loss: 509.6307
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 60.4644
                       Mean reward: 595.30
               Mean episode length: 182.57
    Episode_Reward/reaching_object: 1.0706
     Episode_Reward/lifting_object: 113.7849
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 2.10s
                      Time elapsed: 00:40:35
                               ETA: 00:36:24

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 46747 steps/s (collection: 2.014s, learning 0.089s)
             Mean action noise std: 2.51
          Mean value_function loss: 465.1034
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 60.4656
                       Mean reward: 575.26
               Mean episode length: 176.70
    Episode_Reward/reaching_object: 1.0872
     Episode_Reward/lifting_object: 116.3422
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 2.10s
                      Time elapsed: 00:40:37
                               ETA: 00:36:21

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 46705 steps/s (collection: 2.013s, learning 0.092s)
             Mean action noise std: 2.51
          Mean value_function loss: 473.0493
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 60.4679
                       Mean reward: 596.18
               Mean episode length: 179.88
    Episode_Reward/reaching_object: 1.0201
     Episode_Reward/lifting_object: 108.6479
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 2.10s
                      Time elapsed: 00:40:39
                               ETA: 00:36:19

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 46869 steps/s (collection: 2.003s, learning 0.094s)
             Mean action noise std: 2.51
          Mean value_function loss: 502.9389
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 60.4706
                       Mean reward: 527.61
               Mean episode length: 167.83
    Episode_Reward/reaching_object: 1.0676
     Episode_Reward/lifting_object: 115.0322
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 2.10s
                      Time elapsed: 00:40:42
                               ETA: 00:36:16

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 46644 steps/s (collection: 2.010s, learning 0.098s)
             Mean action noise std: 2.51
          Mean value_function loss: 463.3843
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 60.4714
                       Mean reward: 538.53
               Mean episode length: 169.47
    Episode_Reward/reaching_object: 1.0625
     Episode_Reward/lifting_object: 113.8682
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 9.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 2.11s
                      Time elapsed: 00:40:44
                               ETA: 00:36:14

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 45623 steps/s (collection: 2.030s, learning 0.125s)
             Mean action noise std: 2.51
          Mean value_function loss: 458.0559
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 60.4726
                       Mean reward: 642.74
               Mean episode length: 190.73
    Episode_Reward/reaching_object: 1.1531
     Episode_Reward/lifting_object: 124.5167
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 2.15s
                      Time elapsed: 00:40:46
                               ETA: 00:36:11

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 46143 steps/s (collection: 2.020s, learning 0.110s)
             Mean action noise std: 2.51
          Mean value_function loss: 505.5225
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 60.4735
                       Mean reward: 591.89
               Mean episode length: 183.27
    Episode_Reward/reaching_object: 1.0982
     Episode_Reward/lifting_object: 117.6238
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 2.13s
                      Time elapsed: 00:40:48
                               ETA: 00:36:09

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 46369 steps/s (collection: 2.016s, learning 0.104s)
             Mean action noise std: 2.51
          Mean value_function loss: 467.2605
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 60.4744
                       Mean reward: 565.57
               Mean episode length: 176.40
    Episode_Reward/reaching_object: 1.1226
     Episode_Reward/lifting_object: 120.8985
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 2.12s
                      Time elapsed: 00:40:50
                               ETA: 00:36:06

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 46207 steps/s (collection: 2.040s, learning 0.087s)
             Mean action noise std: 2.51
          Mean value_function loss: 491.6873
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 60.4753
                       Mean reward: 605.22
               Mean episode length: 184.48
    Episode_Reward/reaching_object: 1.0958
     Episode_Reward/lifting_object: 117.2740
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 2.13s
                      Time elapsed: 00:40:52
                               ETA: 00:36:04

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 46865 steps/s (collection: 2.007s, learning 0.091s)
             Mean action noise std: 2.51
          Mean value_function loss: 490.8945
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 60.4768
                       Mean reward: 562.84
               Mean episode length: 169.44
    Episode_Reward/reaching_object: 1.0854
     Episode_Reward/lifting_object: 116.4007
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 2.10s
                      Time elapsed: 00:40:54
                               ETA: 00:36:01

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 46426 steps/s (collection: 2.031s, learning 0.086s)
             Mean action noise std: 2.51
          Mean value_function loss: 520.5387
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 60.4781
                       Mean reward: 636.66
               Mean episode length: 190.35
    Episode_Reward/reaching_object: 1.0410
     Episode_Reward/lifting_object: 110.7760
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 2.12s
                      Time elapsed: 00:40:56
                               ETA: 00:35:59

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 46154 steps/s (collection: 2.038s, learning 0.092s)
             Mean action noise std: 2.51
          Mean value_function loss: 554.1339
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 60.4795
                       Mean reward: 584.26
               Mean episode length: 176.14
    Episode_Reward/reaching_object: 1.0708
     Episode_Reward/lifting_object: 114.2482
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 2.13s
                      Time elapsed: 00:40:59
                               ETA: 00:35:56

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 46675 steps/s (collection: 2.018s, learning 0.089s)
             Mean action noise std: 2.51
          Mean value_function loss: 546.9254
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 60.4811
                       Mean reward: 544.74
               Mean episode length: 166.64
    Episode_Reward/reaching_object: 1.1215
     Episode_Reward/lifting_object: 119.9418
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 2.11s
                      Time elapsed: 00:41:01
                               ETA: 00:35:54

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 45545 steps/s (collection: 2.054s, learning 0.105s)
             Mean action noise std: 2.51
          Mean value_function loss: 548.9412
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 60.4817
                       Mean reward: 578.49
               Mean episode length: 172.95
    Episode_Reward/reaching_object: 1.0279
     Episode_Reward/lifting_object: 108.3170
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 14.5833
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 2.16s
                      Time elapsed: 00:41:03
                               ETA: 00:35:51

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 44978 steps/s (collection: 2.072s, learning 0.114s)
             Mean action noise std: 2.51
          Mean value_function loss: 562.3054
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 60.4825
                       Mean reward: 589.51
               Mean episode length: 179.04
    Episode_Reward/reaching_object: 1.0062
     Episode_Reward/lifting_object: 105.5881
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 13.9167
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 2.19s
                      Time elapsed: 00:41:05
                               ETA: 00:35:49

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 44904 steps/s (collection: 2.080s, learning 0.109s)
             Mean action noise std: 2.51
          Mean value_function loss: 551.4102
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 60.4855
                       Mean reward: 515.76
               Mean episode length: 166.16
    Episode_Reward/reaching_object: 1.0547
     Episode_Reward/lifting_object: 111.7661
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 2.19s
                      Time elapsed: 00:41:07
                               ETA: 00:35:47

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 45361 steps/s (collection: 2.070s, learning 0.098s)
             Mean action noise std: 2.51
          Mean value_function loss: 520.2924
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.4906
                       Mean reward: 567.93
               Mean episode length: 175.62
    Episode_Reward/reaching_object: 1.0637
     Episode_Reward/lifting_object: 112.2796
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 2.17s
                      Time elapsed: 00:41:09
                               ETA: 00:35:44

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 43956 steps/s (collection: 2.136s, learning 0.101s)
             Mean action noise std: 2.51
          Mean value_function loss: 477.5790
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 60.4985
                       Mean reward: 574.77
               Mean episode length: 178.13
    Episode_Reward/reaching_object: 1.0698
     Episode_Reward/lifting_object: 113.8865
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 2.24s
                      Time elapsed: 00:41:12
                               ETA: 00:35:42

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 44641 steps/s (collection: 2.107s, learning 0.096s)
             Mean action noise std: 2.51
          Mean value_function loss: 444.3847
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 60.5102
                       Mean reward: 630.99
               Mean episode length: 187.66
    Episode_Reward/reaching_object: 1.1090
     Episode_Reward/lifting_object: 118.1802
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 2.20s
                      Time elapsed: 00:41:14
                               ETA: 00:35:39

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 45734 steps/s (collection: 2.051s, learning 0.099s)
             Mean action noise std: 2.51
          Mean value_function loss: 503.2038
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 60.5181
                       Mean reward: 588.60
               Mean episode length: 178.49
    Episode_Reward/reaching_object: 1.0725
     Episode_Reward/lifting_object: 114.8307
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 2.15s
                      Time elapsed: 00:41:16
                               ETA: 00:35:37

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 46326 steps/s (collection: 2.034s, learning 0.088s)
             Mean action noise std: 2.52
          Mean value_function loss: 523.9620
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 60.5332
                       Mean reward: 580.98
               Mean episode length: 177.84
    Episode_Reward/reaching_object: 1.1135
     Episode_Reward/lifting_object: 118.4593
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.4583
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 2.12s
                      Time elapsed: 00:41:18
                               ETA: 00:35:35

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 44676 steps/s (collection: 2.094s, learning 0.107s)
             Mean action noise std: 2.52
          Mean value_function loss: 488.8296
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 60.5475
                       Mean reward: 620.16
               Mean episode length: 191.77
    Episode_Reward/reaching_object: 1.1488
     Episode_Reward/lifting_object: 122.8775
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 2.20s
                      Time elapsed: 00:41:20
                               ETA: 00:35:32

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 45740 steps/s (collection: 2.059s, learning 0.090s)
             Mean action noise std: 2.52
          Mean value_function loss: 510.6668
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 60.5582
                       Mean reward: 520.13
               Mean episode length: 162.92
    Episode_Reward/reaching_object: 1.0833
     Episode_Reward/lifting_object: 114.5729
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 2.15s
                      Time elapsed: 00:41:22
                               ETA: 00:35:30

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 46373 steps/s (collection: 2.018s, learning 0.102s)
             Mean action noise std: 2.52
          Mean value_function loss: 474.8036
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 60.5680
                       Mean reward: 609.85
               Mean episode length: 181.14
    Episode_Reward/reaching_object: 1.1719
     Episode_Reward/lifting_object: 124.9316
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 2.12s
                      Time elapsed: 00:41:25
                               ETA: 00:35:27

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 46054 steps/s (collection: 2.043s, learning 0.092s)
             Mean action noise std: 2.52
          Mean value_function loss: 495.7729
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.5746
                       Mean reward: 486.49
               Mean episode length: 157.22
    Episode_Reward/reaching_object: 1.1019
     Episode_Reward/lifting_object: 116.4633
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 2.13s
                      Time elapsed: 00:41:27
                               ETA: 00:35:25

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 46304 steps/s (collection: 2.033s, learning 0.090s)
             Mean action noise std: 2.52
          Mean value_function loss: 518.6914
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 60.5884
                       Mean reward: 587.15
               Mean episode length: 177.59
    Episode_Reward/reaching_object: 1.1288
     Episode_Reward/lifting_object: 120.6382
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 2.12s
                      Time elapsed: 00:41:29
                               ETA: 00:35:22

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 46544 steps/s (collection: 2.018s, learning 0.095s)
             Mean action noise std: 2.52
          Mean value_function loss: 484.7867
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.6032
                       Mean reward: 557.87
               Mean episode length: 174.90
    Episode_Reward/reaching_object: 1.1285
     Episode_Reward/lifting_object: 120.1678
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 2.11s
                      Time elapsed: 00:41:31
                               ETA: 00:35:20

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 46337 steps/s (collection: 2.035s, learning 0.086s)
             Mean action noise std: 2.53
          Mean value_function loss: 466.8474
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 60.6209
                       Mean reward: 647.96
               Mean episode length: 198.05
    Episode_Reward/reaching_object: 1.1710
     Episode_Reward/lifting_object: 126.4543
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 2.12s
                      Time elapsed: 00:41:33
                               ETA: 00:35:17

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 46590 steps/s (collection: 2.016s, learning 0.094s)
             Mean action noise std: 2.53
          Mean value_function loss: 508.2929
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 60.6379
                       Mean reward: 600.50
               Mean episode length: 182.73
    Episode_Reward/reaching_object: 1.1646
     Episode_Reward/lifting_object: 124.8164
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 10.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 2.11s
                      Time elapsed: 00:41:35
                               ETA: 00:35:15

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 46057 steps/s (collection: 2.023s, learning 0.111s)
             Mean action noise std: 2.53
          Mean value_function loss: 498.1605
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 60.6522
                       Mean reward: 551.67
               Mean episode length: 170.36
    Episode_Reward/reaching_object: 1.0617
     Episode_Reward/lifting_object: 112.4679
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 10.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 2.13s
                      Time elapsed: 00:41:37
                               ETA: 00:35:12

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 45291 steps/s (collection: 2.079s, learning 0.091s)
             Mean action noise std: 2.53
          Mean value_function loss: 426.8139
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 60.6620
                       Mean reward: 569.47
               Mean episode length: 172.72
    Episode_Reward/reaching_object: 1.0869
     Episode_Reward/lifting_object: 116.5494
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 2.17s
                      Time elapsed: 00:41:39
                               ETA: 00:35:10

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 46729 steps/s (collection: 2.010s, learning 0.094s)
             Mean action noise std: 2.53
          Mean value_function loss: 456.8043
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 60.6729
                       Mean reward: 582.92
               Mean episode length: 178.97
    Episode_Reward/reaching_object: 1.1207
     Episode_Reward/lifting_object: 119.6843
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 2.10s
                      Time elapsed: 00:41:42
                               ETA: 00:35:08

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 44911 steps/s (collection: 2.088s, learning 0.101s)
             Mean action noise std: 2.53
          Mean value_function loss: 442.6980
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 60.6820
                       Mean reward: 616.06
               Mean episode length: 185.54
    Episode_Reward/reaching_object: 1.0970
     Episode_Reward/lifting_object: 116.4968
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 2.19s
                      Time elapsed: 00:41:44
                               ETA: 00:35:05

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 44841 steps/s (collection: 2.098s, learning 0.095s)
             Mean action noise std: 2.53
          Mean value_function loss: 470.6488
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 60.6855
                       Mean reward: 606.85
               Mean episode length: 184.23
    Episode_Reward/reaching_object: 1.1539
     Episode_Reward/lifting_object: 122.9017
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 2.19s
                      Time elapsed: 00:41:46
                               ETA: 00:35:03

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 45570 steps/s (collection: 2.066s, learning 0.091s)
             Mean action noise std: 2.53
          Mean value_function loss: 579.9431
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 60.6902
                       Mean reward: 559.09
               Mean episode length: 169.91
    Episode_Reward/reaching_object: 1.1336
     Episode_Reward/lifting_object: 121.8521
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 2.16s
                      Time elapsed: 00:41:48
                               ETA: 00:35:00

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 46350 steps/s (collection: 2.027s, learning 0.094s)
             Mean action noise std: 2.53
          Mean value_function loss: 510.5982
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 60.6974
                       Mean reward: 605.14
               Mean episode length: 181.41
    Episode_Reward/reaching_object: 1.1398
     Episode_Reward/lifting_object: 122.2563
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 11.3750
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 2.12s
                      Time elapsed: 00:41:50
                               ETA: 00:34:58

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 46285 steps/s (collection: 2.037s, learning 0.087s)
             Mean action noise std: 2.53
          Mean value_function loss: 486.6272
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 60.7065
                       Mean reward: 611.08
               Mean episode length: 186.28
    Episode_Reward/reaching_object: 1.1096
     Episode_Reward/lifting_object: 117.8079
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 2.12s
                      Time elapsed: 00:41:52
                               ETA: 00:34:55

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 46377 steps/s (collection: 2.027s, learning 0.093s)
             Mean action noise std: 2.54
          Mean value_function loss: 527.8401
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 60.7135
                       Mean reward: 574.54
               Mean episode length: 175.51
    Episode_Reward/reaching_object: 1.1270
     Episode_Reward/lifting_object: 121.1473
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 2.12s
                      Time elapsed: 00:41:54
                               ETA: 00:34:53

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 46174 steps/s (collection: 2.040s, learning 0.089s)
             Mean action noise std: 2.54
          Mean value_function loss: 550.8557
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 60.7222
                       Mean reward: 619.03
               Mean episode length: 184.27
    Episode_Reward/reaching_object: 1.1079
     Episode_Reward/lifting_object: 118.3275
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 13.0833
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 2.13s
                      Time elapsed: 00:41:57
                               ETA: 00:34:51

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 45624 steps/s (collection: 2.064s, learning 0.090s)
             Mean action noise std: 2.54
          Mean value_function loss: 506.9441
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 60.7296
                       Mean reward: 579.98
               Mean episode length: 174.03
    Episode_Reward/reaching_object: 1.0631
     Episode_Reward/lifting_object: 113.2452
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.2917
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 2.15s
                      Time elapsed: 00:41:59
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 44866 steps/s (collection: 2.101s, learning 0.090s)
             Mean action noise std: 2.54
          Mean value_function loss: 518.5690
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 60.7366
                       Mean reward: 583.79
               Mean episode length: 179.63
    Episode_Reward/reaching_object: 1.0379
     Episode_Reward/lifting_object: 108.8476
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.3333
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 2.19s
                      Time elapsed: 00:42:01
                               ETA: 00:34:46

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 46438 steps/s (collection: 2.028s, learning 0.089s)
             Mean action noise std: 2.54
          Mean value_function loss: 499.6633
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 60.7426
                       Mean reward: 558.71
               Mean episode length: 173.64
    Episode_Reward/reaching_object: 1.0598
     Episode_Reward/lifting_object: 112.5162
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 2.12s
                      Time elapsed: 00:42:03
                               ETA: 00:34:43

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 44992 steps/s (collection: 2.073s, learning 0.112s)
             Mean action noise std: 2.54
          Mean value_function loss: 499.6890
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 60.7446
                       Mean reward: 548.74
               Mean episode length: 170.54
    Episode_Reward/reaching_object: 1.0578
     Episode_Reward/lifting_object: 112.3591
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 2.18s
                      Time elapsed: 00:42:05
                               ETA: 00:34:41

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 45944 steps/s (collection: 2.027s, learning 0.113s)
             Mean action noise std: 2.54
          Mean value_function loss: 435.6914
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 60.7474
                       Mean reward: 592.39
               Mean episode length: 183.20
    Episode_Reward/reaching_object: 1.0571
     Episode_Reward/lifting_object: 111.9869
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 2.14s
                      Time elapsed: 00:42:07
                               ETA: 00:34:38

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 45437 steps/s (collection: 2.058s, learning 0.105s)
             Mean action noise std: 2.54
          Mean value_function loss: 430.0509
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 60.7490
                       Mean reward: 549.26
               Mean episode length: 174.43
    Episode_Reward/reaching_object: 1.1167
     Episode_Reward/lifting_object: 118.9542
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 2.16s
                      Time elapsed: 00:42:10
                               ETA: 00:34:36

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 46295 steps/s (collection: 2.024s, learning 0.100s)
             Mean action noise std: 2.54
          Mean value_function loss: 479.0217
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 60.7506
                       Mean reward: 589.07
               Mean episode length: 181.56
    Episode_Reward/reaching_object: 1.1119
     Episode_Reward/lifting_object: 118.4155
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 2.12s
                      Time elapsed: 00:42:12
                               ETA: 00:34:34

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 45988 steps/s (collection: 2.049s, learning 0.089s)
             Mean action noise std: 2.54
          Mean value_function loss: 422.1632
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 60.7512
                       Mean reward: 656.78
               Mean episode length: 196.56
    Episode_Reward/reaching_object: 1.1580
     Episode_Reward/lifting_object: 123.1232
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 2.14s
                      Time elapsed: 00:42:14
                               ETA: 00:34:31

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 45573 steps/s (collection: 2.063s, learning 0.094s)
             Mean action noise std: 2.54
          Mean value_function loss: 452.4296
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 60.7520
                       Mean reward: 604.73
               Mean episode length: 187.94
    Episode_Reward/reaching_object: 1.1520
     Episode_Reward/lifting_object: 123.2845
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 2.16s
                      Time elapsed: 00:42:16
                               ETA: 00:34:29

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 46788 steps/s (collection: 2.012s, learning 0.089s)
             Mean action noise std: 2.54
          Mean value_function loss: 440.2608
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 60.7532
                       Mean reward: 611.88
               Mean episode length: 185.80
    Episode_Reward/reaching_object: 1.1731
     Episode_Reward/lifting_object: 126.7963
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 2.10s
                      Time elapsed: 00:42:18
                               ETA: 00:34:26

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 46458 steps/s (collection: 2.025s, learning 0.091s)
             Mean action noise std: 2.54
          Mean value_function loss: 435.4263
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 60.7545
                       Mean reward: 618.94
               Mean episode length: 184.13
    Episode_Reward/reaching_object: 1.1464
     Episode_Reward/lifting_object: 123.2411
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 2.12s
                      Time elapsed: 00:42:20
                               ETA: 00:34:24

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 46768 steps/s (collection: 2.009s, learning 0.093s)
             Mean action noise std: 2.54
          Mean value_function loss: 442.9674
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 60.7556
                       Mean reward: 539.35
               Mean episode length: 173.16
    Episode_Reward/reaching_object: 1.1108
     Episode_Reward/lifting_object: 118.2584
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 2.10s
                      Time elapsed: 00:42:22
                               ETA: 00:34:21

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 45889 steps/s (collection: 2.048s, learning 0.095s)
             Mean action noise std: 2.54
          Mean value_function loss: 442.2843
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 60.7573
                       Mean reward: 663.99
               Mean episode length: 193.60
    Episode_Reward/reaching_object: 1.1782
     Episode_Reward/lifting_object: 127.3867
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 2.14s
                      Time elapsed: 00:42:24
                               ETA: 00:34:19

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 46585 steps/s (collection: 2.018s, learning 0.093s)
             Mean action noise std: 2.54
          Mean value_function loss: 468.1169
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 60.7585
                       Mean reward: 568.19
               Mean episode length: 177.93
    Episode_Reward/reaching_object: 1.1380
     Episode_Reward/lifting_object: 122.2195
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 2.11s
                      Time elapsed: 00:42:27
                               ETA: 00:34:16

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 46499 steps/s (collection: 2.022s, learning 0.093s)
             Mean action noise std: 2.54
          Mean value_function loss: 425.3711
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 60.7596
                       Mean reward: 603.66
               Mean episode length: 180.11
    Episode_Reward/reaching_object: 1.1332
     Episode_Reward/lifting_object: 122.4755
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 2.11s
                      Time elapsed: 00:42:29
                               ETA: 00:34:14

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 43055 steps/s (collection: 2.187s, learning 0.097s)
             Mean action noise std: 2.54
          Mean value_function loss: 480.3194
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 60.7604
                       Mean reward: 541.00
               Mean episode length: 167.73
    Episode_Reward/reaching_object: 1.1048
     Episode_Reward/lifting_object: 118.5014
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 2.28s
                      Time elapsed: 00:42:31
                               ETA: 00:34:12

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 45320 steps/s (collection: 2.076s, learning 0.094s)
             Mean action noise std: 2.54
          Mean value_function loss: 519.4896
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 60.7618
                       Mean reward: 635.76
               Mean episode length: 189.48
    Episode_Reward/reaching_object: 1.1292
     Episode_Reward/lifting_object: 122.3277
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.2500
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 2.17s
                      Time elapsed: 00:42:33
                               ETA: 00:34:09

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 44175 steps/s (collection: 2.116s, learning 0.110s)
             Mean action noise std: 2.54
          Mean value_function loss: 501.6987
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 60.7629
                       Mean reward: 579.64
               Mean episode length: 172.51
    Episode_Reward/reaching_object: 1.0781
     Episode_Reward/lifting_object: 115.3540
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 12.0417
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 2.23s
                      Time elapsed: 00:42:35
                               ETA: 00:34:07

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 44568 steps/s (collection: 2.086s, learning 0.120s)
             Mean action noise std: 2.54
          Mean value_function loss: 473.3038
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 60.7643
                       Mean reward: 586.40
               Mean episode length: 176.32
    Episode_Reward/reaching_object: 1.0950
     Episode_Reward/lifting_object: 118.2843
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.6250
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 2.21s
                      Time elapsed: 00:42:38
                               ETA: 00:34:05

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 46172 steps/s (collection: 2.037s, learning 0.092s)
             Mean action noise std: 2.54
          Mean value_function loss: 481.1963
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 60.7659
                       Mean reward: 619.54
               Mean episode length: 186.01
    Episode_Reward/reaching_object: 1.0617
     Episode_Reward/lifting_object: 113.7390
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 2.13s
                      Time elapsed: 00:42:40
                               ETA: 00:34:02

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 46258 steps/s (collection: 2.028s, learning 0.097s)
             Mean action noise std: 2.54
          Mean value_function loss: 480.8773
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 60.7717
                       Mean reward: 613.62
               Mean episode length: 183.01
    Episode_Reward/reaching_object: 1.1487
     Episode_Reward/lifting_object: 123.9054
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.6250
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 2.13s
                      Time elapsed: 00:42:42
                               ETA: 00:34:00

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 46491 steps/s (collection: 2.025s, learning 0.090s)
             Mean action noise std: 2.54
          Mean value_function loss: 460.3739
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 60.7813
                       Mean reward: 637.96
               Mean episode length: 186.94
    Episode_Reward/reaching_object: 1.1620
     Episode_Reward/lifting_object: 126.0205
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 2.11s
                      Time elapsed: 00:42:44
                               ETA: 00:33:57

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 46290 steps/s (collection: 2.033s, learning 0.091s)
             Mean action noise std: 2.54
          Mean value_function loss: 419.3881
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 60.7890
                       Mean reward: 593.44
               Mean episode length: 180.66
    Episode_Reward/reaching_object: 1.1202
     Episode_Reward/lifting_object: 121.7001
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 2.12s
                      Time elapsed: 00:42:46
                               ETA: 00:33:55

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 45752 steps/s (collection: 2.057s, learning 0.092s)
             Mean action noise std: 2.54
          Mean value_function loss: 485.1463
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 60.7934
                       Mean reward: 568.66
               Mean episode length: 172.92
    Episode_Reward/reaching_object: 1.1105
     Episode_Reward/lifting_object: 119.4713
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 2.15s
                      Time elapsed: 00:42:48
                               ETA: 00:33:52

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 46202 steps/s (collection: 2.028s, learning 0.100s)
             Mean action noise std: 2.55
          Mean value_function loss: 481.2894
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 60.7957
                       Mean reward: 546.38
               Mean episode length: 165.40
    Episode_Reward/reaching_object: 1.1475
     Episode_Reward/lifting_object: 124.4471
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 2.13s
                      Time elapsed: 00:42:50
                               ETA: 00:33:50

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 46290 steps/s (collection: 2.011s, learning 0.113s)
             Mean action noise std: 2.55
          Mean value_function loss: 486.2981
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 60.8002
                       Mean reward: 565.60
               Mean episode length: 170.56
    Episode_Reward/reaching_object: 1.1011
     Episode_Reward/lifting_object: 117.9460
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 2.12s
                      Time elapsed: 00:42:52
                               ETA: 00:33:47

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 45285 steps/s (collection: 2.051s, learning 0.120s)
             Mean action noise std: 2.55
          Mean value_function loss: 483.0240
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 60.8082
                       Mean reward: 551.88
               Mean episode length: 173.27
    Episode_Reward/reaching_object: 1.1415
     Episode_Reward/lifting_object: 123.9005
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 2.17s
                      Time elapsed: 00:42:55
                               ETA: 00:33:45

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 45743 steps/s (collection: 2.025s, learning 0.124s)
             Mean action noise std: 2.55
          Mean value_function loss: 459.8354
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 60.8196
                       Mean reward: 634.60
               Mean episode length: 188.21
    Episode_Reward/reaching_object: 1.1037
     Episode_Reward/lifting_object: 119.8841
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 2.15s
                      Time elapsed: 00:42:57
                               ETA: 00:33:43

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 45945 steps/s (collection: 2.047s, learning 0.093s)
             Mean action noise std: 2.55
          Mean value_function loss: 553.2356
               Mean surrogate loss: 0.0103
                 Mean entropy loss: 60.8316
                       Mean reward: 583.59
               Mean episode length: 174.17
    Episode_Reward/reaching_object: 1.1347
     Episode_Reward/lifting_object: 123.2419
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 2.14s
                      Time elapsed: 00:42:59
                               ETA: 00:33:40

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 45771 steps/s (collection: 2.049s, learning 0.099s)
             Mean action noise std: 2.55
          Mean value_function loss: 563.7952
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 60.8337
                       Mean reward: 613.49
               Mean episode length: 181.85
    Episode_Reward/reaching_object: 1.0674
     Episode_Reward/lifting_object: 115.1083
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 2.15s
                      Time elapsed: 00:43:01
                               ETA: 00:33:38

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 45773 steps/s (collection: 2.056s, learning 0.092s)
             Mean action noise std: 2.55
          Mean value_function loss: 470.2730
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 60.8404
                       Mean reward: 569.23
               Mean episode length: 172.66
    Episode_Reward/reaching_object: 1.0805
     Episode_Reward/lifting_object: 116.4953
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 2.15s
                      Time elapsed: 00:43:03
                               ETA: 00:33:35

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 44895 steps/s (collection: 2.086s, learning 0.103s)
             Mean action noise std: 2.55
          Mean value_function loss: 501.5821
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 60.8554
                       Mean reward: 615.39
               Mean episode length: 182.99
    Episode_Reward/reaching_object: 1.0679
     Episode_Reward/lifting_object: 114.8133
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 2.19s
                      Time elapsed: 00:43:05
                               ETA: 00:33:33

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 45723 steps/s (collection: 2.039s, learning 0.111s)
             Mean action noise std: 2.55
          Mean value_function loss: 501.9192
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 60.8714
                       Mean reward: 559.70
               Mean episode length: 170.27
    Episode_Reward/reaching_object: 1.0830
     Episode_Reward/lifting_object: 117.0608
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 2.15s
                      Time elapsed: 00:43:08
                               ETA: 00:33:31

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 45993 steps/s (collection: 2.045s, learning 0.093s)
             Mean action noise std: 2.56
          Mean value_function loss: 462.0618
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 60.8813
                       Mean reward: 646.00
               Mean episode length: 186.40
    Episode_Reward/reaching_object: 1.1018
     Episode_Reward/lifting_object: 118.9800
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 2.14s
                      Time elapsed: 00:43:10
                               ETA: 00:33:28

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 45826 steps/s (collection: 2.048s, learning 0.097s)
             Mean action noise std: 2.56
          Mean value_function loss: 438.3609
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 60.8980
                       Mean reward: 607.37
               Mean episode length: 185.14
    Episode_Reward/reaching_object: 1.1216
     Episode_Reward/lifting_object: 120.7198
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 2.15s
                      Time elapsed: 00:43:12
                               ETA: 00:33:26

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 46490 steps/s (collection: 2.012s, learning 0.102s)
             Mean action noise std: 2.56
          Mean value_function loss: 424.5719
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 60.9270
                       Mean reward: 596.11
               Mean episode length: 178.04
    Episode_Reward/reaching_object: 1.1389
     Episode_Reward/lifting_object: 123.0584
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 2.11s
                      Time elapsed: 00:43:14
                               ETA: 00:33:23

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 45911 steps/s (collection: 2.050s, learning 0.092s)
             Mean action noise std: 2.56
          Mean value_function loss: 392.6120
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 60.9448
                       Mean reward: 721.94
               Mean episode length: 205.66
    Episode_Reward/reaching_object: 1.1826
     Episode_Reward/lifting_object: 128.5837
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 2.14s
                      Time elapsed: 00:43:16
                               ETA: 00:33:21

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 46728 steps/s (collection: 2.009s, learning 0.095s)
             Mean action noise std: 2.56
          Mean value_function loss: 434.5456
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 60.9544
                       Mean reward: 596.13
               Mean episode length: 179.33
    Episode_Reward/reaching_object: 1.2104
     Episode_Reward/lifting_object: 132.0060
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 2.10s
                      Time elapsed: 00:43:18
                               ETA: 00:33:18

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 46614 steps/s (collection: 2.023s, learning 0.086s)
             Mean action noise std: 2.56
          Mean value_function loss: 455.5771
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 60.9658
                       Mean reward: 651.34
               Mean episode length: 193.18
    Episode_Reward/reaching_object: 1.1301
     Episode_Reward/lifting_object: 121.7733
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 2.11s
                      Time elapsed: 00:43:20
                               ETA: 00:33:16

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 46094 steps/s (collection: 2.040s, learning 0.093s)
             Mean action noise std: 2.56
          Mean value_function loss: 470.4831
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 60.9722
                       Mean reward: 624.67
               Mean episode length: 182.48
    Episode_Reward/reaching_object: 1.1573
     Episode_Reward/lifting_object: 125.3476
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 2.13s
                      Time elapsed: 00:43:22
                               ETA: 00:33:14

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 46063 steps/s (collection: 2.031s, learning 0.104s)
             Mean action noise std: 2.57
          Mean value_function loss: 431.6825
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 60.9740
                       Mean reward: 579.09
               Mean episode length: 174.65
    Episode_Reward/reaching_object: 1.1418
     Episode_Reward/lifting_object: 124.6973
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 2.13s
                      Time elapsed: 00:43:25
                               ETA: 00:33:11

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 46260 steps/s (collection: 2.031s, learning 0.094s)
             Mean action noise std: 2.57
          Mean value_function loss: 396.4088
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 60.9808
                       Mean reward: 603.96
               Mean episode length: 181.35
    Episode_Reward/reaching_object: 1.1355
     Episode_Reward/lifting_object: 122.1379
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 9.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 2.13s
                      Time elapsed: 00:43:27
                               ETA: 00:33:09

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 46441 steps/s (collection: 2.029s, learning 0.088s)
             Mean action noise std: 2.57
          Mean value_function loss: 389.0033
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 60.9873
                       Mean reward: 607.42
               Mean episode length: 181.79
    Episode_Reward/reaching_object: 1.1447
     Episode_Reward/lifting_object: 124.3950
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 2.12s
                      Time elapsed: 00:43:29
                               ETA: 00:33:06

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 45293 steps/s (collection: 2.083s, learning 0.088s)
             Mean action noise std: 2.57
          Mean value_function loss: 400.1364
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 61.0003
                       Mean reward: 650.21
               Mean episode length: 192.84
    Episode_Reward/reaching_object: 1.1986
     Episode_Reward/lifting_object: 131.0759
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 2.17s
                      Time elapsed: 00:43:31
                               ETA: 00:33:04

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 46385 steps/s (collection: 2.033s, learning 0.087s)
             Mean action noise std: 2.57
          Mean value_function loss: 454.8196
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 61.0171
                       Mean reward: 622.08
               Mean episode length: 181.92
    Episode_Reward/reaching_object: 1.2230
     Episode_Reward/lifting_object: 133.2111
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 2.12s
                      Time elapsed: 00:43:33
                               ETA: 00:33:01

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 45171 steps/s (collection: 2.068s, learning 0.108s)
             Mean action noise std: 2.57
          Mean value_function loss: 446.1389
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 61.0257
                       Mean reward: 642.19
               Mean episode length: 188.61
    Episode_Reward/reaching_object: 1.2063
     Episode_Reward/lifting_object: 132.1453
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.3333
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 2.18s
                      Time elapsed: 00:43:35
                               ETA: 00:32:59

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 45452 steps/s (collection: 2.074s, learning 0.089s)
             Mean action noise std: 2.57
          Mean value_function loss: 429.6862
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 61.0293
                       Mean reward: 672.04
               Mean episode length: 196.58
    Episode_Reward/reaching_object: 1.1839
     Episode_Reward/lifting_object: 129.1333
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 2.16s
                      Time elapsed: 00:43:37
                               ETA: 00:32:57

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 46432 steps/s (collection: 2.026s, learning 0.091s)
             Mean action noise std: 2.57
          Mean value_function loss: 434.8077
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 61.0356
                       Mean reward: 597.88
               Mean episode length: 181.19
    Episode_Reward/reaching_object: 1.1539
     Episode_Reward/lifting_object: 124.3039
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 2.12s
                      Time elapsed: 00:43:40
                               ETA: 00:32:54

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 46157 steps/s (collection: 2.040s, learning 0.090s)
             Mean action noise std: 2.57
          Mean value_function loss: 404.2831
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 61.0399
                       Mean reward: 596.18
               Mean episode length: 178.91
    Episode_Reward/reaching_object: 1.1510
     Episode_Reward/lifting_object: 124.5610
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 2.13s
                      Time elapsed: 00:43:42
                               ETA: 00:32:52

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 46688 steps/s (collection: 2.016s, learning 0.090s)
             Mean action noise std: 2.57
          Mean value_function loss: 471.7900
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 61.0430
                       Mean reward: 647.02
               Mean episode length: 190.12
    Episode_Reward/reaching_object: 1.1823
     Episode_Reward/lifting_object: 128.5336
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 2.11s
                      Time elapsed: 00:43:44
                               ETA: 00:32:49

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 45974 steps/s (collection: 2.040s, learning 0.099s)
             Mean action noise std: 2.57
          Mean value_function loss: 460.4332
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 61.0518
                       Mean reward: 633.66
               Mean episode length: 187.87
    Episode_Reward/reaching_object: 1.1852
     Episode_Reward/lifting_object: 128.4642
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 2.14s
                      Time elapsed: 00:43:46
                               ETA: 00:32:47

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 44893 steps/s (collection: 2.100s, learning 0.090s)
             Mean action noise std: 2.58
          Mean value_function loss: 419.2427
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 61.0682
                       Mean reward: 643.40
               Mean episode length: 188.70
    Episode_Reward/reaching_object: 1.1332
     Episode_Reward/lifting_object: 121.8243
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 10.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 2.19s
                      Time elapsed: 00:43:48
                               ETA: 00:32:45

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 46999 steps/s (collection: 1.996s, learning 0.096s)
             Mean action noise std: 2.58
          Mean value_function loss: 436.6356
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 61.0818
                       Mean reward: 618.86
               Mean episode length: 186.61
    Episode_Reward/reaching_object: 1.1149
     Episode_Reward/lifting_object: 119.6493
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.4167
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 2.09s
                      Time elapsed: 00:43:50
                               ETA: 00:32:42

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 46753 steps/s (collection: 2.010s, learning 0.093s)
             Mean action noise std: 2.58
          Mean value_function loss: 435.3475
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 61.0967
                       Mean reward: 637.77
               Mean episode length: 188.93
    Episode_Reward/reaching_object: 1.1784
     Episode_Reward/lifting_object: 127.4745
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 2.10s
                      Time elapsed: 00:43:52
                               ETA: 00:32:40

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 46449 steps/s (collection: 2.030s, learning 0.086s)
             Mean action noise std: 2.58
          Mean value_function loss: 439.3240
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 61.1153
                       Mean reward: 680.65
               Mean episode length: 195.80
    Episode_Reward/reaching_object: 1.2133
     Episode_Reward/lifting_object: 131.5059
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 2.12s
                      Time elapsed: 00:43:54
                               ETA: 00:32:37

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 46743 steps/s (collection: 2.003s, learning 0.100s)
             Mean action noise std: 2.58
          Mean value_function loss: 450.4402
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.1368
                       Mean reward: 578.59
               Mean episode length: 176.46
    Episode_Reward/reaching_object: 1.1735
     Episode_Reward/lifting_object: 125.9079
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 2.10s
                      Time elapsed: 00:43:56
                               ETA: 00:32:35

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 45837 steps/s (collection: 2.053s, learning 0.092s)
             Mean action noise std: 2.59
          Mean value_function loss: 471.5100
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 61.1535
                       Mean reward: 647.26
               Mean episode length: 189.47
    Episode_Reward/reaching_object: 1.1703
     Episode_Reward/lifting_object: 125.9645
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 2.14s
                      Time elapsed: 00:43:59
                               ETA: 00:32:32

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 44847 steps/s (collection: 2.094s, learning 0.098s)
             Mean action noise std: 2.59
          Mean value_function loss: 440.6988
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 61.1675
                       Mean reward: 631.49
               Mean episode length: 187.79
    Episode_Reward/reaching_object: 1.1795
     Episode_Reward/lifting_object: 127.6859
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 2.19s
                      Time elapsed: 00:44:01
                               ETA: 00:32:30

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 44916 steps/s (collection: 2.099s, learning 0.090s)
             Mean action noise std: 2.59
          Mean value_function loss: 449.5157
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 61.1782
                       Mean reward: 671.14
               Mean episode length: 198.89
    Episode_Reward/reaching_object: 1.1582
     Episode_Reward/lifting_object: 124.3501
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.5417
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 2.19s
                      Time elapsed: 00:44:03
                               ETA: 00:32:28

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 46063 steps/s (collection: 2.044s, learning 0.090s)
             Mean action noise std: 2.59
          Mean value_function loss: 477.6445
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.1832
                       Mean reward: 628.99
               Mean episode length: 189.24
    Episode_Reward/reaching_object: 1.1895
     Episode_Reward/lifting_object: 128.7458
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 2.13s
                      Time elapsed: 00:44:05
                               ETA: 00:32:25

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 45595 steps/s (collection: 2.056s, learning 0.100s)
             Mean action noise std: 2.59
          Mean value_function loss: 422.3723
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 61.2016
                       Mean reward: 610.68
               Mean episode length: 184.01
    Episode_Reward/reaching_object: 1.1560
     Episode_Reward/lifting_object: 125.0043
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 2.16s
                      Time elapsed: 00:44:07
                               ETA: 00:32:23

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 45817 steps/s (collection: 2.046s, learning 0.100s)
             Mean action noise std: 2.59
          Mean value_function loss: 407.4113
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 61.2158
                       Mean reward: 623.68
               Mean episode length: 184.85
    Episode_Reward/reaching_object: 1.2095
     Episode_Reward/lifting_object: 131.1547
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 2.15s
                      Time elapsed: 00:44:09
                               ETA: 00:32:21

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 46564 steps/s (collection: 2.015s, learning 0.097s)
             Mean action noise std: 2.59
          Mean value_function loss: 385.7852
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 61.2240
                       Mean reward: 644.71
               Mean episode length: 197.34
    Episode_Reward/reaching_object: 1.1540
     Episode_Reward/lifting_object: 124.4546
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 2.11s
                      Time elapsed: 00:44:12
                               ETA: 00:32:18

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 46260 steps/s (collection: 2.036s, learning 0.089s)
             Mean action noise std: 2.59
          Mean value_function loss: 410.4770
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 61.2333
                       Mean reward: 700.62
               Mean episode length: 203.63
    Episode_Reward/reaching_object: 1.1733
     Episode_Reward/lifting_object: 127.2926
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 2.12s
                      Time elapsed: 00:44:14
                               ETA: 00:32:16

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 46454 steps/s (collection: 2.028s, learning 0.089s)
             Mean action noise std: 2.60
          Mean value_function loss: 433.8693
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 61.2415
                       Mean reward: 731.18
               Mean episode length: 206.12
    Episode_Reward/reaching_object: 1.2434
     Episode_Reward/lifting_object: 136.4755
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 2.12s
                      Time elapsed: 00:44:16
                               ETA: 00:32:13

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 46148 steps/s (collection: 2.037s, learning 0.093s)
             Mean action noise std: 2.60
          Mean value_function loss: 465.5514
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 61.2576
                       Mean reward: 630.16
               Mean episode length: 187.43
    Episode_Reward/reaching_object: 1.1671
     Episode_Reward/lifting_object: 126.4143
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.5417
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 2.13s
                      Time elapsed: 00:44:18
                               ETA: 00:32:11

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 46481 steps/s (collection: 2.027s, learning 0.088s)
             Mean action noise std: 2.60
          Mean value_function loss: 428.5879
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 61.2714
                       Mean reward: 658.10
               Mean episode length: 191.27
    Episode_Reward/reaching_object: 1.2041
     Episode_Reward/lifting_object: 130.7285
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 2.11s
                      Time elapsed: 00:44:20
                               ETA: 00:32:08

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 46042 steps/s (collection: 2.048s, learning 0.088s)
             Mean action noise std: 2.60
          Mean value_function loss: 401.3631
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 61.2836
                       Mean reward: 610.78
               Mean episode length: 184.52
    Episode_Reward/reaching_object: 1.1713
     Episode_Reward/lifting_object: 126.6738
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 2.14s
                      Time elapsed: 00:44:22
                               ETA: 00:32:06

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 46146 steps/s (collection: 2.037s, learning 0.094s)
             Mean action noise std: 2.60
          Mean value_function loss: 422.1540
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 61.2906
                       Mean reward: 685.33
               Mean episode length: 197.47
    Episode_Reward/reaching_object: 1.2262
     Episode_Reward/lifting_object: 134.2784
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 2.13s
                      Time elapsed: 00:44:24
                               ETA: 00:32:04

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 45668 steps/s (collection: 2.062s, learning 0.091s)
             Mean action noise std: 2.60
          Mean value_function loss: 453.0866
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 61.2987
                       Mean reward: 678.15
               Mean episode length: 196.85
    Episode_Reward/reaching_object: 1.1627
     Episode_Reward/lifting_object: 125.8772
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 2.15s
                      Time elapsed: 00:44:26
                               ETA: 00:32:01

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 46411 steps/s (collection: 2.032s, learning 0.086s)
             Mean action noise std: 2.60
          Mean value_function loss: 441.3731
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 61.3036
                       Mean reward: 611.32
               Mean episode length: 180.72
    Episode_Reward/reaching_object: 1.2084
     Episode_Reward/lifting_object: 132.2972
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 2.12s
                      Time elapsed: 00:44:29
                               ETA: 00:31:59

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 45530 steps/s (collection: 2.072s, learning 0.087s)
             Mean action noise std: 2.60
          Mean value_function loss: 418.8933
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 61.3098
                       Mean reward: 672.25
               Mean episode length: 199.36
    Episode_Reward/reaching_object: 1.2086
     Episode_Reward/lifting_object: 131.0674
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 2.16s
                      Time elapsed: 00:44:31
                               ETA: 00:31:56

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 45977 steps/s (collection: 2.049s, learning 0.089s)
             Mean action noise std: 2.60
          Mean value_function loss: 380.1980
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 61.3192
                       Mean reward: 686.32
               Mean episode length: 199.37
    Episode_Reward/reaching_object: 1.1894
     Episode_Reward/lifting_object: 128.4926
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 10.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 2.14s
                      Time elapsed: 00:44:33
                               ETA: 00:31:54

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 45200 steps/s (collection: 2.053s, learning 0.122s)
             Mean action noise std: 2.60
          Mean value_function loss: 373.1393
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 61.3225
                       Mean reward: 694.83
               Mean episode length: 198.48
    Episode_Reward/reaching_object: 1.1953
     Episode_Reward/lifting_object: 129.2415
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 2.17s
                      Time elapsed: 00:44:35
                               ETA: 00:31:52

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 44598 steps/s (collection: 2.095s, learning 0.110s)
             Mean action noise std: 2.60
          Mean value_function loss: 376.1471
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 61.3250
                       Mean reward: 715.70
               Mean episode length: 203.53
    Episode_Reward/reaching_object: 1.1985
     Episode_Reward/lifting_object: 129.9033
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 2.20s
                      Time elapsed: 00:44:37
                               ETA: 00:31:49

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 46546 steps/s (collection: 2.023s, learning 0.089s)
             Mean action noise std: 2.60
          Mean value_function loss: 430.2504
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 61.3273
                       Mean reward: 589.44
               Mean episode length: 178.00
    Episode_Reward/reaching_object: 1.2132
     Episode_Reward/lifting_object: 131.2366
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 2.11s
                      Time elapsed: 00:44:39
                               ETA: 00:31:47

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 46861 steps/s (collection: 2.009s, learning 0.089s)
             Mean action noise std: 2.61
          Mean value_function loss: 440.7208
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 61.3363
                       Mean reward: 578.36
               Mean episode length: 173.52
    Episode_Reward/reaching_object: 1.1783
     Episode_Reward/lifting_object: 127.6404
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 2.10s
                      Time elapsed: 00:44:41
                               ETA: 00:31:44

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 46611 steps/s (collection: 2.019s, learning 0.090s)
             Mean action noise std: 2.61
          Mean value_function loss: 412.5871
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 61.3481
                       Mean reward: 623.71
               Mean episode length: 185.60
    Episode_Reward/reaching_object: 1.2271
     Episode_Reward/lifting_object: 133.7638
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 2.11s
                      Time elapsed: 00:44:44
                               ETA: 00:31:42

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 46097 steps/s (collection: 2.031s, learning 0.101s)
             Mean action noise std: 2.61
          Mean value_function loss: 427.8144
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 61.3555
                       Mean reward: 629.58
               Mean episode length: 189.54
    Episode_Reward/reaching_object: 1.1919
     Episode_Reward/lifting_object: 129.2625
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 2.13s
                      Time elapsed: 00:44:46
                               ETA: 00:31:40

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 44889 steps/s (collection: 2.090s, learning 0.100s)
             Mean action noise std: 2.61
          Mean value_function loss: 471.6547
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 61.3635
                       Mean reward: 708.27
               Mean episode length: 201.51
    Episode_Reward/reaching_object: 1.2242
     Episode_Reward/lifting_object: 134.0363
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 2.19s
                      Time elapsed: 00:44:48
                               ETA: 00:31:37

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 44500 steps/s (collection: 2.106s, learning 0.103s)
             Mean action noise std: 2.61
          Mean value_function loss: 447.3341
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 61.3735
                       Mean reward: 677.34
               Mean episode length: 196.62
    Episode_Reward/reaching_object: 1.1952
     Episode_Reward/lifting_object: 129.5032
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 2.21s
                      Time elapsed: 00:44:50
                               ETA: 00:31:35

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 45265 steps/s (collection: 2.082s, learning 0.090s)
             Mean action noise std: 2.61
          Mean value_function loss: 437.1618
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 61.3838
                       Mean reward: 589.06
               Mean episode length: 176.97
    Episode_Reward/reaching_object: 1.1567
     Episode_Reward/lifting_object: 125.0586
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 2.17s
                      Time elapsed: 00:44:52
                               ETA: 00:31:32

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 46427 steps/s (collection: 2.024s, learning 0.094s)
             Mean action noise std: 2.61
          Mean value_function loss: 416.0181
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.3900
                       Mean reward: 596.46
               Mean episode length: 181.16
    Episode_Reward/reaching_object: 1.1743
     Episode_Reward/lifting_object: 126.8783
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 2.12s
                      Time elapsed: 00:44:54
                               ETA: 00:31:30

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 46091 steps/s (collection: 2.040s, learning 0.093s)
             Mean action noise std: 2.61
          Mean value_function loss: 439.8660
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 61.4061
                       Mean reward: 626.11
               Mean episode length: 186.53
    Episode_Reward/reaching_object: 1.1607
     Episode_Reward/lifting_object: 125.3953
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 2.13s
                      Time elapsed: 00:44:57
                               ETA: 00:31:28

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 45655 steps/s (collection: 2.064s, learning 0.089s)
             Mean action noise std: 2.62
          Mean value_function loss: 425.1512
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 61.4214
                       Mean reward: 676.74
               Mean episode length: 199.94
    Episode_Reward/reaching_object: 1.2012
     Episode_Reward/lifting_object: 129.8381
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 2.15s
                      Time elapsed: 00:44:59
                               ETA: 00:31:25

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 46009 steps/s (collection: 2.046s, learning 0.090s)
             Mean action noise std: 2.62
          Mean value_function loss: 430.9777
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 61.4325
                       Mean reward: 661.66
               Mean episode length: 197.30
    Episode_Reward/reaching_object: 1.2198
     Episode_Reward/lifting_object: 132.3148
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 2.14s
                      Time elapsed: 00:45:01
                               ETA: 00:31:23

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 46062 steps/s (collection: 2.043s, learning 0.091s)
             Mean action noise std: 2.62
          Mean value_function loss: 436.7231
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 61.4374
                       Mean reward: 662.21
               Mean episode length: 192.09
    Episode_Reward/reaching_object: 1.2106
     Episode_Reward/lifting_object: 131.7310
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 2.13s
                      Time elapsed: 00:45:03
                               ETA: 00:31:20

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 45658 steps/s (collection: 2.040s, learning 0.113s)
             Mean action noise std: 2.62
          Mean value_function loss: 384.4315
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 61.4541
                       Mean reward: 615.08
               Mean episode length: 183.01
    Episode_Reward/reaching_object: 1.2048
     Episode_Reward/lifting_object: 131.1687
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 2.15s
                      Time elapsed: 00:45:05
                               ETA: 00:31:18

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 46117 steps/s (collection: 2.020s, learning 0.112s)
             Mean action noise std: 2.62
          Mean value_function loss: 448.9263
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 61.4749
                       Mean reward: 646.09
               Mean episode length: 188.77
    Episode_Reward/reaching_object: 1.1782
     Episode_Reward/lifting_object: 127.2598
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 2.13s
                      Time elapsed: 00:45:07
                               ETA: 00:31:16

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 46643 steps/s (collection: 2.021s, learning 0.086s)
             Mean action noise std: 2.62
          Mean value_function loss: 436.4717
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 61.4936
                       Mean reward: 677.46
               Mean episode length: 197.12
    Episode_Reward/reaching_object: 1.2236
     Episode_Reward/lifting_object: 133.8286
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 2.11s
                      Time elapsed: 00:45:09
                               ETA: 00:31:13

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 45893 steps/s (collection: 2.038s, learning 0.104s)
             Mean action noise std: 2.63
          Mean value_function loss: 469.3793
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 61.5152
                       Mean reward: 684.02
               Mean episode length: 197.00
    Episode_Reward/reaching_object: 1.2407
     Episode_Reward/lifting_object: 134.9837
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 2.14s
                      Time elapsed: 00:45:11
                               ETA: 00:31:11

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 42803 steps/s (collection: 2.197s, learning 0.100s)
             Mean action noise std: 2.63
          Mean value_function loss: 409.9805
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 61.5319
                       Mean reward: 579.93
               Mean episode length: 174.34
    Episode_Reward/reaching_object: 1.1369
     Episode_Reward/lifting_object: 122.7148
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 2.30s
                      Time elapsed: 00:45:14
                               ETA: 00:31:09

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 44794 steps/s (collection: 2.086s, learning 0.108s)
             Mean action noise std: 2.63
          Mean value_function loss: 440.9534
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 61.5429
                       Mean reward: 647.64
               Mean episode length: 193.66
    Episode_Reward/reaching_object: 1.2258
     Episode_Reward/lifting_object: 134.0901
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 2.19s
                      Time elapsed: 00:45:16
                               ETA: 00:31:06

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 46135 steps/s (collection: 2.041s, learning 0.090s)
             Mean action noise std: 2.63
          Mean value_function loss: 441.8167
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 61.5470
                       Mean reward: 653.33
               Mean episode length: 192.47
    Episode_Reward/reaching_object: 1.2089
     Episode_Reward/lifting_object: 130.7220
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 2.13s
                      Time elapsed: 00:45:18
                               ETA: 00:31:04

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 46028 steps/s (collection: 2.041s, learning 0.095s)
             Mean action noise std: 2.63
          Mean value_function loss: 381.0853
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 61.5509
                       Mean reward: 624.17
               Mean episode length: 184.96
    Episode_Reward/reaching_object: 1.1929
     Episode_Reward/lifting_object: 129.3907
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 2.14s
                      Time elapsed: 00:45:20
                               ETA: 00:31:01

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 46599 steps/s (collection: 2.016s, learning 0.093s)
             Mean action noise std: 2.63
          Mean value_function loss: 415.3838
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 61.5536
                       Mean reward: 641.47
               Mean episode length: 186.34
    Episode_Reward/reaching_object: 1.2017
     Episode_Reward/lifting_object: 131.5591
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 2.11s
                      Time elapsed: 00:45:22
                               ETA: 00:30:59

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 45816 steps/s (collection: 2.055s, learning 0.091s)
             Mean action noise std: 2.63
          Mean value_function loss: 476.5245
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 61.5571
                       Mean reward: 610.82
               Mean episode length: 181.87
    Episode_Reward/reaching_object: 1.2128
     Episode_Reward/lifting_object: 131.0216
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 2.15s
                      Time elapsed: 00:45:25
                               ETA: 00:30:57

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 45438 steps/s (collection: 2.075s, learning 0.089s)
             Mean action noise std: 2.63
          Mean value_function loss: 460.5231
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 61.5622
                       Mean reward: 614.14
               Mean episode length: 185.04
    Episode_Reward/reaching_object: 1.1821
     Episode_Reward/lifting_object: 127.6464
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 2.16s
                      Time elapsed: 00:45:27
                               ETA: 00:30:54

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 46185 steps/s (collection: 2.040s, learning 0.089s)
             Mean action noise std: 2.63
          Mean value_function loss: 479.8208
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 61.5658
                       Mean reward: 592.43
               Mean episode length: 178.05
    Episode_Reward/reaching_object: 1.2002
     Episode_Reward/lifting_object: 130.5418
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 2.13s
                      Time elapsed: 00:45:29
                               ETA: 00:30:52

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 45209 steps/s (collection: 2.081s, learning 0.093s)
             Mean action noise std: 2.63
          Mean value_function loss: 439.2704
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 61.5671
                       Mean reward: 647.56
               Mean episode length: 194.74
    Episode_Reward/reaching_object: 1.1952
     Episode_Reward/lifting_object: 129.8700
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 2.17s
                      Time elapsed: 00:45:31
                               ETA: 00:30:49

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 45873 steps/s (collection: 2.053s, learning 0.090s)
             Mean action noise std: 2.63
          Mean value_function loss: 445.6010
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 61.5688
                       Mean reward: 639.33
               Mean episode length: 190.54
    Episode_Reward/reaching_object: 1.1528
     Episode_Reward/lifting_object: 124.9503
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.6250
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 2.14s
                      Time elapsed: 00:45:33
                               ETA: 00:30:47

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 45989 steps/s (collection: 2.047s, learning 0.091s)
             Mean action noise std: 2.63
          Mean value_function loss: 446.7567
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 61.5733
                       Mean reward: 595.74
               Mean episode length: 181.56
    Episode_Reward/reaching_object: 1.1293
     Episode_Reward/lifting_object: 121.4079
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 2.14s
                      Time elapsed: 00:45:35
                               ETA: 00:30:45

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 44807 steps/s (collection: 2.079s, learning 0.115s)
             Mean action noise std: 2.63
          Mean value_function loss: 430.2565
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 61.5783
                       Mean reward: 666.40
               Mean episode length: 198.96
    Episode_Reward/reaching_object: 1.1800
     Episode_Reward/lifting_object: 128.4263
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 2.19s
                      Time elapsed: 00:45:37
                               ETA: 00:30:42

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 44674 steps/s (collection: 2.108s, learning 0.093s)
             Mean action noise std: 2.63
          Mean value_function loss: 446.4827
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 61.5846
                       Mean reward: 607.05
               Mean episode length: 184.58
    Episode_Reward/reaching_object: 1.1051
     Episode_Reward/lifting_object: 119.0777
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 9.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 2.20s
                      Time elapsed: 00:45:40
                               ETA: 00:30:40

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 46007 steps/s (collection: 2.046s, learning 0.091s)
             Mean action noise std: 2.64
          Mean value_function loss: 415.5264
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 61.5996
                       Mean reward: 648.35
               Mean episode length: 189.50
    Episode_Reward/reaching_object: 1.1425
     Episode_Reward/lifting_object: 123.3400
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 2.14s
                      Time elapsed: 00:45:42
                               ETA: 00:30:38

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 45132 steps/s (collection: 2.086s, learning 0.093s)
             Mean action noise std: 2.64
          Mean value_function loss: 405.5927
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 61.6140
                       Mean reward: 637.36
               Mean episode length: 186.96
    Episode_Reward/reaching_object: 1.1912
     Episode_Reward/lifting_object: 129.0362
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 2.18s
                      Time elapsed: 00:45:44
                               ETA: 00:30:35

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 45584 steps/s (collection: 2.053s, learning 0.103s)
             Mean action noise std: 2.64
          Mean value_function loss: 423.8913
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 61.6178
                       Mean reward: 733.67
               Mean episode length: 210.72
    Episode_Reward/reaching_object: 1.2358
     Episode_Reward/lifting_object: 134.9610
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 2.16s
                      Time elapsed: 00:45:46
                               ETA: 00:30:33

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 45744 steps/s (collection: 2.055s, learning 0.094s)
             Mean action noise std: 2.64
          Mean value_function loss: 452.3314
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 61.6283
                       Mean reward: 654.50
               Mean episode length: 194.50
    Episode_Reward/reaching_object: 1.1726
     Episode_Reward/lifting_object: 126.5779
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 2.15s
                      Time elapsed: 00:45:48
                               ETA: 00:30:30

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 45772 steps/s (collection: 2.052s, learning 0.096s)
             Mean action noise std: 2.64
          Mean value_function loss: 418.7057
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 61.6366
                       Mean reward: 672.85
               Mean episode length: 199.24
    Episode_Reward/reaching_object: 1.1708
     Episode_Reward/lifting_object: 126.6771
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 2.15s
                      Time elapsed: 00:45:50
                               ETA: 00:30:28

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 45759 steps/s (collection: 2.053s, learning 0.095s)
             Mean action noise std: 2.64
          Mean value_function loss: 450.8033
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 61.6398
                       Mean reward: 619.30
               Mean episode length: 187.79
    Episode_Reward/reaching_object: 1.1936
     Episode_Reward/lifting_object: 129.0486
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 2.15s
                      Time elapsed: 00:45:53
                               ETA: 00:30:26

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 45518 steps/s (collection: 2.070s, learning 0.090s)
             Mean action noise std: 2.64
          Mean value_function loss: 458.2644
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 61.6412
                       Mean reward: 623.67
               Mean episode length: 184.53
    Episode_Reward/reaching_object: 1.1576
     Episode_Reward/lifting_object: 124.8959
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 2.16s
                      Time elapsed: 00:45:55
                               ETA: 00:30:23

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 45886 steps/s (collection: 2.052s, learning 0.091s)
             Mean action noise std: 2.64
          Mean value_function loss: 487.4752
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 61.6452
                       Mean reward: 670.91
               Mean episode length: 191.50
    Episode_Reward/reaching_object: 1.2199
     Episode_Reward/lifting_object: 133.2932
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 2.14s
                      Time elapsed: 00:45:57
                               ETA: 00:30:21

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 44680 steps/s (collection: 2.108s, learning 0.093s)
             Mean action noise std: 2.64
          Mean value_function loss: 495.4042
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 61.6479
                       Mean reward: 647.71
               Mean episode length: 190.76
    Episode_Reward/reaching_object: 1.1645
     Episode_Reward/lifting_object: 125.8597
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 11.5417
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 2.20s
                      Time elapsed: 00:45:59
                               ETA: 00:30:19

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 45099 steps/s (collection: 2.082s, learning 0.098s)
             Mean action noise std: 2.64
          Mean value_function loss: 446.3700
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 61.6526
                       Mean reward: 617.07
               Mean episode length: 183.32
    Episode_Reward/reaching_object: 1.1316
     Episode_Reward/lifting_object: 121.5325
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 2.18s
                      Time elapsed: 00:46:01
                               ETA: 00:30:16

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 44317 steps/s (collection: 2.112s, learning 0.106s)
             Mean action noise std: 2.64
          Mean value_function loss: 472.0763
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 61.6577
                       Mean reward: 606.55
               Mean episode length: 181.52
    Episode_Reward/reaching_object: 1.1481
     Episode_Reward/lifting_object: 124.5134
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 2.22s
                      Time elapsed: 00:46:03
                               ETA: 00:30:14

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 44137 steps/s (collection: 2.127s, learning 0.101s)
             Mean action noise std: 2.64
          Mean value_function loss: 507.8975
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.6601
                       Mean reward: 623.90
               Mean episode length: 189.04
    Episode_Reward/reaching_object: 1.1033
     Episode_Reward/lifting_object: 118.1967
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 2.23s
                      Time elapsed: 00:46:06
                               ETA: 00:30:12

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 44817 steps/s (collection: 2.088s, learning 0.105s)
             Mean action noise std: 2.64
          Mean value_function loss: 474.5331
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.6652
                       Mean reward: 614.22
               Mean episode length: 183.35
    Episode_Reward/reaching_object: 1.0814
     Episode_Reward/lifting_object: 115.9325
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 11.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 12.5833
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 2.19s
                      Time elapsed: 00:46:08
                               ETA: 00:30:09

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 44307 steps/s (collection: 2.129s, learning 0.090s)
             Mean action noise std: 2.65
          Mean value_function loss: 444.6937
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 61.6702
                       Mean reward: 546.24
               Mean episode length: 175.06
    Episode_Reward/reaching_object: 1.0482
     Episode_Reward/lifting_object: 111.9512
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 10.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 2.22s
                      Time elapsed: 00:46:10
                               ETA: 00:30:07

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 45248 steps/s (collection: 2.078s, learning 0.095s)
             Mean action noise std: 2.65
          Mean value_function loss: 439.1273
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 61.6765
                       Mean reward: 634.76
               Mean episode length: 188.57
    Episode_Reward/reaching_object: 1.1678
     Episode_Reward/lifting_object: 126.7440
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 10.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 2.17s
                      Time elapsed: 00:46:12
                               ETA: 00:30:05

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 44532 steps/s (collection: 2.113s, learning 0.094s)
             Mean action noise std: 2.65
          Mean value_function loss: 413.6212
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 61.6852
                       Mean reward: 645.65
               Mean episode length: 189.03
    Episode_Reward/reaching_object: 1.1815
     Episode_Reward/lifting_object: 128.4025
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 2.21s
                      Time elapsed: 00:46:14
                               ETA: 00:30:02

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 45331 steps/s (collection: 2.076s, learning 0.093s)
             Mean action noise std: 2.65
          Mean value_function loss: 379.4780
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 61.6896
                       Mean reward: 643.43
               Mean episode length: 189.27
    Episode_Reward/reaching_object: 1.2107
     Episode_Reward/lifting_object: 132.1158
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 2.17s
                      Time elapsed: 00:46:17
                               ETA: 00:30:00

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 44414 steps/s (collection: 2.117s, learning 0.096s)
             Mean action noise std: 2.65
          Mean value_function loss: 427.4303
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 61.6946
                       Mean reward: 667.75
               Mean episode length: 196.08
    Episode_Reward/reaching_object: 1.1738
     Episode_Reward/lifting_object: 127.2254
      Episode_Reward/object_height: 0.0100
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 2.21s
                      Time elapsed: 00:46:19
                               ETA: 00:29:58

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 44791 steps/s (collection: 2.097s, learning 0.098s)
             Mean action noise std: 2.65
          Mean value_function loss: 434.3424
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 61.6985
                       Mean reward: 638.40
               Mean episode length: 189.79
    Episode_Reward/reaching_object: 1.1719
     Episode_Reward/lifting_object: 127.0251
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 2.19s
                      Time elapsed: 00:46:21
                               ETA: 00:29:55

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 44927 steps/s (collection: 2.080s, learning 0.109s)
             Mean action noise std: 2.65
          Mean value_function loss: 487.5678
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 61.7013
                       Mean reward: 627.95
               Mean episode length: 186.81
    Episode_Reward/reaching_object: 1.1711
     Episode_Reward/lifting_object: 126.6451
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 2.19s
                      Time elapsed: 00:46:23
                               ETA: 00:29:53

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 45152 steps/s (collection: 2.089s, learning 0.088s)
             Mean action noise std: 2.65
          Mean value_function loss: 427.2191
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 61.7040
                       Mean reward: 521.56
               Mean episode length: 165.53
    Episode_Reward/reaching_object: 1.0688
     Episode_Reward/lifting_object: 114.4110
      Episode_Reward/object_height: 0.0086
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 10.6667
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 2.18s
                      Time elapsed: 00:46:25
                               ETA: 00:29:50

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 45028 steps/s (collection: 2.090s, learning 0.093s)
             Mean action noise std: 2.65
          Mean value_function loss: 420.6093
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 61.7058
                       Mean reward: 594.83
               Mean episode length: 181.94
    Episode_Reward/reaching_object: 1.1616
     Episode_Reward/lifting_object: 125.6183
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 2.18s
                      Time elapsed: 00:46:28
                               ETA: 00:29:48

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 43814 steps/s (collection: 2.151s, learning 0.093s)
             Mean action noise std: 2.65
          Mean value_function loss: 452.6653
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 61.7069
                       Mean reward: 593.04
               Mean episode length: 180.66
    Episode_Reward/reaching_object: 1.1440
     Episode_Reward/lifting_object: 123.2719
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.5833
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 2.24s
                      Time elapsed: 00:46:30
                               ETA: 00:29:46

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 44948 steps/s (collection: 2.097s, learning 0.091s)
             Mean action noise std: 2.65
          Mean value_function loss: 400.0916
               Mean surrogate loss: 0.0044
                 Mean entropy loss: 61.7084
                       Mean reward: 671.22
               Mean episode length: 197.99
    Episode_Reward/reaching_object: 1.1947
     Episode_Reward/lifting_object: 129.7944
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 2.19s
                      Time elapsed: 00:46:32
                               ETA: 00:29:43

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 44602 steps/s (collection: 2.098s, learning 0.106s)
             Mean action noise std: 2.65
          Mean value_function loss: 389.3678
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 61.7100
                       Mean reward: 687.61
               Mean episode length: 200.84
    Episode_Reward/reaching_object: 1.2515
     Episode_Reward/lifting_object: 136.6761
      Episode_Reward/object_height: 0.0103
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 2.20s
                      Time elapsed: 00:46:34
                               ETA: 00:29:41

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 44928 steps/s (collection: 2.087s, learning 0.101s)
             Mean action noise std: 2.65
          Mean value_function loss: 371.4919
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 61.7120
                       Mean reward: 708.11
               Mean episode length: 204.31
    Episode_Reward/reaching_object: 1.1993
     Episode_Reward/lifting_object: 129.5473
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 2.19s
                      Time elapsed: 00:46:36
                               ETA: 00:29:39

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 44899 steps/s (collection: 2.082s, learning 0.108s)
             Mean action noise std: 2.65
          Mean value_function loss: 405.0979
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 61.7141
                       Mean reward: 606.17
               Mean episode length: 183.07
    Episode_Reward/reaching_object: 1.1721
     Episode_Reward/lifting_object: 126.3372
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 2.19s
                      Time elapsed: 00:46:39
                               ETA: 00:29:36

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 44029 steps/s (collection: 2.107s, learning 0.126s)
             Mean action noise std: 2.65
          Mean value_function loss: 346.9013
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 61.7165
                       Mean reward: 683.62
               Mean episode length: 203.59
    Episode_Reward/reaching_object: 1.2254
     Episode_Reward/lifting_object: 132.5614
      Episode_Reward/object_height: 0.0101
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 2.23s
                      Time elapsed: 00:46:41
                               ETA: 00:29:34

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 45675 steps/s (collection: 2.053s, learning 0.100s)
             Mean action noise std: 2.65
          Mean value_function loss: 393.4659
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 61.7193
                       Mean reward: 673.88
               Mean episode length: 197.98
    Episode_Reward/reaching_object: 1.2633
     Episode_Reward/lifting_object: 137.9166
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 2.15s
                      Time elapsed: 00:46:43
                               ETA: 00:29:32

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 44453 steps/s (collection: 2.109s, learning 0.103s)
             Mean action noise std: 2.65
          Mean value_function loss: 344.9039
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 61.7272
                       Mean reward: 747.61
               Mean episode length: 214.41
    Episode_Reward/reaching_object: 1.2597
     Episode_Reward/lifting_object: 136.0169
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 2.21s
                      Time elapsed: 00:46:45
                               ETA: 00:29:29

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 44758 steps/s (collection: 2.092s, learning 0.104s)
             Mean action noise std: 2.65
          Mean value_function loss: 345.1346
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 61.7358
                       Mean reward: 779.17
               Mean episode length: 223.62
    Episode_Reward/reaching_object: 1.3095
     Episode_Reward/lifting_object: 142.2844
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 2.20s
                      Time elapsed: 00:46:47
                               ETA: 00:29:27

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 44265 steps/s (collection: 2.112s, learning 0.109s)
             Mean action noise std: 2.65
          Mean value_function loss: 386.1609
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 61.7403
                       Mean reward: 711.74
               Mean episode length: 208.00
    Episode_Reward/reaching_object: 1.2659
     Episode_Reward/lifting_object: 136.7919
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 2.22s
                      Time elapsed: 00:46:50
                               ETA: 00:29:25

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 44750 steps/s (collection: 2.108s, learning 0.089s)
             Mean action noise std: 2.66
          Mean value_function loss: 377.5229
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 61.7532
                       Mean reward: 739.01
               Mean episode length: 212.79
    Episode_Reward/reaching_object: 1.2988
     Episode_Reward/lifting_object: 140.6284
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 2.20s
                      Time elapsed: 00:46:52
                               ETA: 00:29:22

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 44105 steps/s (collection: 2.119s, learning 0.110s)
             Mean action noise std: 2.66
          Mean value_function loss: 481.4139
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 61.7656
                       Mean reward: 661.87
               Mean episode length: 191.28
    Episode_Reward/reaching_object: 1.2726
     Episode_Reward/lifting_object: 137.4647
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 2.23s
                      Time elapsed: 00:46:54
                               ETA: 00:29:20

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 43873 steps/s (collection: 2.140s, learning 0.101s)
             Mean action noise std: 2.66
          Mean value_function loss: 357.3786
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 61.7715
                       Mean reward: 740.58
               Mean episode length: 212.63
    Episode_Reward/reaching_object: 1.2993
     Episode_Reward/lifting_object: 139.9697
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 2.24s
                      Time elapsed: 00:46:56
                               ETA: 00:29:18

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 44702 steps/s (collection: 2.099s, learning 0.100s)
             Mean action noise std: 2.66
          Mean value_function loss: 374.7817
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 61.7838
                       Mean reward: 633.94
               Mean episode length: 186.98
    Episode_Reward/reaching_object: 1.2420
     Episode_Reward/lifting_object: 133.5508
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.7917
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 2.20s
                      Time elapsed: 00:46:58
                               ETA: 00:29:15

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 44359 steps/s (collection: 2.117s, learning 0.099s)
             Mean action noise std: 2.66
          Mean value_function loss: 360.9448
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 61.7989
                       Mean reward: 736.97
               Mean episode length: 206.70
    Episode_Reward/reaching_object: 1.2374
     Episode_Reward/lifting_object: 133.3067
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 2.22s
                      Time elapsed: 00:47:01
                               ETA: 00:29:13

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 44570 steps/s (collection: 2.116s, learning 0.090s)
             Mean action noise std: 2.66
          Mean value_function loss: 344.3291
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 61.8157
                       Mean reward: 700.46
               Mean episode length: 202.93
    Episode_Reward/reaching_object: 1.2947
     Episode_Reward/lifting_object: 140.2285
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 2.21s
                      Time elapsed: 00:47:03
                               ETA: 00:29:11

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 45600 steps/s (collection: 2.062s, learning 0.094s)
             Mean action noise std: 2.66
          Mean value_function loss: 339.0916
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 61.8259
                       Mean reward: 711.15
               Mean episode length: 203.39
    Episode_Reward/reaching_object: 1.2983
     Episode_Reward/lifting_object: 140.7184
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 2.16s
                      Time elapsed: 00:47:05
                               ETA: 00:29:08

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 44919 steps/s (collection: 2.092s, learning 0.096s)
             Mean action noise std: 2.67
          Mean value_function loss: 366.8662
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 61.8423
                       Mean reward: 702.49
               Mean episode length: 205.78
    Episode_Reward/reaching_object: 1.2761
     Episode_Reward/lifting_object: 136.9545
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 2.19s
                      Time elapsed: 00:47:07
                               ETA: 00:29:06

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 44024 steps/s (collection: 2.099s, learning 0.134s)
             Mean action noise std: 2.67
          Mean value_function loss: 374.7646
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 61.8650
                       Mean reward: 681.84
               Mean episode length: 197.93
    Episode_Reward/reaching_object: 1.2912
     Episode_Reward/lifting_object: 139.4351
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 2.23s
                      Time elapsed: 00:47:09
                               ETA: 00:29:04

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 44590 steps/s (collection: 2.107s, learning 0.098s)
             Mean action noise std: 2.67
          Mean value_function loss: 357.3903
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 61.8777
                       Mean reward: 724.45
               Mean episode length: 210.99
    Episode_Reward/reaching_object: 1.2928
     Episode_Reward/lifting_object: 138.8097
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 2.20s
                      Time elapsed: 00:47:12
                               ETA: 00:29:01

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 45069 steps/s (collection: 2.084s, learning 0.097s)
             Mean action noise std: 2.67
          Mean value_function loss: 375.5996
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.8931
                       Mean reward: 722.99
               Mean episode length: 209.64
    Episode_Reward/reaching_object: 1.2867
     Episode_Reward/lifting_object: 138.4632
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 2.18s
                      Time elapsed: 00:47:14
                               ETA: 00:28:59

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 44963 steps/s (collection: 2.090s, learning 0.096s)
             Mean action noise std: 2.67
          Mean value_function loss: 323.3464
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.9076
                       Mean reward: 654.02
               Mean episode length: 192.01
    Episode_Reward/reaching_object: 1.3136
     Episode_Reward/lifting_object: 142.1815
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 2.19s
                      Time elapsed: 00:47:16
                               ETA: 00:28:57

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 45415 steps/s (collection: 2.068s, learning 0.097s)
             Mean action noise std: 2.68
          Mean value_function loss: 392.9594
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.9222
                       Mean reward: 660.95
               Mean episode length: 194.51
    Episode_Reward/reaching_object: 1.2882
     Episode_Reward/lifting_object: 138.9967
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 2.16s
                      Time elapsed: 00:47:18
                               ETA: 00:28:54

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 44998 steps/s (collection: 2.067s, learning 0.118s)
             Mean action noise std: 2.68
          Mean value_function loss: 362.2057
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 61.9361
                       Mean reward: 628.69
               Mean episode length: 185.28
    Episode_Reward/reaching_object: 1.2617
     Episode_Reward/lifting_object: 135.6366
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 2.18s
                      Time elapsed: 00:47:20
                               ETA: 00:28:52

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 44300 steps/s (collection: 2.099s, learning 0.120s)
             Mean action noise std: 2.68
          Mean value_function loss: 334.1090
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 61.9441
                       Mean reward: 743.40
               Mean episode length: 210.03
    Episode_Reward/reaching_object: 1.2955
     Episode_Reward/lifting_object: 139.2815
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 2.22s
                      Time elapsed: 00:47:23
                               ETA: 00:28:50

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 44862 steps/s (collection: 2.087s, learning 0.104s)
             Mean action noise std: 2.68
          Mean value_function loss: 362.0484
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 61.9529
                       Mean reward: 646.13
               Mean episode length: 191.89
    Episode_Reward/reaching_object: 1.2559
     Episode_Reward/lifting_object: 134.5831
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 2.19s
                      Time elapsed: 00:47:25
                               ETA: 00:28:47

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 44702 steps/s (collection: 2.100s, learning 0.100s)
             Mean action noise std: 2.68
          Mean value_function loss: 326.9160
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 61.9675
                       Mean reward: 734.28
               Mean episode length: 210.18
    Episode_Reward/reaching_object: 1.2769
     Episode_Reward/lifting_object: 137.0058
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 2.20s
                      Time elapsed: 00:47:27
                               ETA: 00:28:45

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 45403 steps/s (collection: 2.067s, learning 0.098s)
             Mean action noise std: 2.68
          Mean value_function loss: 353.6926
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 61.9852
                       Mean reward: 724.97
               Mean episode length: 208.83
    Episode_Reward/reaching_object: 1.2932
     Episode_Reward/lifting_object: 140.0105
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 2.17s
                      Time elapsed: 00:47:29
                               ETA: 00:28:43

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 43959 steps/s (collection: 2.137s, learning 0.100s)
             Mean action noise std: 2.68
          Mean value_function loss: 358.1317
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 61.9967
                       Mean reward: 700.42
               Mean episode length: 205.23
    Episode_Reward/reaching_object: 1.3676
     Episode_Reward/lifting_object: 148.8602
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 2.24s
                      Time elapsed: 00:47:31
                               ETA: 00:28:40

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 43149 steps/s (collection: 2.146s, learning 0.132s)
             Mean action noise std: 2.69
          Mean value_function loss: 358.3666
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.0038
                       Mean reward: 672.23
               Mean episode length: 200.56
    Episode_Reward/reaching_object: 1.2641
     Episode_Reward/lifting_object: 135.8366
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 11.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 2.28s
                      Time elapsed: 00:47:34
                               ETA: 00:28:38

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 44701 steps/s (collection: 2.089s, learning 0.111s)
             Mean action noise std: 2.69
          Mean value_function loss: 334.8477
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 62.0120
                       Mean reward: 729.02
               Mean episode length: 209.71
    Episode_Reward/reaching_object: 1.3576
     Episode_Reward/lifting_object: 147.1699
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 2.20s
                      Time elapsed: 00:47:36
                               ETA: 00:28:36

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 44385 steps/s (collection: 2.112s, learning 0.103s)
             Mean action noise std: 2.69
          Mean value_function loss: 366.4284
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.0199
                       Mean reward: 753.31
               Mean episode length: 216.09
    Episode_Reward/reaching_object: 1.3450
     Episode_Reward/lifting_object: 146.1088
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 2.21s
                      Time elapsed: 00:47:38
                               ETA: 00:28:33

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 43602 steps/s (collection: 2.144s, learning 0.111s)
             Mean action noise std: 2.69
          Mean value_function loss: 410.8676
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.0389
                       Mean reward: 662.26
               Mean episode length: 196.96
    Episode_Reward/reaching_object: 1.2882
     Episode_Reward/lifting_object: 138.5248
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.25s
                      Time elapsed: 00:47:40
                               ETA: 00:28:31

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 45396 steps/s (collection: 2.072s, learning 0.093s)
             Mean action noise std: 2.69
          Mean value_function loss: 383.7553
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 62.0584
                       Mean reward: 673.14
               Mean episode length: 193.63
    Episode_Reward/reaching_object: 1.2254
     Episode_Reward/lifting_object: 131.7005
      Episode_Reward/object_height: 0.0123
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 2.17s
                      Time elapsed: 00:47:43
                               ETA: 00:28:29

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 44791 steps/s (collection: 2.088s, learning 0.107s)
             Mean action noise std: 2.69
          Mean value_function loss: 404.5716
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 62.0694
                       Mean reward: 704.87
               Mean episode length: 202.90
    Episode_Reward/reaching_object: 1.3175
     Episode_Reward/lifting_object: 142.2146
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.19s
                      Time elapsed: 00:47:45
                               ETA: 00:28:26

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 44360 steps/s (collection: 2.119s, learning 0.097s)
             Mean action noise std: 2.70
          Mean value_function loss: 366.4238
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 62.0893
                       Mean reward: 663.80
               Mean episode length: 193.36
    Episode_Reward/reaching_object: 1.2449
     Episode_Reward/lifting_object: 133.3092
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 2.22s
                      Time elapsed: 00:47:47
                               ETA: 00:28:24

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 44898 steps/s (collection: 2.091s, learning 0.099s)
             Mean action noise std: 2.70
          Mean value_function loss: 420.3061
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 62.1065
                       Mean reward: 661.37
               Mean episode length: 195.36
    Episode_Reward/reaching_object: 1.2415
     Episode_Reward/lifting_object: 133.6561
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 2.19s
                      Time elapsed: 00:47:49
                               ETA: 00:28:22

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 44602 steps/s (collection: 2.095s, learning 0.109s)
             Mean action noise std: 2.70
          Mean value_function loss: 390.6474
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 62.1196
                       Mean reward: 706.94
               Mean episode length: 201.74
    Episode_Reward/reaching_object: 1.3012
     Episode_Reward/lifting_object: 140.8353
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.5000
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 2.20s
                      Time elapsed: 00:47:51
                               ETA: 00:28:19

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 44694 steps/s (collection: 2.101s, learning 0.099s)
             Mean action noise std: 2.70
          Mean value_function loss: 345.9270
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 62.1356
                       Mean reward: 734.86
               Mean episode length: 209.20
    Episode_Reward/reaching_object: 1.3089
     Episode_Reward/lifting_object: 142.6859
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 2.20s
                      Time elapsed: 00:47:54
                               ETA: 00:28:17

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 44927 steps/s (collection: 2.079s, learning 0.109s)
             Mean action noise std: 2.70
          Mean value_function loss: 305.3634
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 62.1517
                       Mean reward: 660.83
               Mean episode length: 195.51
    Episode_Reward/reaching_object: 1.3202
     Episode_Reward/lifting_object: 143.9011
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 2.19s
                      Time elapsed: 00:47:56
                               ETA: 00:28:15

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 44351 steps/s (collection: 2.110s, learning 0.107s)
             Mean action noise std: 2.71
          Mean value_function loss: 341.2620
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 62.1696
                       Mean reward: 698.59
               Mean episode length: 204.77
    Episode_Reward/reaching_object: 1.2880
     Episode_Reward/lifting_object: 139.4553
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 2.22s
                      Time elapsed: 00:47:58
                               ETA: 00:28:12

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 42062 steps/s (collection: 2.190s, learning 0.147s)
             Mean action noise std: 2.71
          Mean value_function loss: 348.8430
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 62.1812
                       Mean reward: 681.44
               Mean episode length: 197.49
    Episode_Reward/reaching_object: 1.3045
     Episode_Reward/lifting_object: 141.7569
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 2.34s
                      Time elapsed: 00:48:00
                               ETA: 00:28:10

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 37571 steps/s (collection: 2.389s, learning 0.228s)
             Mean action noise std: 2.71
          Mean value_function loss: 379.0258
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 62.1838
                       Mean reward: 759.63
               Mean episode length: 213.61
    Episode_Reward/reaching_object: 1.3034
     Episode_Reward/lifting_object: 142.0676
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 2.62s
                      Time elapsed: 00:48:03
                               ETA: 00:28:08

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 39362 steps/s (collection: 2.332s, learning 0.166s)
             Mean action noise std: 2.71
          Mean value_function loss: 314.6705
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 62.1875
                       Mean reward: 703.84
               Mean episode length: 204.30
    Episode_Reward/reaching_object: 1.3001
     Episode_Reward/lifting_object: 142.0310
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 2.50s
                      Time elapsed: 00:48:05
                               ETA: 00:28:06

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 41103 steps/s (collection: 2.267s, learning 0.125s)
             Mean action noise std: 2.71
          Mean value_function loss: 361.3015
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.1960
                       Mean reward: 717.47
               Mean episode length: 203.45
    Episode_Reward/reaching_object: 1.2637
     Episode_Reward/lifting_object: 137.4889
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 2.39s
                      Time elapsed: 00:48:08
                               ETA: 00:28:04

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 44150 steps/s (collection: 2.107s, learning 0.120s)
             Mean action noise std: 2.71
          Mean value_function loss: 374.2283
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 62.2113
                       Mean reward: 682.33
               Mean episode length: 195.54
    Episode_Reward/reaching_object: 1.3058
     Episode_Reward/lifting_object: 142.8372
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0370
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 2.23s
                      Time elapsed: 00:48:10
                               ETA: 00:28:01

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 44410 steps/s (collection: 2.114s, learning 0.100s)
             Mean action noise std: 2.71
          Mean value_function loss: 303.0438
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 62.2234
                       Mean reward: 744.36
               Mean episode length: 212.04
    Episode_Reward/reaching_object: 1.3216
     Episode_Reward/lifting_object: 144.2789
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 2.21s
                      Time elapsed: 00:48:12
                               ETA: 00:27:59

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 42562 steps/s (collection: 2.212s, learning 0.098s)
             Mean action noise std: 2.71
          Mean value_function loss: 301.0665
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 62.2271
                       Mean reward: 764.09
               Mean episode length: 215.45
    Episode_Reward/reaching_object: 1.3283
     Episode_Reward/lifting_object: 144.8650
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 2.31s
                      Time elapsed: 00:48:15
                               ETA: 00:27:57

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 40515 steps/s (collection: 2.293s, learning 0.133s)
             Mean action noise std: 2.71
          Mean value_function loss: 327.7094
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 62.2294
                       Mean reward: 714.26
               Mean episode length: 205.45
    Episode_Reward/reaching_object: 1.3059
     Episode_Reward/lifting_object: 141.7086
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 2.43s
                      Time elapsed: 00:48:17
                               ETA: 00:27:54

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 43217 steps/s (collection: 2.172s, learning 0.103s)
             Mean action noise std: 2.71
          Mean value_function loss: 344.5427
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 62.2351
                       Mean reward: 639.67
               Mean episode length: 188.55
    Episode_Reward/reaching_object: 1.2783
     Episode_Reward/lifting_object: 137.7162
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 2.27s
                      Time elapsed: 00:48:19
                               ETA: 00:27:52

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 43329 steps/s (collection: 2.163s, learning 0.106s)
             Mean action noise std: 2.71
          Mean value_function loss: 333.8288
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 62.2403
                       Mean reward: 765.09
               Mean episode length: 215.71
    Episode_Reward/reaching_object: 1.2856
     Episode_Reward/lifting_object: 138.7032
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 2.27s
                      Time elapsed: 00:48:22
                               ETA: 00:27:50

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 36713 steps/s (collection: 2.534s, learning 0.144s)
             Mean action noise std: 2.71
          Mean value_function loss: 354.1633
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 62.2425
                       Mean reward: 736.93
               Mean episode length: 212.39
    Episode_Reward/reaching_object: 1.3092
     Episode_Reward/lifting_object: 140.8853
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 2.68s
                      Time elapsed: 00:48:24
                               ETA: 00:27:48

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 36978 steps/s (collection: 2.520s, learning 0.138s)
             Mean action noise std: 2.71
          Mean value_function loss: 366.7473
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 62.2480
                       Mean reward: 700.13
               Mean episode length: 199.84
    Episode_Reward/reaching_object: 1.2782
     Episode_Reward/lifting_object: 137.8174
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 2.66s
                      Time elapsed: 00:48:27
                               ETA: 00:27:46

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 39502 steps/s (collection: 2.382s, learning 0.107s)
             Mean action noise std: 2.72
          Mean value_function loss: 367.1999
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.2619
                       Mean reward: 728.26
               Mean episode length: 206.19
    Episode_Reward/reaching_object: 1.3115
     Episode_Reward/lifting_object: 141.8426
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 2.49s
                      Time elapsed: 00:48:29
                               ETA: 00:27:44

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 39390 steps/s (collection: 2.361s, learning 0.135s)
             Mean action noise std: 2.72
          Mean value_function loss: 344.8515
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 62.2796
                       Mean reward: 717.52
               Mean episode length: 208.28
    Episode_Reward/reaching_object: 1.3319
     Episode_Reward/lifting_object: 144.4192
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 2.50s
                      Time elapsed: 00:48:32
                               ETA: 00:27:41

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 36709 steps/s (collection: 2.466s, learning 0.212s)
             Mean action noise std: 2.72
          Mean value_function loss: 355.2656
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 62.2857
                       Mean reward: 665.34
               Mean episode length: 196.01
    Episode_Reward/reaching_object: 1.2616
     Episode_Reward/lifting_object: 136.4027
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 2.68s
                      Time elapsed: 00:48:35
                               ETA: 00:27:39

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 43524 steps/s (collection: 2.148s, learning 0.111s)
             Mean action noise std: 2.72
          Mean value_function loss: 338.7290
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 62.2885
                       Mean reward: 658.42
               Mean episode length: 194.02
    Episode_Reward/reaching_object: 1.3129
     Episode_Reward/lifting_object: 142.5547
      Episode_Reward/object_height: 0.0119
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 2.26s
                      Time elapsed: 00:48:37
                               ETA: 00:27:37

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 44296 steps/s (collection: 2.107s, learning 0.112s)
             Mean action noise std: 2.72
          Mean value_function loss: 377.9590
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 62.2897
                       Mean reward: 651.85
               Mean episode length: 192.81
    Episode_Reward/reaching_object: 1.2435
     Episode_Reward/lifting_object: 134.6844
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 2.22s
                      Time elapsed: 00:48:39
                               ETA: 00:27:35

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 43592 steps/s (collection: 2.163s, learning 0.093s)
             Mean action noise std: 2.72
          Mean value_function loss: 428.9229
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 62.2903
                       Mean reward: 681.90
               Mean episode length: 198.59
    Episode_Reward/reaching_object: 1.2400
     Episode_Reward/lifting_object: 134.5928
      Episode_Reward/object_height: 0.0112
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 2.26s
                      Time elapsed: 00:48:41
                               ETA: 00:27:32

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 42653 steps/s (collection: 2.194s, learning 0.111s)
             Mean action noise std: 2.72
          Mean value_function loss: 520.4401
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 62.2906
                       Mean reward: 578.01
               Mean episode length: 175.23
    Episode_Reward/reaching_object: 1.1803
     Episode_Reward/lifting_object: 127.1228
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 2.30s
                      Time elapsed: 00:48:44
                               ETA: 00:27:30

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 43928 steps/s (collection: 2.117s, learning 0.121s)
             Mean action noise std: 2.72
          Mean value_function loss: 480.9850
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 62.2910
                       Mean reward: 682.56
               Mean episode length: 201.35
    Episode_Reward/reaching_object: 1.2177
     Episode_Reward/lifting_object: 132.3706
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 2.24s
                      Time elapsed: 00:48:46
                               ETA: 00:27:28

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 44120 steps/s (collection: 2.129s, learning 0.100s)
             Mean action noise std: 2.72
          Mean value_function loss: 524.5492
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 62.2932
                       Mean reward: 621.49
               Mean episode length: 185.81
    Episode_Reward/reaching_object: 1.1494
     Episode_Reward/lifting_object: 123.8175
      Episode_Reward/object_height: 0.0099
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 2.23s
                      Time elapsed: 00:48:48
                               ETA: 00:27:26

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 43596 steps/s (collection: 2.158s, learning 0.097s)
             Mean action noise std: 2.72
          Mean value_function loss: 476.0471
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 62.3027
                       Mean reward: 592.60
               Mean episode length: 184.62
    Episode_Reward/reaching_object: 1.1555
     Episode_Reward/lifting_object: 124.0638
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0351
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 2.25s
                      Time elapsed: 00:48:50
                               ETA: 00:27:23

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 43737 steps/s (collection: 2.151s, learning 0.097s)
             Mean action noise std: 2.72
          Mean value_function loss: 468.8528
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 62.3123
                       Mean reward: 674.00
               Mean episode length: 196.38
    Episode_Reward/reaching_object: 1.1745
     Episode_Reward/lifting_object: 126.5229
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 2.25s
                      Time elapsed: 00:48:53
                               ETA: 00:27:21

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 44643 steps/s (collection: 2.104s, learning 0.098s)
             Mean action noise std: 2.72
          Mean value_function loss: 442.6774
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 62.3233
                       Mean reward: 646.13
               Mean episode length: 194.48
    Episode_Reward/reaching_object: 1.1474
     Episode_Reward/lifting_object: 123.2346
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.20s
                      Time elapsed: 00:48:55
                               ETA: 00:27:19

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 44035 steps/s (collection: 2.133s, learning 0.099s)
             Mean action noise std: 2.72
          Mean value_function loss: 384.6338
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 62.3339
                       Mean reward: 664.33
               Mean episode length: 206.71
    Episode_Reward/reaching_object: 1.1727
     Episode_Reward/lifting_object: 126.6111
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 2.23s
                      Time elapsed: 00:48:57
                               ETA: 00:27:16

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 43973 steps/s (collection: 2.108s, learning 0.127s)
             Mean action noise std: 2.73
          Mean value_function loss: 342.1946
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 62.3391
                       Mean reward: 662.10
               Mean episode length: 196.18
    Episode_Reward/reaching_object: 1.1350
     Episode_Reward/lifting_object: 123.0062
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.24s
                      Time elapsed: 00:48:59
                               ETA: 00:27:14

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 44492 steps/s (collection: 2.114s, learning 0.095s)
             Mean action noise std: 2.73
          Mean value_function loss: 309.4335
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 62.3456
                       Mean reward: 689.25
               Mean episode length: 206.26
    Episode_Reward/reaching_object: 1.1862
     Episode_Reward/lifting_object: 128.9181
      Episode_Reward/object_height: 0.0097
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 2.21s
                      Time elapsed: 00:49:01
                               ETA: 00:27:12

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 44618 steps/s (collection: 2.105s, learning 0.098s)
             Mean action noise std: 2.73
          Mean value_function loss: 403.1068
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 62.3560
                       Mean reward: 651.05
               Mean episode length: 196.15
    Episode_Reward/reaching_object: 1.1443
     Episode_Reward/lifting_object: 123.3112
      Episode_Reward/object_height: 0.0094
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 2.20s
                      Time elapsed: 00:49:04
                               ETA: 00:27:09

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 44309 steps/s (collection: 2.117s, learning 0.102s)
             Mean action noise std: 2.73
          Mean value_function loss: 305.7114
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 62.3676
                       Mean reward: 700.59
               Mean episode length: 202.91
    Episode_Reward/reaching_object: 1.2202
     Episode_Reward/lifting_object: 133.3475
      Episode_Reward/object_height: 0.0102
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 2.22s
                      Time elapsed: 00:49:06
                               ETA: 00:27:07

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 42344 steps/s (collection: 2.194s, learning 0.127s)
             Mean action noise std: 2.73
          Mean value_function loss: 346.2245
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 62.3795
                       Mean reward: 667.75
               Mean episode length: 204.90
    Episode_Reward/reaching_object: 1.2423
     Episode_Reward/lifting_object: 135.0479
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.32s
                      Time elapsed: 00:49:08
                               ETA: 00:27:05

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 42445 steps/s (collection: 2.202s, learning 0.114s)
             Mean action noise std: 2.73
          Mean value_function loss: 329.3116
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 62.3926
                       Mean reward: 634.02
               Mean episode length: 196.84
    Episode_Reward/reaching_object: 1.2512
     Episode_Reward/lifting_object: 136.7720
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 2.32s
                      Time elapsed: 00:49:10
                               ETA: 00:27:02

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 42485 steps/s (collection: 2.217s, learning 0.097s)
             Mean action noise std: 2.73
          Mean value_function loss: 341.6777
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 62.4051
                       Mean reward: 679.75
               Mean episode length: 198.56
    Episode_Reward/reaching_object: 1.2752
     Episode_Reward/lifting_object: 139.6467
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 2.31s
                      Time elapsed: 00:49:13
                               ETA: 00:27:00

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 43144 steps/s (collection: 2.173s, learning 0.106s)
             Mean action noise std: 2.73
          Mean value_function loss: 348.6677
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 62.4168
                       Mean reward: 734.48
               Mean episode length: 209.51
    Episode_Reward/reaching_object: 1.3057
     Episode_Reward/lifting_object: 142.9654
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 2.28s
                      Time elapsed: 00:49:15
                               ETA: 00:26:58

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 34603 steps/s (collection: 2.654s, learning 0.187s)
             Mean action noise std: 2.74
          Mean value_function loss: 329.2431
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 62.4248
                       Mean reward: 669.76
               Mean episode length: 201.13
    Episode_Reward/reaching_object: 1.2881
     Episode_Reward/lifting_object: 140.5123
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 2.84s
                      Time elapsed: 00:49:18
                               ETA: 00:26:56

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 36863 steps/s (collection: 2.557s, learning 0.110s)
             Mean action noise std: 2.74
          Mean value_function loss: 357.9259
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 62.4348
                       Mean reward: 674.61
               Mean episode length: 199.56
    Episode_Reward/reaching_object: 1.2702
     Episode_Reward/lifting_object: 137.4743
      Episode_Reward/object_height: 0.0110
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 2.67s
                      Time elapsed: 00:49:21
                               ETA: 00:26:54

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 41169 steps/s (collection: 2.281s, learning 0.107s)
             Mean action noise std: 2.74
          Mean value_function loss: 345.4846
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 62.4431
                       Mean reward: 647.33
               Mean episode length: 194.84
    Episode_Reward/reaching_object: 1.2547
     Episode_Reward/lifting_object: 134.7947
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 2.39s
                      Time elapsed: 00:49:23
                               ETA: 00:26:52

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 42357 steps/s (collection: 2.217s, learning 0.104s)
             Mean action noise std: 2.74
          Mean value_function loss: 361.9444
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 62.4584
                       Mean reward: 712.81
               Mean episode length: 207.34
    Episode_Reward/reaching_object: 1.2143
     Episode_Reward/lifting_object: 129.7491
      Episode_Reward/object_height: 0.0106
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 2.32s
                      Time elapsed: 00:49:25
                               ETA: 00:26:49

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 42907 steps/s (collection: 2.179s, learning 0.112s)
             Mean action noise std: 2.74
          Mean value_function loss: 330.4644
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 62.4710
                       Mean reward: 761.64
               Mean episode length: 216.42
    Episode_Reward/reaching_object: 1.2880
     Episode_Reward/lifting_object: 139.5455
      Episode_Reward/object_height: 0.0114
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 2.29s
                      Time elapsed: 00:49:28
                               ETA: 00:26:47

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 42105 steps/s (collection: 2.219s, learning 0.116s)
             Mean action noise std: 2.74
          Mean value_function loss: 336.1154
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 62.4783
                       Mean reward: 663.15
               Mean episode length: 195.25
    Episode_Reward/reaching_object: 1.2111
     Episode_Reward/lifting_object: 130.3570
      Episode_Reward/object_height: 0.0105
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 2.33s
                      Time elapsed: 00:49:30
                               ETA: 00:26:45

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 41404 steps/s (collection: 2.219s, learning 0.155s)
             Mean action noise std: 2.74
          Mean value_function loss: 351.7678
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 62.4879
                       Mean reward: 737.66
               Mean episode length: 216.10
    Episode_Reward/reaching_object: 1.3016
     Episode_Reward/lifting_object: 140.3912
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 2.37s
                      Time elapsed: 00:49:32
                               ETA: 00:26:43

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 44449 steps/s (collection: 2.120s, learning 0.092s)
             Mean action noise std: 2.74
          Mean value_function loss: 339.2554
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 62.4943
                       Mean reward: 723.90
               Mean episode length: 202.77
    Episode_Reward/reaching_object: 1.2727
     Episode_Reward/lifting_object: 137.7777
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 2.21s
                      Time elapsed: 00:49:34
                               ETA: 00:26:40

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 44294 steps/s (collection: 2.113s, learning 0.106s)
             Mean action noise std: 2.74
          Mean value_function loss: 374.3839
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 62.5020
                       Mean reward: 684.72
               Mean episode length: 195.76
    Episode_Reward/reaching_object: 1.2762
     Episode_Reward/lifting_object: 138.1018
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.22s
                      Time elapsed: 00:49:37
                               ETA: 00:26:38

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 43399 steps/s (collection: 2.164s, learning 0.101s)
             Mean action noise std: 2.75
          Mean value_function loss: 343.2343
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 62.5118
                       Mean reward: 698.29
               Mean episode length: 202.91
    Episode_Reward/reaching_object: 1.3123
     Episode_Reward/lifting_object: 142.2859
      Episode_Reward/object_height: 0.0115
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.27s
                      Time elapsed: 00:49:39
                               ETA: 00:26:36

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 44180 steps/s (collection: 2.112s, learning 0.114s)
             Mean action noise std: 2.75
          Mean value_function loss: 369.1761
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.5167
                       Mean reward: 703.42
               Mean episode length: 200.77
    Episode_Reward/reaching_object: 1.2903
     Episode_Reward/lifting_object: 140.2236
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.23s
                      Time elapsed: 00:49:41
                               ETA: 00:26:33

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 43941 steps/s (collection: 2.135s, learning 0.103s)
             Mean action noise std: 2.75
          Mean value_function loss: 322.4484
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.5251
                       Mean reward: 770.74
               Mean episode length: 219.70
    Episode_Reward/reaching_object: 1.2929
     Episode_Reward/lifting_object: 140.5151
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 2.24s
                      Time elapsed: 00:49:43
                               ETA: 00:26:31

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 44536 steps/s (collection: 2.114s, learning 0.093s)
             Mean action noise std: 2.75
          Mean value_function loss: 333.4478
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 62.5339
                       Mean reward: 702.35
               Mean episode length: 200.27
    Episode_Reward/reaching_object: 1.2972
     Episode_Reward/lifting_object: 140.5829
      Episode_Reward/object_height: 0.0113
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 2.21s
                      Time elapsed: 00:49:46
                               ETA: 00:26:29

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 45030 steps/s (collection: 2.074s, learning 0.109s)
             Mean action noise std: 2.75
          Mean value_function loss: 350.9580
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 62.5387
                       Mean reward: 649.37
               Mean episode length: 190.91
    Episode_Reward/reaching_object: 1.2771
     Episode_Reward/lifting_object: 137.9069
      Episode_Reward/object_height: 0.0109
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.18s
                      Time elapsed: 00:49:48
                               ETA: 00:26:26

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 44636 steps/s (collection: 2.103s, learning 0.099s)
             Mean action noise std: 2.75
          Mean value_function loss: 309.0390
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 62.5466
                       Mean reward: 718.76
               Mean episode length: 207.50
    Episode_Reward/reaching_object: 1.2911
     Episode_Reward/lifting_object: 139.7516
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 2.20s
                      Time elapsed: 00:49:50
                               ETA: 00:26:24

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 44384 steps/s (collection: 2.088s, learning 0.127s)
             Mean action noise std: 2.75
          Mean value_function loss: 258.8272
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 62.5592
                       Mean reward: 722.86
               Mean episode length: 210.00
    Episode_Reward/reaching_object: 1.3367
     Episode_Reward/lifting_object: 144.7850
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 2.21s
                      Time elapsed: 00:49:52
                               ETA: 00:26:22

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 44460 steps/s (collection: 2.107s, learning 0.104s)
             Mean action noise std: 2.75
          Mean value_function loss: 312.6154
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 62.5777
                       Mean reward: 709.71
               Mean episode length: 207.19
    Episode_Reward/reaching_object: 1.3551
     Episode_Reward/lifting_object: 146.7503
      Episode_Reward/object_height: 0.0118
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.21s
                      Time elapsed: 00:49:54
                               ETA: 00:26:19

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 44357 steps/s (collection: 2.120s, learning 0.097s)
             Mean action noise std: 2.76
          Mean value_function loss: 288.0654
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 62.5931
                       Mean reward: 701.39
               Mean episode length: 202.23
    Episode_Reward/reaching_object: 1.3495
     Episode_Reward/lifting_object: 146.8287
      Episode_Reward/object_height: 0.0120
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.22s
                      Time elapsed: 00:49:57
                               ETA: 00:26:17

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 44672 steps/s (collection: 2.094s, learning 0.107s)
             Mean action noise std: 2.76
          Mean value_function loss: 340.5424
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.5999
                       Mean reward: 671.54
               Mean episode length: 197.68
    Episode_Reward/reaching_object: 1.3119
     Episode_Reward/lifting_object: 142.0310
      Episode_Reward/object_height: 0.0121
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 2.20s
                      Time elapsed: 00:49:59
                               ETA: 00:26:15

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 43904 steps/s (collection: 2.143s, learning 0.096s)
             Mean action noise std: 2.76
          Mean value_function loss: 335.3107
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.6119
                       Mean reward: 779.58
               Mean episode length: 221.63
    Episode_Reward/reaching_object: 1.3542
     Episode_Reward/lifting_object: 147.3707
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 2.24s
                      Time elapsed: 00:50:01
                               ETA: 00:26:12

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 44537 steps/s (collection: 2.112s, learning 0.095s)
             Mean action noise std: 2.76
          Mean value_function loss: 335.4403
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.6280
                       Mean reward: 724.16
               Mean episode length: 204.75
    Episode_Reward/reaching_object: 1.3297
     Episode_Reward/lifting_object: 145.0372
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 2.21s
                      Time elapsed: 00:50:03
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 43847 steps/s (collection: 2.144s, learning 0.098s)
             Mean action noise std: 2.76
          Mean value_function loss: 312.4501
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.6435
                       Mean reward: 689.29
               Mean episode length: 201.28
    Episode_Reward/reaching_object: 1.2864
     Episode_Reward/lifting_object: 139.0193
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 2.24s
                      Time elapsed: 00:50:06
                               ETA: 00:26:08

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 44817 steps/s (collection: 2.095s, learning 0.099s)
             Mean action noise std: 2.76
          Mean value_function loss: 338.2394
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 62.6559
                       Mean reward: 703.72
               Mean episode length: 202.37
    Episode_Reward/reaching_object: 1.3362
     Episode_Reward/lifting_object: 145.8246
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 2.19s
                      Time elapsed: 00:50:08
                               ETA: 00:26:05

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 44591 steps/s (collection: 2.093s, learning 0.112s)
             Mean action noise std: 2.77
          Mean value_function loss: 349.6234
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 62.6684
                       Mean reward: 694.33
               Mean episode length: 204.32
    Episode_Reward/reaching_object: 1.2813
     Episode_Reward/lifting_object: 138.8585
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 2.20s
                      Time elapsed: 00:50:10
                               ETA: 00:26:03

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 43717 steps/s (collection: 2.148s, learning 0.101s)
             Mean action noise std: 2.77
          Mean value_function loss: 373.1786
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 62.6827
                       Mean reward: 700.00
               Mean episode length: 198.81
    Episode_Reward/reaching_object: 1.3238
     Episode_Reward/lifting_object: 144.8384
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 2.25s
                      Time elapsed: 00:50:12
                               ETA: 00:26:01

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 44899 steps/s (collection: 2.094s, learning 0.096s)
             Mean action noise std: 2.77
          Mean value_function loss: 308.0907
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.6927
                       Mean reward: 686.78
               Mean episode length: 200.86
    Episode_Reward/reaching_object: 1.3065
     Episode_Reward/lifting_object: 142.0910
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 2.19s
                      Time elapsed: 00:50:14
                               ETA: 00:25:58

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 44052 steps/s (collection: 2.131s, learning 0.101s)
             Mean action noise std: 2.77
          Mean value_function loss: 286.9419
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 62.7003
                       Mean reward: 731.68
               Mean episode length: 208.24
    Episode_Reward/reaching_object: 1.3585
     Episode_Reward/lifting_object: 148.4103
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 2.23s
                      Time elapsed: 00:50:17
                               ETA: 00:25:56

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 43548 steps/s (collection: 2.145s, learning 0.112s)
             Mean action noise std: 2.77
          Mean value_function loss: 342.3246
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 62.7090
                       Mean reward: 647.28
               Mean episode length: 189.93
    Episode_Reward/reaching_object: 1.2634
     Episode_Reward/lifting_object: 136.7227
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 2.26s
                      Time elapsed: 00:50:19
                               ETA: 00:25:54

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 44801 steps/s (collection: 2.102s, learning 0.092s)
             Mean action noise std: 2.77
          Mean value_function loss: 310.0332
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 62.7269
                       Mean reward: 738.75
               Mean episode length: 210.04
    Episode_Reward/reaching_object: 1.3056
     Episode_Reward/lifting_object: 142.0466
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 2.19s
                      Time elapsed: 00:50:21
                               ETA: 00:25:51

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 44066 steps/s (collection: 2.132s, learning 0.099s)
             Mean action noise std: 2.77
          Mean value_function loss: 288.0954
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 62.7395
                       Mean reward: 785.32
               Mean episode length: 222.80
    Episode_Reward/reaching_object: 1.3665
     Episode_Reward/lifting_object: 148.2353
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.23s
                      Time elapsed: 00:50:23
                               ETA: 00:25:49

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 44888 steps/s (collection: 2.094s, learning 0.096s)
             Mean action noise std: 2.78
          Mean value_function loss: 318.9449
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 62.7562
                       Mean reward: 727.06
               Mean episode length: 209.21
    Episode_Reward/reaching_object: 1.3575
     Episode_Reward/lifting_object: 148.6033
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.19s
                      Time elapsed: 00:50:25
                               ETA: 00:25:47

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 44987 steps/s (collection: 2.087s, learning 0.099s)
             Mean action noise std: 2.78
          Mean value_function loss: 336.5111
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 62.7700
                       Mean reward: 756.11
               Mean episode length: 212.72
    Episode_Reward/reaching_object: 1.3508
     Episode_Reward/lifting_object: 147.0755
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.19s
                      Time elapsed: 00:50:28
                               ETA: 00:25:44

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 44924 steps/s (collection: 2.093s, learning 0.095s)
             Mean action noise std: 2.78
          Mean value_function loss: 319.4443
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 62.7770
                       Mean reward: 717.89
               Mean episode length: 205.15
    Episode_Reward/reaching_object: 1.3062
     Episode_Reward/lifting_object: 142.1667
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.19s
                      Time elapsed: 00:50:30
                               ETA: 00:25:42

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 44590 steps/s (collection: 2.112s, learning 0.093s)
             Mean action noise std: 2.78
          Mean value_function loss: 320.4428
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 62.7831
                       Mean reward: 688.38
               Mean episode length: 201.16
    Episode_Reward/reaching_object: 1.2843
     Episode_Reward/lifting_object: 139.0434
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.20s
                      Time elapsed: 00:50:32
                               ETA: 00:25:40

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 44751 steps/s (collection: 2.098s, learning 0.099s)
             Mean action noise std: 2.78
          Mean value_function loss: 330.1033
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 62.7947
                       Mean reward: 658.09
               Mean episode length: 194.60
    Episode_Reward/reaching_object: 1.3282
     Episode_Reward/lifting_object: 143.2580
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.20s
                      Time elapsed: 00:50:34
                               ETA: 00:25:37

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 44639 steps/s (collection: 2.096s, learning 0.106s)
             Mean action noise std: 2.78
          Mean value_function loss: 286.3281
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 62.8042
                       Mean reward: 754.03
               Mean episode length: 213.38
    Episode_Reward/reaching_object: 1.3502
     Episode_Reward/lifting_object: 146.5161
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 2.20s
                      Time elapsed: 00:50:36
                               ETA: 00:25:35

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 45227 steps/s (collection: 2.064s, learning 0.109s)
             Mean action noise std: 2.78
          Mean value_function loss: 307.4671
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 62.8173
                       Mean reward: 649.49
               Mean episode length: 189.92
    Episode_Reward/reaching_object: 1.2750
     Episode_Reward/lifting_object: 137.1293
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 2.17s
                      Time elapsed: 00:50:39
                               ETA: 00:25:33

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 44309 steps/s (collection: 2.111s, learning 0.108s)
             Mean action noise std: 2.79
          Mean value_function loss: 301.9079
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 62.8373
                       Mean reward: 737.44
               Mean episode length: 211.11
    Episode_Reward/reaching_object: 1.3274
     Episode_Reward/lifting_object: 143.5643
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.22s
                      Time elapsed: 00:50:41
                               ETA: 00:25:30

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 44538 steps/s (collection: 2.099s, learning 0.108s)
             Mean action noise std: 2.79
          Mean value_function loss: 307.8915
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.8493
                       Mean reward: 672.04
               Mean episode length: 191.68
    Episode_Reward/reaching_object: 1.2850
     Episode_Reward/lifting_object: 140.2694
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 2.21s
                      Time elapsed: 00:50:43
                               ETA: 00:25:28

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 44773 steps/s (collection: 2.099s, learning 0.097s)
             Mean action noise std: 2.79
          Mean value_function loss: 279.8542
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.8616
                       Mean reward: 792.06
               Mean episode length: 218.81
    Episode_Reward/reaching_object: 1.3506
     Episode_Reward/lifting_object: 147.3949
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 2.20s
                      Time elapsed: 00:50:45
                               ETA: 00:25:26

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 18833 steps/s (collection: 5.099s, learning 0.121s)
             Mean action noise std: 2.79
          Mean value_function loss: 324.5958
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 62.8711
                       Mean reward: 715.65
               Mean episode length: 205.03
    Episode_Reward/reaching_object: 1.3182
     Episode_Reward/lifting_object: 144.4211
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 5.22s
                      Time elapsed: 00:50:50
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 14427 steps/s (collection: 6.697s, learning 0.116s)
             Mean action noise std: 2.79
          Mean value_function loss: 308.4032
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 62.8744
                       Mean reward: 723.55
               Mean episode length: 205.64
    Episode_Reward/reaching_object: 1.3203
     Episode_Reward/lifting_object: 143.8042
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 6.81s
                      Time elapsed: 00:50:57
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 14252 steps/s (collection: 6.788s, learning 0.109s)
             Mean action noise std: 2.79
          Mean value_function loss: 295.3988
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 62.8766
                       Mean reward: 735.03
               Mean episode length: 209.34
    Episode_Reward/reaching_object: 1.3231
     Episode_Reward/lifting_object: 144.8422
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 6.90s
                      Time elapsed: 00:51:04
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 14109 steps/s (collection: 6.841s, learning 0.127s)
             Mean action noise std: 2.79
          Mean value_function loss: 266.2261
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 62.8795
                       Mean reward: 778.81
               Mean episode length: 219.96
    Episode_Reward/reaching_object: 1.3660
     Episode_Reward/lifting_object: 149.8587
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 6.97s
                      Time elapsed: 00:51:11
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 14117 steps/s (collection: 6.848s, learning 0.115s)
             Mean action noise std: 2.79
          Mean value_function loss: 282.6391
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.8853
                       Mean reward: 737.47
               Mean episode length: 212.26
    Episode_Reward/reaching_object: 1.3496
     Episode_Reward/lifting_object: 148.4487
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 6.96s
                      Time elapsed: 00:51:18
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 14087 steps/s (collection: 6.862s, learning 0.116s)
             Mean action noise std: 2.79
          Mean value_function loss: 313.9699
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.8953
                       Mean reward: 720.00
               Mean episode length: 206.49
    Episode_Reward/reaching_object: 1.3123
     Episode_Reward/lifting_object: 143.6340
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 6.98s
                      Time elapsed: 00:51:25
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 14478 steps/s (collection: 6.670s, learning 0.119s)
             Mean action noise std: 2.79
          Mean value_function loss: 269.1385
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 62.9093
                       Mean reward: 740.03
               Mean episode length: 211.16
    Episode_Reward/reaching_object: 1.3107
     Episode_Reward/lifting_object: 142.9158
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 6.79s
                      Time elapsed: 00:51:32
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 14268 steps/s (collection: 6.768s, learning 0.122s)
             Mean action noise std: 2.80
          Mean value_function loss: 277.9720
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 62.9279
                       Mean reward: 718.49
               Mean episode length: 211.44
    Episode_Reward/reaching_object: 1.3146
     Episode_Reward/lifting_object: 143.5337
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 6.89s
                      Time elapsed: 00:51:39
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 13501 steps/s (collection: 7.175s, learning 0.106s)
             Mean action noise std: 2.80
          Mean value_function loss: 262.9256
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 62.9397
                       Mean reward: 777.46
               Mean episode length: 216.64
    Episode_Reward/reaching_object: 1.3788
     Episode_Reward/lifting_object: 150.9585
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 7.28s
                      Time elapsed: 00:51:46
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 47277 steps/s (collection: 1.988s, learning 0.091s)
             Mean action noise std: 2.80
          Mean value_function loss: 312.7241
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 62.9449
                       Mean reward: 758.12
               Mean episode length: 212.72
    Episode_Reward/reaching_object: 1.3302
     Episode_Reward/lifting_object: 145.7080
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 2.08s
                      Time elapsed: 00:51:48
                               ETA: 00:25:23

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 47263 steps/s (collection: 1.967s, learning 0.113s)
             Mean action noise std: 2.80
          Mean value_function loss: 312.0321
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 62.9516
                       Mean reward: 746.98
               Mean episode length: 212.77
    Episode_Reward/reaching_object: 1.3102
     Episode_Reward/lifting_object: 142.9976
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 2.08s
                      Time elapsed: 00:51:50
                               ETA: 00:25:20

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 47432 steps/s (collection: 1.974s, learning 0.099s)
             Mean action noise std: 2.80
          Mean value_function loss: 286.7693
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 62.9622
                       Mean reward: 701.15
               Mean episode length: 201.44
    Episode_Reward/reaching_object: 1.2922
     Episode_Reward/lifting_object: 140.6184
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 2.07s
                      Time elapsed: 00:51:52
                               ETA: 00:25:18

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 45452 steps/s (collection: 2.063s, learning 0.100s)
             Mean action noise std: 2.80
          Mean value_function loss: 336.5442
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 62.9752
                       Mean reward: 693.85
               Mean episode length: 201.37
    Episode_Reward/reaching_object: 1.3374
     Episode_Reward/lifting_object: 146.5386
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 2.16s
                      Time elapsed: 00:51:54
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 47199 steps/s (collection: 1.986s, learning 0.097s)
             Mean action noise std: 2.80
          Mean value_function loss: 294.8922
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 62.9865
                       Mean reward: 795.21
               Mean episode length: 219.87
    Episode_Reward/reaching_object: 1.3396
     Episode_Reward/lifting_object: 146.6400
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 2.08s
                      Time elapsed: 00:51:57
                               ETA: 00:25:13

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 46744 steps/s (collection: 2.012s, learning 0.091s)
             Mean action noise std: 2.80
          Mean value_function loss: 300.3507
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 63.0018
                       Mean reward: 729.53
               Mean episode length: 209.16
    Episode_Reward/reaching_object: 1.3264
     Episode_Reward/lifting_object: 144.8079
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 2.10s
                      Time elapsed: 00:51:59
                               ETA: 00:25:10

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 47876 steps/s (collection: 1.962s, learning 0.092s)
             Mean action noise std: 2.81
          Mean value_function loss: 317.1197
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 63.0091
                       Mean reward: 736.25
               Mean episode length: 208.90
    Episode_Reward/reaching_object: 1.2938
     Episode_Reward/lifting_object: 140.7461
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 2.05s
                      Time elapsed: 00:52:01
                               ETA: 00:25:08

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 47697 steps/s (collection: 1.961s, learning 0.100s)
             Mean action noise std: 2.81
          Mean value_function loss: 297.6224
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 63.0214
                       Mean reward: 665.86
               Mean episode length: 195.55
    Episode_Reward/reaching_object: 1.3043
     Episode_Reward/lifting_object: 141.5756
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 2.06s
                      Time elapsed: 00:52:03
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 47694 steps/s (collection: 1.954s, learning 0.108s)
             Mean action noise std: 2.81
          Mean value_function loss: 254.7593
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 63.0390
                       Mean reward: 747.28
               Mean episode length: 210.94
    Episode_Reward/reaching_object: 1.3455
     Episode_Reward/lifting_object: 147.4650
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 2.06s
                      Time elapsed: 00:52:05
                               ETA: 00:25:03

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 47328 steps/s (collection: 1.984s, learning 0.093s)
             Mean action noise std: 2.81
          Mean value_function loss: 261.4819
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 63.0455
                       Mean reward: 754.87
               Mean episode length: 213.99
    Episode_Reward/reaching_object: 1.3805
     Episode_Reward/lifting_object: 151.6030
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 2.08s
                      Time elapsed: 00:52:07
                               ETA: 00:25:01

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 47648 steps/s (collection: 1.976s, learning 0.087s)
             Mean action noise std: 2.81
          Mean value_function loss: 284.6211
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 63.0477
                       Mean reward: 710.33
               Mean episode length: 211.38
    Episode_Reward/reaching_object: 1.3423
     Episode_Reward/lifting_object: 146.0822
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 2.06s
                      Time elapsed: 00:52:09
                               ETA: 00:24:58

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 47916 steps/s (collection: 1.960s, learning 0.092s)
             Mean action noise std: 2.81
          Mean value_function loss: 312.3758
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 63.0504
                       Mean reward: 702.73
               Mean episode length: 201.28
    Episode_Reward/reaching_object: 1.3476
     Episode_Reward/lifting_object: 147.5108
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 2.05s
                      Time elapsed: 00:52:11
                               ETA: 00:24:56

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 47373 steps/s (collection: 1.982s, learning 0.093s)
             Mean action noise std: 2.81
          Mean value_function loss: 285.8947
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 63.0536
                       Mean reward: 748.04
               Mean episode length: 216.68
    Episode_Reward/reaching_object: 1.3882
     Episode_Reward/lifting_object: 152.5634
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 2.08s
                      Time elapsed: 00:52:13
                               ETA: 00:24:53

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 48215 steps/s (collection: 1.940s, learning 0.099s)
             Mean action noise std: 2.81
          Mean value_function loss: 332.9242
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 63.0577
                       Mean reward: 731.34
               Mean episode length: 210.94
    Episode_Reward/reaching_object: 1.3498
     Episode_Reward/lifting_object: 148.1160
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 2.04s
                      Time elapsed: 00:52:15
                               ETA: 00:24:51

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 47319 steps/s (collection: 1.985s, learning 0.093s)
             Mean action noise std: 2.81
          Mean value_function loss: 285.3371
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 63.0597
                       Mean reward: 805.01
               Mean episode length: 226.37
    Episode_Reward/reaching_object: 1.3721
     Episode_Reward/lifting_object: 149.8627
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 2.08s
                      Time elapsed: 00:52:17
                               ETA: 00:24:49

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 47501 steps/s (collection: 1.972s, learning 0.098s)
             Mean action noise std: 2.81
          Mean value_function loss: 297.9187
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 63.0615
                       Mean reward: 740.96
               Mean episode length: 209.39
    Episode_Reward/reaching_object: 1.3322
     Episode_Reward/lifting_object: 144.6976
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 2.07s
                      Time elapsed: 00:52:19
                               ETA: 00:24:46

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 47591 steps/s (collection: 1.958s, learning 0.108s)
             Mean action noise std: 2.81
          Mean value_function loss: 257.0742
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 63.0640
                       Mean reward: 755.73
               Mean episode length: 211.93
    Episode_Reward/reaching_object: 1.3581
     Episode_Reward/lifting_object: 148.6859
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 2.07s
                      Time elapsed: 00:52:21
                               ETA: 00:24:44

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 48127 steps/s (collection: 1.936s, learning 0.107s)
             Mean action noise std: 2.81
          Mean value_function loss: 315.6481
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 63.0663
                       Mean reward: 708.08
               Mean episode length: 205.90
    Episode_Reward/reaching_object: 1.3332
     Episode_Reward/lifting_object: 145.3650
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 2.04s
                      Time elapsed: 00:52:23
                               ETA: 00:24:41

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 47837 steps/s (collection: 1.949s, learning 0.106s)
             Mean action noise std: 2.81
          Mean value_function loss: 259.0284
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 63.0698
                       Mean reward: 703.96
               Mean episode length: 203.27
    Episode_Reward/reaching_object: 1.3133
     Episode_Reward/lifting_object: 142.2409
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 2.05s
                      Time elapsed: 00:52:25
                               ETA: 00:24:39

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 48108 steps/s (collection: 1.947s, learning 0.097s)
             Mean action noise std: 2.81
          Mean value_function loss: 304.2834
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 63.0769
                       Mean reward: 722.59
               Mean episode length: 205.77
    Episode_Reward/reaching_object: 1.3612
     Episode_Reward/lifting_object: 148.1555
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 2.04s
                      Time elapsed: 00:52:27
                               ETA: 00:24:36

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 47895 steps/s (collection: 1.963s, learning 0.090s)
             Mean action noise std: 2.81
          Mean value_function loss: 307.0373
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 63.0815
                       Mean reward: 716.63
               Mean episode length: 206.95
    Episode_Reward/reaching_object: 1.3399
     Episode_Reward/lifting_object: 145.2295
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 2.05s
                      Time elapsed: 00:52:30
                               ETA: 00:24:34

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 48108 steps/s (collection: 1.950s, learning 0.093s)
             Mean action noise std: 2.81
          Mean value_function loss: 311.7899
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 63.0878
                       Mean reward: 755.88
               Mean episode length: 214.46
    Episode_Reward/reaching_object: 1.3284
     Episode_Reward/lifting_object: 144.3871
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 2.04s
                      Time elapsed: 00:52:32
                               ETA: 00:24:32

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 47170 steps/s (collection: 1.979s, learning 0.105s)
             Mean action noise std: 2.81
          Mean value_function loss: 364.4474
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 63.0910
                       Mean reward: 708.09
               Mean episode length: 203.81
    Episode_Reward/reaching_object: 1.3783
     Episode_Reward/lifting_object: 150.5664
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 2.08s
                      Time elapsed: 00:52:34
                               ETA: 00:24:29

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 47143 steps/s (collection: 1.974s, learning 0.112s)
             Mean action noise std: 2.81
          Mean value_function loss: 325.7314
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 63.0925
                       Mean reward: 767.45
               Mean episode length: 213.06
    Episode_Reward/reaching_object: 1.3710
     Episode_Reward/lifting_object: 150.0038
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 2.09s
                      Time elapsed: 00:52:36
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 47197 steps/s (collection: 1.976s, learning 0.107s)
             Mean action noise std: 2.82
          Mean value_function loss: 308.1053
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 63.0945
                       Mean reward: 719.37
               Mean episode length: 207.33
    Episode_Reward/reaching_object: 1.3602
     Episode_Reward/lifting_object: 148.5567
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 2.08s
                      Time elapsed: 00:52:38
                               ETA: 00:24:24

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 47111 steps/s (collection: 1.974s, learning 0.113s)
             Mean action noise std: 2.82
          Mean value_function loss: 324.9435
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 63.0959
                       Mean reward: 685.83
               Mean episode length: 197.80
    Episode_Reward/reaching_object: 1.3126
     Episode_Reward/lifting_object: 142.5037
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 2.09s
                      Time elapsed: 00:52:40
                               ETA: 00:24:22

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 46585 steps/s (collection: 2.020s, learning 0.090s)
             Mean action noise std: 2.82
          Mean value_function loss: 269.2137
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 63.0972
                       Mean reward: 731.04
               Mean episode length: 204.88
    Episode_Reward/reaching_object: 1.3842
     Episode_Reward/lifting_object: 150.8331
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 2.11s
                      Time elapsed: 00:52:42
                               ETA: 00:24:19

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 46252 steps/s (collection: 2.039s, learning 0.086s)
             Mean action noise std: 2.82
          Mean value_function loss: 279.9839
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 63.1011
                       Mean reward: 689.14
               Mean episode length: 200.02
    Episode_Reward/reaching_object: 1.2946
     Episode_Reward/lifting_object: 139.8587
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 2.13s
                      Time elapsed: 00:52:44
                               ETA: 00:24:17

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 47680 steps/s (collection: 1.978s, learning 0.084s)
             Mean action noise std: 2.82
          Mean value_function loss: 291.4455
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 63.1065
                       Mean reward: 753.71
               Mean episode length: 214.48
    Episode_Reward/reaching_object: 1.3621
     Episode_Reward/lifting_object: 147.8671
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 2.06s
                      Time elapsed: 00:52:46
                               ETA: 00:24:15

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 47765 steps/s (collection: 1.971s, learning 0.087s)
             Mean action noise std: 2.82
          Mean value_function loss: 302.3735
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 63.1114
                       Mean reward: 681.81
               Mean episode length: 202.46
    Episode_Reward/reaching_object: 1.3222
     Episode_Reward/lifting_object: 142.4854
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 2.06s
                      Time elapsed: 00:52:48
                               ETA: 00:24:12

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 47247 steps/s (collection: 1.995s, learning 0.086s)
             Mean action noise std: 2.82
          Mean value_function loss: 301.9396
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 63.1156
                       Mean reward: 686.19
               Mean episode length: 199.85
    Episode_Reward/reaching_object: 1.3027
     Episode_Reward/lifting_object: 141.3071
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 2.08s
                      Time elapsed: 00:52:50
                               ETA: 00:24:10

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 46890 steps/s (collection: 2.001s, learning 0.095s)
             Mean action noise std: 2.82
          Mean value_function loss: 298.7490
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 63.1238
                       Mean reward: 755.90
               Mean episode length: 211.79
    Episode_Reward/reaching_object: 1.4060
     Episode_Reward/lifting_object: 153.5536
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 2.10s
                      Time elapsed: 00:52:52
                               ETA: 00:24:07

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 47431 steps/s (collection: 1.984s, learning 0.089s)
             Mean action noise std: 2.82
          Mean value_function loss: 356.8676
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 63.1381
                       Mean reward: 727.22
               Mean episode length: 208.55
    Episode_Reward/reaching_object: 1.3749
     Episode_Reward/lifting_object: 148.9636
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 2.07s
                      Time elapsed: 00:52:55
                               ETA: 00:24:05

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 46339 steps/s (collection: 2.016s, learning 0.106s)
             Mean action noise std: 2.82
          Mean value_function loss: 327.1462
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 63.1473
                       Mean reward: 730.79
               Mean episode length: 206.65
    Episode_Reward/reaching_object: 1.3474
     Episode_Reward/lifting_object: 146.5801
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 2.12s
                      Time elapsed: 00:52:57
                               ETA: 00:24:03

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 45499 steps/s (collection: 2.057s, learning 0.104s)
             Mean action noise std: 2.82
          Mean value_function loss: 271.3624
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 63.1583
                       Mean reward: 705.76
               Mean episode length: 204.99
    Episode_Reward/reaching_object: 1.3570
     Episode_Reward/lifting_object: 146.3736
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 2.16s
                      Time elapsed: 00:52:59
                               ETA: 00:24:00

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 46678 steps/s (collection: 2.007s, learning 0.099s)
             Mean action noise std: 2.83
          Mean value_function loss: 295.7211
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 63.1730
                       Mean reward: 732.25
               Mean episode length: 207.24
    Episode_Reward/reaching_object: 1.3488
     Episode_Reward/lifting_object: 146.3828
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 2.11s
                      Time elapsed: 00:53:01
                               ETA: 00:23:58

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 46322 steps/s (collection: 2.020s, learning 0.103s)
             Mean action noise std: 2.83
          Mean value_function loss: 295.2543
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 63.1889
                       Mean reward: 764.20
               Mean episode length: 216.51
    Episode_Reward/reaching_object: 1.3651
     Episode_Reward/lifting_object: 148.1848
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 2.12s
                      Time elapsed: 00:53:03
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 46105 steps/s (collection: 2.017s, learning 0.116s)
             Mean action noise std: 2.83
          Mean value_function loss: 288.1507
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.2034
                       Mean reward: 723.43
               Mean episode length: 210.39
    Episode_Reward/reaching_object: 1.3242
     Episode_Reward/lifting_object: 143.5452
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 2.13s
                      Time elapsed: 00:53:05
                               ETA: 00:23:53

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 45163 steps/s (collection: 2.056s, learning 0.121s)
             Mean action noise std: 2.83
          Mean value_function loss: 289.4395
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 63.2157
                       Mean reward: 784.99
               Mean episode length: 217.38
    Episode_Reward/reaching_object: 1.3857
     Episode_Reward/lifting_object: 151.9240
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 2.18s
                      Time elapsed: 00:53:07
                               ETA: 00:23:51

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 46503 steps/s (collection: 2.012s, learning 0.102s)
             Mean action noise std: 2.83
          Mean value_function loss: 315.7864
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 63.2260
                       Mean reward: 725.03
               Mean episode length: 206.75
    Episode_Reward/reaching_object: 1.3224
     Episode_Reward/lifting_object: 143.3257
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 2.11s
                      Time elapsed: 00:53:09
                               ETA: 00:23:48

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 47126 steps/s (collection: 1.977s, learning 0.109s)
             Mean action noise std: 2.83
          Mean value_function loss: 296.6374
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 63.2459
                       Mean reward: 781.39
               Mean episode length: 215.96
    Episode_Reward/reaching_object: 1.3285
     Episode_Reward/lifting_object: 143.3091
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 2.09s
                      Time elapsed: 00:53:12
                               ETA: 00:23:46

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 46544 steps/s (collection: 2.017s, learning 0.095s)
             Mean action noise std: 2.83
          Mean value_function loss: 280.6066
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 63.2537
                       Mean reward: 782.10
               Mean episode length: 217.72
    Episode_Reward/reaching_object: 1.3805
     Episode_Reward/lifting_object: 150.7493
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 2.11s
                      Time elapsed: 00:53:14
                               ETA: 00:23:43

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 47547 steps/s (collection: 1.980s, learning 0.088s)
             Mean action noise std: 2.83
          Mean value_function loss: 288.8154
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 63.2566
                       Mean reward: 728.64
               Mean episode length: 209.73
    Episode_Reward/reaching_object: 1.3678
     Episode_Reward/lifting_object: 149.3555
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 2.07s
                      Time elapsed: 00:53:16
                               ETA: 00:23:41

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 47091 steps/s (collection: 1.996s, learning 0.092s)
             Mean action noise std: 2.84
          Mean value_function loss: 307.8380
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 63.2586
                       Mean reward: 703.11
               Mean episode length: 201.57
    Episode_Reward/reaching_object: 1.3909
     Episode_Reward/lifting_object: 151.9092
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 2.09s
                      Time elapsed: 00:53:18
                               ETA: 00:23:39

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 47023 steps/s (collection: 1.990s, learning 0.101s)
             Mean action noise std: 2.84
          Mean value_function loss: 347.4847
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.2628
                       Mean reward: 724.20
               Mean episode length: 206.50
    Episode_Reward/reaching_object: 1.3491
     Episode_Reward/lifting_object: 147.3609
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 2.09s
                      Time elapsed: 00:53:20
                               ETA: 00:23:36

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 47649 steps/s (collection: 1.965s, learning 0.098s)
             Mean action noise std: 2.84
          Mean value_function loss: 331.9926
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 63.2713
                       Mean reward: 678.39
               Mean episode length: 197.73
    Episode_Reward/reaching_object: 1.2909
     Episode_Reward/lifting_object: 140.4064
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 2.06s
                      Time elapsed: 00:53:22
                               ETA: 00:23:34

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 46922 steps/s (collection: 1.984s, learning 0.111s)
             Mean action noise std: 2.84
          Mean value_function loss: 273.3463
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 63.2758
                       Mean reward: 757.67
               Mean episode length: 214.80
    Episode_Reward/reaching_object: 1.3456
     Episode_Reward/lifting_object: 145.4742
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 2.10s
                      Time elapsed: 00:53:24
                               ETA: 00:23:31

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 46466 steps/s (collection: 2.010s, learning 0.106s)
             Mean action noise std: 2.84
          Mean value_function loss: 308.1495
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 63.2869
                       Mean reward: 691.88
               Mean episode length: 199.34
    Episode_Reward/reaching_object: 1.3388
     Episode_Reward/lifting_object: 145.1069
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 2.12s
                      Time elapsed: 00:53:26
                               ETA: 00:23:29

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 44457 steps/s (collection: 2.121s, learning 0.090s)
             Mean action noise std: 2.84
          Mean value_function loss: 291.9013
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 63.2989
                       Mean reward: 776.32
               Mean episode length: 216.46
    Episode_Reward/reaching_object: 1.3691
     Episode_Reward/lifting_object: 149.3302
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 2.21s
                      Time elapsed: 00:53:28
                               ETA: 00:23:27

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 46163 steps/s (collection: 2.040s, learning 0.090s)
             Mean action noise std: 2.84
          Mean value_function loss: 331.9662
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 63.3107
                       Mean reward: 703.05
               Mean episode length: 202.60
    Episode_Reward/reaching_object: 1.3525
     Episode_Reward/lifting_object: 146.9992
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 2.13s
                      Time elapsed: 00:53:30
                               ETA: 00:23:24

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 46967 steps/s (collection: 2.000s, learning 0.093s)
             Mean action noise std: 2.84
          Mean value_function loss: 290.5038
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 63.3270
                       Mean reward: 724.19
               Mean episode length: 203.78
    Episode_Reward/reaching_object: 1.3246
     Episode_Reward/lifting_object: 143.6270
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 2.09s
                      Time elapsed: 00:53:33
                               ETA: 00:23:22

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 41770 steps/s (collection: 2.197s, learning 0.157s)
             Mean action noise std: 2.85
          Mean value_function loss: 303.0001
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 63.3416
                       Mean reward: 671.24
               Mean episode length: 195.76
    Episode_Reward/reaching_object: 1.3306
     Episode_Reward/lifting_object: 144.0921
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 2.35s
                      Time elapsed: 00:53:35
                               ETA: 00:23:20

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 45529 steps/s (collection: 2.053s, learning 0.106s)
             Mean action noise std: 2.85
          Mean value_function loss: 301.5104
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.3526
                       Mean reward: 720.90
               Mean episode length: 208.61
    Episode_Reward/reaching_object: 1.3211
     Episode_Reward/lifting_object: 142.4309
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 2.16s
                      Time elapsed: 00:53:37
                               ETA: 00:23:17

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 45645 steps/s (collection: 2.055s, learning 0.099s)
             Mean action noise std: 2.85
          Mean value_function loss: 343.3615
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 63.3695
                       Mean reward: 722.11
               Mean episode length: 207.03
    Episode_Reward/reaching_object: 1.3687
     Episode_Reward/lifting_object: 149.2861
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 2.15s
                      Time elapsed: 00:53:39
                               ETA: 00:23:15

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 46459 steps/s (collection: 2.026s, learning 0.090s)
             Mean action noise std: 2.85
          Mean value_function loss: 355.7063
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.3844
                       Mean reward: 730.39
               Mean episode length: 210.25
    Episode_Reward/reaching_object: 1.3278
     Episode_Reward/lifting_object: 143.7789
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 2.12s
                      Time elapsed: 00:53:41
                               ETA: 00:23:12

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 40843 steps/s (collection: 2.249s, learning 0.158s)
             Mean action noise std: 2.85
          Mean value_function loss: 291.3256
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 63.3995
                       Mean reward: 767.45
               Mean episode length: 217.90
    Episode_Reward/reaching_object: 1.3091
     Episode_Reward/lifting_object: 141.5630
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 2.41s
                      Time elapsed: 00:53:44
                               ETA: 00:23:10

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 45911 steps/s (collection: 2.019s, learning 0.122s)
             Mean action noise std: 2.85
          Mean value_function loss: 271.0416
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 63.4134
                       Mean reward: 715.32
               Mean episode length: 205.27
    Episode_Reward/reaching_object: 1.3936
     Episode_Reward/lifting_object: 151.8249
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 2.14s
                      Time elapsed: 00:53:46
                               ETA: 00:23:08

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 46513 steps/s (collection: 2.013s, learning 0.100s)
             Mean action noise std: 2.86
          Mean value_function loss: 318.8250
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 63.4315
                       Mean reward: 746.01
               Mean episode length: 212.04
    Episode_Reward/reaching_object: 1.3295
     Episode_Reward/lifting_object: 144.8976
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 2.11s
                      Time elapsed: 00:53:48
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 45587 steps/s (collection: 2.071s, learning 0.085s)
             Mean action noise std: 2.86
          Mean value_function loss: 269.7070
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 63.4437
                       Mean reward: 775.45
               Mean episode length: 219.33
    Episode_Reward/reaching_object: 1.3899
     Episode_Reward/lifting_object: 151.2330
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 2.16s
                      Time elapsed: 00:53:50
                               ETA: 00:23:03

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 45193 steps/s (collection: 2.081s, learning 0.094s)
             Mean action noise std: 2.86
          Mean value_function loss: 260.8135
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 63.4489
                       Mean reward: 738.33
               Mean episode length: 210.98
    Episode_Reward/reaching_object: 1.3597
     Episode_Reward/lifting_object: 148.2449
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 2.18s
                      Time elapsed: 00:53:52
                               ETA: 00:23:01

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 45749 steps/s (collection: 2.042s, learning 0.107s)
             Mean action noise std: 2.86
          Mean value_function loss: 267.7521
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 63.4572
                       Mean reward: 755.76
               Mean episode length: 215.22
    Episode_Reward/reaching_object: 1.3445
     Episode_Reward/lifting_object: 146.0485
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 2.15s
                      Time elapsed: 00:53:55
                               ETA: 00:22:58

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 45423 steps/s (collection: 2.062s, learning 0.102s)
             Mean action noise std: 2.86
          Mean value_function loss: 257.0096
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 63.4682
                       Mean reward: 800.70
               Mean episode length: 224.28
    Episode_Reward/reaching_object: 1.3701
     Episode_Reward/lifting_object: 149.8579
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 2.16s
                      Time elapsed: 00:53:57
                               ETA: 00:22:56

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 46570 steps/s (collection: 2.013s, learning 0.098s)
             Mean action noise std: 2.86
          Mean value_function loss: 299.1178
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 63.4757
                       Mean reward: 694.92
               Mean episode length: 199.43
    Episode_Reward/reaching_object: 1.3075
     Episode_Reward/lifting_object: 142.2633
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 2.11s
                      Time elapsed: 00:53:59
                               ETA: 00:22:54

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 46058 steps/s (collection: 2.007s, learning 0.127s)
             Mean action noise std: 2.86
          Mean value_function loss: 289.3011
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 63.4793
                       Mean reward: 785.34
               Mean episode length: 217.46
    Episode_Reward/reaching_object: 1.3877
     Episode_Reward/lifting_object: 152.1563
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 2.13s
                      Time elapsed: 00:54:01
                               ETA: 00:22:51

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 46579 steps/s (collection: 2.012s, learning 0.098s)
             Mean action noise std: 2.86
          Mean value_function loss: 291.5844
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 63.4814
                       Mean reward: 786.32
               Mean episode length: 220.46
    Episode_Reward/reaching_object: 1.3654
     Episode_Reward/lifting_object: 149.0541
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 2.11s
                      Time elapsed: 00:54:03
                               ETA: 00:22:49

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 45445 steps/s (collection: 2.076s, learning 0.087s)
             Mean action noise std: 2.86
          Mean value_function loss: 265.0667
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 63.4846
                       Mean reward: 764.23
               Mean episode length: 216.29
    Episode_Reward/reaching_object: 1.3794
     Episode_Reward/lifting_object: 151.1087
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 2.16s
                      Time elapsed: 00:54:05
                               ETA: 00:22:46

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 45981 steps/s (collection: 2.043s, learning 0.095s)
             Mean action noise std: 2.86
          Mean value_function loss: 294.0371
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 63.4923
                       Mean reward: 774.63
               Mean episode length: 218.09
    Episode_Reward/reaching_object: 1.3545
     Episode_Reward/lifting_object: 147.7443
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 2.14s
                      Time elapsed: 00:54:07
                               ETA: 00:22:44

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 46662 steps/s (collection: 2.022s, learning 0.085s)
             Mean action noise std: 2.87
          Mean value_function loss: 255.2965
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 63.5053
                       Mean reward: 744.02
               Mean episode length: 212.72
    Episode_Reward/reaching_object: 1.3510
     Episode_Reward/lifting_object: 146.9932
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 2.11s
                      Time elapsed: 00:54:09
                               ETA: 00:22:42

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 46679 steps/s (collection: 2.021s, learning 0.085s)
             Mean action noise std: 2.87
          Mean value_function loss: 259.0986
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 63.5228
                       Mean reward: 777.65
               Mean episode length: 219.58
    Episode_Reward/reaching_object: 1.3784
     Episode_Reward/lifting_object: 150.6248
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 2.11s
                      Time elapsed: 00:54:12
                               ETA: 00:22:39

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 46657 steps/s (collection: 2.015s, learning 0.092s)
             Mean action noise std: 2.87
          Mean value_function loss: 247.7476
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.5347
                       Mean reward: 705.19
               Mean episode length: 205.15
    Episode_Reward/reaching_object: 1.3432
     Episode_Reward/lifting_object: 145.4743
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 2.11s
                      Time elapsed: 00:54:14
                               ETA: 00:22:37

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 46315 steps/s (collection: 2.014s, learning 0.109s)
             Mean action noise std: 2.87
          Mean value_function loss: 269.6017
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 63.5416
                       Mean reward: 769.86
               Mean episode length: 214.25
    Episode_Reward/reaching_object: 1.3552
     Episode_Reward/lifting_object: 147.7821
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 2.12s
                      Time elapsed: 00:54:16
                               ETA: 00:22:35

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 45824 steps/s (collection: 2.025s, learning 0.121s)
             Mean action noise std: 2.87
          Mean value_function loss: 273.4560
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 63.5524
                       Mean reward: 741.57
               Mean episode length: 210.13
    Episode_Reward/reaching_object: 1.3704
     Episode_Reward/lifting_object: 149.9338
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 2.15s
                      Time elapsed: 00:54:18
                               ETA: 00:22:32

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 46480 steps/s (collection: 2.005s, learning 0.110s)
             Mean action noise std: 2.87
          Mean value_function loss: 258.9002
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 63.5624
                       Mean reward: 725.65
               Mean episode length: 207.47
    Episode_Reward/reaching_object: 1.3773
     Episode_Reward/lifting_object: 150.0768
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 2.11s
                      Time elapsed: 00:54:20
                               ETA: 00:22:30

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 43839 steps/s (collection: 2.108s, learning 0.134s)
             Mean action noise std: 2.88
          Mean value_function loss: 302.2998
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 63.5759
                       Mean reward: 737.95
               Mean episode length: 206.79
    Episode_Reward/reaching_object: 1.3610
     Episode_Reward/lifting_object: 149.3648
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 2.24s
                      Time elapsed: 00:54:22
                               ETA: 00:22:27

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 44830 steps/s (collection: 2.097s, learning 0.096s)
             Mean action noise std: 2.88
          Mean value_function loss: 264.4356
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 63.5932
                       Mean reward: 778.02
               Mean episode length: 217.98
    Episode_Reward/reaching_object: 1.3977
     Episode_Reward/lifting_object: 152.3614
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 2.19s
                      Time elapsed: 00:54:24
                               ETA: 00:22:25

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 46353 steps/s (collection: 2.025s, learning 0.096s)
             Mean action noise std: 2.88
          Mean value_function loss: 281.5768
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 63.6126
                       Mean reward: 740.74
               Mean episode length: 209.20
    Episode_Reward/reaching_object: 1.3944
     Episode_Reward/lifting_object: 152.5722
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 2.12s
                      Time elapsed: 00:54:27
                               ETA: 00:22:23

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 42457 steps/s (collection: 2.205s, learning 0.111s)
             Mean action noise std: 2.88
          Mean value_function loss: 288.7275
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 63.6264
                       Mean reward: 725.58
               Mean episode length: 206.01
    Episode_Reward/reaching_object: 1.2996
     Episode_Reward/lifting_object: 139.9410
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 2.32s
                      Time elapsed: 00:54:29
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 46816 steps/s (collection: 1.998s, learning 0.102s)
             Mean action noise std: 2.88
          Mean value_function loss: 253.1790
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 63.6384
                       Mean reward: 675.69
               Mean episode length: 197.31
    Episode_Reward/reaching_object: 1.3202
     Episode_Reward/lifting_object: 143.3069
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 2.10s
                      Time elapsed: 00:54:31
                               ETA: 00:22:18

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 46323 steps/s (collection: 2.004s, learning 0.118s)
             Mean action noise std: 2.88
          Mean value_function loss: 246.6121
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 63.6474
                       Mean reward: 755.69
               Mean episode length: 213.16
    Episode_Reward/reaching_object: 1.3970
     Episode_Reward/lifting_object: 152.3246
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 2.12s
                      Time elapsed: 00:54:33
                               ETA: 00:22:16

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 46793 steps/s (collection: 2.010s, learning 0.091s)
             Mean action noise std: 2.88
          Mean value_function loss: 264.7273
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 63.6500
                       Mean reward: 668.88
               Mean episode length: 194.93
    Episode_Reward/reaching_object: 1.3143
     Episode_Reward/lifting_object: 142.8304
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 2.10s
                      Time elapsed: 00:54:35
                               ETA: 00:22:13

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 45733 steps/s (collection: 2.046s, learning 0.103s)
             Mean action noise std: 2.88
          Mean value_function loss: 300.7810
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 63.6520
                       Mean reward: 731.28
               Mean episode length: 207.04
    Episode_Reward/reaching_object: 1.3682
     Episode_Reward/lifting_object: 148.5010
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 2.15s
                      Time elapsed: 00:54:37
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 47549 steps/s (collection: 1.972s, learning 0.095s)
             Mean action noise std: 2.88
          Mean value_function loss: 235.8063
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 63.6537
                       Mean reward: 755.80
               Mean episode length: 213.16
    Episode_Reward/reaching_object: 1.4208
     Episode_Reward/lifting_object: 155.6677
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 2.07s
                      Time elapsed: 00:54:39
                               ETA: 00:22:09

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 45528 steps/s (collection: 2.067s, learning 0.093s)
             Mean action noise std: 2.88
          Mean value_function loss: 294.7543
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 63.6567
                       Mean reward: 703.23
               Mean episode length: 202.39
    Episode_Reward/reaching_object: 1.3415
     Episode_Reward/lifting_object: 145.7589
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 2.16s
                      Time elapsed: 00:54:42
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 45985 steps/s (collection: 2.051s, learning 0.087s)
             Mean action noise std: 2.89
          Mean value_function loss: 313.7992
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 63.6608
                       Mean reward: 721.00
               Mean episode length: 203.98
    Episode_Reward/reaching_object: 1.3020
     Episode_Reward/lifting_object: 141.9603
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 2.14s
                      Time elapsed: 00:54:44
                               ETA: 00:22:04

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 45685 steps/s (collection: 2.058s, learning 0.094s)
             Mean action noise std: 2.89
          Mean value_function loss: 328.9262
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 63.6689
                       Mean reward: 774.49
               Mean episode length: 214.50
    Episode_Reward/reaching_object: 1.3750
     Episode_Reward/lifting_object: 150.9005
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 2.15s
                      Time elapsed: 00:54:46
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 43223 steps/s (collection: 2.138s, learning 0.136s)
             Mean action noise std: 2.89
          Mean value_function loss: 307.9547
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 63.6766
                       Mean reward: 728.87
               Mean episode length: 211.11
    Episode_Reward/reaching_object: 1.3085
     Episode_Reward/lifting_object: 142.7861
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 2.27s
                      Time elapsed: 00:54:48
                               ETA: 00:21:59

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 45662 steps/s (collection: 2.034s, learning 0.119s)
             Mean action noise std: 2.89
          Mean value_function loss: 322.9362
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 63.6882
                       Mean reward: 718.31
               Mean episode length: 201.89
    Episode_Reward/reaching_object: 1.2740
     Episode_Reward/lifting_object: 138.3367
      Episode_Reward/object_height: 0.0122
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 2.15s
                      Time elapsed: 00:54:50
                               ETA: 00:21:57

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 46398 steps/s (collection: 2.030s, learning 0.089s)
             Mean action noise std: 2.89
          Mean value_function loss: 265.6512
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 63.7034
                       Mean reward: 724.44
               Mean episode length: 204.75
    Episode_Reward/reaching_object: 1.3354
     Episode_Reward/lifting_object: 146.4497
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 2.12s
                      Time elapsed: 00:54:52
                               ETA: 00:21:54

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 44849 steps/s (collection: 2.057s, learning 0.135s)
             Mean action noise std: 2.89
          Mean value_function loss: 244.8983
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 63.7166
                       Mean reward: 731.35
               Mean episode length: 208.85
    Episode_Reward/reaching_object: 1.3351
     Episode_Reward/lifting_object: 146.1621
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 2.19s
                      Time elapsed: 00:54:55
                               ETA: 00:21:52

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 44809 steps/s (collection: 2.053s, learning 0.141s)
             Mean action noise std: 2.89
          Mean value_function loss: 274.0734
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 63.7227
                       Mean reward: 773.17
               Mean episode length: 218.71
    Episode_Reward/reaching_object: 1.3289
     Episode_Reward/lifting_object: 146.1946
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 2.19s
                      Time elapsed: 00:54:57
                               ETA: 00:21:50

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 43376 steps/s (collection: 2.168s, learning 0.098s)
             Mean action noise std: 2.89
          Mean value_function loss: 267.1065
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 63.7257
                       Mean reward: 785.21
               Mean episode length: 218.14
    Episode_Reward/reaching_object: 1.3447
     Episode_Reward/lifting_object: 148.3445
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 2.27s
                      Time elapsed: 00:54:59
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 46520 steps/s (collection: 2.010s, learning 0.103s)
             Mean action noise std: 2.89
          Mean value_function loss: 269.2127
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 63.7347
                       Mean reward: 756.55
               Mean episode length: 210.51
    Episode_Reward/reaching_object: 1.3357
     Episode_Reward/lifting_object: 147.0443
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 2.11s
                      Time elapsed: 00:55:01
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 42913 steps/s (collection: 2.167s, learning 0.124s)
             Mean action noise std: 2.90
          Mean value_function loss: 265.5506
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 63.7468
                       Mean reward: 731.44
               Mean episode length: 204.57
    Episode_Reward/reaching_object: 1.3527
     Episode_Reward/lifting_object: 149.0289
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 2.29s
                      Time elapsed: 00:55:04
                               ETA: 00:21:43

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 45956 steps/s (collection: 2.034s, learning 0.105s)
             Mean action noise std: 2.90
          Mean value_function loss: 286.2163
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 63.7562
                       Mean reward: 759.84
               Mean episode length: 215.99
    Episode_Reward/reaching_object: 1.3769
     Episode_Reward/lifting_object: 150.7516
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 2.14s
                      Time elapsed: 00:55:06
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 42831 steps/s (collection: 2.176s, learning 0.119s)
             Mean action noise std: 2.90
          Mean value_function loss: 319.2316
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 63.7732
                       Mean reward: 748.52
               Mean episode length: 213.51
    Episode_Reward/reaching_object: 1.3400
     Episode_Reward/lifting_object: 146.7555
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0427
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 2.30s
                      Time elapsed: 00:55:08
                               ETA: 00:21:38

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 44927 steps/s (collection: 2.048s, learning 0.140s)
             Mean action noise std: 2.90
          Mean value_function loss: 282.2446
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 63.7855
                       Mean reward: 802.07
               Mean episode length: 222.72
    Episode_Reward/reaching_object: 1.3655
     Episode_Reward/lifting_object: 149.2549
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 2.19s
                      Time elapsed: 00:55:10
                               ETA: 00:21:36

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 44578 steps/s (collection: 2.105s, learning 0.101s)
             Mean action noise std: 2.90
          Mean value_function loss: 308.6292
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 63.8011
                       Mean reward: 782.56
               Mean episode length: 218.18
    Episode_Reward/reaching_object: 1.3839
     Episode_Reward/lifting_object: 150.4906
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 2.21s
                      Time elapsed: 00:55:12
                               ETA: 00:21:33

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 43084 steps/s (collection: 2.156s, learning 0.126s)
             Mean action noise std: 2.90
          Mean value_function loss: 288.0945
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 63.8157
                       Mean reward: 793.59
               Mean episode length: 224.79
    Episode_Reward/reaching_object: 1.3694
     Episode_Reward/lifting_object: 149.1636
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 2.28s
                      Time elapsed: 00:55:15
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 45445 steps/s (collection: 2.073s, learning 0.090s)
             Mean action noise std: 2.91
          Mean value_function loss: 267.6076
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 63.8265
                       Mean reward: 773.55
               Mean episode length: 216.05
    Episode_Reward/reaching_object: 1.3739
     Episode_Reward/lifting_object: 150.3295
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0437
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 2.16s
                      Time elapsed: 00:55:17
                               ETA: 00:21:29

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 46223 steps/s (collection: 2.025s, learning 0.102s)
             Mean action noise std: 2.91
          Mean value_function loss: 293.7249
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 63.8375
                       Mean reward: 792.18
               Mean episode length: 221.13
    Episode_Reward/reaching_object: 1.3852
     Episode_Reward/lifting_object: 151.0076
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 2.13s
                      Time elapsed: 00:55:19
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 45700 steps/s (collection: 2.034s, learning 0.117s)
             Mean action noise std: 2.91
          Mean value_function loss: 240.5981
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 63.8507
                       Mean reward: 741.88
               Mean episode length: 209.07
    Episode_Reward/reaching_object: 1.3790
     Episode_Reward/lifting_object: 150.5038
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 2.15s
                      Time elapsed: 00:55:21
                               ETA: 00:21:24

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 42531 steps/s (collection: 2.178s, learning 0.133s)
             Mean action noise std: 2.91
          Mean value_function loss: 263.9165
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 63.8662
                       Mean reward: 774.52
               Mean episode length: 214.90
    Episode_Reward/reaching_object: 1.3500
     Episode_Reward/lifting_object: 146.9946
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 2.31s
                      Time elapsed: 00:55:23
                               ETA: 00:21:22

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 46995 steps/s (collection: 2.007s, learning 0.085s)
             Mean action noise std: 2.91
          Mean value_function loss: 277.3235
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 63.8809
                       Mean reward: 749.81
               Mean episode length: 209.66
    Episode_Reward/reaching_object: 1.3620
     Episode_Reward/lifting_object: 148.1083
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 2.09s
                      Time elapsed: 00:55:25
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 43397 steps/s (collection: 2.146s, learning 0.119s)
             Mean action noise std: 2.91
          Mean value_function loss: 276.4745
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 63.9015
                       Mean reward: 757.18
               Mean episode length: 210.98
    Episode_Reward/reaching_object: 1.3172
     Episode_Reward/lifting_object: 143.4950
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 2.27s
                      Time elapsed: 00:55:28
                               ETA: 00:21:17

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 46675 steps/s (collection: 2.022s, learning 0.085s)
             Mean action noise std: 2.92
          Mean value_function loss: 262.0591
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 63.9216
                       Mean reward: 819.79
               Mean episode length: 226.69
    Episode_Reward/reaching_object: 1.3955
     Episode_Reward/lifting_object: 152.5517
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 2.11s
                      Time elapsed: 00:55:30
                               ETA: 00:21:15

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 45802 steps/s (collection: 2.059s, learning 0.087s)
             Mean action noise std: 2.92
          Mean value_function loss: 309.8311
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 63.9372
                       Mean reward: 771.54
               Mean episode length: 214.04
    Episode_Reward/reaching_object: 1.3948
     Episode_Reward/lifting_object: 153.4018
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 2.15s
                      Time elapsed: 00:55:32
                               ETA: 00:21:12

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 47120 steps/s (collection: 1.993s, learning 0.094s)
             Mean action noise std: 2.92
          Mean value_function loss: 305.7818
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 63.9520
                       Mean reward: 751.68
               Mean episode length: 212.25
    Episode_Reward/reaching_object: 1.3213
     Episode_Reward/lifting_object: 144.0857
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 2.09s
                      Time elapsed: 00:55:34
                               ETA: 00:21:10

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 45339 steps/s (collection: 2.025s, learning 0.144s)
             Mean action noise std: 2.92
          Mean value_function loss: 267.7227
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 63.9665
                       Mean reward: 728.37
               Mean episode length: 204.81
    Episode_Reward/reaching_object: 1.3300
     Episode_Reward/lifting_object: 145.3242
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 2.17s
                      Time elapsed: 00:55:36
                               ETA: 00:21:07

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 46425 steps/s (collection: 2.017s, learning 0.100s)
             Mean action noise std: 2.92
          Mean value_function loss: 308.5979
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 63.9736
                       Mean reward: 796.90
               Mean episode length: 221.97
    Episode_Reward/reaching_object: 1.3360
     Episode_Reward/lifting_object: 145.0675
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 2.12s
                      Time elapsed: 00:55:38
                               ETA: 00:21:05

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 45126 steps/s (collection: 2.070s, learning 0.109s)
             Mean action noise std: 2.92
          Mean value_function loss: 286.9859
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 63.9907
                       Mean reward: 779.38
               Mean episode length: 216.60
    Episode_Reward/reaching_object: 1.3718
     Episode_Reward/lifting_object: 150.1046
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 2.18s
                      Time elapsed: 00:55:41
                               ETA: 00:21:03

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 43657 steps/s (collection: 2.121s, learning 0.131s)
             Mean action noise std: 2.92
          Mean value_function loss: 266.2462
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 63.9984
                       Mean reward: 676.64
               Mean episode length: 193.50
    Episode_Reward/reaching_object: 1.3532
     Episode_Reward/lifting_object: 148.1531
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 2.25s
                      Time elapsed: 00:55:43
                               ETA: 00:21:00

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 42816 steps/s (collection: 2.144s, learning 0.152s)
             Mean action noise std: 2.93
          Mean value_function loss: 254.5784
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 64.0031
                       Mean reward: 786.16
               Mean episode length: 217.68
    Episode_Reward/reaching_object: 1.3896
     Episode_Reward/lifting_object: 151.6432
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 2.30s
                      Time elapsed: 00:55:45
                               ETA: 00:20:58

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 45640 steps/s (collection: 2.058s, learning 0.096s)
             Mean action noise std: 2.93
          Mean value_function loss: 260.8383
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 64.0048
                       Mean reward: 736.40
               Mean episode length: 210.72
    Episode_Reward/reaching_object: 1.3847
     Episode_Reward/lifting_object: 152.0617
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 2.15s
                      Time elapsed: 00:55:47
                               ETA: 00:20:56

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 45764 steps/s (collection: 2.011s, learning 0.137s)
             Mean action noise std: 2.93
          Mean value_function loss: 276.9671
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 64.0064
                       Mean reward: 740.84
               Mean episode length: 208.36
    Episode_Reward/reaching_object: 1.3458
     Episode_Reward/lifting_object: 148.0280
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 2.15s
                      Time elapsed: 00:55:49
                               ETA: 00:20:53

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 45251 steps/s (collection: 2.031s, learning 0.141s)
             Mean action noise std: 2.93
          Mean value_function loss: 312.9406
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 64.0080
                       Mean reward: 740.96
               Mean episode length: 208.79
    Episode_Reward/reaching_object: 1.3863
     Episode_Reward/lifting_object: 152.6105
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 2.17s
                      Time elapsed: 00:55:52
                               ETA: 00:20:51

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 43016 steps/s (collection: 2.148s, learning 0.137s)
             Mean action noise std: 2.93
          Mean value_function loss: 311.4656
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 64.0105
                       Mean reward: 761.18
               Mean episode length: 214.42
    Episode_Reward/reaching_object: 1.3501
     Episode_Reward/lifting_object: 147.9973
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 2.29s
                      Time elapsed: 00:55:54
                               ETA: 00:20:49

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 43901 steps/s (collection: 2.143s, learning 0.097s)
             Mean action noise std: 2.93
          Mean value_function loss: 286.8457
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 64.0147
                       Mean reward: 784.46
               Mean episode length: 218.24
    Episode_Reward/reaching_object: 1.3859
     Episode_Reward/lifting_object: 151.8842
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 2.24s
                      Time elapsed: 00:55:56
                               ETA: 00:20:46

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 45522 steps/s (collection: 2.052s, learning 0.108s)
             Mean action noise std: 2.93
          Mean value_function loss: 322.5880
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 64.0216
                       Mean reward: 769.25
               Mean episode length: 214.70
    Episode_Reward/reaching_object: 1.3894
     Episode_Reward/lifting_object: 151.7327
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 2.16s
                      Time elapsed: 00:55:58
                               ETA: 00:20:44

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 44884 steps/s (collection: 2.058s, learning 0.132s)
             Mean action noise std: 2.93
          Mean value_function loss: 320.6094
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 64.0298
                       Mean reward: 748.17
               Mean episode length: 212.32
    Episode_Reward/reaching_object: 1.3438
     Episode_Reward/lifting_object: 146.3463
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 2.19s
                      Time elapsed: 00:56:00
                               ETA: 00:20:42

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 46246 steps/s (collection: 2.003s, learning 0.122s)
             Mean action noise std: 2.93
          Mean value_function loss: 326.8504
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 64.0393
                       Mean reward: 705.24
               Mean episode length: 204.93
    Episode_Reward/reaching_object: 1.3137
     Episode_Reward/lifting_object: 142.0017
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 2.13s
                      Time elapsed: 00:56:03
                               ETA: 00:20:39

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 46807 steps/s (collection: 2.010s, learning 0.090s)
             Mean action noise std: 2.93
          Mean value_function loss: 311.9588
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 64.0469
                       Mean reward: 700.96
               Mean episode length: 201.25
    Episode_Reward/reaching_object: 1.3331
     Episode_Reward/lifting_object: 144.4622
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 2.10s
                      Time elapsed: 00:56:05
                               ETA: 00:20:37

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 46155 steps/s (collection: 2.031s, learning 0.099s)
             Mean action noise std: 2.93
          Mean value_function loss: 300.8339
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.0585
                       Mean reward: 767.73
               Mean episode length: 216.61
    Episode_Reward/reaching_object: 1.3708
     Episode_Reward/lifting_object: 148.9828
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 2.13s
                      Time elapsed: 00:56:07
                               ETA: 00:20:35

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 46531 steps/s (collection: 2.019s, learning 0.094s)
             Mean action noise std: 2.93
          Mean value_function loss: 274.1610
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 64.0652
                       Mean reward: 711.84
               Mean episode length: 200.94
    Episode_Reward/reaching_object: 1.3454
     Episode_Reward/lifting_object: 146.3651
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 2.11s
                      Time elapsed: 00:56:09
                               ETA: 00:20:32

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 46805 steps/s (collection: 2.011s, learning 0.089s)
             Mean action noise std: 2.93
          Mean value_function loss: 285.2912
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 64.0685
                       Mean reward: 743.37
               Mean episode length: 215.56
    Episode_Reward/reaching_object: 1.3417
     Episode_Reward/lifting_object: 145.3324
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 2.10s
                      Time elapsed: 00:56:11
                               ETA: 00:20:30

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 46881 steps/s (collection: 1.999s, learning 0.098s)
             Mean action noise std: 2.93
          Mean value_function loss: 291.0688
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 64.0747
                       Mean reward: 714.07
               Mean episode length: 206.93
    Episode_Reward/reaching_object: 1.3526
     Episode_Reward/lifting_object: 147.1060
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 2.10s
                      Time elapsed: 00:56:13
                               ETA: 00:20:28

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 47257 steps/s (collection: 1.994s, learning 0.086s)
             Mean action noise std: 2.94
          Mean value_function loss: 280.0081
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 64.0864
                       Mean reward: 773.31
               Mean episode length: 217.04
    Episode_Reward/reaching_object: 1.3406
     Episode_Reward/lifting_object: 146.1091
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 2.08s
                      Time elapsed: 00:56:15
                               ETA: 00:20:25

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 46568 steps/s (collection: 2.020s, learning 0.091s)
             Mean action noise std: 2.94
          Mean value_function loss: 269.2123
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 64.0999
                       Mean reward: 736.48
               Mean episode length: 207.75
    Episode_Reward/reaching_object: 1.4004
     Episode_Reward/lifting_object: 152.8992
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 2.11s
                      Time elapsed: 00:56:17
                               ETA: 00:20:23

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 45037 steps/s (collection: 2.033s, learning 0.150s)
             Mean action noise std: 2.94
          Mean value_function loss: 287.6604
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 64.1081
                       Mean reward: 754.23
               Mean episode length: 211.88
    Episode_Reward/reaching_object: 1.3786
     Episode_Reward/lifting_object: 150.5641
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 2.18s
                      Time elapsed: 00:56:19
                               ETA: 00:20:20

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 45411 steps/s (collection: 2.047s, learning 0.118s)
             Mean action noise std: 2.94
          Mean value_function loss: 310.9741
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 64.1149
                       Mean reward: 717.12
               Mean episode length: 206.45
    Episode_Reward/reaching_object: 1.3453
     Episode_Reward/lifting_object: 146.1094
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 2.16s
                      Time elapsed: 00:56:22
                               ETA: 00:20:18

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 46606 steps/s (collection: 2.009s, learning 0.101s)
             Mean action noise std: 2.94
          Mean value_function loss: 289.5769
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 64.1204
                       Mean reward: 723.22
               Mean episode length: 207.82
    Episode_Reward/reaching_object: 1.3234
     Episode_Reward/lifting_object: 142.9432
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 2.11s
                      Time elapsed: 00:56:24
                               ETA: 00:20:16

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 47087 steps/s (collection: 2.004s, learning 0.084s)
             Mean action noise std: 2.94
          Mean value_function loss: 269.8910
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 64.1249
                       Mean reward: 770.68
               Mean episode length: 214.35
    Episode_Reward/reaching_object: 1.3810
     Episode_Reward/lifting_object: 150.4118
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 2.09s
                      Time elapsed: 00:56:26
                               ETA: 00:20:13

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 47344 steps/s (collection: 1.988s, learning 0.088s)
             Mean action noise std: 2.94
          Mean value_function loss: 264.1326
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 64.1289
                       Mean reward: 697.99
               Mean episode length: 200.86
    Episode_Reward/reaching_object: 1.3570
     Episode_Reward/lifting_object: 146.4480
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 2.08s
                      Time elapsed: 00:56:28
                               ETA: 00:20:11

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 45890 steps/s (collection: 2.027s, learning 0.115s)
             Mean action noise std: 2.94
          Mean value_function loss: 235.8417
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 64.1331
                       Mean reward: 714.06
               Mean episode length: 208.90
    Episode_Reward/reaching_object: 1.3481
     Episode_Reward/lifting_object: 145.9756
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 2.14s
                      Time elapsed: 00:56:30
                               ETA: 00:20:09

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 45761 steps/s (collection: 2.039s, learning 0.109s)
             Mean action noise std: 2.94
          Mean value_function loss: 263.6105
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 64.1373
                       Mean reward: 731.92
               Mean episode length: 210.42
    Episode_Reward/reaching_object: 1.3749
     Episode_Reward/lifting_object: 148.9669
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 2.15s
                      Time elapsed: 00:56:32
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 46137 steps/s (collection: 2.044s, learning 0.087s)
             Mean action noise std: 2.94
          Mean value_function loss: 226.7925
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.1455
                       Mean reward: 703.11
               Mean episode length: 198.25
    Episode_Reward/reaching_object: 1.3756
     Episode_Reward/lifting_object: 150.8248
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 2.13s
                      Time elapsed: 00:56:34
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 45976 steps/s (collection: 2.019s, learning 0.119s)
             Mean action noise std: 2.94
          Mean value_function loss: 241.7621
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 64.1544
                       Mean reward: 839.86
               Mean episode length: 230.99
    Episode_Reward/reaching_object: 1.3816
     Episode_Reward/lifting_object: 151.1785
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 2.14s
                      Time elapsed: 00:56:36
                               ETA: 00:20:02

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 46704 steps/s (collection: 1.997s, learning 0.108s)
             Mean action noise std: 2.95
          Mean value_function loss: 243.6927
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 64.1633
                       Mean reward: 703.40
               Mean episode length: 204.97
    Episode_Reward/reaching_object: 1.3853
     Episode_Reward/lifting_object: 151.0331
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 2.10s
                      Time elapsed: 00:56:39
                               ETA: 00:19:59

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 46235 steps/s (collection: 2.025s, learning 0.101s)
             Mean action noise std: 2.95
          Mean value_function loss: 265.6514
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 64.1703
                       Mean reward: 823.18
               Mean episode length: 228.74
    Episode_Reward/reaching_object: 1.4079
     Episode_Reward/lifting_object: 154.6597
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 2.13s
                      Time elapsed: 00:56:41
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 46397 steps/s (collection: 2.026s, learning 0.093s)
             Mean action noise std: 2.95
          Mean value_function loss: 267.0687
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 64.1728
                       Mean reward: 748.87
               Mean episode length: 211.37
    Episode_Reward/reaching_object: 1.3585
     Episode_Reward/lifting_object: 147.8557
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 2.12s
                      Time elapsed: 00:56:43
                               ETA: 00:19:54

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 46918 steps/s (collection: 2.005s, learning 0.090s)
             Mean action noise std: 2.95
          Mean value_function loss: 265.2572
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 64.1771
                       Mean reward: 770.72
               Mean episode length: 218.87
    Episode_Reward/reaching_object: 1.3646
     Episode_Reward/lifting_object: 149.0704
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 2.10s
                      Time elapsed: 00:56:45
                               ETA: 00:19:52

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 46531 steps/s (collection: 2.017s, learning 0.096s)
             Mean action noise std: 2.95
          Mean value_function loss: 241.8951
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 64.1820
                       Mean reward: 735.03
               Mean episode length: 206.89
    Episode_Reward/reaching_object: 1.3872
     Episode_Reward/lifting_object: 152.2751
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 2.11s
                      Time elapsed: 00:56:47
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 46325 steps/s (collection: 2.018s, learning 0.105s)
             Mean action noise std: 2.95
          Mean value_function loss: 245.4305
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 64.1855
                       Mean reward: 743.65
               Mean episode length: 210.48
    Episode_Reward/reaching_object: 1.3816
     Episode_Reward/lifting_object: 151.6658
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 2.12s
                      Time elapsed: 00:56:49
                               ETA: 00:19:47

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 46636 steps/s (collection: 2.020s, learning 0.088s)
             Mean action noise std: 2.95
          Mean value_function loss: 263.1285
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 64.1884
                       Mean reward: 794.37
               Mean episode length: 220.88
    Episode_Reward/reaching_object: 1.3711
     Episode_Reward/lifting_object: 150.7560
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 2.11s
                      Time elapsed: 00:56:51
                               ETA: 00:19:45

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 46290 steps/s (collection: 2.023s, learning 0.101s)
             Mean action noise std: 2.95
          Mean value_function loss: 261.1895
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 64.1897
                       Mean reward: 746.15
               Mean episode length: 210.16
    Episode_Reward/reaching_object: 1.3515
     Episode_Reward/lifting_object: 148.0009
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 2.12s
                      Time elapsed: 00:56:53
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 45654 steps/s (collection: 2.059s, learning 0.095s)
             Mean action noise std: 2.95
          Mean value_function loss: 266.3977
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 64.1932
                       Mean reward: 750.94
               Mean episode length: 210.35
    Episode_Reward/reaching_object: 1.3472
     Episode_Reward/lifting_object: 147.1300
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 2.15s
                      Time elapsed: 00:56:56
                               ETA: 00:19:40

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 42032 steps/s (collection: 2.224s, learning 0.115s)
             Mean action noise std: 2.95
          Mean value_function loss: 272.8158
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.2016
                       Mean reward: 744.66
               Mean episode length: 211.95
    Episode_Reward/reaching_object: 1.3770
     Episode_Reward/lifting_object: 150.8181
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 2.34s
                      Time elapsed: 00:56:58
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 45777 steps/s (collection: 2.057s, learning 0.090s)
             Mean action noise std: 2.95
          Mean value_function loss: 262.4597
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 64.2148
                       Mean reward: 773.43
               Mean episode length: 215.93
    Episode_Reward/reaching_object: 1.3672
     Episode_Reward/lifting_object: 150.9595
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 2.15s
                      Time elapsed: 00:57:00
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 42755 steps/s (collection: 2.209s, learning 0.091s)
             Mean action noise std: 2.95
          Mean value_function loss: 293.5742
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 64.2252
                       Mean reward: 695.23
               Mean episode length: 201.13
    Episode_Reward/reaching_object: 1.3587
     Episode_Reward/lifting_object: 148.5641
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 2.30s
                      Time elapsed: 00:57:02
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 45912 steps/s (collection: 2.042s, learning 0.099s)
             Mean action noise std: 2.95
          Mean value_function loss: 285.1582
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 64.2340
                       Mean reward: 762.13
               Mean episode length: 213.15
    Episode_Reward/reaching_object: 1.3551
     Episode_Reward/lifting_object: 148.3683
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 2.14s
                      Time elapsed: 00:57:04
                               ETA: 00:19:31

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 44268 steps/s (collection: 2.114s, learning 0.107s)
             Mean action noise std: 2.96
          Mean value_function loss: 310.1426
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 64.2404
                       Mean reward: 787.06
               Mean episode length: 220.10
    Episode_Reward/reaching_object: 1.3707
     Episode_Reward/lifting_object: 150.5809
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 2.22s
                      Time elapsed: 00:57:07
                               ETA: 00:19:29

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 44956 steps/s (collection: 2.099s, learning 0.088s)
             Mean action noise std: 2.96
          Mean value_function loss: 259.3882
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 64.2468
                       Mean reward: 740.12
               Mean episode length: 205.11
    Episode_Reward/reaching_object: 1.3510
     Episode_Reward/lifting_object: 148.6868
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 2.19s
                      Time elapsed: 00:57:09
                               ETA: 00:19:26

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 44759 steps/s (collection: 2.046s, learning 0.151s)
             Mean action noise std: 2.96
          Mean value_function loss: 305.2599
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 64.2542
                       Mean reward: 667.18
               Mean episode length: 194.83
    Episode_Reward/reaching_object: 1.2858
     Episode_Reward/lifting_object: 139.5360
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 2.20s
                      Time elapsed: 00:57:11
                               ETA: 00:19:24

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 44755 steps/s (collection: 2.098s, learning 0.098s)
             Mean action noise std: 2.96
          Mean value_function loss: 255.4265
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.2667
                       Mean reward: 750.63
               Mean episode length: 214.99
    Episode_Reward/reaching_object: 1.3205
     Episode_Reward/lifting_object: 144.4851
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 2.20s
                      Time elapsed: 00:57:13
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 46860 steps/s (collection: 2.008s, learning 0.090s)
             Mean action noise std: 2.96
          Mean value_function loss: 272.0515
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 64.2780
                       Mean reward: 755.86
               Mean episode length: 212.00
    Episode_Reward/reaching_object: 1.3452
     Episode_Reward/lifting_object: 148.7146
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 2.10s
                      Time elapsed: 00:57:15
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 41374 steps/s (collection: 2.220s, learning 0.156s)
             Mean action noise std: 2.96
          Mean value_function loss: 300.3036
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 64.2854
                       Mean reward: 713.04
               Mean episode length: 205.24
    Episode_Reward/reaching_object: 1.3195
     Episode_Reward/lifting_object: 145.2249
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 2.38s
                      Time elapsed: 00:57:18
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 43673 steps/s (collection: 2.149s, learning 0.102s)
             Mean action noise std: 2.96
          Mean value_function loss: 305.0169
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 64.2932
                       Mean reward: 699.08
               Mean episode length: 202.76
    Episode_Reward/reaching_object: 1.3396
     Episode_Reward/lifting_object: 147.2715
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 2.25s
                      Time elapsed: 00:57:20
                               ETA: 00:19:15

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 44402 steps/s (collection: 2.104s, learning 0.110s)
             Mean action noise std: 2.96
          Mean value_function loss: 285.4434
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 64.3018
                       Mean reward: 801.90
               Mean episode length: 220.00
    Episode_Reward/reaching_object: 1.3651
     Episode_Reward/lifting_object: 151.8778
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 2.21s
                      Time elapsed: 00:57:22
                               ETA: 00:19:12

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 43524 steps/s (collection: 2.164s, learning 0.095s)
             Mean action noise std: 2.96
          Mean value_function loss: 257.4647
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 64.3064
                       Mean reward: 733.76
               Mean episode length: 213.43
    Episode_Reward/reaching_object: 1.3690
     Episode_Reward/lifting_object: 151.3715
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 2.26s
                      Time elapsed: 00:57:24
                               ETA: 00:19:10

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 39926 steps/s (collection: 2.345s, learning 0.117s)
             Mean action noise std: 2.97
          Mean value_function loss: 337.7791
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 64.3142
                       Mean reward: 777.02
               Mean episode length: 219.46
    Episode_Reward/reaching_object: 1.3349
     Episode_Reward/lifting_object: 146.5652
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 2.46s
                      Time elapsed: 00:57:27
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 43124 steps/s (collection: 2.170s, learning 0.110s)
             Mean action noise std: 2.97
          Mean value_function loss: 300.6367
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 64.3264
                       Mean reward: 787.08
               Mean episode length: 217.05
    Episode_Reward/reaching_object: 1.3965
     Episode_Reward/lifting_object: 154.9032
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 2.28s
                      Time elapsed: 00:57:29
                               ETA: 00:19:06

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 43958 steps/s (collection: 2.107s, learning 0.129s)
             Mean action noise std: 2.97
          Mean value_function loss: 289.6981
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.3354
                       Mean reward: 736.01
               Mean episode length: 207.26
    Episode_Reward/reaching_object: 1.3186
     Episode_Reward/lifting_object: 144.9536
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 2.24s
                      Time elapsed: 00:57:31
                               ETA: 00:19:03

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 42023 steps/s (collection: 2.212s, learning 0.127s)
             Mean action noise std: 2.97
          Mean value_function loss: 242.6331
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.3470
                       Mean reward: 755.69
               Mean episode length: 213.45
    Episode_Reward/reaching_object: 1.3425
     Episode_Reward/lifting_object: 148.1470
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 2.34s
                      Time elapsed: 00:57:34
                               ETA: 00:19:01

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 43700 steps/s (collection: 2.086s, learning 0.163s)
             Mean action noise std: 2.97
          Mean value_function loss: 277.9154
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 64.3599
                       Mean reward: 738.81
               Mean episode length: 204.57
    Episode_Reward/reaching_object: 1.3080
     Episode_Reward/lifting_object: 143.5419
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 2.25s
                      Time elapsed: 00:57:36
                               ETA: 00:18:59

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 43837 steps/s (collection: 2.134s, learning 0.109s)
             Mean action noise std: 2.97
          Mean value_function loss: 275.2053
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 64.3649
                       Mean reward: 759.08
               Mean episode length: 210.65
    Episode_Reward/reaching_object: 1.3599
     Episode_Reward/lifting_object: 149.6859
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 2.24s
                      Time elapsed: 00:57:38
                               ETA: 00:18:56

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 45179 steps/s (collection: 2.046s, learning 0.130s)
             Mean action noise std: 2.97
          Mean value_function loss: 260.1782
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 64.3717
                       Mean reward: 735.63
               Mean episode length: 207.79
    Episode_Reward/reaching_object: 1.3139
     Episode_Reward/lifting_object: 144.4850
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 2.18s
                      Time elapsed: 00:57:40
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 45468 steps/s (collection: 2.046s, learning 0.116s)
             Mean action noise std: 2.97
          Mean value_function loss: 293.9096
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.3788
                       Mean reward: 791.11
               Mean episode length: 217.29
    Episode_Reward/reaching_object: 1.3975
     Episode_Reward/lifting_object: 154.1852
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 2.16s
                      Time elapsed: 00:57:43
                               ETA: 00:18:52

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 44399 steps/s (collection: 2.067s, learning 0.147s)
             Mean action noise std: 2.97
          Mean value_function loss: 273.2106
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 64.3883
                       Mean reward: 752.08
               Mean episode length: 208.65
    Episode_Reward/reaching_object: 1.3851
     Episode_Reward/lifting_object: 153.2370
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 2.21s
                      Time elapsed: 00:57:45
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 45111 steps/s (collection: 2.066s, learning 0.114s)
             Mean action noise std: 2.98
          Mean value_function loss: 273.9420
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 64.3956
                       Mean reward: 738.70
               Mean episode length: 209.69
    Episode_Reward/reaching_object: 1.3262
     Episode_Reward/lifting_object: 144.9359
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 2.18s
                      Time elapsed: 00:57:47
                               ETA: 00:18:47

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 46765 steps/s (collection: 2.013s, learning 0.090s)
             Mean action noise std: 2.98
          Mean value_function loss: 229.1639
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 64.4052
                       Mean reward: 796.21
               Mean episode length: 224.11
    Episode_Reward/reaching_object: 1.3907
     Episode_Reward/lifting_object: 152.6328
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 2.10s
                      Time elapsed: 00:57:49
                               ETA: 00:18:45

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 46654 steps/s (collection: 2.017s, learning 0.091s)
             Mean action noise std: 2.98
          Mean value_function loss: 255.9707
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 64.4157
                       Mean reward: 677.19
               Mean episode length: 197.26
    Episode_Reward/reaching_object: 1.3665
     Episode_Reward/lifting_object: 149.8725
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 2.11s
                      Time elapsed: 00:57:51
                               ETA: 00:18:42

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 44778 steps/s (collection: 2.063s, learning 0.132s)
             Mean action noise std: 2.98
          Mean value_function loss: 297.6390
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 64.4247
                       Mean reward: 696.37
               Mean episode length: 200.80
    Episode_Reward/reaching_object: 1.3821
     Episode_Reward/lifting_object: 152.5524
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 2.20s
                      Time elapsed: 00:57:53
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 43056 steps/s (collection: 2.162s, learning 0.122s)
             Mean action noise std: 2.98
          Mean value_function loss: 289.0053
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 64.4297
                       Mean reward: 696.37
               Mean episode length: 201.10
    Episode_Reward/reaching_object: 1.3138
     Episode_Reward/lifting_object: 143.4670
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 2.28s
                      Time elapsed: 00:57:56
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 45240 steps/s (collection: 2.043s, learning 0.130s)
             Mean action noise std: 2.98
          Mean value_function loss: 210.7843
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 64.4452
                       Mean reward: 765.29
               Mean episode length: 214.99
    Episode_Reward/reaching_object: 1.4297
     Episode_Reward/lifting_object: 157.5140
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 2.17s
                      Time elapsed: 00:57:58
                               ETA: 00:18:35

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 45889 steps/s (collection: 2.042s, learning 0.101s)
             Mean action noise std: 2.98
          Mean value_function loss: 222.9108
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.4673
                       Mean reward: 739.45
               Mean episode length: 208.35
    Episode_Reward/reaching_object: 1.4034
     Episode_Reward/lifting_object: 154.7899
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 2.14s
                      Time elapsed: 00:58:00
                               ETA: 00:18:33

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 45370 steps/s (collection: 2.073s, learning 0.094s)
             Mean action noise std: 2.99
          Mean value_function loss: 288.1680
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 64.4801
                       Mean reward: 700.14
               Mean episode length: 202.84
    Episode_Reward/reaching_object: 1.3219
     Episode_Reward/lifting_object: 144.2529
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 2.17s
                      Time elapsed: 00:58:02
                               ETA: 00:18:31

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 46821 steps/s (collection: 1.997s, learning 0.103s)
             Mean action noise std: 2.99
          Mean value_function loss: 298.5655
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 64.4836
                       Mean reward: 769.11
               Mean episode length: 215.08
    Episode_Reward/reaching_object: 1.3681
     Episode_Reward/lifting_object: 150.0899
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 2.10s
                      Time elapsed: 00:58:04
                               ETA: 00:18:28

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 44168 steps/s (collection: 2.087s, learning 0.139s)
             Mean action noise std: 2.99
          Mean value_function loss: 309.3743
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 64.4865
                       Mean reward: 702.75
               Mean episode length: 201.35
    Episode_Reward/reaching_object: 1.3090
     Episode_Reward/lifting_object: 143.4695
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 2.23s
                      Time elapsed: 00:58:06
                               ETA: 00:18:26

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 46043 steps/s (collection: 2.043s, learning 0.092s)
             Mean action noise std: 2.99
          Mean value_function loss: 239.8329
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 64.4879
                       Mean reward: 821.19
               Mean episode length: 227.91
    Episode_Reward/reaching_object: 1.3547
     Episode_Reward/lifting_object: 149.1213
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 2.14s
                      Time elapsed: 00:58:09
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 45274 steps/s (collection: 2.057s, learning 0.114s)
             Mean action noise std: 2.99
          Mean value_function loss: 262.3608
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.4908
                       Mean reward: 735.01
               Mean episode length: 213.25
    Episode_Reward/reaching_object: 1.3848
     Episode_Reward/lifting_object: 152.0963
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 2.17s
                      Time elapsed: 00:58:11
                               ETA: 00:18:21

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 46159 steps/s (collection: 2.015s, learning 0.115s)
             Mean action noise std: 2.99
          Mean value_function loss: 251.5978
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 64.4989
                       Mean reward: 763.38
               Mean episode length: 213.44
    Episode_Reward/reaching_object: 1.3642
     Episode_Reward/lifting_object: 150.3656
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 2.13s
                      Time elapsed: 00:58:13
                               ETA: 00:18:19

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 46585 steps/s (collection: 2.021s, learning 0.090s)
             Mean action noise std: 2.99
          Mean value_function loss: 251.4625
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 64.5093
                       Mean reward: 724.07
               Mean episode length: 208.96
    Episode_Reward/reaching_object: 1.3537
     Episode_Reward/lifting_object: 149.2511
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 2.11s
                      Time elapsed: 00:58:15
                               ETA: 00:18:17

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 45279 steps/s (collection: 2.072s, learning 0.099s)
             Mean action noise std: 2.99
          Mean value_function loss: 284.0420
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 64.5216
                       Mean reward: 753.41
               Mean episode length: 209.21
    Episode_Reward/reaching_object: 1.3367
     Episode_Reward/lifting_object: 148.2903
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.0833
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 2.17s
                      Time elapsed: 00:58:17
                               ETA: 00:18:14

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 45324 steps/s (collection: 2.074s, learning 0.095s)
             Mean action noise std: 2.99
          Mean value_function loss: 257.0003
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 64.5287
                       Mean reward: 742.85
               Mean episode length: 213.07
    Episode_Reward/reaching_object: 1.3457
     Episode_Reward/lifting_object: 148.3347
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 2.17s
                      Time elapsed: 00:58:19
                               ETA: 00:18:12

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 46044 steps/s (collection: 2.043s, learning 0.092s)
             Mean action noise std: 2.99
          Mean value_function loss: 225.6804
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 64.5399
                       Mean reward: 764.92
               Mean episode length: 213.82
    Episode_Reward/reaching_object: 1.3593
     Episode_Reward/lifting_object: 149.3141
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 2.13s
                      Time elapsed: 00:58:21
                               ETA: 00:18:10

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 45811 steps/s (collection: 2.045s, learning 0.101s)
             Mean action noise std: 3.00
          Mean value_function loss: 280.1125
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.5537
                       Mean reward: 750.45
               Mean episode length: 209.46
    Episode_Reward/reaching_object: 1.3925
     Episode_Reward/lifting_object: 152.7559
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 2.15s
                      Time elapsed: 00:58:24
                               ETA: 00:18:07

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 46657 steps/s (collection: 2.008s, learning 0.099s)
             Mean action noise std: 3.00
          Mean value_function loss: 268.3135
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 64.5652
                       Mean reward: 776.04
               Mean episode length: 214.44
    Episode_Reward/reaching_object: 1.3922
     Episode_Reward/lifting_object: 153.1529
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 2.11s
                      Time elapsed: 00:58:26
                               ETA: 00:18:05

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 45478 steps/s (collection: 2.066s, learning 0.096s)
             Mean action noise std: 3.00
          Mean value_function loss: 258.2897
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 64.5714
                       Mean reward: 757.51
               Mean episode length: 213.22
    Episode_Reward/reaching_object: 1.3829
     Episode_Reward/lifting_object: 152.0205
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 2.16s
                      Time elapsed: 00:58:28
                               ETA: 00:18:03

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 45956 steps/s (collection: 2.013s, learning 0.126s)
             Mean action noise std: 3.00
          Mean value_function loss: 237.8978
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 64.5800
                       Mean reward: 786.19
               Mean episode length: 223.02
    Episode_Reward/reaching_object: 1.4252
     Episode_Reward/lifting_object: 156.9118
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 2.14s
                      Time elapsed: 00:58:30
                               ETA: 00:18:00

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 42408 steps/s (collection: 2.218s, learning 0.100s)
             Mean action noise std: 3.00
          Mean value_function loss: 288.2606
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 64.5895
                       Mean reward: 728.58
               Mean episode length: 205.14
    Episode_Reward/reaching_object: 1.3840
     Episode_Reward/lifting_object: 152.2213
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 2.32s
                      Time elapsed: 00:58:32
                               ETA: 00:17:58

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 45554 steps/s (collection: 2.068s, learning 0.090s)
             Mean action noise std: 3.00
          Mean value_function loss: 257.7341
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 64.5921
                       Mean reward: 765.81
               Mean episode length: 216.30
    Episode_Reward/reaching_object: 1.3941
     Episode_Reward/lifting_object: 153.0208
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 2.16s
                      Time elapsed: 00:58:35
                               ETA: 00:17:56

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 46260 steps/s (collection: 2.039s, learning 0.086s)
             Mean action noise std: 3.00
          Mean value_function loss: 209.0791
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 64.5930
                       Mean reward: 772.12
               Mean episode length: 215.30
    Episode_Reward/reaching_object: 1.4089
     Episode_Reward/lifting_object: 154.0555
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 2.13s
                      Time elapsed: 00:58:37
                               ETA: 00:17:53

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 45722 steps/s (collection: 2.052s, learning 0.098s)
             Mean action noise std: 3.00
          Mean value_function loss: 248.8019
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 64.5966
                       Mean reward: 742.71
               Mean episode length: 208.15
    Episode_Reward/reaching_object: 1.4086
     Episode_Reward/lifting_object: 154.0862
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 2.15s
                      Time elapsed: 00:58:39
                               ETA: 00:17:51

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 45165 steps/s (collection: 2.075s, learning 0.102s)
             Mean action noise std: 3.00
          Mean value_function loss: 260.9536
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 64.6015
                       Mean reward: 715.98
               Mean episode length: 207.42
    Episode_Reward/reaching_object: 1.3968
     Episode_Reward/lifting_object: 153.0023
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 2.18s
                      Time elapsed: 00:58:41
                               ETA: 00:17:49

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 43971 steps/s (collection: 2.087s, learning 0.149s)
             Mean action noise std: 3.00
          Mean value_function loss: 285.4128
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 64.6063
                       Mean reward: 781.37
               Mean episode length: 220.94
    Episode_Reward/reaching_object: 1.4094
     Episode_Reward/lifting_object: 154.5258
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 2.24s
                      Time elapsed: 00:58:43
                               ETA: 00:17:46

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 44576 steps/s (collection: 2.115s, learning 0.090s)
             Mean action noise std: 3.00
          Mean value_function loss: 242.8323
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 64.6145
                       Mean reward: 749.95
               Mean episode length: 209.31
    Episode_Reward/reaching_object: 1.3786
     Episode_Reward/lifting_object: 151.5847
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 2.21s
                      Time elapsed: 00:58:45
                               ETA: 00:17:44

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 44663 steps/s (collection: 2.105s, learning 0.096s)
             Mean action noise std: 3.00
          Mean value_function loss: 215.1496
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 64.6194
                       Mean reward: 782.49
               Mean episode length: 218.54
    Episode_Reward/reaching_object: 1.3624
     Episode_Reward/lifting_object: 149.4893
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 2.20s
                      Time elapsed: 00:58:48
                               ETA: 00:17:42

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 46096 steps/s (collection: 2.043s, learning 0.089s)
             Mean action noise std: 3.01
          Mean value_function loss: 266.3113
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.6278
                       Mean reward: 729.14
               Mean episode length: 209.31
    Episode_Reward/reaching_object: 1.3896
     Episode_Reward/lifting_object: 152.1753
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 2.13s
                      Time elapsed: 00:58:50
                               ETA: 00:17:39

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 45309 steps/s (collection: 2.050s, learning 0.120s)
             Mean action noise std: 3.01
          Mean value_function loss: 249.7540
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 64.6354
                       Mean reward: 791.65
               Mean episode length: 219.46
    Episode_Reward/reaching_object: 1.4224
     Episode_Reward/lifting_object: 156.3442
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 2.17s
                      Time elapsed: 00:58:52
                               ETA: 00:17:37

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 42565 steps/s (collection: 2.207s, learning 0.103s)
             Mean action noise std: 3.01
          Mean value_function loss: 236.8266
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 64.6384
                       Mean reward: 870.39
               Mean episode length: 236.68
    Episode_Reward/reaching_object: 1.4116
     Episode_Reward/lifting_object: 155.1553
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 2.31s
                      Time elapsed: 00:58:54
                               ETA: 00:17:35

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 43691 steps/s (collection: 2.149s, learning 0.101s)
             Mean action noise std: 3.01
          Mean value_function loss: 221.9912
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 64.6404
                       Mean reward: 754.66
               Mean episode length: 212.58
    Episode_Reward/reaching_object: 1.4087
     Episode_Reward/lifting_object: 154.9780
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 2.25s
                      Time elapsed: 00:58:56
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 43496 steps/s (collection: 2.137s, learning 0.123s)
             Mean action noise std: 3.01
          Mean value_function loss: 243.0837
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 64.6436
                       Mean reward: 707.41
               Mean episode length: 201.39
    Episode_Reward/reaching_object: 1.4040
     Episode_Reward/lifting_object: 154.1291
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 2.26s
                      Time elapsed: 00:58:59
                               ETA: 00:17:30

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 41199 steps/s (collection: 2.192s, learning 0.194s)
             Mean action noise std: 3.01
          Mean value_function loss: 228.2473
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 64.6501
                       Mean reward: 805.14
               Mean episode length: 222.88
    Episode_Reward/reaching_object: 1.3373
     Episode_Reward/lifting_object: 147.6044
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 2.39s
                      Time elapsed: 00:59:01
                               ETA: 00:17:28

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 44735 steps/s (collection: 2.091s, learning 0.106s)
             Mean action noise std: 3.01
          Mean value_function loss: 248.5079
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.6563
                       Mean reward: 809.16
               Mean episode length: 224.85
    Episode_Reward/reaching_object: 1.4175
     Episode_Reward/lifting_object: 156.6692
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 2.20s
                      Time elapsed: 00:59:03
                               ETA: 00:17:25

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 42836 steps/s (collection: 2.135s, learning 0.160s)
             Mean action noise std: 3.01
          Mean value_function loss: 259.6997
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 64.6669
                       Mean reward: 772.97
               Mean episode length: 215.46
    Episode_Reward/reaching_object: 1.3616
     Episode_Reward/lifting_object: 150.0060
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 2.29s
                      Time elapsed: 00:59:06
                               ETA: 00:17:23

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 43079 steps/s (collection: 2.180s, learning 0.102s)
             Mean action noise std: 3.01
          Mean value_function loss: 256.5469
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 64.6702
                       Mean reward: 765.15
               Mean episode length: 213.74
    Episode_Reward/reaching_object: 1.3420
     Episode_Reward/lifting_object: 146.6183
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 2.28s
                      Time elapsed: 00:59:08
                               ETA: 00:17:21

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 42929 steps/s (collection: 2.117s, learning 0.173s)
             Mean action noise std: 3.01
          Mean value_function loss: 262.7768
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 64.6733
                       Mean reward: 821.68
               Mean episode length: 227.53
    Episode_Reward/reaching_object: 1.3974
     Episode_Reward/lifting_object: 153.5298
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 2.29s
                      Time elapsed: 00:59:10
                               ETA: 00:17:19

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 40587 steps/s (collection: 2.312s, learning 0.110s)
             Mean action noise std: 3.01
          Mean value_function loss: 264.0984
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 64.6779
                       Mean reward: 794.93
               Mean episode length: 221.48
    Episode_Reward/reaching_object: 1.3903
     Episode_Reward/lifting_object: 153.8392
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 2.42s
                      Time elapsed: 00:59:13
                               ETA: 00:17:16

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 45080 steps/s (collection: 2.092s, learning 0.089s)
             Mean action noise std: 3.01
          Mean value_function loss: 248.0947
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 64.6858
                       Mean reward: 829.98
               Mean episode length: 224.36
    Episode_Reward/reaching_object: 1.4086
     Episode_Reward/lifting_object: 155.3185
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 2.18s
                      Time elapsed: 00:59:15
                               ETA: 00:17:14

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 41422 steps/s (collection: 2.265s, learning 0.108s)
             Mean action noise std: 3.02
          Mean value_function loss: 260.0040
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 64.7012
                       Mean reward: 730.35
               Mean episode length: 208.09
    Episode_Reward/reaching_object: 1.3201
     Episode_Reward/lifting_object: 144.8766
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 2.37s
                      Time elapsed: 00:59:17
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 43213 steps/s (collection: 2.168s, learning 0.107s)
             Mean action noise std: 3.02
          Mean value_function loss: 222.4174
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 64.7213
                       Mean reward: 770.41
               Mean episode length: 216.06
    Episode_Reward/reaching_object: 1.4254
     Episode_Reward/lifting_object: 157.2862
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 2.27s
                      Time elapsed: 00:59:19
                               ETA: 00:17:09

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 43519 steps/s (collection: 2.132s, learning 0.127s)
             Mean action noise std: 3.02
          Mean value_function loss: 243.8464
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 64.7399
                       Mean reward: 745.33
               Mean episode length: 208.24
    Episode_Reward/reaching_object: 1.4128
     Episode_Reward/lifting_object: 155.6723
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 2.26s
                      Time elapsed: 00:59:22
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 43836 steps/s (collection: 2.142s, learning 0.101s)
             Mean action noise std: 3.02
          Mean value_function loss: 213.9294
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.7460
                       Mean reward: 725.11
               Mean episode length: 209.08
    Episode_Reward/reaching_object: 1.3491
     Episode_Reward/lifting_object: 148.5213
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 2.24s
                      Time elapsed: 00:59:24
                               ETA: 00:17:05

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 42444 steps/s (collection: 2.176s, learning 0.140s)
             Mean action noise std: 3.02
          Mean value_function loss: 201.6025
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.7560
                       Mean reward: 774.00
               Mean episode length: 218.20
    Episode_Reward/reaching_object: 1.4048
     Episode_Reward/lifting_object: 153.8415
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 2.32s
                      Time elapsed: 00:59:26
                               ETA: 00:17:03

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 43667 steps/s (collection: 2.156s, learning 0.095s)
             Mean action noise std: 3.02
          Mean value_function loss: 253.4055
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 64.7657
                       Mean reward: 749.48
               Mean episode length: 211.15
    Episode_Reward/reaching_object: 1.4074
     Episode_Reward/lifting_object: 155.0998
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 2.25s
                      Time elapsed: 00:59:29
                               ETA: 00:17:00

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 44928 steps/s (collection: 2.085s, learning 0.103s)
             Mean action noise std: 3.02
          Mean value_function loss: 247.8465
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 64.7731
                       Mean reward: 769.34
               Mean episode length: 215.95
    Episode_Reward/reaching_object: 1.4151
     Episode_Reward/lifting_object: 156.3445
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 2.19s
                      Time elapsed: 00:59:31
                               ETA: 00:16:58

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 44060 steps/s (collection: 2.122s, learning 0.110s)
             Mean action noise std: 3.03
          Mean value_function loss: 235.7932
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 64.7847
                       Mean reward: 821.26
               Mean episode length: 227.31
    Episode_Reward/reaching_object: 1.3967
     Episode_Reward/lifting_object: 153.5506
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 2.23s
                      Time elapsed: 00:59:33
                               ETA: 00:16:56

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 42521 steps/s (collection: 2.203s, learning 0.109s)
             Mean action noise std: 3.03
          Mean value_function loss: 228.0998
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 64.8022
                       Mean reward: 752.89
               Mean episode length: 213.01
    Episode_Reward/reaching_object: 1.3862
     Episode_Reward/lifting_object: 153.1009
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 2.31s
                      Time elapsed: 00:59:35
                               ETA: 00:16:53

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 44508 steps/s (collection: 2.103s, learning 0.106s)
             Mean action noise std: 3.03
          Mean value_function loss: 241.0202
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 64.8185
                       Mean reward: 761.81
               Mean episode length: 213.34
    Episode_Reward/reaching_object: 1.4149
     Episode_Reward/lifting_object: 155.7453
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 2.21s
                      Time elapsed: 00:59:37
                               ETA: 00:16:51

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 45230 steps/s (collection: 2.075s, learning 0.098s)
             Mean action noise std: 3.03
          Mean value_function loss: 219.9221
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 64.8272
                       Mean reward: 795.07
               Mean episode length: 222.80
    Episode_Reward/reaching_object: 1.4605
     Episode_Reward/lifting_object: 161.6909
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 2.17s
                      Time elapsed: 00:59:40
                               ETA: 00:16:49

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 44960 steps/s (collection: 2.093s, learning 0.093s)
             Mean action noise std: 3.03
          Mean value_function loss: 254.7354
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 64.8291
                       Mean reward: 783.80
               Mean episode length: 217.19
    Episode_Reward/reaching_object: 1.3775
     Episode_Reward/lifting_object: 150.7785
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 2.19s
                      Time elapsed: 00:59:42
                               ETA: 00:16:46

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 44618 steps/s (collection: 2.101s, learning 0.103s)
             Mean action noise std: 3.03
          Mean value_function loss: 251.9291
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 64.8319
                       Mean reward: 744.04
               Mean episode length: 209.22
    Episode_Reward/reaching_object: 1.3980
     Episode_Reward/lifting_object: 154.1393
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 2.20s
                      Time elapsed: 00:59:44
                               ETA: 00:16:44

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 42389 steps/s (collection: 2.203s, learning 0.116s)
             Mean action noise std: 3.03
          Mean value_function loss: 233.3563
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 64.8361
                       Mean reward: 792.97
               Mean episode length: 218.98
    Episode_Reward/reaching_object: 1.3950
     Episode_Reward/lifting_object: 152.8530
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 2.32s
                      Time elapsed: 00:59:46
                               ETA: 00:16:42

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 45147 steps/s (collection: 2.079s, learning 0.098s)
             Mean action noise std: 3.03
          Mean value_function loss: 216.2744
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 64.8472
                       Mean reward: 826.91
               Mean episode length: 230.23
    Episode_Reward/reaching_object: 1.4228
     Episode_Reward/lifting_object: 156.1436
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 2.18s
                      Time elapsed: 00:59:49
                               ETA: 00:16:39

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 44894 steps/s (collection: 2.094s, learning 0.096s)
             Mean action noise std: 3.03
          Mean value_function loss: 287.4594
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 64.8582
                       Mean reward: 780.49
               Mean episode length: 216.18
    Episode_Reward/reaching_object: 1.3857
     Episode_Reward/lifting_object: 151.6106
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 2.19s
                      Time elapsed: 00:59:51
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 43488 steps/s (collection: 2.105s, learning 0.156s)
             Mean action noise std: 3.04
          Mean value_function loss: 231.1290
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 64.8620
                       Mean reward: 795.28
               Mean episode length: 221.59
    Episode_Reward/reaching_object: 1.4086
     Episode_Reward/lifting_object: 154.3658
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 2.26s
                      Time elapsed: 00:59:53
                               ETA: 00:16:35

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 41840 steps/s (collection: 2.237s, learning 0.113s)
             Mean action noise std: 3.04
          Mean value_function loss: 252.0352
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 64.8682
                       Mean reward: 793.81
               Mean episode length: 221.89
    Episode_Reward/reaching_object: 1.4182
     Episode_Reward/lifting_object: 155.3844
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 2.35s
                      Time elapsed: 00:59:55
                               ETA: 00:16:32

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 44535 steps/s (collection: 2.107s, learning 0.100s)
             Mean action noise std: 3.04
          Mean value_function loss: 270.7696
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 64.8796
                       Mean reward: 750.08
               Mean episode length: 213.69
    Episode_Reward/reaching_object: 1.4091
     Episode_Reward/lifting_object: 154.2919
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 2.21s
                      Time elapsed: 00:59:58
                               ETA: 00:16:30

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 45059 steps/s (collection: 2.086s, learning 0.095s)
             Mean action noise std: 3.04
          Mean value_function loss: 264.5769
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 64.8904
                       Mean reward: 760.56
               Mean episode length: 212.41
    Episode_Reward/reaching_object: 1.4257
     Episode_Reward/lifting_object: 156.8726
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 2.18s
                      Time elapsed: 01:00:00
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 42770 steps/s (collection: 2.208s, learning 0.091s)
             Mean action noise std: 3.04
          Mean value_function loss: 216.0065
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.8959
                       Mean reward: 797.92
               Mean episode length: 220.69
    Episode_Reward/reaching_object: 1.4219
     Episode_Reward/lifting_object: 155.8900
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 2.30s
                      Time elapsed: 01:00:02
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 45303 steps/s (collection: 2.079s, learning 0.091s)
             Mean action noise std: 3.04
          Mean value_function loss: 236.3436
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 64.9059
                       Mean reward: 836.93
               Mean episode length: 230.14
    Episode_Reward/reaching_object: 1.4366
     Episode_Reward/lifting_object: 157.7230
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 2.17s
                      Time elapsed: 01:00:04
                               ETA: 00:16:23

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 45561 steps/s (collection: 2.066s, learning 0.092s)
             Mean action noise std: 3.04
          Mean value_function loss: 253.7070
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.9169
                       Mean reward: 810.47
               Mean episode length: 228.45
    Episode_Reward/reaching_object: 1.4223
     Episode_Reward/lifting_object: 155.4855
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 2.16s
                      Time elapsed: 01:00:06
                               ETA: 00:16:21

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 44021 steps/s (collection: 2.135s, learning 0.099s)
             Mean action noise std: 3.04
          Mean value_function loss: 239.5213
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 64.9216
                       Mean reward: 784.73
               Mean episode length: 219.82
    Episode_Reward/reaching_object: 1.4278
     Episode_Reward/lifting_object: 157.3300
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 2.23s
                      Time elapsed: 01:00:09
                               ETA: 00:16:19

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 41919 steps/s (collection: 2.226s, learning 0.119s)
             Mean action noise std: 3.04
          Mean value_function loss: 257.0499
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 64.9266
                       Mean reward: 723.58
               Mean episode length: 209.71
    Episode_Reward/reaching_object: 1.4019
     Episode_Reward/lifting_object: 153.7020
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 2.35s
                      Time elapsed: 01:00:11
                               ETA: 00:16:16

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 42513 steps/s (collection: 2.193s, learning 0.120s)
             Mean action noise std: 3.04
          Mean value_function loss: 236.1993
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 64.9307
                       Mean reward: 731.08
               Mean episode length: 209.69
    Episode_Reward/reaching_object: 1.4125
     Episode_Reward/lifting_object: 154.1846
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 2.31s
                      Time elapsed: 01:00:13
                               ETA: 00:16:14

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 44579 steps/s (collection: 2.106s, learning 0.099s)
             Mean action noise std: 3.05
          Mean value_function loss: 236.0692
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 64.9380
                       Mean reward: 801.01
               Mean episode length: 221.41
    Episode_Reward/reaching_object: 1.4205
     Episode_Reward/lifting_object: 154.9988
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 2.21s
                      Time elapsed: 01:00:15
                               ETA: 00:16:12

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 43900 steps/s (collection: 2.120s, learning 0.119s)
             Mean action noise std: 3.05
          Mean value_function loss: 221.5598
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 64.9456
                       Mean reward: 733.30
               Mean episode length: 212.79
    Episode_Reward/reaching_object: 1.3785
     Episode_Reward/lifting_object: 150.5136
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 2.24s
                      Time elapsed: 01:00:18
                               ETA: 00:16:09

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 44876 steps/s (collection: 2.080s, learning 0.111s)
             Mean action noise std: 3.05
          Mean value_function loss: 259.1735
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 64.9506
                       Mean reward: 773.13
               Mean episode length: 220.69
    Episode_Reward/reaching_object: 1.4313
     Episode_Reward/lifting_object: 156.1964
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 2.19s
                      Time elapsed: 01:00:20
                               ETA: 00:16:07

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 44438 steps/s (collection: 2.070s, learning 0.142s)
             Mean action noise std: 3.05
          Mean value_function loss: 244.7746
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 64.9578
                       Mean reward: 753.47
               Mean episode length: 208.94
    Episode_Reward/reaching_object: 1.4194
     Episode_Reward/lifting_object: 155.0967
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.21s
                      Time elapsed: 01:00:22
                               ETA: 00:16:05

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 45526 steps/s (collection: 2.063s, learning 0.097s)
             Mean action noise std: 3.05
          Mean value_function loss: 243.5767
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.9640
                       Mean reward: 796.20
               Mean episode length: 223.47
    Episode_Reward/reaching_object: 1.4343
     Episode_Reward/lifting_object: 156.8185
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 2.16s
                      Time elapsed: 01:00:24
                               ETA: 00:16:02

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 44300 steps/s (collection: 2.099s, learning 0.120s)
             Mean action noise std: 3.05
          Mean value_function loss: 279.3652
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 64.9742
                       Mean reward: 767.06
               Mean episode length: 214.08
    Episode_Reward/reaching_object: 1.3712
     Episode_Reward/lifting_object: 149.1174
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 2.22s
                      Time elapsed: 01:00:26
                               ETA: 00:16:00

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 44269 steps/s (collection: 2.101s, learning 0.119s)
             Mean action noise std: 3.05
          Mean value_function loss: 267.6750
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 64.9863
                       Mean reward: 872.92
               Mean episode length: 237.67
    Episode_Reward/reaching_object: 1.4144
     Episode_Reward/lifting_object: 153.3956
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 2.22s
                      Time elapsed: 01:00:29
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 42759 steps/s (collection: 2.190s, learning 0.109s)
             Mean action noise std: 3.05
          Mean value_function loss: 240.1667
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 64.9927
                       Mean reward: 798.04
               Mean episode length: 222.90
    Episode_Reward/reaching_object: 1.4074
     Episode_Reward/lifting_object: 152.8308
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 2.30s
                      Time elapsed: 01:00:31
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 41954 steps/s (collection: 2.212s, learning 0.131s)
             Mean action noise std: 3.05
          Mean value_function loss: 241.9805
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 64.9968
                       Mean reward: 766.69
               Mean episode length: 215.78
    Episode_Reward/reaching_object: 1.4108
     Episode_Reward/lifting_object: 153.3661
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 2.34s
                      Time elapsed: 01:00:33
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 41019 steps/s (collection: 2.259s, learning 0.138s)
             Mean action noise std: 3.05
          Mean value_function loss: 274.0046
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 65.0016
                       Mean reward: 719.88
               Mean episode length: 208.56
    Episode_Reward/reaching_object: 1.3644
     Episode_Reward/lifting_object: 148.6568
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 2.40s
                      Time elapsed: 01:00:36
                               ETA: 00:15:51

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 44357 steps/s (collection: 2.124s, learning 0.092s)
             Mean action noise std: 3.05
          Mean value_function loss: 254.0087
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 65.0048
                       Mean reward: 777.75
               Mean episode length: 221.63
    Episode_Reward/reaching_object: 1.4443
     Episode_Reward/lifting_object: 158.3686
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 2.22s
                      Time elapsed: 01:00:38
                               ETA: 00:15:49

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 44489 steps/s (collection: 2.112s, learning 0.098s)
             Mean action noise std: 3.05
          Mean value_function loss: 228.5901
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 65.0059
                       Mean reward: 782.16
               Mean episode length: 221.57
    Episode_Reward/reaching_object: 1.3709
     Episode_Reward/lifting_object: 149.9462
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 2.21s
                      Time elapsed: 01:00:40
                               ETA: 00:15:46

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 41621 steps/s (collection: 2.199s, learning 0.163s)
             Mean action noise std: 3.05
          Mean value_function loss: 231.4551
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 65.0078
                       Mean reward: 686.70
               Mean episode length: 196.58
    Episode_Reward/reaching_object: 1.3853
     Episode_Reward/lifting_object: 151.1873
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 2.36s
                      Time elapsed: 01:00:42
                               ETA: 00:15:44

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 39142 steps/s (collection: 2.247s, learning 0.265s)
             Mean action noise std: 3.05
          Mean value_function loss: 244.5982
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 65.0105
                       Mean reward: 815.09
               Mean episode length: 225.38
    Episode_Reward/reaching_object: 1.4331
     Episode_Reward/lifting_object: 157.4690
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 2.51s
                      Time elapsed: 01:00:45
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 37586 steps/s (collection: 2.442s, learning 0.174s)
             Mean action noise std: 3.06
          Mean value_function loss: 251.0597
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.0188
                       Mean reward: 776.11
               Mean episode length: 215.71
    Episode_Reward/reaching_object: 1.4051
     Episode_Reward/lifting_object: 153.5782
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.62s
                      Time elapsed: 01:00:48
                               ETA: 00:15:40

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 39850 steps/s (collection: 2.316s, learning 0.151s)
             Mean action noise std: 3.06
          Mean value_function loss: 233.1495
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 65.0328
                       Mean reward: 802.17
               Mean episode length: 222.38
    Episode_Reward/reaching_object: 1.4170
     Episode_Reward/lifting_object: 155.7515
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.47s
                      Time elapsed: 01:00:50
                               ETA: 00:15:37

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 38151 steps/s (collection: 2.424s, learning 0.153s)
             Mean action noise std: 3.06
          Mean value_function loss: 214.2565
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 65.0411
                       Mean reward: 780.92
               Mean episode length: 218.50
    Episode_Reward/reaching_object: 1.4279
     Episode_Reward/lifting_object: 157.7310
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 2.58s
                      Time elapsed: 01:00:53
                               ETA: 00:15:35

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 40139 steps/s (collection: 2.280s, learning 0.169s)
             Mean action noise std: 3.06
          Mean value_function loss: 250.4326
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 65.0446
                       Mean reward: 789.44
               Mean episode length: 220.06
    Episode_Reward/reaching_object: 1.4467
     Episode_Reward/lifting_object: 159.9836
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.45s
                      Time elapsed: 01:00:55
                               ETA: 00:15:33

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 42004 steps/s (collection: 2.231s, learning 0.109s)
             Mean action noise std: 3.06
          Mean value_function loss: 222.0454
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 65.0488
                       Mean reward: 752.98
               Mean episode length: 212.94
    Episode_Reward/reaching_object: 1.3660
     Episode_Reward/lifting_object: 149.7146
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 2.34s
                      Time elapsed: 01:00:57
                               ETA: 00:15:31

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 43627 steps/s (collection: 2.132s, learning 0.121s)
             Mean action noise std: 3.06
          Mean value_function loss: 203.1385
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 65.0583
                       Mean reward: 817.10
               Mean episode length: 220.52
    Episode_Reward/reaching_object: 1.4396
     Episode_Reward/lifting_object: 159.4977
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.25s
                      Time elapsed: 01:01:00
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 44526 steps/s (collection: 2.107s, learning 0.101s)
             Mean action noise std: 3.06
          Mean value_function loss: 219.0293
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 65.0674
                       Mean reward: 825.62
               Mean episode length: 228.28
    Episode_Reward/reaching_object: 1.4234
     Episode_Reward/lifting_object: 156.4252
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.21s
                      Time elapsed: 01:01:02
                               ETA: 00:15:26

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 44700 steps/s (collection: 2.095s, learning 0.104s)
             Mean action noise std: 3.06
          Mean value_function loss: 246.2482
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 65.0855
                       Mean reward: 815.24
               Mean episode length: 225.05
    Episode_Reward/reaching_object: 1.4127
     Episode_Reward/lifting_object: 155.4103
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 2.20s
                      Time elapsed: 01:01:04
                               ETA: 00:15:24

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 43982 steps/s (collection: 2.103s, learning 0.133s)
             Mean action noise std: 3.07
          Mean value_function loss: 223.7567
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 65.1011
                       Mean reward: 745.67
               Mean episode length: 209.77
    Episode_Reward/reaching_object: 1.4068
     Episode_Reward/lifting_object: 154.7793
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.24s
                      Time elapsed: 01:01:06
                               ETA: 00:15:21

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 44089 steps/s (collection: 2.119s, learning 0.111s)
             Mean action noise std: 3.07
          Mean value_function loss: 247.8969
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 65.1063
                       Mean reward: 730.63
               Mean episode length: 209.38
    Episode_Reward/reaching_object: 1.3981
     Episode_Reward/lifting_object: 153.1939
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.23s
                      Time elapsed: 01:01:09
                               ETA: 00:15:19

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 43571 steps/s (collection: 2.149s, learning 0.108s)
             Mean action noise std: 3.07
          Mean value_function loss: 264.5417
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 65.1125
                       Mean reward: 808.88
               Mean episode length: 221.44
    Episode_Reward/reaching_object: 1.4204
     Episode_Reward/lifting_object: 155.9884
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.26s
                      Time elapsed: 01:01:11
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 42916 steps/s (collection: 2.168s, learning 0.123s)
             Mean action noise std: 3.07
          Mean value_function loss: 274.5905
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 65.1189
                       Mean reward: 804.45
               Mean episode length: 223.68
    Episode_Reward/reaching_object: 1.4111
     Episode_Reward/lifting_object: 155.2690
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.29s
                      Time elapsed: 01:01:13
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 40798 steps/s (collection: 2.224s, learning 0.185s)
             Mean action noise std: 3.07
          Mean value_function loss: 275.3838
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 65.1234
                       Mean reward: 755.89
               Mean episode length: 214.70
    Episode_Reward/reaching_object: 1.3928
     Episode_Reward/lifting_object: 152.7450
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.41s
                      Time elapsed: 01:01:16
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 43450 steps/s (collection: 2.154s, learning 0.108s)
             Mean action noise std: 3.07
          Mean value_function loss: 258.1963
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 65.1258
                       Mean reward: 763.61
               Mean episode length: 216.82
    Episode_Reward/reaching_object: 1.3666
     Episode_Reward/lifting_object: 149.5632
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.26s
                      Time elapsed: 01:01:18
                               ETA: 00:15:10

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 42805 steps/s (collection: 2.154s, learning 0.142s)
             Mean action noise std: 3.07
          Mean value_function loss: 239.9353
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 65.1308
                       Mean reward: 725.27
               Mean episode length: 209.89
    Episode_Reward/reaching_object: 1.4067
     Episode_Reward/lifting_object: 153.7679
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.30s
                      Time elapsed: 01:01:20
                               ETA: 00:15:08

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 42050 steps/s (collection: 2.188s, learning 0.150s)
             Mean action noise std: 3.07
          Mean value_function loss: 228.9320
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 65.1392
                       Mean reward: 749.05
               Mean episode length: 210.43
    Episode_Reward/reaching_object: 1.3920
     Episode_Reward/lifting_object: 153.1574
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.34s
                      Time elapsed: 01:01:22
                               ETA: 00:15:05

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 42341 steps/s (collection: 2.181s, learning 0.141s)
             Mean action noise std: 3.07
          Mean value_function loss: 226.4068
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 65.1475
                       Mean reward: 716.50
               Mean episode length: 206.27
    Episode_Reward/reaching_object: 1.3877
     Episode_Reward/lifting_object: 153.0408
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.32s
                      Time elapsed: 01:01:25
                               ETA: 00:15:03

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 41199 steps/s (collection: 2.289s, learning 0.098s)
             Mean action noise std: 3.07
          Mean value_function loss: 232.3861
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 65.1575
                       Mean reward: 813.68
               Mean episode length: 225.21
    Episode_Reward/reaching_object: 1.4094
     Episode_Reward/lifting_object: 155.9557
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 2.39s
                      Time elapsed: 01:01:27
                               ETA: 00:15:01

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 43550 steps/s (collection: 2.137s, learning 0.120s)
             Mean action noise std: 3.07
          Mean value_function loss: 257.4785
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.1628
                       Mean reward: 798.33
               Mean episode length: 221.60
    Episode_Reward/reaching_object: 1.3795
     Episode_Reward/lifting_object: 152.1346
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 2.26s
                      Time elapsed: 01:01:29
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 43285 steps/s (collection: 2.167s, learning 0.105s)
             Mean action noise std: 3.07
          Mean value_function loss: 240.9469
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 65.1685
                       Mean reward: 848.05
               Mean episode length: 231.03
    Episode_Reward/reaching_object: 1.4446
     Episode_Reward/lifting_object: 159.7416
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 2.27s
                      Time elapsed: 01:01:32
                               ETA: 00:14:56

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 44189 steps/s (collection: 2.129s, learning 0.096s)
             Mean action noise std: 3.08
          Mean value_function loss: 241.0317
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 65.1757
                       Mean reward: 720.14
               Mean episode length: 207.23
    Episode_Reward/reaching_object: 1.3640
     Episode_Reward/lifting_object: 150.9760
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 2.22s
                      Time elapsed: 01:01:34
                               ETA: 00:14:54

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 40314 steps/s (collection: 2.274s, learning 0.165s)
             Mean action noise std: 3.08
          Mean value_function loss: 257.9436
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 65.1844
                       Mean reward: 799.63
               Mean episode length: 220.33
    Episode_Reward/reaching_object: 1.4004
     Episode_Reward/lifting_object: 154.1971
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.44s
                      Time elapsed: 01:01:36
                               ETA: 00:14:52

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 42784 steps/s (collection: 2.150s, learning 0.148s)
             Mean action noise std: 3.08
          Mean value_function loss: 271.4789
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 65.1994
                       Mean reward: 777.56
               Mean episode length: 215.66
    Episode_Reward/reaching_object: 1.3507
     Episode_Reward/lifting_object: 149.3007
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 2.30s
                      Time elapsed: 01:01:39
                               ETA: 00:14:49

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 41086 steps/s (collection: 2.272s, learning 0.121s)
             Mean action noise std: 3.08
          Mean value_function loss: 255.4576
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 65.2143
                       Mean reward: 723.91
               Mean episode length: 205.36
    Episode_Reward/reaching_object: 1.3223
     Episode_Reward/lifting_object: 145.4856
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 2.39s
                      Time elapsed: 01:01:41
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 43813 steps/s (collection: 2.142s, learning 0.102s)
             Mean action noise std: 3.08
          Mean value_function loss: 253.1343
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 65.2230
                       Mean reward: 695.54
               Mean episode length: 200.58
    Episode_Reward/reaching_object: 1.3144
     Episode_Reward/lifting_object: 144.1901
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 2.24s
                      Time elapsed: 01:01:43
                               ETA: 00:14:45

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 41824 steps/s (collection: 2.186s, learning 0.164s)
             Mean action noise std: 3.08
          Mean value_function loss: 227.1787
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 65.2295
                       Mean reward: 773.42
               Mean episode length: 216.05
    Episode_Reward/reaching_object: 1.3947
     Episode_Reward/lifting_object: 154.1883
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 2.35s
                      Time elapsed: 01:01:46
                               ETA: 00:14:42

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 43059 steps/s (collection: 2.162s, learning 0.121s)
             Mean action noise std: 3.08
          Mean value_function loss: 234.1756
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 65.2309
                       Mean reward: 800.71
               Mean episode length: 224.75
    Episode_Reward/reaching_object: 1.3832
     Episode_Reward/lifting_object: 153.5161
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 2.28s
                      Time elapsed: 01:01:48
                               ETA: 00:14:40

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 42741 steps/s (collection: 2.185s, learning 0.115s)
             Mean action noise std: 3.08
          Mean value_function loss: 237.1931
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 65.2346
                       Mean reward: 762.38
               Mean episode length: 216.49
    Episode_Reward/reaching_object: 1.3682
     Episode_Reward/lifting_object: 152.0769
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 2.30s
                      Time elapsed: 01:01:50
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 40481 steps/s (collection: 2.259s, learning 0.170s)
             Mean action noise std: 3.08
          Mean value_function loss: 243.2036
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 65.2425
                       Mean reward: 723.30
               Mean episode length: 212.56
    Episode_Reward/reaching_object: 1.3444
     Episode_Reward/lifting_object: 148.4003
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 2.43s
                      Time elapsed: 01:01:53
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 43195 steps/s (collection: 2.162s, learning 0.114s)
             Mean action noise std: 3.09
          Mean value_function loss: 244.9746
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 65.2565
                       Mean reward: 752.04
               Mean episode length: 218.42
    Episode_Reward/reaching_object: 1.3388
     Episode_Reward/lifting_object: 147.2671
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 2.28s
                      Time elapsed: 01:01:55
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 44467 steps/s (collection: 2.092s, learning 0.119s)
             Mean action noise std: 3.09
          Mean value_function loss: 252.0331
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 65.2660
                       Mean reward: 794.35
               Mean episode length: 220.39
    Episode_Reward/reaching_object: 1.3908
     Episode_Reward/lifting_object: 155.1327
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 2.21s
                      Time elapsed: 01:01:57
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 41130 steps/s (collection: 2.221s, learning 0.169s)
             Mean action noise std: 3.09
          Mean value_function loss: 271.8187
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 65.2683
                       Mean reward: 714.54
               Mean episode length: 203.92
    Episode_Reward/reaching_object: 1.3005
     Episode_Reward/lifting_object: 143.4142
      Episode_Reward/object_height: 0.0124
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 2.39s
                      Time elapsed: 01:01:59
                               ETA: 00:14:29

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 38743 steps/s (collection: 2.376s, learning 0.161s)
             Mean action noise std: 3.09
          Mean value_function loss: 237.7081
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 65.2688
                       Mean reward: 757.65
               Mean episode length: 213.17
    Episode_Reward/reaching_object: 1.3522
     Episode_Reward/lifting_object: 149.7094
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 2.54s
                      Time elapsed: 01:02:02
                               ETA: 00:14:26

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 44777 steps/s (collection: 2.102s, learning 0.093s)
             Mean action noise std: 3.09
          Mean value_function loss: 268.5108
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 65.2704
                       Mean reward: 805.84
               Mean episode length: 224.97
    Episode_Reward/reaching_object: 1.3670
     Episode_Reward/lifting_object: 151.6789
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.20s
                      Time elapsed: 01:02:04
                               ETA: 00:14:24

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 39933 steps/s (collection: 2.241s, learning 0.221s)
             Mean action noise std: 3.09
          Mean value_function loss: 247.0003
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 65.2747
                       Mean reward: 697.41
               Mean episode length: 201.93
    Episode_Reward/reaching_object: 1.3271
     Episode_Reward/lifting_object: 146.7809
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.46s
                      Time elapsed: 01:02:07
                               ETA: 00:14:22

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 41734 steps/s (collection: 2.264s, learning 0.091s)
             Mean action noise std: 3.09
          Mean value_function loss: 257.6038
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 65.2815
                       Mean reward: 772.18
               Mean episode length: 217.21
    Episode_Reward/reaching_object: 1.3383
     Episode_Reward/lifting_object: 147.6944
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 2.36s
                      Time elapsed: 01:02:09
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 44335 steps/s (collection: 2.124s, learning 0.093s)
             Mean action noise std: 3.09
          Mean value_function loss: 246.5352
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 65.2915
                       Mean reward: 771.74
               Mean episode length: 215.35
    Episode_Reward/reaching_object: 1.3363
     Episode_Reward/lifting_object: 147.8732
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 2.22s
                      Time elapsed: 01:02:11
                               ETA: 00:14:17

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 44326 steps/s (collection: 2.124s, learning 0.094s)
             Mean action noise std: 3.09
          Mean value_function loss: 188.7187
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 65.3067
                       Mean reward: 720.52
               Mean episode length: 206.65
    Episode_Reward/reaching_object: 1.3432
     Episode_Reward/lifting_object: 148.8365
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 2.22s
                      Time elapsed: 01:02:13
                               ETA: 00:14:15

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 38729 steps/s (collection: 2.403s, learning 0.135s)
             Mean action noise std: 3.09
          Mean value_function loss: 223.3368
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 65.3174
                       Mean reward: 789.76
               Mean episode length: 222.43
    Episode_Reward/reaching_object: 1.3876
     Episode_Reward/lifting_object: 153.9566
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.54s
                      Time elapsed: 01:02:16
                               ETA: 00:14:13

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 38311 steps/s (collection: 2.440s, learning 0.126s)
             Mean action noise std: 3.09
          Mean value_function loss: 246.9006
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 65.3224
                       Mean reward: 713.28
               Mean episode length: 201.77
    Episode_Reward/reaching_object: 1.3351
     Episode_Reward/lifting_object: 146.9905
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.57s
                      Time elapsed: 01:02:19
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 43109 steps/s (collection: 2.113s, learning 0.167s)
             Mean action noise std: 3.10
          Mean value_function loss: 230.6207
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 65.3308
                       Mean reward: 830.27
               Mean episode length: 227.76
    Episode_Reward/reaching_object: 1.4051
     Episode_Reward/lifting_object: 155.8350
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 2.28s
                      Time elapsed: 01:02:21
                               ETA: 00:14:08

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 44098 steps/s (collection: 2.118s, learning 0.112s)
             Mean action noise std: 3.10
          Mean value_function loss: 247.2387
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 65.3449
                       Mean reward: 776.66
               Mean episode length: 217.76
    Episode_Reward/reaching_object: 1.3669
     Episode_Reward/lifting_object: 150.9287
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.23s
                      Time elapsed: 01:02:23
                               ETA: 00:14:06

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 40112 steps/s (collection: 2.279s, learning 0.172s)
             Mean action noise std: 3.10
          Mean value_function loss: 230.0459
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 65.3522
                       Mean reward: 775.59
               Mean episode length: 219.91
    Episode_Reward/reaching_object: 1.4146
     Episode_Reward/lifting_object: 156.7901
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.45s
                      Time elapsed: 01:02:26
                               ETA: 00:14:04

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 42030 steps/s (collection: 2.204s, learning 0.135s)
             Mean action noise std: 3.10
          Mean value_function loss: 229.5369
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 65.3600
                       Mean reward: 759.43
               Mean episode length: 212.74
    Episode_Reward/reaching_object: 1.3452
     Episode_Reward/lifting_object: 148.5935
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 2.34s
                      Time elapsed: 01:02:28
                               ETA: 00:14:01

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 42315 steps/s (collection: 2.213s, learning 0.110s)
             Mean action noise std: 3.10
          Mean value_function loss: 276.2199
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 65.3748
                       Mean reward: 779.84
               Mean episode length: 221.33
    Episode_Reward/reaching_object: 1.3717
     Episode_Reward/lifting_object: 150.7353
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.32s
                      Time elapsed: 01:02:30
                               ETA: 00:13:59

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 38770 steps/s (collection: 2.306s, learning 0.229s)
             Mean action noise std: 3.10
          Mean value_function loss: 231.8918
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.3885
                       Mean reward: 745.97
               Mean episode length: 211.06
    Episode_Reward/reaching_object: 1.3358
     Episode_Reward/lifting_object: 147.8387
      Episode_Reward/object_height: 0.0127
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.54s
                      Time elapsed: 01:02:33
                               ETA: 00:13:57

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 40402 steps/s (collection: 2.283s, learning 0.150s)
             Mean action noise std: 3.10
          Mean value_function loss: 235.6582
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 65.3918
                       Mean reward: 761.99
               Mean episode length: 211.90
    Episode_Reward/reaching_object: 1.3500
     Episode_Reward/lifting_object: 148.6997
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 2.43s
                      Time elapsed: 01:02:35
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 41620 steps/s (collection: 2.246s, learning 0.116s)
             Mean action noise std: 3.10
          Mean value_function loss: 219.5899
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 65.3950
                       Mean reward: 789.08
               Mean episode length: 215.96
    Episode_Reward/reaching_object: 1.3850
     Episode_Reward/lifting_object: 153.4770
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 2.36s
                      Time elapsed: 01:02:38
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 42858 steps/s (collection: 2.111s, learning 0.183s)
             Mean action noise std: 3.10
          Mean value_function loss: 205.4715
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 65.4005
                       Mean reward: 794.67
               Mean episode length: 220.36
    Episode_Reward/reaching_object: 1.3840
     Episode_Reward/lifting_object: 152.5147
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 2.29s
                      Time elapsed: 01:02:40
                               ETA: 00:13:50

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 41293 steps/s (collection: 2.197s, learning 0.184s)
             Mean action noise std: 3.10
          Mean value_function loss: 211.7245
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 65.4073
                       Mean reward: 783.69
               Mean episode length: 217.71
    Episode_Reward/reaching_object: 1.4206
     Episode_Reward/lifting_object: 156.7342
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 2.38s
                      Time elapsed: 01:02:42
                               ETA: 00:13:48

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 43329 steps/s (collection: 2.172s, learning 0.097s)
             Mean action noise std: 3.11
          Mean value_function loss: 208.9346
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 65.4122
                       Mean reward: 798.60
               Mean episode length: 223.19
    Episode_Reward/reaching_object: 1.3674
     Episode_Reward/lifting_object: 151.3305
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 2.27s
                      Time elapsed: 01:02:44
                               ETA: 00:13:45

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 44073 steps/s (collection: 2.134s, learning 0.097s)
             Mean action noise std: 3.11
          Mean value_function loss: 244.2601
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 65.4160
                       Mean reward: 773.48
               Mean episode length: 215.04
    Episode_Reward/reaching_object: 1.3811
     Episode_Reward/lifting_object: 152.7405
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 2.23s
                      Time elapsed: 01:02:47
                               ETA: 00:13:43

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 44332 steps/s (collection: 2.116s, learning 0.101s)
             Mean action noise std: 3.11
          Mean value_function loss: 189.8446
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 65.4224
                       Mean reward: 799.74
               Mean episode length: 223.49
    Episode_Reward/reaching_object: 1.4236
     Episode_Reward/lifting_object: 157.7501
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.22s
                      Time elapsed: 01:02:49
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 43852 steps/s (collection: 2.121s, learning 0.121s)
             Mean action noise std: 3.11
          Mean value_function loss: 222.0357
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 65.4294
                       Mean reward: 720.23
               Mean episode length: 202.62
    Episode_Reward/reaching_object: 1.3447
     Episode_Reward/lifting_object: 148.0114
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.24s
                      Time elapsed: 01:02:51
                               ETA: 00:13:39

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 42709 steps/s (collection: 2.178s, learning 0.124s)
             Mean action noise std: 3.11
          Mean value_function loss: 218.6155
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 65.4397
                       Mean reward: 833.71
               Mean episode length: 229.71
    Episode_Reward/reaching_object: 1.4582
     Episode_Reward/lifting_object: 161.9563
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 2.30s
                      Time elapsed: 01:02:53
                               ETA: 00:13:36

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 40268 steps/s (collection: 2.310s, learning 0.132s)
             Mean action noise std: 3.11
          Mean value_function loss: 281.7056
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 65.4479
                       Mean reward: 773.09
               Mean episode length: 213.74
    Episode_Reward/reaching_object: 1.4081
     Episode_Reward/lifting_object: 155.9833
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 2.44s
                      Time elapsed: 01:02:56
                               ETA: 00:13:34

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 43550 steps/s (collection: 2.139s, learning 0.118s)
             Mean action noise std: 3.11
          Mean value_function loss: 222.5274
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 65.4563
                       Mean reward: 726.04
               Mean episode length: 205.02
    Episode_Reward/reaching_object: 1.3714
     Episode_Reward/lifting_object: 151.8629
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 2.26s
                      Time elapsed: 01:02:58
                               ETA: 00:13:32

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 44021 steps/s (collection: 2.120s, learning 0.113s)
             Mean action noise std: 3.11
          Mean value_function loss: 231.9901
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 65.4596
                       Mean reward: 758.57
               Mean episode length: 211.39
    Episode_Reward/reaching_object: 1.3908
     Episode_Reward/lifting_object: 153.7288
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 2.23s
                      Time elapsed: 01:03:00
                               ETA: 00:13:29

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 44248 steps/s (collection: 2.109s, learning 0.113s)
             Mean action noise std: 3.11
          Mean value_function loss: 253.5842
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 65.4688
                       Mean reward: 762.81
               Mean episode length: 213.26
    Episode_Reward/reaching_object: 1.3612
     Episode_Reward/lifting_object: 150.7610
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 2.22s
                      Time elapsed: 01:03:03
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 43655 steps/s (collection: 2.114s, learning 0.138s)
             Mean action noise std: 3.11
          Mean value_function loss: 281.8444
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 65.4796
                       Mean reward: 786.18
               Mean episode length: 218.25
    Episode_Reward/reaching_object: 1.4113
     Episode_Reward/lifting_object: 157.2329
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 2.25s
                      Time elapsed: 01:03:05
                               ETA: 00:13:25

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 43924 steps/s (collection: 2.117s, learning 0.121s)
             Mean action noise std: 3.12
          Mean value_function loss: 238.4616
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 65.4848
                       Mean reward: 782.67
               Mean episode length: 217.07
    Episode_Reward/reaching_object: 1.3597
     Episode_Reward/lifting_object: 150.3875
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 2.24s
                      Time elapsed: 01:03:07
                               ETA: 00:13:22

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 41733 steps/s (collection: 2.259s, learning 0.097s)
             Mean action noise std: 3.12
          Mean value_function loss: 259.7479
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 65.4983
                       Mean reward: 757.60
               Mean episode length: 210.60
    Episode_Reward/reaching_object: 1.3627
     Episode_Reward/lifting_object: 151.2361
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.36s
                      Time elapsed: 01:03:09
                               ETA: 00:13:20

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 45024 steps/s (collection: 2.080s, learning 0.103s)
             Mean action noise std: 3.12
          Mean value_function loss: 220.2468
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 65.5197
                       Mean reward: 809.31
               Mean episode length: 220.80
    Episode_Reward/reaching_object: 1.3849
     Episode_Reward/lifting_object: 153.9895
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 2.18s
                      Time elapsed: 01:03:12
                               ETA: 00:13:18

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 43879 steps/s (collection: 2.129s, learning 0.111s)
             Mean action noise std: 3.12
          Mean value_function loss: 264.4478
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 65.5271
                       Mean reward: 786.77
               Mean episode length: 219.86
    Episode_Reward/reaching_object: 1.3845
     Episode_Reward/lifting_object: 153.3778
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 2.24s
                      Time elapsed: 01:03:14
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 43614 steps/s (collection: 2.126s, learning 0.128s)
             Mean action noise std: 3.12
          Mean value_function loss: 224.7320
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 65.5294
                       Mean reward: 746.21
               Mean episode length: 208.82
    Episode_Reward/reaching_object: 1.3565
     Episode_Reward/lifting_object: 150.6224
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 2.25s
                      Time elapsed: 01:03:16
                               ETA: 00:13:13

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 40954 steps/s (collection: 2.244s, learning 0.156s)
             Mean action noise std: 3.12
          Mean value_function loss: 224.0726
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 65.5361
                       Mean reward: 785.69
               Mean episode length: 217.59
    Episode_Reward/reaching_object: 1.3743
     Episode_Reward/lifting_object: 152.5648
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 2.40s
                      Time elapsed: 01:03:19
                               ETA: 00:13:11

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 41978 steps/s (collection: 2.172s, learning 0.170s)
             Mean action noise std: 3.12
          Mean value_function loss: 232.2484
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 65.5506
                       Mean reward: 733.13
               Mean episode length: 202.87
    Episode_Reward/reaching_object: 1.3762
     Episode_Reward/lifting_object: 153.8405
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.34s
                      Time elapsed: 01:03:21
                               ETA: 00:13:09

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 44687 steps/s (collection: 2.097s, learning 0.103s)
             Mean action noise std: 3.13
          Mean value_function loss: 232.4728
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 65.5665
                       Mean reward: 743.85
               Mean episode length: 208.94
    Episode_Reward/reaching_object: 1.3686
     Episode_Reward/lifting_object: 152.4155
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 2.20s
                      Time elapsed: 01:03:23
                               ETA: 00:13:06

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 42896 steps/s (collection: 2.163s, learning 0.129s)
             Mean action noise std: 3.13
          Mean value_function loss: 213.7363
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.5796
                       Mean reward: 837.52
               Mean episode length: 228.65
    Episode_Reward/reaching_object: 1.3650
     Episode_Reward/lifting_object: 151.5285
      Episode_Reward/object_height: 0.0129
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 2.29s
                      Time elapsed: 01:03:25
                               ETA: 00:13:04

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 44665 steps/s (collection: 2.102s, learning 0.099s)
             Mean action noise std: 3.13
          Mean value_function loss: 234.4425
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 65.5861
                       Mean reward: 772.20
               Mean episode length: 215.07
    Episode_Reward/reaching_object: 1.3834
     Episode_Reward/lifting_object: 154.3152
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 2.20s
                      Time elapsed: 01:03:28
                               ETA: 00:13:02

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 43834 steps/s (collection: 2.107s, learning 0.135s)
             Mean action noise std: 3.13
          Mean value_function loss: 218.5625
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 65.5949
                       Mean reward: 819.67
               Mean episode length: 222.71
    Episode_Reward/reaching_object: 1.4123
     Episode_Reward/lifting_object: 157.5787
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 2.24s
                      Time elapsed: 01:03:30
                               ETA: 00:12:59

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 43748 steps/s (collection: 2.120s, learning 0.127s)
             Mean action noise std: 3.13
          Mean value_function loss: 262.2474
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 65.5981
                       Mean reward: 776.73
               Mean episode length: 213.41
    Episode_Reward/reaching_object: 1.3468
     Episode_Reward/lifting_object: 149.4886
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 2.25s
                      Time elapsed: 01:03:32
                               ETA: 00:12:57

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 44016 steps/s (collection: 2.125s, learning 0.109s)
             Mean action noise std: 3.13
          Mean value_function loss: 220.3514
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 65.6069
                       Mean reward: 740.64
               Mean episode length: 210.47
    Episode_Reward/reaching_object: 1.4092
     Episode_Reward/lifting_object: 156.2620
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 2.23s
                      Time elapsed: 01:03:34
                               ETA: 00:12:55

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 44041 steps/s (collection: 2.133s, learning 0.099s)
             Mean action noise std: 3.13
          Mean value_function loss: 239.3247
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 65.6154
                       Mean reward: 776.89
               Mean episode length: 216.63
    Episode_Reward/reaching_object: 1.4158
     Episode_Reward/lifting_object: 157.0588
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 2.23s
                      Time elapsed: 01:03:37
                               ETA: 00:12:53

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 44278 steps/s (collection: 2.109s, learning 0.112s)
             Mean action noise std: 3.13
          Mean value_function loss: 243.4662
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 65.6216
                       Mean reward: 815.02
               Mean episode length: 220.44
    Episode_Reward/reaching_object: 1.4240
     Episode_Reward/lifting_object: 157.4592
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 2.22s
                      Time elapsed: 01:03:39
                               ETA: 00:12:50

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 44366 steps/s (collection: 2.101s, learning 0.115s)
             Mean action noise std: 3.13
          Mean value_function loss: 232.1779
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.6338
                       Mean reward: 774.57
               Mean episode length: 212.64
    Episode_Reward/reaching_object: 1.3780
     Episode_Reward/lifting_object: 152.0237
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 2.22s
                      Time elapsed: 01:03:41
                               ETA: 00:12:48

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 26785 steps/s (collection: 3.513s, learning 0.157s)
             Mean action noise std: 3.13
          Mean value_function loss: 265.8652
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.6439
                       Mean reward: 772.65
               Mean episode length: 216.51
    Episode_Reward/reaching_object: 1.3861
     Episode_Reward/lifting_object: 152.6498
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 3.67s
                      Time elapsed: 01:03:45
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 13977 steps/s (collection: 6.897s, learning 0.136s)
             Mean action noise std: 3.14
          Mean value_function loss: 243.2885
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 65.6545
                       Mean reward: 742.15
               Mean episode length: 209.06
    Episode_Reward/reaching_object: 1.3633
     Episode_Reward/lifting_object: 149.4479
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 7.03s
                      Time elapsed: 01:03:52
                               ETA: 00:12:45

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 14177 steps/s (collection: 6.819s, learning 0.115s)
             Mean action noise std: 3.14
          Mean value_function loss: 201.9971
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 65.6647
                       Mean reward: 784.00
               Mean episode length: 218.20
    Episode_Reward/reaching_object: 1.4365
     Episode_Reward/lifting_object: 159.0928
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 6.93s
                      Time elapsed: 01:03:59
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 13983 steps/s (collection: 6.909s, learning 0.121s)
             Mean action noise std: 3.14
          Mean value_function loss: 285.6971
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 65.6749
                       Mean reward: 841.31
               Mean episode length: 228.10
    Episode_Reward/reaching_object: 1.4082
     Episode_Reward/lifting_object: 155.8002
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 7.03s
                      Time elapsed: 01:04:06
                               ETA: 00:12:42

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 14162 steps/s (collection: 6.821s, learning 0.120s)
             Mean action noise std: 3.14
          Mean value_function loss: 334.3301
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 65.6798
                       Mean reward: 719.60
               Mean episode length: 206.17
    Episode_Reward/reaching_object: 1.3560
     Episode_Reward/lifting_object: 148.6226
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 6.94s
                      Time elapsed: 01:04:13
                               ETA: 00:12:40

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 13973 steps/s (collection: 6.909s, learning 0.126s)
             Mean action noise std: 3.14
          Mean value_function loss: 310.0500
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 65.6881
                       Mean reward: 755.72
               Mean episode length: 210.49
    Episode_Reward/reaching_object: 1.3395
     Episode_Reward/lifting_object: 145.9811
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 7.03s
                      Time elapsed: 01:04:20
                               ETA: 00:12:39

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 14119 steps/s (collection: 6.846s, learning 0.116s)
             Mean action noise std: 3.14
          Mean value_function loss: 273.1972
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 65.6968
                       Mean reward: 805.00
               Mean episode length: 222.87
    Episode_Reward/reaching_object: 1.3994
     Episode_Reward/lifting_object: 153.4223
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 6.96s
                      Time elapsed: 01:04:27
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 13856 steps/s (collection: 6.976s, learning 0.119s)
             Mean action noise std: 3.14
          Mean value_function loss: 267.8844
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 65.7076
                       Mean reward: 734.21
               Mean episode length: 207.52
    Episode_Reward/reaching_object: 1.3660
     Episode_Reward/lifting_object: 149.4980
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 7.09s
                      Time elapsed: 01:04:34
                               ETA: 00:12:36

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 13807 steps/s (collection: 6.997s, learning 0.123s)
             Mean action noise std: 3.15
          Mean value_function loss: 312.9158
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 65.7219
                       Mean reward: 735.06
               Mean episode length: 209.68
    Episode_Reward/reaching_object: 1.3631
     Episode_Reward/lifting_object: 149.3781
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 7.12s
                      Time elapsed: 01:04:41
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 22674 steps/s (collection: 4.246s, learning 0.089s)
             Mean action noise std: 3.15
          Mean value_function loss: 262.9896
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 65.7382
                       Mean reward: 771.74
               Mean episode length: 213.34
    Episode_Reward/reaching_object: 1.3663
     Episode_Reward/lifting_object: 149.6444
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 4.34s
                      Time elapsed: 01:04:45
                               ETA: 00:12:33

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 45464 steps/s (collection: 2.063s, learning 0.099s)
             Mean action noise std: 3.15
          Mean value_function loss: 251.3489
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 65.7505
                       Mean reward: 737.01
               Mean episode length: 209.52
    Episode_Reward/reaching_object: 1.4279
     Episode_Reward/lifting_object: 156.2883
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 2.16s
                      Time elapsed: 01:04:47
                               ETA: 00:12:31

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 44891 steps/s (collection: 2.061s, learning 0.129s)
             Mean action noise std: 3.15
          Mean value_function loss: 271.0044
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 65.7571
                       Mean reward: 790.14
               Mean episode length: 217.06
    Episode_Reward/reaching_object: 1.4183
     Episode_Reward/lifting_object: 156.4795
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 2.19s
                      Time elapsed: 01:04:49
                               ETA: 00:12:28

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 42121 steps/s (collection: 2.203s, learning 0.131s)
             Mean action noise std: 3.15
          Mean value_function loss: 311.5473
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.7652
                       Mean reward: 769.26
               Mean episode length: 215.91
    Episode_Reward/reaching_object: 1.3872
     Episode_Reward/lifting_object: 151.8957
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 2.33s
                      Time elapsed: 01:04:52
                               ETA: 00:12:26

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 46731 steps/s (collection: 2.016s, learning 0.088s)
             Mean action noise std: 3.15
          Mean value_function loss: 300.4076
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 65.7783
                       Mean reward: 788.11
               Mean episode length: 218.98
    Episode_Reward/reaching_object: 1.4044
     Episode_Reward/lifting_object: 153.6625
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 2.10s
                      Time elapsed: 01:04:54
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 45989 steps/s (collection: 2.024s, learning 0.113s)
             Mean action noise std: 3.15
          Mean value_function loss: 256.5197
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 65.7839
                       Mean reward: 768.69
               Mean episode length: 213.21
    Episode_Reward/reaching_object: 1.3998
     Episode_Reward/lifting_object: 153.1815
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 2.14s
                      Time elapsed: 01:04:56
                               ETA: 00:12:21

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 47263 steps/s (collection: 1.976s, learning 0.104s)
             Mean action noise std: 3.15
          Mean value_function loss: 261.3072
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 65.7906
                       Mean reward: 806.80
               Mean episode length: 225.61
    Episode_Reward/reaching_object: 1.4340
     Episode_Reward/lifting_object: 157.7790
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 2.08s
                      Time elapsed: 01:04:58
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 46525 steps/s (collection: 2.023s, learning 0.090s)
             Mean action noise std: 3.15
          Mean value_function loss: 279.5244
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 65.8011
                       Mean reward: 776.05
               Mean episode length: 214.47
    Episode_Reward/reaching_object: 1.3653
     Episode_Reward/lifting_object: 149.4863
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 2.11s
                      Time elapsed: 01:05:00
                               ETA: 00:12:17

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 46201 steps/s (collection: 1.987s, learning 0.141s)
             Mean action noise std: 3.16
          Mean value_function loss: 254.1118
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 65.8113
                       Mean reward: 732.63
               Mean episode length: 208.96
    Episode_Reward/reaching_object: 1.3511
     Episode_Reward/lifting_object: 148.0629
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 2.13s
                      Time elapsed: 01:05:02
                               ETA: 00:12:14

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 44184 steps/s (collection: 2.017s, learning 0.208s)
             Mean action noise std: 3.16
          Mean value_function loss: 260.9088
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 65.8209
                       Mean reward: 819.48
               Mean episode length: 226.14
    Episode_Reward/reaching_object: 1.4048
     Episode_Reward/lifting_object: 154.0562
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 2.22s
                      Time elapsed: 01:05:05
                               ETA: 00:12:12

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 45774 steps/s (collection: 2.062s, learning 0.086s)
             Mean action noise std: 3.16
          Mean value_function loss: 252.5117
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 65.8349
                       Mean reward: 771.70
               Mean episode length: 216.78
    Episode_Reward/reaching_object: 1.4248
     Episode_Reward/lifting_object: 156.6526
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 2.15s
                      Time elapsed: 01:05:07
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 47221 steps/s (collection: 1.996s, learning 0.086s)
             Mean action noise std: 3.16
          Mean value_function loss: 281.1763
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 65.8452
                       Mean reward: 728.34
               Mean episode length: 209.38
    Episode_Reward/reaching_object: 1.3891
     Episode_Reward/lifting_object: 151.5077
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 2.08s
                      Time elapsed: 01:05:09
                               ETA: 00:12:07

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 46586 steps/s (collection: 1.990s, learning 0.120s)
             Mean action noise std: 3.16
          Mean value_function loss: 284.7484
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 65.8559
                       Mean reward: 712.59
               Mean episode length: 199.73
    Episode_Reward/reaching_object: 1.3498
     Episode_Reward/lifting_object: 146.9863
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 2.11s
                      Time elapsed: 01:05:11
                               ETA: 00:12:05

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 46548 steps/s (collection: 2.016s, learning 0.096s)
             Mean action noise std: 3.16
          Mean value_function loss: 253.3020
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 65.8732
                       Mean reward: 821.28
               Mean episode length: 226.54
    Episode_Reward/reaching_object: 1.3804
     Episode_Reward/lifting_object: 149.8676
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 2.11s
                      Time elapsed: 01:05:13
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 47410 steps/s (collection: 1.976s, learning 0.097s)
             Mean action noise std: 3.17
          Mean value_function loss: 311.0710
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.8973
                       Mean reward: 729.75
               Mean episode length: 203.80
    Episode_Reward/reaching_object: 1.3821
     Episode_Reward/lifting_object: 150.4153
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 2.07s
                      Time elapsed: 01:05:15
                               ETA: 00:12:00

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 43454 steps/s (collection: 2.135s, learning 0.127s)
             Mean action noise std: 3.17
          Mean value_function loss: 260.1775
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 65.9155
                       Mean reward: 705.25
               Mean episode length: 199.72
    Episode_Reward/reaching_object: 1.3551
     Episode_Reward/lifting_object: 147.7289
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 2.26s
                      Time elapsed: 01:05:17
                               ETA: 00:11:58

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 46611 steps/s (collection: 1.960s, learning 0.149s)
             Mean action noise std: 3.17
          Mean value_function loss: 238.3731
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 65.9277
                       Mean reward: 799.59
               Mean episode length: 219.63
    Episode_Reward/reaching_object: 1.4030
     Episode_Reward/lifting_object: 154.2922
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 2.11s
                      Time elapsed: 01:05:19
                               ETA: 00:11:55

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 45036 steps/s (collection: 2.070s, learning 0.113s)
             Mean action noise std: 3.17
          Mean value_function loss: 288.1086
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 65.9323
                       Mean reward: 772.97
               Mean episode length: 213.37
    Episode_Reward/reaching_object: 1.3478
     Episode_Reward/lifting_object: 147.6696
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 2.18s
                      Time elapsed: 01:05:22
                               ETA: 00:11:53

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 47077 steps/s (collection: 1.965s, learning 0.124s)
             Mean action noise std: 3.17
          Mean value_function loss: 283.3102
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 65.9352
                       Mean reward: 827.77
               Mean episode length: 228.13
    Episode_Reward/reaching_object: 1.3878
     Episode_Reward/lifting_object: 152.4181
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 2.09s
                      Time elapsed: 01:05:24
                               ETA: 00:11:51

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 45783 steps/s (collection: 2.049s, learning 0.098s)
             Mean action noise std: 3.17
          Mean value_function loss: 296.4152
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 65.9399
                       Mean reward: 698.82
               Mean episode length: 198.65
    Episode_Reward/reaching_object: 1.2834
     Episode_Reward/lifting_object: 139.4692
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 2.15s
                      Time elapsed: 01:05:26
                               ETA: 00:11:48

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 47349 steps/s (collection: 1.969s, learning 0.108s)
             Mean action noise std: 3.17
          Mean value_function loss: 294.8043
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 65.9454
                       Mean reward: 825.12
               Mean episode length: 225.16
    Episode_Reward/reaching_object: 1.3979
     Episode_Reward/lifting_object: 153.9733
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 2.08s
                      Time elapsed: 01:05:28
                               ETA: 00:11:46

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 47156 steps/s (collection: 1.987s, learning 0.098s)
             Mean action noise std: 3.17
          Mean value_function loss: 293.4358
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 65.9481
                       Mean reward: 735.90
               Mean episode length: 205.06
    Episode_Reward/reaching_object: 1.3888
     Episode_Reward/lifting_object: 152.5233
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 2.08s
                      Time elapsed: 01:05:30
                               ETA: 00:11:44

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 47521 steps/s (collection: 1.972s, learning 0.097s)
             Mean action noise std: 3.17
          Mean value_function loss: 266.7019
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 65.9522
                       Mean reward: 762.44
               Mean episode length: 210.61
    Episode_Reward/reaching_object: 1.4051
     Episode_Reward/lifting_object: 154.7382
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 2.07s
                      Time elapsed: 01:05:32
                               ETA: 00:11:41

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 47500 steps/s (collection: 1.979s, learning 0.091s)
             Mean action noise std: 3.17
          Mean value_function loss: 314.4256
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 65.9591
                       Mean reward: 803.56
               Mean episode length: 219.11
    Episode_Reward/reaching_object: 1.3792
     Episode_Reward/lifting_object: 152.0474
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 2.07s
                      Time elapsed: 01:05:34
                               ETA: 00:11:39

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 47488 steps/s (collection: 1.981s, learning 0.089s)
             Mean action noise std: 3.18
          Mean value_function loss: 292.9994
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 65.9682
                       Mean reward: 685.82
               Mean episode length: 196.69
    Episode_Reward/reaching_object: 1.3103
     Episode_Reward/lifting_object: 143.0196
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 2.07s
                      Time elapsed: 01:05:36
                               ETA: 00:11:37

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 47494 steps/s (collection: 1.975s, learning 0.095s)
             Mean action noise std: 3.18
          Mean value_function loss: 234.9652
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 65.9759
                       Mean reward: 792.72
               Mean episode length: 215.63
    Episode_Reward/reaching_object: 1.4371
     Episode_Reward/lifting_object: 159.1060
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 2.07s
                      Time elapsed: 01:05:38
                               ETA: 00:11:34

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 46168 steps/s (collection: 2.035s, learning 0.094s)
             Mean action noise std: 3.18
          Mean value_function loss: 310.4359
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 65.9780
                       Mean reward: 735.96
               Mean episode length: 208.73
    Episode_Reward/reaching_object: 1.3447
     Episode_Reward/lifting_object: 147.6218
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 2.13s
                      Time elapsed: 01:05:40
                               ETA: 00:11:32

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 43595 steps/s (collection: 2.170s, learning 0.085s)
             Mean action noise std: 3.18
          Mean value_function loss: 306.9679
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 65.9804
                       Mean reward: 764.55
               Mean episode length: 217.05
    Episode_Reward/reaching_object: 1.3774
     Episode_Reward/lifting_object: 150.9603
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 2.25s
                      Time elapsed: 01:05:43
                               ETA: 00:11:30

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 46542 steps/s (collection: 2.021s, learning 0.091s)
             Mean action noise std: 3.18
          Mean value_function loss: 296.7870
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 65.9817
                       Mean reward: 779.91
               Mean episode length: 215.99
    Episode_Reward/reaching_object: 1.3352
     Episode_Reward/lifting_object: 145.2255
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 2.11s
                      Time elapsed: 01:05:45
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 46957 steps/s (collection: 2.006s, learning 0.087s)
             Mean action noise std: 3.18
          Mean value_function loss: 244.0906
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 65.9837
                       Mean reward: 761.64
               Mean episode length: 211.61
    Episode_Reward/reaching_object: 1.3716
     Episode_Reward/lifting_object: 150.2375
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 2.09s
                      Time elapsed: 01:05:47
                               ETA: 00:11:25

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 44540 steps/s (collection: 2.022s, learning 0.185s)
             Mean action noise std: 3.18
          Mean value_function loss: 249.5475
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 65.9897
                       Mean reward: 798.78
               Mean episode length: 223.20
    Episode_Reward/reaching_object: 1.4183
     Episode_Reward/lifting_object: 156.0860
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 2.21s
                      Time elapsed: 01:05:49
                               ETA: 00:11:22

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 43981 steps/s (collection: 2.112s, learning 0.123s)
             Mean action noise std: 3.18
          Mean value_function loss: 267.2048
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 66.0006
                       Mean reward: 741.01
               Mean episode length: 206.64
    Episode_Reward/reaching_object: 1.3645
     Episode_Reward/lifting_object: 148.9864
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 2.24s
                      Time elapsed: 01:05:51
                               ETA: 00:11:20

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 44051 steps/s (collection: 2.111s, learning 0.121s)
             Mean action noise std: 3.18
          Mean value_function loss: 276.8818
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 66.0078
                       Mean reward: 741.34
               Mean episode length: 208.93
    Episode_Reward/reaching_object: 1.3412
     Episode_Reward/lifting_object: 145.9131
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 2.23s
                      Time elapsed: 01:05:54
                               ETA: 00:11:18

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 45235 steps/s (collection: 2.037s, learning 0.136s)
             Mean action noise std: 3.18
          Mean value_function loss: 253.5938
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 66.0166
                       Mean reward: 800.51
               Mean episode length: 222.91
    Episode_Reward/reaching_object: 1.4198
     Episode_Reward/lifting_object: 156.3136
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 2.17s
                      Time elapsed: 01:05:56
                               ETA: 00:11:15

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 45971 steps/s (collection: 2.038s, learning 0.100s)
             Mean action noise std: 3.18
          Mean value_function loss: 269.8429
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 66.0240
                       Mean reward: 764.38
               Mean episode length: 212.38
    Episode_Reward/reaching_object: 1.3602
     Episode_Reward/lifting_object: 148.3954
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 2.14s
                      Time elapsed: 01:05:58
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 46074 steps/s (collection: 2.010s, learning 0.123s)
             Mean action noise std: 3.19
          Mean value_function loss: 260.1737
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.0331
                       Mean reward: 752.40
               Mean episode length: 213.06
    Episode_Reward/reaching_object: 1.3475
     Episode_Reward/lifting_object: 146.8537
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 2.13s
                      Time elapsed: 01:06:00
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 46109 steps/s (collection: 2.016s, learning 0.116s)
             Mean action noise std: 3.19
          Mean value_function loss: 288.0139
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.0481
                       Mean reward: 790.63
               Mean episode length: 219.89
    Episode_Reward/reaching_object: 1.3347
     Episode_Reward/lifting_object: 145.5398
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 2.13s
                      Time elapsed: 01:06:02
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 46254 steps/s (collection: 2.015s, learning 0.110s)
             Mean action noise std: 3.19
          Mean value_function loss: 234.8504
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 66.0607
                       Mean reward: 825.08
               Mean episode length: 227.13
    Episode_Reward/reaching_object: 1.4450
     Episode_Reward/lifting_object: 159.1610
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 2.13s
                      Time elapsed: 01:06:04
                               ETA: 00:11:06

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 45897 steps/s (collection: 2.028s, learning 0.114s)
             Mean action noise std: 3.19
          Mean value_function loss: 244.6884
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.0724
                       Mean reward: 752.88
               Mean episode length: 210.00
    Episode_Reward/reaching_object: 1.3728
     Episode_Reward/lifting_object: 150.0893
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 2.14s
                      Time elapsed: 01:06:06
                               ETA: 00:11:04

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 46404 steps/s (collection: 2.027s, learning 0.092s)
             Mean action noise std: 3.19
          Mean value_function loss: 226.7094
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 66.0851
                       Mean reward: 779.50
               Mean episode length: 214.91
    Episode_Reward/reaching_object: 1.3911
     Episode_Reward/lifting_object: 152.0831
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 2.12s
                      Time elapsed: 01:06:09
                               ETA: 00:11:01

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 46451 steps/s (collection: 1.972s, learning 0.145s)
             Mean action noise std: 3.19
          Mean value_function loss: 319.0371
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 66.0986
                       Mean reward: 756.63
               Mean episode length: 213.06
    Episode_Reward/reaching_object: 1.3685
     Episode_Reward/lifting_object: 150.3801
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 2.12s
                      Time elapsed: 01:06:11
                               ETA: 00:10:59

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 47137 steps/s (collection: 1.974s, learning 0.111s)
             Mean action noise std: 3.20
          Mean value_function loss: 275.7229
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 66.1154
                       Mean reward: 637.39
               Mean episode length: 188.69
    Episode_Reward/reaching_object: 1.3191
     Episode_Reward/lifting_object: 144.0482
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 2.09s
                      Time elapsed: 01:06:13
                               ETA: 00:10:57

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 46426 steps/s (collection: 2.006s, learning 0.111s)
             Mean action noise std: 3.20
          Mean value_function loss: 274.2423
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 66.1236
                       Mean reward: 740.89
               Mean episode length: 207.85
    Episode_Reward/reaching_object: 1.3716
     Episode_Reward/lifting_object: 150.2823
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 2.12s
                      Time elapsed: 01:06:15
                               ETA: 00:10:54

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 45710 steps/s (collection: 2.051s, learning 0.100s)
             Mean action noise std: 3.20
          Mean value_function loss: 279.9436
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 66.1333
                       Mean reward: 744.24
               Mean episode length: 210.65
    Episode_Reward/reaching_object: 1.4024
     Episode_Reward/lifting_object: 153.4702
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 2.15s
                      Time elapsed: 01:06:17
                               ETA: 00:10:52

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 46224 steps/s (collection: 2.039s, learning 0.088s)
             Mean action noise std: 3.20
          Mean value_function loss: 269.6522
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 66.1406
                       Mean reward: 799.21
               Mean episode length: 219.63
    Episode_Reward/reaching_object: 1.3930
     Episode_Reward/lifting_object: 152.9824
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 2.13s
                      Time elapsed: 01:06:19
                               ETA: 00:10:50

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 44729 steps/s (collection: 2.039s, learning 0.159s)
             Mean action noise std: 3.20
          Mean value_function loss: 235.5604
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 66.1490
                       Mean reward: 729.74
               Mean episode length: 206.51
    Episode_Reward/reaching_object: 1.4288
     Episode_Reward/lifting_object: 157.1298
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 2.20s
                      Time elapsed: 01:06:21
                               ETA: 00:10:47

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 45308 steps/s (collection: 2.049s, learning 0.121s)
             Mean action noise std: 3.20
          Mean value_function loss: 241.1046
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 66.1547
                       Mean reward: 768.92
               Mean episode length: 211.02
    Episode_Reward/reaching_object: 1.3779
     Episode_Reward/lifting_object: 150.8523
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 2.17s
                      Time elapsed: 01:06:24
                               ETA: 00:10:45

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 45861 steps/s (collection: 2.014s, learning 0.129s)
             Mean action noise std: 3.20
          Mean value_function loss: 257.5000
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 66.1556
                       Mean reward: 787.15
               Mean episode length: 220.99
    Episode_Reward/reaching_object: 1.4292
     Episode_Reward/lifting_object: 157.4964
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 2.14s
                      Time elapsed: 01:06:26
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 45922 steps/s (collection: 2.007s, learning 0.134s)
             Mean action noise std: 3.20
          Mean value_function loss: 237.9091
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 66.1628
                       Mean reward: 713.41
               Mean episode length: 199.76
    Episode_Reward/reaching_object: 1.3761
     Episode_Reward/lifting_object: 150.9613
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 2.14s
                      Time elapsed: 01:06:28
                               ETA: 00:10:40

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 47121 steps/s (collection: 1.988s, learning 0.098s)
             Mean action noise std: 3.20
          Mean value_function loss: 238.8694
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.1789
                       Mean reward: 758.02
               Mean episode length: 212.25
    Episode_Reward/reaching_object: 1.3798
     Episode_Reward/lifting_object: 151.4017
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 2.09s
                      Time elapsed: 01:06:30
                               ETA: 00:10:38

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 46888 steps/s (collection: 1.993s, learning 0.103s)
             Mean action noise std: 3.20
          Mean value_function loss: 272.9621
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 66.1897
                       Mean reward: 790.00
               Mean episode length: 219.28
    Episode_Reward/reaching_object: 1.4063
     Episode_Reward/lifting_object: 154.5130
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 2.10s
                      Time elapsed: 01:06:32
                               ETA: 00:10:36

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 46689 steps/s (collection: 2.019s, learning 0.087s)
             Mean action noise std: 3.21
          Mean value_function loss: 226.4505
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 66.1970
                       Mean reward: 761.35
               Mean episode length: 214.87
    Episode_Reward/reaching_object: 1.3683
     Episode_Reward/lifting_object: 149.2368
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 2.11s
                      Time elapsed: 01:06:34
                               ETA: 00:10:33

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 47397 steps/s (collection: 1.985s, learning 0.089s)
             Mean action noise std: 3.21
          Mean value_function loss: 253.7877
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 66.2085
                       Mean reward: 738.76
               Mean episode length: 208.36
    Episode_Reward/reaching_object: 1.4017
     Episode_Reward/lifting_object: 154.7681
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 2.07s
                      Time elapsed: 01:06:36
                               ETA: 00:10:31

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 47626 steps/s (collection: 1.976s, learning 0.089s)
             Mean action noise std: 3.21
          Mean value_function loss: 247.1432
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 66.2253
                       Mean reward: 792.47
               Mean episode length: 221.98
    Episode_Reward/reaching_object: 1.3748
     Episode_Reward/lifting_object: 151.1397
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 2.06s
                      Time elapsed: 01:06:38
                               ETA: 00:10:29

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 46293 steps/s (collection: 2.025s, learning 0.098s)
             Mean action noise std: 3.21
          Mean value_function loss: 227.5324
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 66.2356
                       Mean reward: 731.46
               Mean episode length: 206.36
    Episode_Reward/reaching_object: 1.4114
     Episode_Reward/lifting_object: 155.6685
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 2.12s
                      Time elapsed: 01:06:40
                               ETA: 00:10:26

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 46571 steps/s (collection: 2.010s, learning 0.101s)
             Mean action noise std: 3.21
          Mean value_function loss: 235.8873
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 66.2499
                       Mean reward: 756.26
               Mean episode length: 212.72
    Episode_Reward/reaching_object: 1.4190
     Episode_Reward/lifting_object: 156.9977
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 2.11s
                      Time elapsed: 01:06:42
                               ETA: 00:10:24

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 46244 steps/s (collection: 2.001s, learning 0.125s)
             Mean action noise std: 3.22
          Mean value_function loss: 276.4618
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 66.2703
                       Mean reward: 810.32
               Mean episode length: 222.73
    Episode_Reward/reaching_object: 1.4169
     Episode_Reward/lifting_object: 156.4263
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 2.13s
                      Time elapsed: 01:06:45
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 44956 steps/s (collection: 2.032s, learning 0.155s)
             Mean action noise std: 3.22
          Mean value_function loss: 259.9776
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 66.2803
                       Mean reward: 710.57
               Mean episode length: 206.67
    Episode_Reward/reaching_object: 1.3756
     Episode_Reward/lifting_object: 151.2268
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 2.19s
                      Time elapsed: 01:06:47
                               ETA: 00:10:19

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 45344 steps/s (collection: 2.057s, learning 0.111s)
             Mean action noise std: 3.22
          Mean value_function loss: 206.5057
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 66.2896
                       Mean reward: 843.23
               Mean episode length: 229.15
    Episode_Reward/reaching_object: 1.4098
     Episode_Reward/lifting_object: 155.9117
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 2.17s
                      Time elapsed: 01:06:49
                               ETA: 00:10:17

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 45628 steps/s (collection: 2.031s, learning 0.123s)
             Mean action noise std: 3.22
          Mean value_function loss: 218.7350
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 66.2927
                       Mean reward: 750.80
               Mean episode length: 209.73
    Episode_Reward/reaching_object: 1.4197
     Episode_Reward/lifting_object: 157.6296
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 2.15s
                      Time elapsed: 01:06:51
                               ETA: 00:10:15

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 45744 steps/s (collection: 2.055s, learning 0.094s)
             Mean action noise std: 3.22
          Mean value_function loss: 261.1471
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 66.2959
                       Mean reward: 766.74
               Mean episode length: 212.98
    Episode_Reward/reaching_object: 1.3971
     Episode_Reward/lifting_object: 154.5618
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 2.15s
                      Time elapsed: 01:06:53
                               ETA: 00:10:12

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 46258 steps/s (collection: 2.019s, learning 0.107s)
             Mean action noise std: 3.22
          Mean value_function loss: 257.3362
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 66.2984
                       Mean reward: 731.68
               Mean episode length: 206.70
    Episode_Reward/reaching_object: 1.3504
     Episode_Reward/lifting_object: 148.6656
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 2.13s
                      Time elapsed: 01:06:55
                               ETA: 00:10:10

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 46007 steps/s (collection: 2.041s, learning 0.096s)
             Mean action noise std: 3.22
          Mean value_function loss: 219.4874
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 66.3043
                       Mean reward: 796.74
               Mean episode length: 218.82
    Episode_Reward/reaching_object: 1.4097
     Episode_Reward/lifting_object: 156.3394
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 2.14s
                      Time elapsed: 01:06:58
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 46680 steps/s (collection: 2.019s, learning 0.087s)
             Mean action noise std: 3.22
          Mean value_function loss: 216.9567
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.3127
                       Mean reward: 750.24
               Mean episode length: 211.23
    Episode_Reward/reaching_object: 1.4275
     Episode_Reward/lifting_object: 158.2260
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 2.11s
                      Time elapsed: 01:07:00
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 47152 steps/s (collection: 1.995s, learning 0.090s)
             Mean action noise std: 3.22
          Mean value_function loss: 243.9265
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 66.3198
                       Mean reward: 782.28
               Mean episode length: 215.03
    Episode_Reward/reaching_object: 1.3932
     Episode_Reward/lifting_object: 154.4395
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 2.08s
                      Time elapsed: 01:07:02
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 46563 steps/s (collection: 2.010s, learning 0.101s)
             Mean action noise std: 3.22
          Mean value_function loss: 220.3729
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 66.3262
                       Mean reward: 794.73
               Mean episode length: 220.09
    Episode_Reward/reaching_object: 1.3570
     Episode_Reward/lifting_object: 150.3972
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 2.11s
                      Time elapsed: 01:07:04
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 45889 steps/s (collection: 2.039s, learning 0.104s)
             Mean action noise std: 3.22
          Mean value_function loss: 199.9942
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 66.3377
                       Mean reward: 763.65
               Mean episode length: 212.03
    Episode_Reward/reaching_object: 1.4034
     Episode_Reward/lifting_object: 155.9797
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 2.14s
                      Time elapsed: 01:07:06
                               ETA: 00:09:58

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 46504 steps/s (collection: 2.022s, learning 0.092s)
             Mean action noise std: 3.23
          Mean value_function loss: 241.8145
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 66.3475
                       Mean reward: 779.08
               Mean episode length: 218.36
    Episode_Reward/reaching_object: 1.3707
     Episode_Reward/lifting_object: 152.0961
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 2.11s
                      Time elapsed: 01:07:08
                               ETA: 00:09:56

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 46641 steps/s (collection: 2.016s, learning 0.092s)
             Mean action noise std: 3.23
          Mean value_function loss: 272.0846
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 66.3581
                       Mean reward: 749.69
               Mean episode length: 211.24
    Episode_Reward/reaching_object: 1.3853
     Episode_Reward/lifting_object: 153.7436
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 2.11s
                      Time elapsed: 01:07:10
                               ETA: 00:09:53

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 47213 steps/s (collection: 1.994s, learning 0.088s)
             Mean action noise std: 3.23
          Mean value_function loss: 246.6229
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 66.3659
                       Mean reward: 726.60
               Mean episode length: 203.50
    Episode_Reward/reaching_object: 1.3231
     Episode_Reward/lifting_object: 146.4596
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 2.08s
                      Time elapsed: 01:07:12
                               ETA: 00:09:51

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 47007 steps/s (collection: 2.005s, learning 0.086s)
             Mean action noise std: 3.23
          Mean value_function loss: 273.1561
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 66.3844
                       Mean reward: 734.99
               Mean episode length: 209.17
    Episode_Reward/reaching_object: 1.3389
     Episode_Reward/lifting_object: 147.9092
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 2.09s
                      Time elapsed: 01:07:14
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 46885 steps/s (collection: 2.006s, learning 0.091s)
             Mean action noise std: 3.23
          Mean value_function loss: 307.7402
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 66.3984
                       Mean reward: 734.27
               Mean episode length: 202.94
    Episode_Reward/reaching_object: 1.3580
     Episode_Reward/lifting_object: 150.4715
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 2.10s
                      Time elapsed: 01:07:16
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 46619 steps/s (collection: 2.004s, learning 0.105s)
             Mean action noise std: 3.23
          Mean value_function loss: 260.4302
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 66.4020
                       Mean reward: 789.92
               Mean episode length: 219.70
    Episode_Reward/reaching_object: 1.3626
     Episode_Reward/lifting_object: 150.9023
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 2.11s
                      Time elapsed: 01:07:19
                               ETA: 00:09:44

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 46259 steps/s (collection: 2.005s, learning 0.120s)
             Mean action noise std: 3.23
          Mean value_function loss: 248.6389
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 66.4038
                       Mean reward: 737.72
               Mean episode length: 212.10
    Episode_Reward/reaching_object: 1.3582
     Episode_Reward/lifting_object: 149.5531
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 2.13s
                      Time elapsed: 01:07:21
                               ETA: 00:09:42

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 46177 steps/s (collection: 2.016s, learning 0.113s)
             Mean action noise std: 3.23
          Mean value_function loss: 237.7815
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 66.4059
                       Mean reward: 760.65
               Mean episode length: 214.08
    Episode_Reward/reaching_object: 1.3811
     Episode_Reward/lifting_object: 152.0842
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 2.13s
                      Time elapsed: 01:07:23
                               ETA: 00:09:39

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 43708 steps/s (collection: 2.140s, learning 0.109s)
             Mean action noise std: 3.23
          Mean value_function loss: 253.2304
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 66.4081
                       Mean reward: 776.72
               Mean episode length: 214.32
    Episode_Reward/reaching_object: 1.3873
     Episode_Reward/lifting_object: 152.7983
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 2.25s
                      Time elapsed: 01:07:25
                               ETA: 00:09:37

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 41249 steps/s (collection: 2.297s, learning 0.087s)
             Mean action noise std: 3.24
          Mean value_function loss: 226.0002
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.4138
                       Mean reward: 816.87
               Mean episode length: 221.74
    Episode_Reward/reaching_object: 1.3713
     Episode_Reward/lifting_object: 151.1879
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 2.38s
                      Time elapsed: 01:07:27
                               ETA: 00:09:35

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 43044 steps/s (collection: 2.183s, learning 0.101s)
             Mean action noise std: 3.24
          Mean value_function loss: 232.0425
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 66.4238
                       Mean reward: 737.64
               Mean episode length: 207.64
    Episode_Reward/reaching_object: 1.3487
     Episode_Reward/lifting_object: 147.2648
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 2.28s
                      Time elapsed: 01:07:30
                               ETA: 00:09:32

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 41493 steps/s (collection: 2.230s, learning 0.139s)
             Mean action noise std: 3.24
          Mean value_function loss: 254.7947
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 66.4275
                       Mean reward: 732.50
               Mean episode length: 203.25
    Episode_Reward/reaching_object: 1.3840
     Episode_Reward/lifting_object: 153.3002
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 2.37s
                      Time elapsed: 01:07:32
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 38213 steps/s (collection: 2.412s, learning 0.161s)
             Mean action noise std: 3.24
          Mean value_function loss: 273.7282
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 66.4328
                       Mean reward: 829.71
               Mean episode length: 223.35
    Episode_Reward/reaching_object: 1.3834
     Episode_Reward/lifting_object: 153.1464
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 2.57s
                      Time elapsed: 01:07:35
                               ETA: 00:09:28

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 38922 steps/s (collection: 2.338s, learning 0.188s)
             Mean action noise std: 3.24
          Mean value_function loss: 300.4291
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 66.4419
                       Mean reward: 770.31
               Mean episode length: 215.37
    Episode_Reward/reaching_object: 1.3219
     Episode_Reward/lifting_object: 145.2165
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 2.53s
                      Time elapsed: 01:07:37
                               ETA: 00:09:26

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 41911 steps/s (collection: 2.191s, learning 0.155s)
             Mean action noise std: 3.24
          Mean value_function loss: 259.0107
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 66.4473
                       Mean reward: 788.66
               Mean episode length: 217.23
    Episode_Reward/reaching_object: 1.3722
     Episode_Reward/lifting_object: 151.1790
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 2.35s
                      Time elapsed: 01:07:40
                               ETA: 00:09:23

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 44774 steps/s (collection: 2.109s, learning 0.086s)
             Mean action noise std: 3.24
          Mean value_function loss: 242.8408
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.4509
                       Mean reward: 757.16
               Mean episode length: 207.83
    Episode_Reward/reaching_object: 1.3916
     Episode_Reward/lifting_object: 154.1393
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 2.20s
                      Time elapsed: 01:07:42
                               ETA: 00:09:21

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 44414 steps/s (collection: 2.070s, learning 0.144s)
             Mean action noise std: 3.24
          Mean value_function loss: 228.1501
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.4551
                       Mean reward: 801.44
               Mean episode length: 219.40
    Episode_Reward/reaching_object: 1.4123
     Episode_Reward/lifting_object: 157.4912
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 2.21s
                      Time elapsed: 01:07:44
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 45233 steps/s (collection: 2.042s, learning 0.132s)
             Mean action noise std: 3.24
          Mean value_function loss: 238.6922
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 66.4647
                       Mean reward: 778.20
               Mean episode length: 214.70
    Episode_Reward/reaching_object: 1.3715
     Episode_Reward/lifting_object: 152.0362
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 2.17s
                      Time elapsed: 01:07:46
                               ETA: 00:09:16

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 44118 steps/s (collection: 2.074s, learning 0.154s)
             Mean action noise std: 3.24
          Mean value_function loss: 224.3204
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 66.4744
                       Mean reward: 780.80
               Mean episode length: 218.43
    Episode_Reward/reaching_object: 1.4079
     Episode_Reward/lifting_object: 156.0107
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 2.23s
                      Time elapsed: 01:07:48
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 39245 steps/s (collection: 2.373s, learning 0.132s)
             Mean action noise std: 3.25
          Mean value_function loss: 222.0219
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 66.4843
                       Mean reward: 747.51
               Mean episode length: 212.97
    Episode_Reward/reaching_object: 1.3515
     Episode_Reward/lifting_object: 149.1655
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 2.50s
                      Time elapsed: 01:07:51
                               ETA: 00:09:12

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 45267 steps/s (collection: 2.048s, learning 0.124s)
             Mean action noise std: 3.25
          Mean value_function loss: 224.1834
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 66.4950
                       Mean reward: 776.99
               Mean episode length: 215.13
    Episode_Reward/reaching_object: 1.3458
     Episode_Reward/lifting_object: 148.4677
      Episode_Reward/object_height: 0.0126
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 2.17s
                      Time elapsed: 01:07:53
                               ETA: 00:09:09

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 46019 steps/s (collection: 1.993s, learning 0.143s)
             Mean action noise std: 3.25
          Mean value_function loss: 269.0115
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 66.5018
                       Mean reward: 813.85
               Mean episode length: 222.15
    Episode_Reward/reaching_object: 1.3996
     Episode_Reward/lifting_object: 155.0262
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 2.14s
                      Time elapsed: 01:07:55
                               ETA: 00:09:07

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 45579 steps/s (collection: 2.053s, learning 0.104s)
             Mean action noise std: 3.25
          Mean value_function loss: 232.0993
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 66.5120
                       Mean reward: 749.56
               Mean episode length: 212.20
    Episode_Reward/reaching_object: 1.3563
     Episode_Reward/lifting_object: 150.4327
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 2.16s
                      Time elapsed: 01:07:57
                               ETA: 00:09:05

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 46300 steps/s (collection: 2.035s, learning 0.088s)
             Mean action noise std: 3.25
          Mean value_function loss: 262.5458
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 66.5225
                       Mean reward: 715.75
               Mean episode length: 201.21
    Episode_Reward/reaching_object: 1.3798
     Episode_Reward/lifting_object: 152.9955
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 2.12s
                      Time elapsed: 01:07:59
                               ETA: 00:09:02

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 45143 steps/s (collection: 2.075s, learning 0.103s)
             Mean action noise std: 3.25
          Mean value_function loss: 267.3671
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 66.5308
                       Mean reward: 775.57
               Mean episode length: 214.94
    Episode_Reward/reaching_object: 1.4239
     Episode_Reward/lifting_object: 157.8922
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 2.18s
                      Time elapsed: 01:08:02
                               ETA: 00:09:00

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 46234 steps/s (collection: 2.037s, learning 0.089s)
             Mean action noise std: 3.25
          Mean value_function loss: 243.8604
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 66.5369
                       Mean reward: 795.57
               Mean episode length: 219.54
    Episode_Reward/reaching_object: 1.3761
     Episode_Reward/lifting_object: 152.7274
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 2.13s
                      Time elapsed: 01:08:04
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 46310 steps/s (collection: 2.021s, learning 0.102s)
             Mean action noise std: 3.25
          Mean value_function loss: 269.5916
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 66.5470
                       Mean reward: 791.52
               Mean episode length: 216.38
    Episode_Reward/reaching_object: 1.3680
     Episode_Reward/lifting_object: 151.5542
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 2.12s
                      Time elapsed: 01:08:06
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 45287 steps/s (collection: 2.071s, learning 0.100s)
             Mean action noise std: 3.26
          Mean value_function loss: 234.4772
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 66.5539
                       Mean reward: 837.86
               Mean episode length: 229.41
    Episode_Reward/reaching_object: 1.3938
     Episode_Reward/lifting_object: 155.2374
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 2.17s
                      Time elapsed: 01:08:08
                               ETA: 00:08:53

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 39998 steps/s (collection: 2.370s, learning 0.088s)
             Mean action noise std: 3.26
          Mean value_function loss: 222.0080
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 66.5574
                       Mean reward: 757.57
               Mean episode length: 212.76
    Episode_Reward/reaching_object: 1.3395
     Episode_Reward/lifting_object: 148.4989
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 2.46s
                      Time elapsed: 01:08:11
                               ETA: 00:08:51

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 45544 steps/s (collection: 2.044s, learning 0.114s)
             Mean action noise std: 3.26
          Mean value_function loss: 210.0436
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 66.5604
                       Mean reward: 770.56
               Mean episode length: 211.23
    Episode_Reward/reaching_object: 1.3564
     Episode_Reward/lifting_object: 150.3097
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 2.16s
                      Time elapsed: 01:08:13
                               ETA: 00:08:48

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 45980 steps/s (collection: 2.036s, learning 0.102s)
             Mean action noise std: 3.26
          Mean value_function loss: 214.2763
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 66.5618
                       Mean reward: 730.31
               Mean episode length: 205.75
    Episode_Reward/reaching_object: 1.3276
     Episode_Reward/lifting_object: 147.8764
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 2.14s
                      Time elapsed: 01:08:15
                               ETA: 00:08:46

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 46027 steps/s (collection: 2.039s, learning 0.097s)
             Mean action noise std: 3.26
          Mean value_function loss: 207.7827
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 66.5645
                       Mean reward: 802.91
               Mean episode length: 219.74
    Episode_Reward/reaching_object: 1.3960
     Episode_Reward/lifting_object: 155.3684
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 2.14s
                      Time elapsed: 01:08:17
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 41814 steps/s (collection: 2.196s, learning 0.155s)
             Mean action noise std: 3.26
          Mean value_function loss: 211.9353
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 66.5710
                       Mean reward: 804.60
               Mean episode length: 217.34
    Episode_Reward/reaching_object: 1.3859
     Episode_Reward/lifting_object: 153.9500
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 2.35s
                      Time elapsed: 01:08:19
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 45164 steps/s (collection: 2.062s, learning 0.115s)
             Mean action noise std: 3.26
          Mean value_function loss: 200.6781
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 66.5763
                       Mean reward: 755.19
               Mean episode length: 209.07
    Episode_Reward/reaching_object: 1.3793
     Episode_Reward/lifting_object: 152.6248
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 2.18s
                      Time elapsed: 01:08:21
                               ETA: 00:08:39

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 41231 steps/s (collection: 2.232s, learning 0.153s)
             Mean action noise std: 3.26
          Mean value_function loss: 247.3502
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.5821
                       Mean reward: 718.54
               Mean episode length: 206.08
    Episode_Reward/reaching_object: 1.3715
     Episode_Reward/lifting_object: 152.0835
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 2.38s
                      Time elapsed: 01:08:24
                               ETA: 00:08:37

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 43911 steps/s (collection: 2.092s, learning 0.147s)
             Mean action noise std: 3.26
          Mean value_function loss: 258.4181
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 66.5901
                       Mean reward: 825.35
               Mean episode length: 225.38
    Episode_Reward/reaching_object: 1.4214
     Episode_Reward/lifting_object: 157.7977
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 2.24s
                      Time elapsed: 01:08:26
                               ETA: 00:08:35

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 45246 steps/s (collection: 2.077s, learning 0.096s)
             Mean action noise std: 3.26
          Mean value_function loss: 287.4399
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 66.5981
                       Mean reward: 761.19
               Mean episode length: 213.90
    Episode_Reward/reaching_object: 1.3505
     Episode_Reward/lifting_object: 148.5307
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 2.17s
                      Time elapsed: 01:08:28
                               ETA: 00:08:32

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 45666 steps/s (collection: 2.044s, learning 0.109s)
             Mean action noise std: 3.26
          Mean value_function loss: 247.1591
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 66.6044
                       Mean reward: 708.76
               Mean episode length: 208.46
    Episode_Reward/reaching_object: 1.3314
     Episode_Reward/lifting_object: 145.5326
      Episode_Reward/object_height: 0.0128
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 2.15s
                      Time elapsed: 01:08:30
                               ETA: 00:08:30

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 45175 steps/s (collection: 2.069s, learning 0.107s)
             Mean action noise std: 3.26
          Mean value_function loss: 217.1548
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 66.6092
                       Mean reward: 787.45
               Mean episode length: 215.99
    Episode_Reward/reaching_object: 1.3402
     Episode_Reward/lifting_object: 147.8325
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 2.18s
                      Time elapsed: 01:08:33
                               ETA: 00:08:28

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 45287 steps/s (collection: 2.070s, learning 0.101s)
             Mean action noise std: 3.26
          Mean value_function loss: 248.7464
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.6181
                       Mean reward: 741.58
               Mean episode length: 208.46
    Episode_Reward/reaching_object: 1.3616
     Episode_Reward/lifting_object: 150.7185
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 2.17s
                      Time elapsed: 01:08:35
                               ETA: 00:08:25

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 45551 steps/s (collection: 2.057s, learning 0.101s)
             Mean action noise std: 3.27
          Mean value_function loss: 203.8680
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 66.6259
                       Mean reward: 789.86
               Mean episode length: 218.92
    Episode_Reward/reaching_object: 1.4066
     Episode_Reward/lifting_object: 156.3034
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 2.16s
                      Time elapsed: 01:08:37
                               ETA: 00:08:23

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 46461 steps/s (collection: 2.006s, learning 0.110s)
             Mean action noise std: 3.27
          Mean value_function loss: 251.3097
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 66.6345
                       Mean reward: 766.43
               Mean episode length: 215.43
    Episode_Reward/reaching_object: 1.3515
     Episode_Reward/lifting_object: 149.5811
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 2.12s
                      Time elapsed: 01:08:39
                               ETA: 00:08:21

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 45784 steps/s (collection: 2.023s, learning 0.124s)
             Mean action noise std: 3.27
          Mean value_function loss: 222.6167
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 66.6490
                       Mean reward: 757.33
               Mean episode length: 217.30
    Episode_Reward/reaching_object: 1.3858
     Episode_Reward/lifting_object: 153.2274
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 2.15s
                      Time elapsed: 01:08:41
                               ETA: 00:08:18

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 44446 steps/s (collection: 2.036s, learning 0.176s)
             Mean action noise std: 3.27
          Mean value_function loss: 238.7356
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.6555
                       Mean reward: 707.74
               Mean episode length: 200.07
    Episode_Reward/reaching_object: 1.3206
     Episode_Reward/lifting_object: 145.0580
      Episode_Reward/object_height: 0.0130
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 2.21s
                      Time elapsed: 01:08:43
                               ETA: 00:08:16

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 42423 steps/s (collection: 2.201s, learning 0.117s)
             Mean action noise std: 3.27
          Mean value_function loss: 236.8592
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.6622
                       Mean reward: 814.80
               Mean episode length: 225.20
    Episode_Reward/reaching_object: 1.4057
     Episode_Reward/lifting_object: 155.4520
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 2.32s
                      Time elapsed: 01:08:46
                               ETA: 00:08:14

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 44851 steps/s (collection: 2.100s, learning 0.092s)
             Mean action noise std: 3.27
          Mean value_function loss: 196.6013
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 66.6691
                       Mean reward: 735.66
               Mean episode length: 209.58
    Episode_Reward/reaching_object: 1.4299
     Episode_Reward/lifting_object: 158.5656
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 2.19s
                      Time elapsed: 01:08:48
                               ETA: 00:08:11

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 44861 steps/s (collection: 2.094s, learning 0.098s)
             Mean action noise std: 3.27
          Mean value_function loss: 206.4193
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 66.6734
                       Mean reward: 775.40
               Mean episode length: 215.40
    Episode_Reward/reaching_object: 1.3861
     Episode_Reward/lifting_object: 152.5470
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 2.19s
                      Time elapsed: 01:08:50
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 44307 steps/s (collection: 2.080s, learning 0.139s)
             Mean action noise std: 3.27
          Mean value_function loss: 254.3410
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 66.6756
                       Mean reward: 783.15
               Mean episode length: 221.84
    Episode_Reward/reaching_object: 1.4165
     Episode_Reward/lifting_object: 156.6748
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 2.22s
                      Time elapsed: 01:08:52
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 44315 steps/s (collection: 2.111s, learning 0.108s)
             Mean action noise std: 3.27
          Mean value_function loss: 243.6815
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 66.6769
                       Mean reward: 842.30
               Mean episode length: 227.20
    Episode_Reward/reaching_object: 1.3909
     Episode_Reward/lifting_object: 153.5576
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 2.22s
                      Time elapsed: 01:08:55
                               ETA: 00:08:04

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 44911 steps/s (collection: 2.074s, learning 0.115s)
             Mean action noise std: 3.27
          Mean value_function loss: 284.9989
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 66.6788
                       Mean reward: 710.65
               Mean episode length: 199.21
    Episode_Reward/reaching_object: 1.3856
     Episode_Reward/lifting_object: 153.3385
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 2.19s
                      Time elapsed: 01:08:57
                               ETA: 00:08:02

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 44927 steps/s (collection: 2.103s, learning 0.086s)
             Mean action noise std: 3.27
          Mean value_function loss: 256.2294
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 66.6827
                       Mean reward: 775.43
               Mean episode length: 217.75
    Episode_Reward/reaching_object: 1.3532
     Episode_Reward/lifting_object: 148.7363
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 2.19s
                      Time elapsed: 01:08:59
                               ETA: 00:08:00

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 45547 steps/s (collection: 2.050s, learning 0.108s)
             Mean action noise std: 3.27
          Mean value_function loss: 253.6385
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 66.6867
                       Mean reward: 739.26
               Mean episode length: 205.27
    Episode_Reward/reaching_object: 1.3570
     Episode_Reward/lifting_object: 150.2924
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 2.16s
                      Time elapsed: 01:09:01
                               ETA: 00:07:57

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 44399 steps/s (collection: 2.080s, learning 0.135s)
             Mean action noise std: 3.28
          Mean value_function loss: 203.0751
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 66.6883
                       Mean reward: 786.96
               Mean episode length: 219.46
    Episode_Reward/reaching_object: 1.3853
     Episode_Reward/lifting_object: 153.3022
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 2.21s
                      Time elapsed: 01:09:03
                               ETA: 00:07:55

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 42914 steps/s (collection: 2.154s, learning 0.137s)
             Mean action noise std: 3.28
          Mean value_function loss: 228.1379
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 66.6905
                       Mean reward: 780.71
               Mean episode length: 219.08
    Episode_Reward/reaching_object: 1.3587
     Episode_Reward/lifting_object: 150.1280
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 2.29s
                      Time elapsed: 01:09:06
                               ETA: 00:07:53

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 42952 steps/s (collection: 2.177s, learning 0.112s)
             Mean action noise std: 3.28
          Mean value_function loss: 245.1868
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 66.6972
                       Mean reward: 785.03
               Mean episode length: 220.66
    Episode_Reward/reaching_object: 1.3940
     Episode_Reward/lifting_object: 154.8381
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 2.29s
                      Time elapsed: 01:09:08
                               ETA: 00:07:50

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 44938 steps/s (collection: 2.093s, learning 0.095s)
             Mean action noise std: 3.28
          Mean value_function loss: 205.3068
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 66.7092
                       Mean reward: 759.91
               Mean episode length: 213.09
    Episode_Reward/reaching_object: 1.3959
     Episode_Reward/lifting_object: 155.3436
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 2.19s
                      Time elapsed: 01:09:10
                               ETA: 00:07:48

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 42041 steps/s (collection: 2.166s, learning 0.173s)
             Mean action noise std: 3.28
          Mean value_function loss: 184.0888
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 66.7166
                       Mean reward: 772.63
               Mean episode length: 213.16
    Episode_Reward/reaching_object: 1.4184
     Episode_Reward/lifting_object: 158.7160
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 2.34s
                      Time elapsed: 01:09:12
                               ETA: 00:07:46

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 45056 steps/s (collection: 2.088s, learning 0.094s)
             Mean action noise std: 3.28
          Mean value_function loss: 242.8490
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 66.7232
                       Mean reward: 736.17
               Mean episode length: 204.39
    Episode_Reward/reaching_object: 1.3726
     Episode_Reward/lifting_object: 152.8304
      Episode_Reward/object_height: 0.0132
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 2.18s
                      Time elapsed: 01:09:15
                               ETA: 00:07:43

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 40199 steps/s (collection: 2.355s, learning 0.090s)
             Mean action noise std: 3.28
          Mean value_function loss: 223.2136
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 66.7318
                       Mean reward: 752.59
               Mean episode length: 208.33
    Episode_Reward/reaching_object: 1.4031
     Episode_Reward/lifting_object: 155.8980
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 2.45s
                      Time elapsed: 01:09:17
                               ETA: 00:07:41

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 45093 steps/s (collection: 2.076s, learning 0.104s)
             Mean action noise std: 3.28
          Mean value_function loss: 221.3524
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 66.7385
                       Mean reward: 825.76
               Mean episode length: 227.20
    Episode_Reward/reaching_object: 1.4740
     Episode_Reward/lifting_object: 164.8876
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 2.18s
                      Time elapsed: 01:09:19
                               ETA: 00:07:39

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 44807 steps/s (collection: 2.082s, learning 0.112s)
             Mean action noise std: 3.28
          Mean value_function loss: 230.7702
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 66.7424
                       Mean reward: 725.99
               Mean episode length: 204.96
    Episode_Reward/reaching_object: 1.3866
     Episode_Reward/lifting_object: 153.9737
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 2.19s
                      Time elapsed: 01:09:21
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 43147 steps/s (collection: 2.179s, learning 0.100s)
             Mean action noise std: 3.28
          Mean value_function loss: 192.9313
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.7491
                       Mean reward: 844.55
               Mean episode length: 228.74
    Episode_Reward/reaching_object: 1.4052
     Episode_Reward/lifting_object: 156.1932
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 2.28s
                      Time elapsed: 01:09:24
                               ETA: 00:07:34

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 45390 steps/s (collection: 2.052s, learning 0.114s)
             Mean action noise std: 3.29
          Mean value_function loss: 212.2458
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 66.7566
                       Mean reward: 809.73
               Mean episode length: 221.94
    Episode_Reward/reaching_object: 1.4370
     Episode_Reward/lifting_object: 159.3178
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 2.17s
                      Time elapsed: 01:09:26
                               ETA: 00:07:32

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 46433 steps/s (collection: 2.019s, learning 0.099s)
             Mean action noise std: 3.29
          Mean value_function loss: 211.6054
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 66.7655
                       Mean reward: 782.79
               Mean episode length: 213.68
    Episode_Reward/reaching_object: 1.4455
     Episode_Reward/lifting_object: 160.8790
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 2.12s
                      Time elapsed: 01:09:28
                               ETA: 00:07:30

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 47396 steps/s (collection: 1.985s, learning 0.089s)
             Mean action noise std: 3.29
          Mean value_function loss: 209.8114
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 66.7748
                       Mean reward: 709.44
               Mean episode length: 201.36
    Episode_Reward/reaching_object: 1.3985
     Episode_Reward/lifting_object: 154.4884
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 2.07s
                      Time elapsed: 01:09:30
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 45625 steps/s (collection: 2.054s, learning 0.101s)
             Mean action noise std: 3.29
          Mean value_function loss: 204.8713
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 66.7846
                       Mean reward: 804.09
               Mean episode length: 218.87
    Episode_Reward/reaching_object: 1.4312
     Episode_Reward/lifting_object: 158.2110
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 2.15s
                      Time elapsed: 01:09:32
                               ETA: 00:07:25

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 46610 steps/s (collection: 2.003s, learning 0.106s)
             Mean action noise std: 3.29
          Mean value_function loss: 231.4224
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 66.7927
                       Mean reward: 827.17
               Mean episode length: 224.69
    Episode_Reward/reaching_object: 1.3885
     Episode_Reward/lifting_object: 152.5243
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 2.11s
                      Time elapsed: 01:09:34
                               ETA: 00:07:23

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 46418 steps/s (collection: 2.030s, learning 0.088s)
             Mean action noise std: 3.29
          Mean value_function loss: 235.9850
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 66.7979
                       Mean reward: 756.72
               Mean episode length: 210.93
    Episode_Reward/reaching_object: 1.3957
     Episode_Reward/lifting_object: 153.5298
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 2.12s
                      Time elapsed: 01:09:36
                               ETA: 00:07:20

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 46535 steps/s (collection: 2.019s, learning 0.093s)
             Mean action noise std: 3.29
          Mean value_function loss: 196.0380
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 66.8070
                       Mean reward: 831.84
               Mean episode length: 227.58
    Episode_Reward/reaching_object: 1.4598
     Episode_Reward/lifting_object: 161.5691
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 2.11s
                      Time elapsed: 01:09:39
                               ETA: 00:07:18

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 45613 steps/s (collection: 2.068s, learning 0.087s)
             Mean action noise std: 3.29
          Mean value_function loss: 240.4292
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 66.8223
                       Mean reward: 779.32
               Mean episode length: 215.36
    Episode_Reward/reaching_object: 1.3805
     Episode_Reward/lifting_object: 152.6482
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 2.16s
                      Time elapsed: 01:09:41
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 46109 steps/s (collection: 2.021s, learning 0.111s)
             Mean action noise std: 3.30
          Mean value_function loss: 200.5346
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 66.8393
                       Mean reward: 809.83
               Mean episode length: 223.58
    Episode_Reward/reaching_object: 1.4440
     Episode_Reward/lifting_object: 159.7068
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 2.13s
                      Time elapsed: 01:09:43
                               ETA: 00:07:13

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 46551 steps/s (collection: 2.022s, learning 0.090s)
             Mean action noise std: 3.30
          Mean value_function loss: 202.2684
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.8558
                       Mean reward: 753.08
               Mean episode length: 211.30
    Episode_Reward/reaching_object: 1.4232
     Episode_Reward/lifting_object: 157.6436
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 2.11s
                      Time elapsed: 01:09:45
                               ETA: 00:07:11

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 44000 steps/s (collection: 2.148s, learning 0.087s)
             Mean action noise std: 3.30
          Mean value_function loss: 182.2438
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 66.8651
                       Mean reward: 837.54
               Mean episode length: 227.18
    Episode_Reward/reaching_object: 1.4758
     Episode_Reward/lifting_object: 163.1124
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 2.23s
                      Time elapsed: 01:09:47
                               ETA: 00:07:09

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 45977 steps/s (collection: 2.026s, learning 0.113s)
             Mean action noise std: 3.30
          Mean value_function loss: 231.4024
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 66.8730
                       Mean reward: 803.63
               Mean episode length: 221.71
    Episode_Reward/reaching_object: 1.4085
     Episode_Reward/lifting_object: 155.6665
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0573
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 2.14s
                      Time elapsed: 01:09:49
                               ETA: 00:07:06

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 40838 steps/s (collection: 2.235s, learning 0.172s)
             Mean action noise std: 3.30
          Mean value_function loss: 213.6289
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.8868
                       Mean reward: 765.19
               Mean episode length: 218.42
    Episode_Reward/reaching_object: 1.4520
     Episode_Reward/lifting_object: 160.3197
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 2.41s
                      Time elapsed: 01:09:52
                               ETA: 00:07:04

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 46211 steps/s (collection: 2.039s, learning 0.088s)
             Mean action noise std: 3.30
          Mean value_function loss: 215.7611
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 66.8994
                       Mean reward: 847.67
               Mean episode length: 226.57
    Episode_Reward/reaching_object: 1.4403
     Episode_Reward/lifting_object: 159.9254
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 2.13s
                      Time elapsed: 01:09:54
                               ETA: 00:07:02

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 43649 steps/s (collection: 2.125s, learning 0.128s)
             Mean action noise std: 3.31
          Mean value_function loss: 204.9691
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 66.9073
                       Mean reward: 805.67
               Mean episode length: 222.76
    Episode_Reward/reaching_object: 1.4463
     Episode_Reward/lifting_object: 160.1350
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 2.25s
                      Time elapsed: 01:09:56
                               ETA: 00:06:59

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 43576 steps/s (collection: 2.170s, learning 0.086s)
             Mean action noise std: 3.31
          Mean value_function loss: 212.3836
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 66.9181
                       Mean reward: 832.17
               Mean episode length: 226.98
    Episode_Reward/reaching_object: 1.3728
     Episode_Reward/lifting_object: 151.6505
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 2.26s
                      Time elapsed: 01:09:58
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 44534 steps/s (collection: 2.112s, learning 0.095s)
             Mean action noise std: 3.31
          Mean value_function loss: 217.2978
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 66.9229
                       Mean reward: 817.19
               Mean episode length: 223.02
    Episode_Reward/reaching_object: 1.3886
     Episode_Reward/lifting_object: 154.0716
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 2.21s
                      Time elapsed: 01:10:01
                               ETA: 00:06:55

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 43967 steps/s (collection: 2.144s, learning 0.092s)
             Mean action noise std: 3.31
          Mean value_function loss: 219.0929
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 66.9315
                       Mean reward: 729.46
               Mean episode length: 202.83
    Episode_Reward/reaching_object: 1.4421
     Episode_Reward/lifting_object: 159.4449
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 2.24s
                      Time elapsed: 01:10:03
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 43422 steps/s (collection: 2.174s, learning 0.090s)
             Mean action noise std: 3.31
          Mean value_function loss: 225.6660
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 66.9429
                       Mean reward: 813.00
               Mean episode length: 225.26
    Episode_Reward/reaching_object: 1.4217
     Episode_Reward/lifting_object: 156.9532
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 2.26s
                      Time elapsed: 01:10:05
                               ETA: 00:06:50

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 44718 steps/s (collection: 2.074s, learning 0.125s)
             Mean action noise std: 3.31
          Mean value_function loss: 187.8808
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 66.9539
                       Mean reward: 862.42
               Mean episode length: 234.75
    Episode_Reward/reaching_object: 1.4520
     Episode_Reward/lifting_object: 160.9165
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 2.20s
                      Time elapsed: 01:10:07
                               ETA: 00:06:48

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 44077 steps/s (collection: 2.074s, learning 0.156s)
             Mean action noise std: 3.31
          Mean value_function loss: 229.6862
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 66.9602
                       Mean reward: 819.33
               Mean episode length: 225.19
    Episode_Reward/reaching_object: 1.4220
     Episode_Reward/lifting_object: 157.6521
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 2.23s
                      Time elapsed: 01:10:09
                               ETA: 00:06:46

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 43482 steps/s (collection: 2.124s, learning 0.137s)
             Mean action noise std: 3.31
          Mean value_function loss: 197.1486
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 66.9660
                       Mean reward: 774.15
               Mean episode length: 216.16
    Episode_Reward/reaching_object: 1.4040
     Episode_Reward/lifting_object: 155.6644
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 2.26s
                      Time elapsed: 01:10:12
                               ETA: 00:06:43

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 45517 steps/s (collection: 2.067s, learning 0.093s)
             Mean action noise std: 3.32
          Mean value_function loss: 229.9519
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 66.9736
                       Mean reward: 856.52
               Mean episode length: 229.57
    Episode_Reward/reaching_object: 1.4558
     Episode_Reward/lifting_object: 162.4353
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 2.16s
                      Time elapsed: 01:10:14
                               ETA: 00:06:41

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 45999 steps/s (collection: 2.038s, learning 0.100s)
             Mean action noise std: 3.32
          Mean value_function loss: 219.7852
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 66.9795
                       Mean reward: 774.67
               Mean episode length: 215.26
    Episode_Reward/reaching_object: 1.4116
     Episode_Reward/lifting_object: 156.6004
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 2.14s
                      Time elapsed: 01:10:16
                               ETA: 00:06:39

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 46313 steps/s (collection: 2.022s, learning 0.100s)
             Mean action noise std: 3.32
          Mean value_function loss: 246.0457
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 66.9871
                       Mean reward: 763.87
               Mean episode length: 211.60
    Episode_Reward/reaching_object: 1.3825
     Episode_Reward/lifting_object: 152.7341
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 2.12s
                      Time elapsed: 01:10:18
                               ETA: 00:06:36

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 45922 steps/s (collection: 2.019s, learning 0.122s)
             Mean action noise std: 3.32
          Mean value_function loss: 216.9367
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 66.9979
                       Mean reward: 768.03
               Mean episode length: 215.68
    Episode_Reward/reaching_object: 1.4155
     Episode_Reward/lifting_object: 157.1695
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 2.14s
                      Time elapsed: 01:10:20
                               ETA: 00:06:34

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 44341 steps/s (collection: 2.127s, learning 0.090s)
             Mean action noise std: 3.32
          Mean value_function loss: 195.0746
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 67.0057
                       Mean reward: 741.46
               Mean episode length: 206.40
    Episode_Reward/reaching_object: 1.4479
     Episode_Reward/lifting_object: 160.5450
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 2.22s
                      Time elapsed: 01:10:23
                               ETA: 00:06:32

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 45792 steps/s (collection: 2.037s, learning 0.110s)
             Mean action noise std: 3.32
          Mean value_function loss: 252.8291
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 67.0164
                       Mean reward: 727.25
               Mean episode length: 204.44
    Episode_Reward/reaching_object: 1.4121
     Episode_Reward/lifting_object: 155.5635
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 2.15s
                      Time elapsed: 01:10:25
                               ETA: 00:06:29

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 45308 steps/s (collection: 2.050s, learning 0.120s)
             Mean action noise std: 3.32
          Mean value_function loss: 230.7783
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.0309
                       Mean reward: 766.27
               Mean episode length: 211.20
    Episode_Reward/reaching_object: 1.4070
     Episode_Reward/lifting_object: 155.3479
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 2.17s
                      Time elapsed: 01:10:27
                               ETA: 00:06:27

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 46565 steps/s (collection: 2.008s, learning 0.104s)
             Mean action noise std: 3.32
          Mean value_function loss: 219.2436
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 67.0427
                       Mean reward: 850.83
               Mean episode length: 230.85
    Episode_Reward/reaching_object: 1.4512
     Episode_Reward/lifting_object: 161.2252
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 2.11s
                      Time elapsed: 01:10:29
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 45976 steps/s (collection: 2.039s, learning 0.100s)
             Mean action noise std: 3.33
          Mean value_function loss: 246.4308
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 67.0540
                       Mean reward: 784.98
               Mean episode length: 215.96
    Episode_Reward/reaching_object: 1.4023
     Episode_Reward/lifting_object: 154.5123
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 2.14s
                      Time elapsed: 01:10:31
                               ETA: 00:06:22

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 45846 steps/s (collection: 2.040s, learning 0.105s)
             Mean action noise std: 3.33
          Mean value_function loss: 205.4713
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 67.0662
                       Mean reward: 791.43
               Mean episode length: 219.25
    Episode_Reward/reaching_object: 1.4156
     Episode_Reward/lifting_object: 156.0909
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 2.14s
                      Time elapsed: 01:10:33
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 43996 steps/s (collection: 2.121s, learning 0.114s)
             Mean action noise std: 3.33
          Mean value_function loss: 193.0787
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.0729
                       Mean reward: 844.88
               Mean episode length: 228.37
    Episode_Reward/reaching_object: 1.4542
     Episode_Reward/lifting_object: 161.4055
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 2.23s
                      Time elapsed: 01:10:35
                               ETA: 00:06:18

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 44445 steps/s (collection: 2.090s, learning 0.122s)
             Mean action noise std: 3.33
          Mean value_function loss: 230.9959
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 67.0818
                       Mean reward: 765.22
               Mean episode length: 213.86
    Episode_Reward/reaching_object: 1.4398
     Episode_Reward/lifting_object: 158.7552
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 2.21s
                      Time elapsed: 01:10:38
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 44618 steps/s (collection: 2.110s, learning 0.093s)
             Mean action noise std: 3.33
          Mean value_function loss: 226.2726
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 67.0993
                       Mean reward: 867.77
               Mean episode length: 234.65
    Episode_Reward/reaching_object: 1.4031
     Episode_Reward/lifting_object: 154.6628
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 2.20s
                      Time elapsed: 01:10:40
                               ETA: 00:06:13

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 44855 steps/s (collection: 2.087s, learning 0.104s)
             Mean action noise std: 3.33
          Mean value_function loss: 251.7628
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 67.1131
                       Mean reward: 736.13
               Mean episode length: 207.24
    Episode_Reward/reaching_object: 1.3765
     Episode_Reward/lifting_object: 151.3001
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 2.19s
                      Time elapsed: 01:10:42
                               ETA: 00:06:11

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 45729 steps/s (collection: 2.046s, learning 0.104s)
             Mean action noise std: 3.34
          Mean value_function loss: 242.0524
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 67.1220
                       Mean reward: 780.62
               Mean episode length: 215.45
    Episode_Reward/reaching_object: 1.3891
     Episode_Reward/lifting_object: 153.6292
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0580
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 2.15s
                      Time elapsed: 01:10:44
                               ETA: 00:06:08

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 44961 steps/s (collection: 2.067s, learning 0.120s)
             Mean action noise std: 3.34
          Mean value_function loss: 215.6609
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 67.1317
                       Mean reward: 788.03
               Mean episode length: 216.80
    Episode_Reward/reaching_object: 1.4193
     Episode_Reward/lifting_object: 157.1474
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 2.19s
                      Time elapsed: 01:10:46
                               ETA: 00:06:06

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 44460 steps/s (collection: 2.104s, learning 0.107s)
             Mean action noise std: 3.34
          Mean value_function loss: 218.1251
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 67.1394
                       Mean reward: 733.14
               Mean episode length: 203.58
    Episode_Reward/reaching_object: 1.4147
     Episode_Reward/lifting_object: 157.1683
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 2.21s
                      Time elapsed: 01:10:49
                               ETA: 00:06:04

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 43416 steps/s (collection: 2.099s, learning 0.165s)
             Mean action noise std: 3.34
          Mean value_function loss: 209.7492
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 67.1429
                       Mean reward: 810.62
               Mean episode length: 219.64
    Episode_Reward/reaching_object: 1.4203
     Episode_Reward/lifting_object: 158.2253
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 2.26s
                      Time elapsed: 01:10:51
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 44399 steps/s (collection: 2.114s, learning 0.101s)
             Mean action noise std: 3.34
          Mean value_function loss: 232.2832
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 67.1476
                       Mean reward: 733.94
               Mean episode length: 203.18
    Episode_Reward/reaching_object: 1.3950
     Episode_Reward/lifting_object: 155.2612
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0581
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 2.21s
                      Time elapsed: 01:10:53
                               ETA: 00:05:59

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 45061 steps/s (collection: 2.093s, learning 0.089s)
             Mean action noise std: 3.34
          Mean value_function loss: 207.3243
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 67.1547
                       Mean reward: 703.81
               Mean episode length: 199.81
    Episode_Reward/reaching_object: 1.3649
     Episode_Reward/lifting_object: 150.3325
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 2.18s
                      Time elapsed: 01:10:55
                               ETA: 00:05:57

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 41753 steps/s (collection: 2.219s, learning 0.136s)
             Mean action noise std: 3.34
          Mean value_function loss: 197.2387
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 67.1645
                       Mean reward: 816.15
               Mean episode length: 222.87
    Episode_Reward/reaching_object: 1.4247
     Episode_Reward/lifting_object: 158.6940
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 2.35s
                      Time elapsed: 01:10:58
                               ETA: 00:05:55

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 44729 steps/s (collection: 2.108s, learning 0.090s)
             Mean action noise std: 3.34
          Mean value_function loss: 239.8497
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 67.1709
                       Mean reward: 820.43
               Mean episode length: 224.55
    Episode_Reward/reaching_object: 1.4028
     Episode_Reward/lifting_object: 155.9527
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 2.20s
                      Time elapsed: 01:11:00
                               ETA: 00:05:52

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 45589 steps/s (collection: 2.052s, learning 0.105s)
             Mean action noise std: 3.34
          Mean value_function loss: 204.6522
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 67.1788
                       Mean reward: 823.57
               Mean episode length: 228.43
    Episode_Reward/reaching_object: 1.4406
     Episode_Reward/lifting_object: 160.1710
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 2.16s
                      Time elapsed: 01:11:02
                               ETA: 00:05:50

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 43567 steps/s (collection: 2.148s, learning 0.109s)
             Mean action noise std: 3.35
          Mean value_function loss: 206.3772
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 67.1863
                       Mean reward: 796.46
               Mean episode length: 223.51
    Episode_Reward/reaching_object: 1.3937
     Episode_Reward/lifting_object: 154.5186
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 2.26s
                      Time elapsed: 01:11:04
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 45953 steps/s (collection: 2.054s, learning 0.086s)
             Mean action noise std: 3.35
          Mean value_function loss: 214.9656
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 67.1993
                       Mean reward: 819.16
               Mean episode length: 224.95
    Episode_Reward/reaching_object: 1.4620
     Episode_Reward/lifting_object: 162.2570
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 2.14s
                      Time elapsed: 01:11:06
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 43852 steps/s (collection: 2.149s, learning 0.093s)
             Mean action noise std: 3.35
          Mean value_function loss: 232.7507
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 67.2032
                       Mean reward: 801.64
               Mean episode length: 220.39
    Episode_Reward/reaching_object: 1.3798
     Episode_Reward/lifting_object: 152.5945
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 2.24s
                      Time elapsed: 01:11:09
                               ETA: 00:05:43

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 45287 steps/s (collection: 2.058s, learning 0.113s)
             Mean action noise std: 3.35
          Mean value_function loss: 204.9432
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 67.2040
                       Mean reward: 837.38
               Mean episode length: 226.56
    Episode_Reward/reaching_object: 1.4006
     Episode_Reward/lifting_object: 155.1018
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 2.17s
                      Time elapsed: 01:11:11
                               ETA: 00:05:41

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 43975 steps/s (collection: 2.094s, learning 0.141s)
             Mean action noise std: 3.35
          Mean value_function loss: 258.5309
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 67.2060
                       Mean reward: 796.38
               Mean episode length: 217.97
    Episode_Reward/reaching_object: 1.4504
     Episode_Reward/lifting_object: 161.9154
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 2.24s
                      Time elapsed: 01:11:13
                               ETA: 00:05:38

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 45977 steps/s (collection: 2.049s, learning 0.090s)
             Mean action noise std: 3.35
          Mean value_function loss: 252.7752
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 67.2121
                       Mean reward: 816.83
               Mean episode length: 225.17
    Episode_Reward/reaching_object: 1.3929
     Episode_Reward/lifting_object: 153.4951
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 2.14s
                      Time elapsed: 01:11:15
                               ETA: 00:05:36

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 46190 steps/s (collection: 2.040s, learning 0.088s)
             Mean action noise std: 3.35
          Mean value_function loss: 223.5933
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 67.2208
                       Mean reward: 777.27
               Mean episode length: 214.51
    Episode_Reward/reaching_object: 1.3822
     Episode_Reward/lifting_object: 152.3758
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 2.13s
                      Time elapsed: 01:11:17
                               ETA: 00:05:34

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 43422 steps/s (collection: 2.170s, learning 0.094s)
             Mean action noise std: 3.35
          Mean value_function loss: 199.1931
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 67.2246
                       Mean reward: 845.60
               Mean episode length: 228.15
    Episode_Reward/reaching_object: 1.4448
     Episode_Reward/lifting_object: 160.0590
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 2.26s
                      Time elapsed: 01:11:20
                               ETA: 00:05:31

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 43947 steps/s (collection: 2.128s, learning 0.109s)
             Mean action noise std: 3.35
          Mean value_function loss: 244.3939
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 67.2288
                       Mean reward: 793.17
               Mean episode length: 218.93
    Episode_Reward/reaching_object: 1.4207
     Episode_Reward/lifting_object: 157.1214
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0605
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 2.24s
                      Time elapsed: 01:11:22
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 42840 steps/s (collection: 2.208s, learning 0.087s)
             Mean action noise std: 3.35
          Mean value_function loss: 230.6026
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.2366
                       Mean reward: 792.61
               Mean episode length: 218.47
    Episode_Reward/reaching_object: 1.3926
     Episode_Reward/lifting_object: 153.8309
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 2.29s
                      Time elapsed: 01:11:24
                               ETA: 00:05:27

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 45041 steps/s (collection: 2.083s, learning 0.099s)
             Mean action noise std: 3.35
          Mean value_function loss: 212.2618
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 67.2437
                       Mean reward: 809.11
               Mean episode length: 222.04
    Episode_Reward/reaching_object: 1.4271
     Episode_Reward/lifting_object: 158.7993
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 2.18s
                      Time elapsed: 01:11:26
                               ETA: 00:05:24

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 44404 steps/s (collection: 2.121s, learning 0.093s)
             Mean action noise std: 3.35
          Mean value_function loss: 240.6340
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 67.2526
                       Mean reward: 794.43
               Mean episode length: 221.43
    Episode_Reward/reaching_object: 1.4017
     Episode_Reward/lifting_object: 155.6513
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 2.21s
                      Time elapsed: 01:11:28
                               ETA: 00:05:22

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 44760 steps/s (collection: 2.093s, learning 0.103s)
             Mean action noise std: 3.36
          Mean value_function loss: 223.2615
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 67.2668
                       Mean reward: 759.73
               Mean episode length: 211.26
    Episode_Reward/reaching_object: 1.3854
     Episode_Reward/lifting_object: 152.4112
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 2.20s
                      Time elapsed: 01:11:31
                               ETA: 00:05:20

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 42023 steps/s (collection: 2.215s, learning 0.124s)
             Mean action noise std: 3.36
          Mean value_function loss: 245.3799
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 67.2835
                       Mean reward: 819.82
               Mean episode length: 220.61
    Episode_Reward/reaching_object: 1.3948
     Episode_Reward/lifting_object: 155.2908
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0591
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 2.34s
                      Time elapsed: 01:11:33
                               ETA: 00:05:18

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 42112 steps/s (collection: 2.230s, learning 0.104s)
             Mean action noise std: 3.36
          Mean value_function loss: 234.0559
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 67.2928
                       Mean reward: 795.54
               Mean episode length: 220.65
    Episode_Reward/reaching_object: 1.3705
     Episode_Reward/lifting_object: 151.7045
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 2.33s
                      Time elapsed: 01:11:35
                               ETA: 00:05:15

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 43736 steps/s (collection: 2.140s, learning 0.108s)
             Mean action noise std: 3.36
          Mean value_function loss: 251.9544
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 67.2954
                       Mean reward: 755.40
               Mean episode length: 213.42
    Episode_Reward/reaching_object: 1.4167
     Episode_Reward/lifting_object: 157.2516
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 2.25s
                      Time elapsed: 01:11:38
                               ETA: 00:05:13

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 43654 steps/s (collection: 2.141s, learning 0.111s)
             Mean action noise std: 3.36
          Mean value_function loss: 194.8511
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 67.2958
                       Mean reward: 789.28
               Mean episode length: 214.90
    Episode_Reward/reaching_object: 1.3889
     Episode_Reward/lifting_object: 154.6079
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 2.25s
                      Time elapsed: 01:11:40
                               ETA: 00:05:11

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 43695 steps/s (collection: 2.120s, learning 0.130s)
             Mean action noise std: 3.36
          Mean value_function loss: 231.5303
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 67.2983
                       Mean reward: 751.56
               Mean episode length: 210.84
    Episode_Reward/reaching_object: 1.4155
     Episode_Reward/lifting_object: 156.9796
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 2.25s
                      Time elapsed: 01:11:42
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 43247 steps/s (collection: 2.173s, learning 0.100s)
             Mean action noise std: 3.36
          Mean value_function loss: 213.3369
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.3034
                       Mean reward: 749.88
               Mean episode length: 208.32
    Episode_Reward/reaching_object: 1.3735
     Episode_Reward/lifting_object: 152.8057
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 2.27s
                      Time elapsed: 01:11:44
                               ETA: 00:05:06

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 45160 steps/s (collection: 2.083s, learning 0.094s)
             Mean action noise std: 3.36
          Mean value_function loss: 221.4921
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 67.3171
                       Mean reward: 776.78
               Mean episode length: 219.02
    Episode_Reward/reaching_object: 1.4037
     Episode_Reward/lifting_object: 155.4421
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0597
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 2.18s
                      Time elapsed: 01:11:47
                               ETA: 00:05:04

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 45373 steps/s (collection: 2.071s, learning 0.095s)
             Mean action noise std: 3.37
          Mean value_function loss: 225.3174
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 67.3311
                       Mean reward: 806.19
               Mean episode length: 220.51
    Episode_Reward/reaching_object: 1.4193
     Episode_Reward/lifting_object: 158.0139
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 2.17s
                      Time elapsed: 01:11:49
                               ETA: 00:05:01

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 43190 steps/s (collection: 2.126s, learning 0.150s)
             Mean action noise std: 3.37
          Mean value_function loss: 227.1529
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 67.3388
                       Mean reward: 807.20
               Mean episode length: 220.13
    Episode_Reward/reaching_object: 1.3936
     Episode_Reward/lifting_object: 155.3600
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 2.28s
                      Time elapsed: 01:11:51
                               ETA: 00:04:59

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 44943 steps/s (collection: 2.079s, learning 0.109s)
             Mean action noise std: 3.37
          Mean value_function loss: 203.7498
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 67.3482
                       Mean reward: 831.10
               Mean episode length: 225.32
    Episode_Reward/reaching_object: 1.4062
     Episode_Reward/lifting_object: 156.7600
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 2.19s
                      Time elapsed: 01:11:53
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 44191 steps/s (collection: 2.099s, learning 0.125s)
             Mean action noise std: 3.37
          Mean value_function loss: 219.7762
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 67.3597
                       Mean reward: 771.69
               Mean episode length: 212.32
    Episode_Reward/reaching_object: 1.3865
     Episode_Reward/lifting_object: 154.4969
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0596
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 2.22s
                      Time elapsed: 01:11:55
                               ETA: 00:04:54

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 42122 steps/s (collection: 2.235s, learning 0.099s)
             Mean action noise std: 3.37
          Mean value_function loss: 236.4783
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 67.3618
                       Mean reward: 800.26
               Mean episode length: 219.50
    Episode_Reward/reaching_object: 1.4183
     Episode_Reward/lifting_object: 158.5752
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 2.33s
                      Time elapsed: 01:11:58
                               ETA: 00:04:52

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 42486 steps/s (collection: 2.165s, learning 0.148s)
             Mean action noise std: 3.37
          Mean value_function loss: 198.2328
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 67.3643
                       Mean reward: 843.97
               Mean episode length: 229.85
    Episode_Reward/reaching_object: 1.4481
     Episode_Reward/lifting_object: 161.4454
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 2.31s
                      Time elapsed: 01:12:00
                               ETA: 00:04:50

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 41224 steps/s (collection: 2.247s, learning 0.138s)
             Mean action noise std: 3.37
          Mean value_function loss: 181.1439
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 67.3715
                       Mean reward: 765.12
               Mean episode length: 209.96
    Episode_Reward/reaching_object: 1.4516
     Episode_Reward/lifting_object: 162.2896
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0617
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 2.38s
                      Time elapsed: 01:12:02
                               ETA: 00:04:48

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 43091 steps/s (collection: 2.184s, learning 0.097s)
             Mean action noise std: 3.37
          Mean value_function loss: 203.7165
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 67.3831
                       Mean reward: 814.01
               Mean episode length: 224.08
    Episode_Reward/reaching_object: 1.4186
     Episode_Reward/lifting_object: 158.7877
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 2.28s
                      Time elapsed: 01:12:05
                               ETA: 00:04:45

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 41179 steps/s (collection: 2.253s, learning 0.135s)
             Mean action noise std: 3.37
          Mean value_function loss: 244.1390
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 67.3874
                       Mean reward: 818.10
               Mean episode length: 226.23
    Episode_Reward/reaching_object: 1.3747
     Episode_Reward/lifting_object: 151.5776
      Episode_Reward/object_height: 0.0135
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 2.39s
                      Time elapsed: 01:12:07
                               ETA: 00:04:43

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 42564 steps/s (collection: 2.218s, learning 0.092s)
             Mean action noise std: 3.37
          Mean value_function loss: 224.5376
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 67.3890
                       Mean reward: 755.04
               Mean episode length: 207.95
    Episode_Reward/reaching_object: 1.3906
     Episode_Reward/lifting_object: 154.7932
      Episode_Reward/object_height: 0.0134
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 2.31s
                      Time elapsed: 01:12:09
                               ETA: 00:04:41

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 42786 steps/s (collection: 2.193s, learning 0.104s)
             Mean action noise std: 3.37
          Mean value_function loss: 225.8593
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 67.3928
                       Mean reward: 793.59
               Mean episode length: 219.88
    Episode_Reward/reaching_object: 1.3963
     Episode_Reward/lifting_object: 153.9020
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 2.30s
                      Time elapsed: 01:12:12
                               ETA: 00:04:38

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 42312 steps/s (collection: 2.206s, learning 0.117s)
             Mean action noise std: 3.38
          Mean value_function loss: 241.1845
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 67.4004
                       Mean reward: 784.82
               Mean episode length: 216.89
    Episode_Reward/reaching_object: 1.4426
     Episode_Reward/lifting_object: 159.9639
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 2.32s
                      Time elapsed: 01:12:14
                               ETA: 00:04:36

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 42105 steps/s (collection: 2.232s, learning 0.103s)
             Mean action noise std: 3.38
          Mean value_function loss: 251.4937
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 67.4103
                       Mean reward: 757.43
               Mean episode length: 209.67
    Episode_Reward/reaching_object: 1.3961
     Episode_Reward/lifting_object: 153.4606
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 2.33s
                      Time elapsed: 01:12:16
                               ETA: 00:04:34

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 42249 steps/s (collection: 2.218s, learning 0.109s)
             Mean action noise std: 3.38
          Mean value_function loss: 223.5740
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 67.4209
                       Mean reward: 782.44
               Mean episode length: 216.08
    Episode_Reward/reaching_object: 1.4114
     Episode_Reward/lifting_object: 155.5708
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 2.33s
                      Time elapsed: 01:12:19
                               ETA: 00:04:31

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 40781 steps/s (collection: 2.199s, learning 0.211s)
             Mean action noise std: 3.38
          Mean value_function loss: 260.4589
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 67.4238
                       Mean reward: 797.46
               Mean episode length: 220.48
    Episode_Reward/reaching_object: 1.3901
     Episode_Reward/lifting_object: 151.7928
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 2.41s
                      Time elapsed: 01:12:21
                               ETA: 00:04:29

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 41328 steps/s (collection: 2.244s, learning 0.135s)
             Mean action noise std: 3.38
          Mean value_function loss: 227.3840
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 67.4268
                       Mean reward: 784.25
               Mean episode length: 217.45
    Episode_Reward/reaching_object: 1.4205
     Episode_Reward/lifting_object: 156.3982
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 2.38s
                      Time elapsed: 01:12:23
                               ETA: 00:04:27

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 43338 steps/s (collection: 2.169s, learning 0.099s)
             Mean action noise std: 3.38
          Mean value_function loss: 249.7720
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 67.4315
                       Mean reward: 789.12
               Mean episode length: 215.75
    Episode_Reward/reaching_object: 1.4310
     Episode_Reward/lifting_object: 157.0473
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 2.27s
                      Time elapsed: 01:12:26
                               ETA: 00:04:25

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 40302 steps/s (collection: 2.323s, learning 0.117s)
             Mean action noise std: 3.38
          Mean value_function loss: 251.4466
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 67.4336
                       Mean reward: 731.69
               Mean episode length: 205.06
    Episode_Reward/reaching_object: 1.4343
     Episode_Reward/lifting_object: 158.0930
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 2.44s
                      Time elapsed: 01:12:28
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 40316 steps/s (collection: 2.256s, learning 0.183s)
             Mean action noise std: 3.38
          Mean value_function loss: 269.3236
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 67.4356
                       Mean reward: 790.94
               Mean episode length: 220.66
    Episode_Reward/reaching_object: 1.4109
     Episode_Reward/lifting_object: 154.6051
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 2.44s
                      Time elapsed: 01:12:31
                               ETA: 00:04:20

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 35760 steps/s (collection: 2.557s, learning 0.192s)
             Mean action noise std: 3.38
          Mean value_function loss: 286.4294
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 67.4434
                       Mean reward: 724.68
               Mean episode length: 204.55
    Episode_Reward/reaching_object: 1.3695
     Episode_Reward/lifting_object: 149.9903
      Episode_Reward/object_height: 0.0137
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 2.75s
                      Time elapsed: 01:12:33
                               ETA: 00:04:18

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 40510 steps/s (collection: 2.308s, learning 0.119s)
             Mean action noise std: 3.38
          Mean value_function loss: 274.1597
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 67.4528
                       Mean reward: 772.74
               Mean episode length: 214.67
    Episode_Reward/reaching_object: 1.4025
     Episode_Reward/lifting_object: 153.7118
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 2.43s
                      Time elapsed: 01:12:36
                               ETA: 00:04:15

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 42913 steps/s (collection: 2.156s, learning 0.135s)
             Mean action noise std: 3.38
          Mean value_function loss: 230.0096
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 67.4575
                       Mean reward: 801.66
               Mean episode length: 221.75
    Episode_Reward/reaching_object: 1.3963
     Episode_Reward/lifting_object: 152.8885
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 2.29s
                      Time elapsed: 01:12:38
                               ETA: 00:04:13

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 43398 steps/s (collection: 2.149s, learning 0.116s)
             Mean action noise std: 3.38
          Mean value_function loss: 254.6904
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 67.4637
                       Mean reward: 815.85
               Mean episode length: 223.44
    Episode_Reward/reaching_object: 1.4442
     Episode_Reward/lifting_object: 158.9363
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 2.27s
                      Time elapsed: 01:12:40
                               ETA: 00:04:11

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 44450 steps/s (collection: 2.122s, learning 0.089s)
             Mean action noise std: 3.39
          Mean value_function loss: 299.7927
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.4707
                       Mean reward: 764.26
               Mean episode length: 213.36
    Episode_Reward/reaching_object: 1.3798
     Episode_Reward/lifting_object: 151.2881
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 2.21s
                      Time elapsed: 01:12:43
                               ETA: 00:04:08

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 43924 steps/s (collection: 2.141s, learning 0.098s)
             Mean action noise std: 3.39
          Mean value_function loss: 275.4697
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 67.4835
                       Mean reward: 834.57
               Mean episode length: 228.93
    Episode_Reward/reaching_object: 1.4187
     Episode_Reward/lifting_object: 155.8610
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 2.24s
                      Time elapsed: 01:12:45
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 42280 steps/s (collection: 2.211s, learning 0.114s)
             Mean action noise std: 3.39
          Mean value_function loss: 262.0565
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 67.4940
                       Mean reward: 743.86
               Mean episode length: 208.03
    Episode_Reward/reaching_object: 1.4047
     Episode_Reward/lifting_object: 154.6386
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 2.33s
                      Time elapsed: 01:12:47
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 42286 steps/s (collection: 2.167s, learning 0.158s)
             Mean action noise std: 3.39
          Mean value_function loss: 253.2726
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 67.4975
                       Mean reward: 778.69
               Mean episode length: 217.76
    Episode_Reward/reaching_object: 1.4125
     Episode_Reward/lifting_object: 155.3280
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 2.32s
                      Time elapsed: 01:12:49
                               ETA: 00:04:02

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 43382 steps/s (collection: 2.168s, learning 0.098s)
             Mean action noise std: 3.39
          Mean value_function loss: 253.0503
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 67.5013
                       Mean reward: 771.09
               Mean episode length: 216.85
    Episode_Reward/reaching_object: 1.3628
     Episode_Reward/lifting_object: 149.8029
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0595
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 2.27s
                      Time elapsed: 01:12:52
                               ETA: 00:03:59

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 43822 steps/s (collection: 2.145s, learning 0.098s)
             Mean action noise std: 3.39
          Mean value_function loss: 279.2652
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 67.5049
                       Mean reward: 771.01
               Mean episode length: 215.46
    Episode_Reward/reaching_object: 1.4149
     Episode_Reward/lifting_object: 156.7788
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 2.24s
                      Time elapsed: 01:12:54
                               ETA: 00:03:57

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 41751 steps/s (collection: 2.201s, learning 0.153s)
             Mean action noise std: 3.39
          Mean value_function loss: 285.3069
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 67.5134
                       Mean reward: 740.41
               Mean episode length: 206.81
    Episode_Reward/reaching_object: 1.3724
     Episode_Reward/lifting_object: 151.7477
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 2.35s
                      Time elapsed: 01:12:56
                               ETA: 00:03:55

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 43094 steps/s (collection: 2.152s, learning 0.129s)
             Mean action noise std: 3.39
          Mean value_function loss: 268.5609
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 67.5211
                       Mean reward: 783.51
               Mean episode length: 221.97
    Episode_Reward/reaching_object: 1.3926
     Episode_Reward/lifting_object: 153.5336
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 2.28s
                      Time elapsed: 01:12:59
                               ETA: 00:03:52

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 37217 steps/s (collection: 2.396s, learning 0.245s)
             Mean action noise std: 3.39
          Mean value_function loss: 265.5533
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 67.5222
                       Mean reward: 758.04
               Mean episode length: 210.79
    Episode_Reward/reaching_object: 1.4097
     Episode_Reward/lifting_object: 156.2986
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 2.64s
                      Time elapsed: 01:13:01
                               ETA: 00:03:50

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 41000 steps/s (collection: 2.260s, learning 0.138s)
             Mean action noise std: 3.39
          Mean value_function loss: 243.4824
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 67.5232
                       Mean reward: 795.25
               Mean episode length: 219.41
    Episode_Reward/reaching_object: 1.3747
     Episode_Reward/lifting_object: 151.1857
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.40s
                      Time elapsed: 01:13:04
                               ETA: 00:03:48

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 43940 steps/s (collection: 2.143s, learning 0.095s)
             Mean action noise std: 3.39
          Mean value_function loss: 236.7486
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 67.5243
                       Mean reward: 802.59
               Mean episode length: 217.49
    Episode_Reward/reaching_object: 1.4542
     Episode_Reward/lifting_object: 161.6366
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 2.24s
                      Time elapsed: 01:13:06
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 42960 steps/s (collection: 2.149s, learning 0.140s)
             Mean action noise std: 3.39
          Mean value_function loss: 268.2063
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 67.5263
                       Mean reward: 796.71
               Mean episode length: 218.81
    Episode_Reward/reaching_object: 1.4349
     Episode_Reward/lifting_object: 158.9103
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0617
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 2.29s
                      Time elapsed: 01:13:08
                               ETA: 00:03:43

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 42664 steps/s (collection: 2.188s, learning 0.117s)
             Mean action noise std: 3.39
          Mean value_function loss: 179.1604
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 67.5296
                       Mean reward: 790.86
               Mean episode length: 220.74
    Episode_Reward/reaching_object: 1.4769
     Episode_Reward/lifting_object: 163.8239
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 2.30s
                      Time elapsed: 01:13:10
                               ETA: 00:03:41

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 42670 steps/s (collection: 2.203s, learning 0.101s)
             Mean action noise std: 3.40
          Mean value_function loss: 204.1002
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 67.5374
                       Mean reward: 818.45
               Mean episode length: 224.60
    Episode_Reward/reaching_object: 1.4491
     Episode_Reward/lifting_object: 159.5798
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 2.30s
                      Time elapsed: 01:13:13
                               ETA: 00:03:38

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 40472 steps/s (collection: 2.291s, learning 0.138s)
             Mean action noise std: 3.40
          Mean value_function loss: 197.6491
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.5521
                       Mean reward: 753.35
               Mean episode length: 212.24
    Episode_Reward/reaching_object: 1.4127
     Episode_Reward/lifting_object: 155.1873
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.43s
                      Time elapsed: 01:13:15
                               ETA: 00:03:36

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 41433 steps/s (collection: 2.275s, learning 0.098s)
             Mean action noise std: 3.40
          Mean value_function loss: 210.2033
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 67.5664
                       Mean reward: 821.32
               Mean episode length: 224.53
    Episode_Reward/reaching_object: 1.4412
     Episode_Reward/lifting_object: 159.2367
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 2.37s
                      Time elapsed: 01:13:18
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 43060 steps/s (collection: 2.176s, learning 0.107s)
             Mean action noise std: 3.40
          Mean value_function loss: 249.0617
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 67.5729
                       Mean reward: 730.78
               Mean episode length: 201.29
    Episode_Reward/reaching_object: 1.4031
     Episode_Reward/lifting_object: 155.0124
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 2.28s
                      Time elapsed: 01:13:20
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 39806 steps/s (collection: 2.313s, learning 0.157s)
             Mean action noise std: 3.40
          Mean value_function loss: 219.4404
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 67.5761
                       Mean reward: 813.22
               Mean episode length: 222.41
    Episode_Reward/reaching_object: 1.4715
     Episode_Reward/lifting_object: 163.1365
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 2.47s
                      Time elapsed: 01:13:22
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 37243 steps/s (collection: 2.464s, learning 0.175s)
             Mean action noise std: 3.40
          Mean value_function loss: 245.6861
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 67.5832
                       Mean reward: 803.86
               Mean episode length: 218.88
    Episode_Reward/reaching_object: 1.4341
     Episode_Reward/lifting_object: 158.8228
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 2.64s
                      Time elapsed: 01:13:25
                               ETA: 00:03:27

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 39140 steps/s (collection: 2.383s, learning 0.128s)
             Mean action noise std: 3.40
          Mean value_function loss: 253.3902
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 67.5886
                       Mean reward: 798.92
               Mean episode length: 219.08
    Episode_Reward/reaching_object: 1.4316
     Episode_Reward/lifting_object: 158.3699
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0617
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 2.51s
                      Time elapsed: 01:13:27
                               ETA: 00:03:25

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 43717 steps/s (collection: 2.152s, learning 0.097s)
             Mean action noise std: 3.40
          Mean value_function loss: 253.0230
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.5955
                       Mean reward: 770.70
               Mean episode length: 213.14
    Episode_Reward/reaching_object: 1.3970
     Episode_Reward/lifting_object: 154.6696
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 2.25s
                      Time elapsed: 01:13:30
                               ETA: 00:03:22

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 43290 steps/s (collection: 2.138s, learning 0.132s)
             Mean action noise std: 3.40
          Mean value_function loss: 241.5566
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 67.6043
                       Mean reward: 787.08
               Mean episode length: 214.59
    Episode_Reward/reaching_object: 1.4296
     Episode_Reward/lifting_object: 158.6768
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 2.27s
                      Time elapsed: 01:13:32
                               ETA: 00:03:20

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 37395 steps/s (collection: 2.473s, learning 0.156s)
             Mean action noise std: 3.41
          Mean value_function loss: 257.8533
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 67.6116
                       Mean reward: 778.34
               Mean episode length: 211.85
    Episode_Reward/reaching_object: 1.4391
     Episode_Reward/lifting_object: 159.7688
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 2.63s
                      Time elapsed: 01:13:35
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 41324 steps/s (collection: 2.276s, learning 0.103s)
             Mean action noise std: 3.41
          Mean value_function loss: 227.1303
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 67.6213
                       Mean reward: 818.60
               Mean episode length: 224.57
    Episode_Reward/reaching_object: 1.3941
     Episode_Reward/lifting_object: 154.3561
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0604
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 2.38s
                      Time elapsed: 01:13:37
                               ETA: 00:03:15

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 41109 steps/s (collection: 2.225s, learning 0.167s)
             Mean action noise std: 3.41
          Mean value_function loss: 231.0031
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 67.6260
                       Mean reward: 771.78
               Mean episode length: 212.07
    Episode_Reward/reaching_object: 1.4163
     Episode_Reward/lifting_object: 157.4633
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 2.39s
                      Time elapsed: 01:13:39
                               ETA: 00:03:13

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 38820 steps/s (collection: 2.426s, learning 0.106s)
             Mean action noise std: 3.41
          Mean value_function loss: 248.1098
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 67.6296
                       Mean reward: 811.08
               Mean episode length: 220.93
    Episode_Reward/reaching_object: 1.4519
     Episode_Reward/lifting_object: 161.1845
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.53s
                      Time elapsed: 01:13:42
                               ETA: 00:03:11

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 39920 steps/s (collection: 2.339s, learning 0.123s)
             Mean action noise std: 3.41
          Mean value_function loss: 252.1531
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 67.6332
                       Mean reward: 795.75
               Mean episode length: 218.84
    Episode_Reward/reaching_object: 1.4338
     Episode_Reward/lifting_object: 158.9681
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 2.46s
                      Time elapsed: 01:13:44
                               ETA: 00:03:09

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 40623 steps/s (collection: 2.316s, learning 0.104s)
             Mean action noise std: 3.41
          Mean value_function loss: 242.2906
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 67.6409
                       Mean reward: 783.67
               Mean episode length: 213.23
    Episode_Reward/reaching_object: 1.4193
     Episode_Reward/lifting_object: 156.7982
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0617
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 2.42s
                      Time elapsed: 01:13:47
                               ETA: 00:03:06

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 42773 steps/s (collection: 2.157s, learning 0.142s)
             Mean action noise std: 3.41
          Mean value_function loss: 300.5169
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 67.6537
                       Mean reward: 777.03
               Mean episode length: 213.84
    Episode_Reward/reaching_object: 1.4605
     Episode_Reward/lifting_object: 161.9821
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 2.30s
                      Time elapsed: 01:13:49
                               ETA: 00:03:04

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 38660 steps/s (collection: 2.339s, learning 0.204s)
             Mean action noise std: 3.41
          Mean value_function loss: 276.6430
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 67.6662
                       Mean reward: 793.79
               Mean episode length: 219.45
    Episode_Reward/reaching_object: 1.3918
     Episode_Reward/lifting_object: 152.8879
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.6250
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 2.54s
                      Time elapsed: 01:13:52
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 36099 steps/s (collection: 2.616s, learning 0.107s)
             Mean action noise std: 3.41
          Mean value_function loss: 222.3289
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 67.6744
                       Mean reward: 756.19
               Mean episode length: 210.48
    Episode_Reward/reaching_object: 1.4731
     Episode_Reward/lifting_object: 163.3222
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 2.72s
                      Time elapsed: 01:13:54
                               ETA: 00:02:59

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 30485 steps/s (collection: 2.817s, learning 0.407s)
             Mean action noise std: 3.42
          Mean value_function loss: 243.1999
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 67.6803
                       Mean reward: 811.53
               Mean episode length: 224.53
    Episode_Reward/reaching_object: 1.4180
     Episode_Reward/lifting_object: 156.3983
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 3.22s
                      Time elapsed: 01:13:58
                               ETA: 00:02:57

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 15288 steps/s (collection: 5.994s, learning 0.436s)
             Mean action noise std: 3.42
          Mean value_function loss: 238.1342
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 67.6856
                       Mean reward: 829.39
               Mean episode length: 224.39
    Episode_Reward/reaching_object: 1.4612
     Episode_Reward/lifting_object: 162.2216
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 6.43s
                      Time elapsed: 01:14:04
                               ETA: 00:02:55

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 15195 steps/s (collection: 6.171s, learning 0.298s)
             Mean action noise std: 3.42
          Mean value_function loss: 251.7741
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 67.6950
                       Mean reward: 742.83
               Mean episode length: 211.46
    Episode_Reward/reaching_object: 1.4175
     Episode_Reward/lifting_object: 154.9477
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0626
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 6.47s
                      Time elapsed: 01:14:11
                               ETA: 00:02:53

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 15678 steps/s (collection: 5.751s, learning 0.519s)
             Mean action noise std: 3.42
          Mean value_function loss: 235.4490
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 67.7046
                       Mean reward: 795.79
               Mean episode length: 217.75
    Episode_Reward/reaching_object: 1.4285
     Episode_Reward/lifting_object: 157.5398
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0668
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 6.27s
                      Time elapsed: 01:14:17
                               ETA: 00:02:51

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 17129 steps/s (collection: 5.362s, learning 0.377s)
             Mean action noise std: 3.42
          Mean value_function loss: 243.3445
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 67.7079
                       Mean reward: 773.11
               Mean episode length: 214.57
    Episode_Reward/reaching_object: 1.4059
     Episode_Reward/lifting_object: 154.8401
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 5.74s
                      Time elapsed: 01:14:23
                               ETA: 00:02:48

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 16273 steps/s (collection: 5.532s, learning 0.509s)
             Mean action noise std: 3.42
          Mean value_function loss: 230.6047
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 67.7128
                       Mean reward: 739.73
               Mean episode length: 205.78
    Episode_Reward/reaching_object: 1.4231
     Episode_Reward/lifting_object: 157.4695
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 6.04s
                      Time elapsed: 01:14:29
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 15037 steps/s (collection: 6.025s, learning 0.512s)
             Mean action noise std: 3.42
          Mean value_function loss: 230.4483
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 67.7178
                       Mean reward: 790.70
               Mean episode length: 220.48
    Episode_Reward/reaching_object: 1.4632
     Episode_Reward/lifting_object: 160.5715
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0690
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 6.54s
                      Time elapsed: 01:14:35
                               ETA: 00:02:44

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 15068 steps/s (collection: 6.090s, learning 0.434s)
             Mean action noise std: 3.42
          Mean value_function loss: 301.0175
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 67.7234
                       Mean reward: 774.07
               Mean episode length: 213.50
    Episode_Reward/reaching_object: 1.3957
     Episode_Reward/lifting_object: 153.2222
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.5833
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 6.52s
                      Time elapsed: 01:14:42
                               ETA: 00:02:42

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 14888 steps/s (collection: 6.265s, learning 0.338s)
             Mean action noise std: 3.42
          Mean value_function loss: 229.9253
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 67.7281
                       Mean reward: 799.77
               Mean episode length: 222.38
    Episode_Reward/reaching_object: 1.4420
     Episode_Reward/lifting_object: 158.7349
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 6.60s
                      Time elapsed: 01:14:48
                               ETA: 00:02:40

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 19741 steps/s (collection: 4.677s, learning 0.302s)
             Mean action noise std: 3.42
          Mean value_function loss: 232.4305
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 67.7303
                       Mean reward: 751.16
               Mean episode length: 211.48
    Episode_Reward/reaching_object: 1.3895
     Episode_Reward/lifting_object: 152.9614
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0618
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 4.98s
                      Time elapsed: 01:14:53
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 15809 steps/s (collection: 5.760s, learning 0.458s)
             Mean action noise std: 3.42
          Mean value_function loss: 214.8873
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 67.7332
                       Mean reward: 826.00
               Mean episode length: 227.89
    Episode_Reward/reaching_object: 1.4696
     Episode_Reward/lifting_object: 162.8791
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 6.22s
                      Time elapsed: 01:14:59
                               ETA: 00:02:35

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 16367 steps/s (collection: 5.574s, learning 0.432s)
             Mean action noise std: 3.42
          Mean value_function loss: 241.4866
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 67.7374
                       Mean reward: 796.73
               Mean episode length: 217.78
    Episode_Reward/reaching_object: 1.3984
     Episode_Reward/lifting_object: 154.5008
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 6.01s
                      Time elapsed: 01:15:05
                               ETA: 00:02:33

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 18365 steps/s (collection: 5.064s, learning 0.289s)
             Mean action noise std: 3.42
          Mean value_function loss: 257.3111
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 67.7397
                       Mean reward: 757.96
               Mean episode length: 210.43
    Episode_Reward/reaching_object: 1.4315
     Episode_Reward/lifting_object: 158.5576
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 5.35s
                      Time elapsed: 01:15:11
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 16882 steps/s (collection: 5.333s, learning 0.489s)
             Mean action noise std: 3.42
          Mean value_function loss: 246.1108
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.7434
                       Mean reward: 733.22
               Mean episode length: 206.71
    Episode_Reward/reaching_object: 1.4185
     Episode_Reward/lifting_object: 156.7711
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0680
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 5.82s
                      Time elapsed: 01:15:17
                               ETA: 00:02:29

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 16109 steps/s (collection: 5.559s, learning 0.543s)
             Mean action noise std: 3.43
          Mean value_function loss: 237.9726
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.7527
                       Mean reward: 776.99
               Mean episode length: 216.75
    Episode_Reward/reaching_object: 1.3979
     Episode_Reward/lifting_object: 155.2641
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 6.10s
                      Time elapsed: 01:15:23
                               ETA: 00:02:27

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 16178 steps/s (collection: 5.635s, learning 0.442s)
             Mean action noise std: 3.43
          Mean value_function loss: 248.0119
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 67.7671
                       Mean reward: 777.00
               Mean episode length: 216.66
    Episode_Reward/reaching_object: 1.4042
     Episode_Reward/lifting_object: 155.7791
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 6.08s
                      Time elapsed: 01:15:29
                               ETA: 00:02:24

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 17476 steps/s (collection: 5.230s, learning 0.395s)
             Mean action noise std: 3.43
          Mean value_function loss: 246.4468
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 67.7751
                       Mean reward: 765.47
               Mean episode length: 211.84
    Episode_Reward/reaching_object: 1.3955
     Episode_Reward/lifting_object: 154.5066
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0617
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 5.62s
                      Time elapsed: 01:15:34
                               ETA: 00:02:22

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 16666 steps/s (collection: 5.432s, learning 0.466s)
             Mean action noise std: 3.43
          Mean value_function loss: 263.6632
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 67.7810
                       Mean reward: 796.32
               Mean episode length: 217.32
    Episode_Reward/reaching_object: 1.4300
     Episode_Reward/lifting_object: 158.0501
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 5.90s
                      Time elapsed: 01:15:40
                               ETA: 00:02:20

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 14831 steps/s (collection: 6.243s, learning 0.385s)
             Mean action noise std: 3.43
          Mean value_function loss: 192.0730
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 67.7884
                       Mean reward: 822.66
               Mean episode length: 224.55
    Episode_Reward/reaching_object: 1.4594
     Episode_Reward/lifting_object: 161.4364
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0637
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 6.63s
                      Time elapsed: 01:15:47
                               ETA: 00:02:18

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 15681 steps/s (collection: 5.822s, learning 0.447s)
             Mean action noise std: 3.43
          Mean value_function loss: 233.7623
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 67.7957
                       Mean reward: 823.61
               Mean episode length: 223.32
    Episode_Reward/reaching_object: 1.4371
     Episode_Reward/lifting_object: 159.1422
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 6.27s
                      Time elapsed: 01:15:53
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 14289 steps/s (collection: 6.461s, learning 0.418s)
             Mean action noise std: 3.43
          Mean value_function loss: 207.6051
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 67.8047
                       Mean reward: 773.35
               Mean episode length: 210.89
    Episode_Reward/reaching_object: 1.4303
     Episode_Reward/lifting_object: 158.1070
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 6.88s
                      Time elapsed: 01:16:00
                               ETA: 00:02:13

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 17253 steps/s (collection: 5.305s, learning 0.393s)
             Mean action noise std: 3.43
          Mean value_function loss: 202.3282
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 67.8091
                       Mean reward: 762.83
               Mean episode length: 210.46
    Episode_Reward/reaching_object: 1.3518
     Episode_Reward/lifting_object: 149.2161
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 5.70s
                      Time elapsed: 01:16:06
                               ETA: 00:02:11

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 16770 steps/s (collection: 5.552s, learning 0.309s)
             Mean action noise std: 3.43
          Mean value_function loss: 196.1546
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 67.8119
                       Mean reward: 813.60
               Mean episode length: 222.22
    Episode_Reward/reaching_object: 1.4613
     Episode_Reward/lifting_object: 162.7728
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 5.86s
                      Time elapsed: 01:16:12
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 19032 steps/s (collection: 4.849s, learning 0.316s)
             Mean action noise std: 3.44
          Mean value_function loss: 235.7474
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 67.8185
                       Mean reward: 784.54
               Mean episode length: 214.75
    Episode_Reward/reaching_object: 1.4049
     Episode_Reward/lifting_object: 156.1661
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 5.16s
                      Time elapsed: 01:16:17
                               ETA: 00:02:06

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 14703 steps/s (collection: 6.221s, learning 0.465s)
             Mean action noise std: 3.44
          Mean value_function loss: 186.9698
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 67.8306
                       Mean reward: 815.17
               Mean episode length: 223.19
    Episode_Reward/reaching_object: 1.4638
     Episode_Reward/lifting_object: 162.2748
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 6.69s
                      Time elapsed: 01:16:23
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 16069 steps/s (collection: 5.671s, learning 0.446s)
             Mean action noise std: 3.44
          Mean value_function loss: 237.2391
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.8383
                       Mean reward: 764.68
               Mean episode length: 215.09
    Episode_Reward/reaching_object: 1.4222
     Episode_Reward/lifting_object: 156.5391
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 6.12s
                      Time elapsed: 01:16:30
                               ETA: 00:02:02

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 17063 steps/s (collection: 5.415s, learning 0.346s)
             Mean action noise std: 3.44
          Mean value_function loss: 208.5154
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 67.8453
                       Mean reward: 830.29
               Mean episode length: 227.44
    Episode_Reward/reaching_object: 1.4628
     Episode_Reward/lifting_object: 161.9677
      Episode_Reward/object_height: 0.0150
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 5.76s
                      Time elapsed: 01:16:35
                               ETA: 00:02:00

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 15976 steps/s (collection: 5.759s, learning 0.394s)
             Mean action noise std: 3.44
          Mean value_function loss: 200.9394
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 67.8524
                       Mean reward: 787.42
               Mean episode length: 217.29
    Episode_Reward/reaching_object: 1.4746
     Episode_Reward/lifting_object: 163.2988
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 6.15s
                      Time elapsed: 01:16:42
                               ETA: 00:01:57

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 15637 steps/s (collection: 5.925s, learning 0.361s)
             Mean action noise std: 3.44
          Mean value_function loss: 230.8545
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 67.8635
                       Mean reward: 818.35
               Mean episode length: 221.53
    Episode_Reward/reaching_object: 1.4429
     Episode_Reward/lifting_object: 159.7803
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 6.29s
                      Time elapsed: 01:16:48
                               ETA: 00:01:55

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 20546 steps/s (collection: 4.412s, learning 0.373s)
             Mean action noise std: 3.44
          Mean value_function loss: 203.6350
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 67.8734
                       Mean reward: 821.17
               Mean episode length: 226.29
    Episode_Reward/reaching_object: 1.4939
     Episode_Reward/lifting_object: 165.2460
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 4.78s
                      Time elapsed: 01:16:53
                               ETA: 00:01:53

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 16753 steps/s (collection: 5.431s, learning 0.436s)
             Mean action noise std: 3.44
          Mean value_function loss: 212.5881
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 67.8765
                       Mean reward: 800.98
               Mean episode length: 222.23
    Episode_Reward/reaching_object: 1.4306
     Episode_Reward/lifting_object: 157.9586
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 5.87s
                      Time elapsed: 01:16:58
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 16276 steps/s (collection: 5.669s, learning 0.370s)
             Mean action noise std: 3.44
          Mean value_function loss: 201.0842
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 67.8775
                       Mean reward: 812.64
               Mean episode length: 221.13
    Episode_Reward/reaching_object: 1.4827
     Episode_Reward/lifting_object: 163.9490
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 6.04s
                      Time elapsed: 01:17:04
                               ETA: 00:01:48

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 14866 steps/s (collection: 6.213s, learning 0.400s)
             Mean action noise std: 3.44
          Mean value_function loss: 218.9503
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 67.8796
                       Mean reward: 820.21
               Mean episode length: 221.07
    Episode_Reward/reaching_object: 1.4801
     Episode_Reward/lifting_object: 163.9902
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 6.61s
                      Time elapsed: 01:17:11
                               ETA: 00:01:46

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 16819 steps/s (collection: 5.425s, learning 0.420s)
             Mean action noise std: 3.45
          Mean value_function loss: 165.4164
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 67.8850
                       Mean reward: 807.53
               Mean episode length: 222.61
    Episode_Reward/reaching_object: 1.4755
     Episode_Reward/lifting_object: 162.7054
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 5.84s
                      Time elapsed: 01:17:17
                               ETA: 00:01:44

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 17366 steps/s (collection: 5.228s, learning 0.432s)
             Mean action noise std: 3.45
          Mean value_function loss: 218.8296
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 67.8950
                       Mean reward: 782.25
               Mean episode length: 215.46
    Episode_Reward/reaching_object: 1.4515
     Episode_Reward/lifting_object: 159.5832
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 5.66s
                      Time elapsed: 01:17:23
                               ETA: 00:01:41

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 18306 steps/s (collection: 4.953s, learning 0.417s)
             Mean action noise std: 3.45
          Mean value_function loss: 230.0299
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 67.9038
                       Mean reward: 838.48
               Mean episode length: 229.16
    Episode_Reward/reaching_object: 1.4615
     Episode_Reward/lifting_object: 161.3764
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 5.37s
                      Time elapsed: 01:17:28
                               ETA: 00:01:39

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 17060 steps/s (collection: 5.338s, learning 0.424s)
             Mean action noise std: 3.45
          Mean value_function loss: 232.1616
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 67.9156
                       Mean reward: 825.88
               Mean episode length: 224.58
    Episode_Reward/reaching_object: 1.4391
     Episode_Reward/lifting_object: 159.6917
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 5.76s
                      Time elapsed: 01:17:34
                               ETA: 00:01:37

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 15729 steps/s (collection: 5.812s, learning 0.438s)
             Mean action noise std: 3.45
          Mean value_function loss: 226.0226
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 67.9297
                       Mean reward: 802.17
               Mean episode length: 216.98
    Episode_Reward/reaching_object: 1.4221
     Episode_Reward/lifting_object: 156.2001
      Episode_Reward/object_height: 0.0144
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 6.25s
                      Time elapsed: 01:17:40
                               ETA: 00:01:35

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 17407 steps/s (collection: 5.240s, learning 0.408s)
             Mean action noise std: 3.45
          Mean value_function loss: 214.3285
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 67.9463
                       Mean reward: 837.35
               Mean episode length: 225.88
    Episode_Reward/reaching_object: 1.4494
     Episode_Reward/lifting_object: 160.5827
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 5.65s
                      Time elapsed: 01:17:46
                               ETA: 00:01:32

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 18768 steps/s (collection: 4.839s, learning 0.399s)
             Mean action noise std: 3.45
          Mean value_function loss: 236.2615
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 67.9546
                       Mean reward: 802.19
               Mean episode length: 218.45
    Episode_Reward/reaching_object: 1.4162
     Episode_Reward/lifting_object: 156.5999
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 5.24s
                      Time elapsed: 01:17:51
                               ETA: 00:01:30

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 17083 steps/s (collection: 5.359s, learning 0.396s)
             Mean action noise std: 3.46
          Mean value_function loss: 243.6364
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 67.9636
                       Mean reward: 774.73
               Mean episode length: 217.51
    Episode_Reward/reaching_object: 1.4120
     Episode_Reward/lifting_object: 155.8160
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 5.75s
                      Time elapsed: 01:17:57
                               ETA: 00:01:28

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 18105 steps/s (collection: 5.199s, learning 0.231s)
             Mean action noise std: 3.46
          Mean value_function loss: 236.4296
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 67.9726
                       Mean reward: 815.21
               Mean episode length: 223.55
    Episode_Reward/reaching_object: 1.4325
     Episode_Reward/lifting_object: 158.2287
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0631
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 5.43s
                      Time elapsed: 01:18:02
                               ETA: 00:01:25

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 19830 steps/s (collection: 4.539s, learning 0.419s)
             Mean action noise std: 3.46
          Mean value_function loss: 199.2236
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 67.9836
                       Mean reward: 839.40
               Mean episode length: 225.64
    Episode_Reward/reaching_object: 1.4565
     Episode_Reward/lifting_object: 161.1287
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 4.96s
                      Time elapsed: 01:18:07
                               ETA: 00:01:23

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 15997 steps/s (collection: 5.752s, learning 0.393s)
             Mean action noise std: 3.46
          Mean value_function loss: 207.8139
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 67.9910
                       Mean reward: 807.41
               Mean episode length: 219.55
    Episode_Reward/reaching_object: 1.4475
     Episode_Reward/lifting_object: 160.7079
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 6.14s
                      Time elapsed: 01:18:13
                               ETA: 00:01:21

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 13902 steps/s (collection: 6.628s, learning 0.443s)
             Mean action noise std: 3.46
          Mean value_function loss: 223.0861
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 67.9966
                       Mean reward: 770.27
               Mean episode length: 212.56
    Episode_Reward/reaching_object: 1.4607
     Episode_Reward/lifting_object: 161.7349
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 7.07s
                      Time elapsed: 01:18:20
                               ETA: 00:01:18

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 14049 steps/s (collection: 6.551s, learning 0.446s)
             Mean action noise std: 3.46
          Mean value_function loss: 226.6546
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.0031
                       Mean reward: 831.53
               Mean episode length: 226.33
    Episode_Reward/reaching_object: 1.4119
     Episode_Reward/lifting_object: 155.9769
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 7.00s
                      Time elapsed: 01:18:27
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 16540 steps/s (collection: 5.697s, learning 0.247s)
             Mean action noise std: 3.46
          Mean value_function loss: 271.0446
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 68.0174
                       Mean reward: 778.21
               Mean episode length: 212.79
    Episode_Reward/reaching_object: 1.4237
     Episode_Reward/lifting_object: 157.4849
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 5.94s
                      Time elapsed: 01:18:33
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 14796 steps/s (collection: 6.143s, learning 0.501s)
             Mean action noise std: 3.46
          Mean value_function loss: 235.0631
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 68.0273
                       Mean reward: 760.99
               Mean episode length: 207.71
    Episode_Reward/reaching_object: 1.4048
     Episode_Reward/lifting_object: 155.3413
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 6.64s
                      Time elapsed: 01:18:40
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 18118 steps/s (collection: 5.120s, learning 0.306s)
             Mean action noise std: 3.47
          Mean value_function loss: 215.7203
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 68.0342
                       Mean reward: 817.39
               Mean episode length: 224.21
    Episode_Reward/reaching_object: 1.4394
     Episode_Reward/lifting_object: 158.7081
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 5.43s
                      Time elapsed: 01:18:45
                               ETA: 00:01:09

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 20706 steps/s (collection: 4.395s, learning 0.353s)
             Mean action noise std: 3.47
          Mean value_function loss: 246.8074
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 68.0490
                       Mean reward: 893.09
               Mean episode length: 240.38
    Episode_Reward/reaching_object: 1.4717
     Episode_Reward/lifting_object: 162.5850
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 4.75s
                      Time elapsed: 01:18:50
                               ETA: 00:01:07

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 20219 steps/s (collection: 4.427s, learning 0.435s)
             Mean action noise std: 3.47
          Mean value_function loss: 228.4837
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 68.0623
                       Mean reward: 779.58
               Mean episode length: 217.54
    Episode_Reward/reaching_object: 1.4115
     Episode_Reward/lifting_object: 155.5649
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 4.86s
                      Time elapsed: 01:18:55
                               ETA: 00:01:04

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 19224 steps/s (collection: 4.979s, learning 0.134s)
             Mean action noise std: 3.47
          Mean value_function loss: 212.9351
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 68.0652
                       Mean reward: 812.70
               Mean episode length: 219.74
    Episode_Reward/reaching_object: 1.4379
     Episode_Reward/lifting_object: 159.3524
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 5.11s
                      Time elapsed: 01:19:00
                               ETA: 00:01:02

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 44752 steps/s (collection: 2.084s, learning 0.113s)
             Mean action noise std: 3.47
          Mean value_function loss: 232.2678
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 68.0687
                       Mean reward: 782.77
               Mean episode length: 213.33
    Episode_Reward/reaching_object: 1.4507
     Episode_Reward/lifting_object: 160.9308
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 2.20s
                      Time elapsed: 01:19:02
                               ETA: 00:01:00

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 46728 steps/s (collection: 1.987s, learning 0.117s)
             Mean action noise std: 3.47
          Mean value_function loss: 240.5795
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 68.0781
                       Mean reward: 805.38
               Mean episode length: 220.65
    Episode_Reward/reaching_object: 1.4523
     Episode_Reward/lifting_object: 159.8407
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 2.10s
                      Time elapsed: 01:19:04
                               ETA: 00:00:57

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 47460 steps/s (collection: 1.967s, learning 0.104s)
             Mean action noise std: 3.47
          Mean value_function loss: 256.5666
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 68.0875
                       Mean reward: 831.90
               Mean episode length: 226.93
    Episode_Reward/reaching_object: 1.4858
     Episode_Reward/lifting_object: 164.8226
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 2.07s
                      Time elapsed: 01:19:06
                               ETA: 00:00:55

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 20058 steps/s (collection: 4.369s, learning 0.532s)
             Mean action noise std: 3.47
          Mean value_function loss: 270.7549
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 68.0974
                       Mean reward: 788.99
               Mean episode length: 219.35
    Episode_Reward/reaching_object: 1.4361
     Episode_Reward/lifting_object: 158.0983
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0650
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 4.90s
                      Time elapsed: 01:19:11
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 14610 steps/s (collection: 6.240s, learning 0.488s)
             Mean action noise std: 3.48
          Mean value_function loss: 236.3256
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 68.1127
                       Mean reward: 858.28
               Mean episode length: 231.67
    Episode_Reward/reaching_object: 1.4399
     Episode_Reward/lifting_object: 158.9346
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 6.73s
                      Time elapsed: 01:19:18
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 17616 steps/s (collection: 5.163s, learning 0.417s)
             Mean action noise std: 3.48
          Mean value_function loss: 191.8184
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 68.1336
                       Mean reward: 817.87
               Mean episode length: 222.93
    Episode_Reward/reaching_object: 1.4427
     Episode_Reward/lifting_object: 158.7756
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 5.58s
                      Time elapsed: 01:19:24
                               ETA: 00:00:48

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 16497 steps/s (collection: 5.506s, learning 0.452s)
             Mean action noise std: 3.48
          Mean value_function loss: 216.1868
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 68.1403
                       Mean reward: 796.35
               Mean episode length: 222.36
    Episode_Reward/reaching_object: 1.4252
     Episode_Reward/lifting_object: 156.8318
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 5.96s
                      Time elapsed: 01:19:30
                               ETA: 00:00:45

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 16513 steps/s (collection: 5.467s, learning 0.487s)
             Mean action noise std: 3.48
          Mean value_function loss: 241.0750
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.1458
                       Mean reward: 800.17
               Mean episode length: 219.21
    Episode_Reward/reaching_object: 1.4551
     Episode_Reward/lifting_object: 160.2163
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 5.95s
                      Time elapsed: 01:19:35
                               ETA: 00:00:43

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 17291 steps/s (collection: 5.235s, learning 0.450s)
             Mean action noise std: 3.48
          Mean value_function loss: 227.2902
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 68.1540
                       Mean reward: 842.70
               Mean episode length: 228.16
    Episode_Reward/reaching_object: 1.4892
     Episode_Reward/lifting_object: 164.4270
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0666
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 5.68s
                      Time elapsed: 01:19:41
                               ETA: 00:00:40

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 15416 steps/s (collection: 5.998s, learning 0.378s)
             Mean action noise std: 3.48
          Mean value_function loss: 252.6886
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 68.1622
                       Mean reward: 845.82
               Mean episode length: 228.80
    Episode_Reward/reaching_object: 1.4310
     Episode_Reward/lifting_object: 156.9796
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 6.38s
                      Time elapsed: 01:19:48
                               ETA: 00:00:38

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 17810 steps/s (collection: 5.137s, learning 0.382s)
             Mean action noise std: 3.48
          Mean value_function loss: 229.7207
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 68.1667
                       Mean reward: 796.77
               Mean episode length: 218.61
    Episode_Reward/reaching_object: 1.4482
     Episode_Reward/lifting_object: 158.5709
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 5.52s
                      Time elapsed: 01:19:53
                               ETA: 00:00:36

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 15747 steps/s (collection: 5.774s, learning 0.468s)
             Mean action noise std: 3.48
          Mean value_function loss: 226.1223
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 68.1716
                       Mean reward: 791.94
               Mean episode length: 216.24
    Episode_Reward/reaching_object: 1.4478
     Episode_Reward/lifting_object: 159.4980
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0661
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 6.24s
                      Time elapsed: 01:19:59
                               ETA: 00:00:33

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 16322 steps/s (collection: 5.672s, learning 0.351s)
             Mean action noise std: 3.49
          Mean value_function loss: 249.7497
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 68.1760
                       Mean reward: 763.62
               Mean episode length: 213.41
    Episode_Reward/reaching_object: 1.4393
     Episode_Reward/lifting_object: 158.5785
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 6.02s
                      Time elapsed: 01:20:05
                               ETA: 00:00:31

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 15757 steps/s (collection: 5.747s, learning 0.492s)
             Mean action noise std: 3.49
          Mean value_function loss: 226.7133
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 68.1844
                       Mean reward: 765.04
               Mean episode length: 213.52
    Episode_Reward/reaching_object: 1.4290
     Episode_Reward/lifting_object: 156.5048
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0650
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 6.24s
                      Time elapsed: 01:20:12
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 15922 steps/s (collection: 5.738s, learning 0.436s)
             Mean action noise std: 3.49
          Mean value_function loss: 246.5157
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 68.1921
                       Mean reward: 828.77
               Mean episode length: 224.80
    Episode_Reward/reaching_object: 1.4468
     Episode_Reward/lifting_object: 160.0123
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0648
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 6.17s
                      Time elapsed: 01:20:18
                               ETA: 00:00:26

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 15192 steps/s (collection: 6.056s, learning 0.415s)
             Mean action noise std: 3.49
          Mean value_function loss: 264.5717
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 68.1971
                       Mean reward: 810.62
               Mean episode length: 222.00
    Episode_Reward/reaching_object: 1.4273
     Episode_Reward/lifting_object: 156.8635
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 6.47s
                      Time elapsed: 01:20:24
                               ETA: 00:00:24

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 16430 steps/s (collection: 5.492s, learning 0.491s)
             Mean action noise std: 3.49
          Mean value_function loss: 239.7360
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 68.2038
                       Mean reward: 734.54
               Mean episode length: 215.37
    Episode_Reward/reaching_object: 1.4301
     Episode_Reward/lifting_object: 155.9880
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0684
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 5.98s
                      Time elapsed: 01:20:30
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 15306 steps/s (collection: 6.060s, learning 0.362s)
             Mean action noise std: 3.49
          Mean value_function loss: 199.2921
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 68.2107
                       Mean reward: 844.77
               Mean episode length: 228.07
    Episode_Reward/reaching_object: 1.4685
     Episode_Reward/lifting_object: 162.2069
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0661
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 6.42s
                      Time elapsed: 01:20:37
                               ETA: 00:00:19

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 19034 steps/s (collection: 4.854s, learning 0.310s)
             Mean action noise std: 3.49
          Mean value_function loss: 236.9116
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 68.2206
                       Mean reward: 702.18
               Mean episode length: 200.95
    Episode_Reward/reaching_object: 1.3746
     Episode_Reward/lifting_object: 150.4427
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 5.16s
                      Time elapsed: 01:20:42
                               ETA: 00:00:16

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 16732 steps/s (collection: 5.568s, learning 0.307s)
             Mean action noise std: 3.49
          Mean value_function loss: 208.3947
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 68.2294
                       Mean reward: 769.38
               Mean episode length: 213.66
    Episode_Reward/reaching_object: 1.4419
     Episode_Reward/lifting_object: 159.7126
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 5.88s
                      Time elapsed: 01:20:48
                               ETA: 00:00:14

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 15436 steps/s (collection: 5.911s, learning 0.457s)
             Mean action noise std: 3.49
          Mean value_function loss: 213.5521
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.2374
                       Mean reward: 792.21
               Mean episode length: 219.12
    Episode_Reward/reaching_object: 1.4143
     Episode_Reward/lifting_object: 155.6199
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 6.37s
                      Time elapsed: 01:20:54
                               ETA: 00:00:12

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 17325 steps/s (collection: 5.273s, learning 0.401s)
             Mean action noise std: 3.50
          Mean value_function loss: 181.0725
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 68.2470
                       Mean reward: 814.59
               Mean episode length: 219.77
    Episode_Reward/reaching_object: 1.4431
     Episode_Reward/lifting_object: 160.8274
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 5.67s
                      Time elapsed: 01:21:00
                               ETA: 00:00:09

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 15225 steps/s (collection: 6.049s, learning 0.408s)
             Mean action noise std: 3.50
          Mean value_function loss: 218.3629
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 68.2538
                       Mean reward: 813.95
               Mean episode length: 221.71
    Episode_Reward/reaching_object: 1.4430
     Episode_Reward/lifting_object: 160.1635
      Episode_Reward/object_height: 0.0149
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 6.46s
                      Time elapsed: 01:21:06
                               ETA: 00:00:07

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 14914 steps/s (collection: 6.172s, learning 0.419s)
             Mean action noise std: 3.50
          Mean value_function loss: 188.8017
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 68.2583
                       Mean reward: 807.17
               Mean episode length: 222.81
    Episode_Reward/reaching_object: 1.4191
     Episode_Reward/lifting_object: 157.4872
      Episode_Reward/object_height: 0.0145
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 6.59s
                      Time elapsed: 01:21:13
                               ETA: 00:00:04

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 14804 steps/s (collection: 6.194s, learning 0.446s)
             Mean action noise std: 3.50
          Mean value_function loss: 241.7060
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 68.2624
                       Mean reward: 749.14
               Mean episode length: 209.71
    Episode_Reward/reaching_object: 1.3752
     Episode_Reward/lifting_object: 151.7789
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 6.64s
                      Time elapsed: 01:21:19
                               ETA: 00:00:02

