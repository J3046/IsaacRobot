################################################################################
                      [1m Learning iteration 0/2000 [0m                       

                       Computation: 7844 steps/s (collection: 12.112s, learning 0.420s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0027
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 36.9246
                       Mean reward: 0.00
               Mean episode length: 21.40
    Episode_Reward/reaching_object: 0.0009
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0003
          Episode_Reward/joint_vel: -0.0004
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 12.53s
                      Time elapsed: 00:00:12
                               ETA: 06:57:44

################################################################################
                      [1m Learning iteration 1/2000 [0m                       

                       Computation: 11373 steps/s (collection: 8.402s, learning 0.241s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 37.0403
                       Mean reward: 0.01
               Mean episode length: 45.09
    Episode_Reward/reaching_object: 0.0024
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0008
          Episode_Reward/joint_vel: -0.0011
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 8.64s
                      Time elapsed: 00:00:21
                               ETA: 05:52:44

################################################################################
                      [1m Learning iteration 2/2000 [0m                       

                       Computation: 11364 steps/s (collection: 8.456s, learning 0.195s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 37.1084
                       Mean reward: 0.01
               Mean episode length: 69.30
    Episode_Reward/reaching_object: 0.0040
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0013
          Episode_Reward/joint_vel: -0.0019
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 8.65s
                      Time elapsed: 00:00:29
                               ETA: 05:31:03

################################################################################
                      [1m Learning iteration 3/2000 [0m                       

                       Computation: 11085 steps/s (collection: 8.664s, learning 0.204s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 37.0975
                       Mean reward: 0.01
               Mean episode length: 93.06
    Episode_Reward/reaching_object: 0.0056
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0018
          Episode_Reward/joint_vel: -0.0026
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 8.87s
                      Time elapsed: 00:00:38
                               ETA: 05:21:57

################################################################################
                      [1m Learning iteration 4/2000 [0m                       

                       Computation: 12365 steps/s (collection: 7.777s, learning 0.173s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 37.0832
                       Mean reward: 0.01
               Mean episode length: 117.24
    Episode_Reward/reaching_object: 0.0072
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0023
          Episode_Reward/joint_vel: -0.0033
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 7.95s
                      Time elapsed: 00:00:46
                               ETA: 05:10:20

################################################################################
                      [1m Learning iteration 5/2000 [0m                       

                       Computation: 11427 steps/s (collection: 8.388s, learning 0.215s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 37.0700
                       Mean reward: 0.01
               Mean episode length: 141.16
    Episode_Reward/reaching_object: 0.0086
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0028
          Episode_Reward/joint_vel: -0.0041
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 8.60s
                      Time elapsed: 00:00:55
                               ETA: 05:06:09

################################################################################
                      [1m Learning iteration 6/2000 [0m                       

                       Computation: 11097 steps/s (collection: 8.653s, learning 0.205s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 37.0595
                       Mean reward: 0.02
               Mean episode length: 165.69
    Episode_Reward/reaching_object: 0.0105
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0048
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 8.86s
                      Time elapsed: 00:01:04
                               ETA: 05:04:20

################################################################################
                      [1m Learning iteration 7/2000 [0m                       

                       Computation: 11568 steps/s (collection: 8.158s, learning 0.339s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 37.0398
                       Mean reward: 0.03
               Mean episode length: 189.43
    Episode_Reward/reaching_object: 0.0123
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0038
          Episode_Reward/joint_vel: -0.0055
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 8.50s
                      Time elapsed: 00:01:12
                               ETA: 05:01:26

################################################################################
                      [1m Learning iteration 8/2000 [0m                       

                       Computation: 10471 steps/s (collection: 9.164s, learning 0.223s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0072
                 Mean entropy loss: 37.0260
                       Mean reward: 0.03
               Mean episode length: 213.24
    Episode_Reward/reaching_object: 0.0152
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0043
          Episode_Reward/joint_vel: -0.0062
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 9.39s
                      Time elapsed: 00:01:21
                               ETA: 05:02:26

################################################################################
                      [1m Learning iteration 9/2000 [0m                       

                       Computation: 33450 steps/s (collection: 2.711s, learning 0.228s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 37.0322
                       Mean reward: 0.05
               Mean episode length: 237.77
    Episode_Reward/reaching_object: 0.0190
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0048
          Episode_Reward/joint_vel: -0.0070
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 2.94s
                      Time elapsed: 00:01:24
                               ETA: 04:41:49

################################################################################
                      [1m Learning iteration 10/2000 [0m                      

                       Computation: 31553 steps/s (collection: 2.765s, learning 0.351s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0077
                 Mean entropy loss: 37.0029
                       Mean reward: 0.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0203
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 3.12s
                      Time elapsed: 00:01:28
                               ETA: 04:25:27

################################################################################
                      [1m Learning iteration 11/2000 [0m                      

                       Computation: 30670 steps/s (collection: 2.944s, learning 0.261s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0074
                 Mean entropy loss: 37.0147
                       Mean reward: 0.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0254
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 3.21s
                      Time elapsed: 00:01:31
                               ETA: 04:12:04

################################################################################
                      [1m Learning iteration 12/2000 [0m                      

                       Computation: 28954 steps/s (collection: 3.089s, learning 0.306s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 37.0298
                       Mean reward: 0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0306
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 3.40s
                      Time elapsed: 00:01:34
                               ETA: 04:01:13

################################################################################
                      [1m Learning iteration 13/2000 [0m                      

                       Computation: 30829 steps/s (collection: 2.897s, learning 0.292s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 37.0598
                       Mean reward: 0.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0375
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 3.19s
                      Time elapsed: 00:01:37
                               ETA: 03:51:25

################################################################################
                      [1m Learning iteration 14/2000 [0m                      

                       Computation: 33435 steps/s (collection: 2.766s, learning 0.174s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 37.1093
                       Mean reward: 0.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0546
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 2.94s
                      Time elapsed: 00:01:40
                               ETA: 03:42:22

################################################################################
                      [1m Learning iteration 15/2000 [0m                      

                       Computation: 37837 steps/s (collection: 2.347s, learning 0.251s)
             Mean action noise std: 1.01
          Mean value_function loss: 1.2688
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 37.1649
                       Mean reward: 0.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0715
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 2.60s
                      Time elapsed: 00:01:43
                               ETA: 03:33:44

################################################################################
                      [1m Learning iteration 16/2000 [0m                      

                       Computation: 32694 steps/s (collection: 2.711s, learning 0.296s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.1371
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 37.2619
                       Mean reward: 0.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0998
     Episode_Reward/lifting_object: -0.1491
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 3.01s
                      Time elapsed: 00:01:46
                               ETA: 03:26:54

################################################################################
                      [1m Learning iteration 17/2000 [0m                      

                       Computation: 34181 steps/s (collection: 2.700s, learning 0.176s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.5398
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 37.5556
                       Mean reward: 0.69
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 0.1284
     Episode_Reward/lifting_object: -0.0468
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 2.88s
                      Time elapsed: 00:01:49
                               ETA: 03:20:36

################################################################################
                      [1m Learning iteration 18/2000 [0m                      

                       Computation: 31048 steps/s (collection: 2.824s, learning 0.342s)
             Mean action noise std: 1.04
          Mean value_function loss: 2.2873
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 37.6991
                       Mean reward: -1.04
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 0.1498
     Episode_Reward/lifting_object: -0.1416
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0074
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 3.17s
                      Time elapsed: 00:01:52
                               ETA: 03:15:27

################################################################################
                      [1m Learning iteration 19/2000 [0m                      

                       Computation: 31057 steps/s (collection: 2.875s, learning 0.291s)
             Mean action noise std: 1.04
          Mean value_function loss: 5.3664
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 37.8730
                       Mean reward: -0.46
               Mean episode length: 249.06
    Episode_Reward/reaching_object: 0.1844
     Episode_Reward/lifting_object: -0.5399
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0053
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 3.17s
                      Time elapsed: 00:01:55
                               ETA: 03:10:48

################################################################################
                      [1m Learning iteration 20/2000 [0m                      

                       Computation: 29838 steps/s (collection: 3.073s, learning 0.221s)
             Mean action noise std: 1.05
          Mean value_function loss: 7.5343
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 37.9881
                       Mean reward: -4.72
               Mean episode length: 248.16
    Episode_Reward/reaching_object: 0.2202
     Episode_Reward/lifting_object: -0.9830
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0075
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 3.29s
                      Time elapsed: 00:01:58
                               ETA: 03:06:48

################################################################################
                      [1m Learning iteration 21/2000 [0m                      

                       Computation: 29598 steps/s (collection: 3.086s, learning 0.236s)
             Mean action noise std: 1.05
          Mean value_function loss: 1.3922
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 38.1221
                       Mean reward: 1.23
               Mean episode length: 249.48
    Episode_Reward/reaching_object: 0.2418
     Episode_Reward/lifting_object: -0.3552
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0054
          Episode_Reward/joint_vel: -0.0076
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 3.32s
                      Time elapsed: 00:02:02
                               ETA: 03:03:12

################################################################################
                      [1m Learning iteration 22/2000 [0m                      

                       Computation: 27268 steps/s (collection: 3.305s, learning 0.300s)
             Mean action noise std: 1.06
          Mean value_function loss: 2.8538
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 38.2756
                       Mean reward: -1.72
               Mean episode length: 249.88
    Episode_Reward/reaching_object: 0.2537
     Episode_Reward/lifting_object: -0.1759
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0055
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 3.61s
                      Time elapsed: 00:02:05
                               ETA: 03:00:19

################################################################################
                      [1m Learning iteration 23/2000 [0m                      

                       Computation: 31676 steps/s (collection: 2.852s, learning 0.251s)
             Mean action noise std: 1.06
          Mean value_function loss: 1.8907
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 38.4360
                       Mean reward: 0.16
               Mean episode length: 249.48
    Episode_Reward/reaching_object: 0.2580
     Episode_Reward/lifting_object: -0.2097
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0078
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 3.10s
                      Time elapsed: 00:02:08
                               ETA: 02:56:58

################################################################################
                      [1m Learning iteration 24/2000 [0m                      

                       Computation: 29409 steps/s (collection: 3.086s, learning 0.257s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.3896
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 38.5642
                       Mean reward: 1.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2289
     Episode_Reward/lifting_object: -0.1383
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0056
          Episode_Reward/joint_vel: -0.0079
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 3.34s
                      Time elapsed: 00:02:12
                               ETA: 02:54:13

################################################################################
                      [1m Learning iteration 25/2000 [0m                      

                       Computation: 31383 steps/s (collection: 2.903s, learning 0.230s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 38.7296
                       Mean reward: 0.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2128
     Episode_Reward/lifting_object: -0.0256
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0057
          Episode_Reward/joint_vel: -0.0080
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 3.13s
                      Time elapsed: 00:02:15
                               ETA: 02:51:23

################################################################################
                      [1m Learning iteration 26/2000 [0m                      

                       Computation: 33789 steps/s (collection: 2.665s, learning 0.245s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 38.8550
                       Mean reward: 0.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1800
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0081
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 2.91s
                      Time elapsed: 00:02:18
                               ETA: 02:48:30

################################################################################
                      [1m Learning iteration 27/2000 [0m                      

                       Computation: 35367 steps/s (collection: 2.591s, learning 0.188s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 38.9544
                       Mean reward: 0.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1706
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0082
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 2.78s
                      Time elapsed: 00:02:21
                               ETA: 02:45:40

################################################################################
                      [1m Learning iteration 28/2000 [0m                      

                       Computation: 32224 steps/s (collection: 2.805s, learning 0.246s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 38.9726
                       Mean reward: 0.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1315
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0059
          Episode_Reward/joint_vel: -0.0083
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 3.05s
                      Time elapsed: 00:02:24
                               ETA: 02:43:20

################################################################################
                      [1m Learning iteration 29/2000 [0m                      

                       Computation: 33301 steps/s (collection: 2.718s, learning 0.234s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0078
                 Mean entropy loss: 38.9931
                       Mean reward: 0.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1161
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0060
          Episode_Reward/joint_vel: -0.0084
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 2.95s
                      Time elapsed: 00:02:27
                               ETA: 02:41:02

################################################################################
                      [1m Learning iteration 30/2000 [0m                      

                       Computation: 32541 steps/s (collection: 2.820s, learning 0.201s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 39.0200
                       Mean reward: 0.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1047
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 3.02s
                      Time elapsed: 00:02:30
                               ETA: 02:38:58

################################################################################
                      [1m Learning iteration 31/2000 [0m                      

                       Computation: 34299 steps/s (collection: 2.635s, learning 0.231s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.4166
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 39.0893
                       Mean reward: -0.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0983
     Episode_Reward/lifting_object: -0.0444
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0061
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 2.87s
                      Time elapsed: 00:02:32
                               ETA: 02:36:51

################################################################################
                      [1m Learning iteration 32/2000 [0m                      

                       Computation: 33643 steps/s (collection: 2.693s, learning 0.229s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 39.1326
                       Mean reward: 0.40
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0953
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 2.92s
                      Time elapsed: 00:02:35
                               ETA: 02:34:56

################################################################################
                      [1m Learning iteration 33/2000 [0m                      

                       Computation: 30037 steps/s (collection: 3.031s, learning 0.242s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0066
                 Mean entropy loss: 39.2171
                       Mean reward: 0.44
               Mean episode length: 249.95
    Episode_Reward/reaching_object: 0.0988
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0062
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 3.27s
                      Time elapsed: 00:02:39
                               ETA: 02:33:27

################################################################################
                      [1m Learning iteration 34/2000 [0m                      

                       Computation: 33399 steps/s (collection: 2.697s, learning 0.247s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0975
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 39.2491
                       Mean reward: 0.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1006
     Episode_Reward/lifting_object: -0.0238
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 2.94s
                      Time elapsed: 00:02:42
                               ETA: 02:31:45

################################################################################
                      [1m Learning iteration 35/2000 [0m                      

                       Computation: 33003 steps/s (collection: 2.764s, learning 0.215s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.1044
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 39.3115
                       Mean reward: 0.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1085
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 2.98s
                      Time elapsed: 00:02:45
                               ETA: 02:30:10

################################################################################
                      [1m Learning iteration 36/2000 [0m                      

                       Computation: 32597 steps/s (collection: 2.825s, learning 0.191s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0067
                 Mean entropy loss: 39.4981
                       Mean reward: 0.62
               Mean episode length: 249.83
    Episode_Reward/reaching_object: 0.1307
     Episode_Reward/lifting_object: -0.0476
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 3.02s
                      Time elapsed: 00:02:48
                               ETA: 02:28:42

################################################################################
                      [1m Learning iteration 37/2000 [0m                      

                       Computation: 31677 steps/s (collection: 2.856s, learning 0.247s)
             Mean action noise std: 1.11
          Mean value_function loss: 1.5441
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.5303
                       Mean reward: -1.63
               Mean episode length: 249.44
    Episode_Reward/reaching_object: 0.1601
     Episode_Reward/lifting_object: -0.1324
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 3.10s
                      Time elapsed: 00:02:51
                               ETA: 02:27:23

################################################################################
                      [1m Learning iteration 38/2000 [0m                      

                       Computation: 30016 steps/s (collection: 3.036s, learning 0.239s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.2769
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 39.5779
                       Mean reward: 0.41
               Mean episode length: 248.92
    Episode_Reward/reaching_object: 0.1763
     Episode_Reward/lifting_object: -0.0356
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 3.27s
                      Time elapsed: 00:02:54
                               ETA: 02:26:17

################################################################################
                      [1m Learning iteration 39/2000 [0m                      

                       Computation: 30758 steps/s (collection: 2.869s, learning 0.327s)
             Mean action noise std: 1.12
          Mean value_function loss: 1.7666
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 39.7386
                       Mean reward: -0.77
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 0.1973
     Episode_Reward/lifting_object: -0.1385
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 3.20s
                      Time elapsed: 00:02:57
                               ETA: 02:25:10

################################################################################
                      [1m Learning iteration 40/2000 [0m                      

                       Computation: 30870 steps/s (collection: 2.872s, learning 0.312s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.4725
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 39.8887
                       Mean reward: 1.01
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 0.2136
     Episode_Reward/lifting_object: -0.1333
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 3.18s
                      Time elapsed: 00:03:00
                               ETA: 02:24:05

################################################################################
                      [1m Learning iteration 41/2000 [0m                      

                       Computation: 29801 steps/s (collection: 3.063s, learning 0.236s)
             Mean action noise std: 1.13
          Mean value_function loss: 1.6446
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 40.0093
                       Mean reward: 0.29
               Mean episode length: 246.28
    Episode_Reward/reaching_object: 0.2223
     Episode_Reward/lifting_object: -0.1432
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 3.30s
                      Time elapsed: 00:03:04
                               ETA: 02:23:09

################################################################################
                      [1m Learning iteration 42/2000 [0m                      

                       Computation: 34542 steps/s (collection: 2.576s, learning 0.270s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.1320
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 40.1057
                       Mean reward: 1.10
               Mean episode length: 247.67
    Episode_Reward/reaching_object: 0.2398
     Episode_Reward/lifting_object: -0.1167
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 2.85s
                      Time elapsed: 00:03:06
                               ETA: 02:21:54

################################################################################
                      [1m Learning iteration 43/2000 [0m                      

                       Computation: 32077 steps/s (collection: 2.833s, learning 0.232s)
             Mean action noise std: 1.14
          Mean value_function loss: 0.4657
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 40.2149
                       Mean reward: 1.21
               Mean episode length: 248.00
    Episode_Reward/reaching_object: 0.2630
     Episode_Reward/lifting_object: -0.0583
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 3.06s
                      Time elapsed: 00:03:10
                               ETA: 02:20:53

################################################################################
                      [1m Learning iteration 44/2000 [0m                      

                       Computation: 33844 steps/s (collection: 2.660s, learning 0.245s)
             Mean action noise std: 1.14
          Mean value_function loss: 1.4159
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 40.3223
                       Mean reward: 0.20
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.2889
     Episode_Reward/lifting_object: -0.1247
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 2.90s
                      Time elapsed: 00:03:12
                               ETA: 02:19:47

################################################################################
                      [1m Learning iteration 45/2000 [0m                      

                       Computation: 41035 steps/s (collection: 2.232s, learning 0.163s)
             Mean action noise std: 1.15
          Mean value_function loss: 0.2922
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 40.4179
                       Mean reward: 1.18
               Mean episode length: 248.81
    Episode_Reward/reaching_object: 0.2466
     Episode_Reward/lifting_object: -0.1909
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 2.40s
                      Time elapsed: 00:03:15
                               ETA: 02:18:22

################################################################################
                      [1m Learning iteration 46/2000 [0m                      

                       Computation: 36111 steps/s (collection: 2.528s, learning 0.195s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.0618
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 40.5510
                       Mean reward: 1.16
               Mean episode length: 249.46
    Episode_Reward/reaching_object: 0.2619
     Episode_Reward/lifting_object: -0.0265
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 2.72s
                      Time elapsed: 00:03:18
                               ETA: 02:17:15

################################################################################
                      [1m Learning iteration 47/2000 [0m                      

                       Computation: 37227 steps/s (collection: 2.364s, learning 0.277s)
             Mean action noise std: 1.16
          Mean value_function loss: 0.1929
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 40.8118
                       Mean reward: 1.08
               Mean episode length: 248.48
    Episode_Reward/reaching_object: 0.2391
     Episode_Reward/lifting_object: -0.0417
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 2.64s
                      Time elapsed: 00:03:20
                               ETA: 02:16:07

################################################################################
                      [1m Learning iteration 48/2000 [0m                      

                       Computation: 31306 steps/s (collection: 2.902s, learning 0.238s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 40.8748
                       Mean reward: 1.08
               Mean episode length: 249.34
    Episode_Reward/reaching_object: 0.2274
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.6667
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 3.14s
                      Time elapsed: 00:03:23
                               ETA: 02:15:21

################################################################################
                      [1m Learning iteration 49/2000 [0m                      

                       Computation: 33692 steps/s (collection: 2.673s, learning 0.245s)
             Mean action noise std: 1.17
          Mean value_function loss: 0.0266
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 40.9661
                       Mean reward: 1.07
               Mean episode length: 249.94
    Episode_Reward/reaching_object: 0.2160
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 2.92s
                      Time elapsed: 00:03:26
                               ETA: 02:14:28

################################################################################
                      [1m Learning iteration 50/2000 [0m                      

                       Computation: 34267 steps/s (collection: 2.624s, learning 0.245s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.1706
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 41.0209
                       Mean reward: 1.01
               Mean episode length: 249.51
    Episode_Reward/reaching_object: 0.2200
     Episode_Reward/lifting_object: -0.0496
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 2.87s
                      Time elapsed: 00:03:29
                               ETA: 02:13:36

################################################################################
                      [1m Learning iteration 51/2000 [0m                      

                       Computation: 30922 steps/s (collection: 2.947s, learning 0.232s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0918
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 41.1525
                       Mean reward: 0.55
               Mean episode length: 249.55
    Episode_Reward/reaching_object: 0.2035
     Episode_Reward/lifting_object: -0.0139
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 3.18s
                      Time elapsed: 00:03:32
                               ETA: 02:12:57

################################################################################
                      [1m Learning iteration 52/2000 [0m                      

                       Computation: 39916 steps/s (collection: 2.269s, learning 0.194s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.2510
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.2427
                       Mean reward: -0.05
               Mean episode length: 249.34
    Episode_Reward/reaching_object: 0.1958
     Episode_Reward/lifting_object: -0.0556
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 2.46s
                      Time elapsed: 00:03:35
                               ETA: 02:11:53

################################################################################
                      [1m Learning iteration 53/2000 [0m                      

                       Computation: 40529 steps/s (collection: 2.268s, learning 0.157s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.4599
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 41.3436
                       Mean reward: 0.51
               Mean episode length: 249.61
    Episode_Reward/reaching_object: 0.1938
     Episode_Reward/lifting_object: -0.0714
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 2.43s
                      Time elapsed: 00:03:37
                               ETA: 02:10:49

################################################################################
                      [1m Learning iteration 54/2000 [0m                      

                       Computation: 41032 steps/s (collection: 2.238s, learning 0.158s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.1681
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 41.4300
                       Mean reward: 0.98
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 0.2064
     Episode_Reward/lifting_object: -0.0444
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 2.40s
                      Time elapsed: 00:03:40
                               ETA: 02:09:48

################################################################################
                      [1m Learning iteration 55/2000 [0m                      

                       Computation: 40703 steps/s (collection: 2.238s, learning 0.178s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.3948
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 41.5116
                       Mean reward: 0.17
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 0.2139
     Episode_Reward/lifting_object: -0.1077
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 2.42s
                      Time elapsed: 00:03:42
                               ETA: 02:08:48

################################################################################
                      [1m Learning iteration 56/2000 [0m                      

                       Computation: 41121 steps/s (collection: 2.244s, learning 0.146s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0024
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 41.5924
                       Mean reward: 1.02
               Mean episode length: 248.85
    Episode_Reward/reaching_object: 0.2166
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 2.39s
                      Time elapsed: 00:03:44
                               ETA: 02:07:50

################################################################################
                      [1m Learning iteration 57/2000 [0m                      

                       Computation: 39312 steps/s (collection: 2.356s, learning 0.145s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0312
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 41.7069
                       Mean reward: 1.18
               Mean episode length: 249.14
    Episode_Reward/reaching_object: 0.2450
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 2.50s
                      Time elapsed: 00:03:47
                               ETA: 02:06:58

################################################################################
                      [1m Learning iteration 58/2000 [0m                      

                       Computation: 41411 steps/s (collection: 2.185s, learning 0.189s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.2702
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 41.7724
                       Mean reward: 0.39
               Mean episode length: 248.53
    Episode_Reward/reaching_object: 0.2527
     Episode_Reward/lifting_object: -0.0912
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 2.37s
                      Time elapsed: 00:03:49
                               ETA: 02:06:03

################################################################################
                      [1m Learning iteration 59/2000 [0m                      

                       Computation: 39584 steps/s (collection: 2.369s, learning 0.115s)
             Mean action noise std: 1.21
          Mean value_function loss: 1.6089
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 41.8256
                       Mean reward: -0.40
               Mean episode length: 247.96
    Episode_Reward/reaching_object: 0.2495
     Episode_Reward/lifting_object: -0.0702
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 2.48s
                      Time elapsed: 00:03:52
                               ETA: 02:05:14

################################################################################
                      [1m Learning iteration 60/2000 [0m                      

                       Computation: 35458 steps/s (collection: 2.542s, learning 0.230s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.3297
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 41.9218
                       Mean reward: 0.65
               Mean episode length: 248.58
    Episode_Reward/reaching_object: 0.2680
     Episode_Reward/lifting_object: -0.0590
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 2.77s
                      Time elapsed: 00:03:55
                               ETA: 02:04:35

################################################################################
                      [1m Learning iteration 61/2000 [0m                      

                       Computation: 40536 steps/s (collection: 2.229s, learning 0.196s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.3017
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 42.0039
                       Mean reward: 1.30
               Mean episode length: 247.65
    Episode_Reward/reaching_object: 0.2655
     Episode_Reward/lifting_object: -0.1090
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 2.43s
                      Time elapsed: 00:03:57
                               ETA: 02:03:46

################################################################################
                      [1m Learning iteration 62/2000 [0m                      

                       Computation: 39431 steps/s (collection: 2.266s, learning 0.227s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0279
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 42.1245
                       Mean reward: 1.22
               Mean episode length: 246.70
    Episode_Reward/reaching_object: 0.2626
     Episode_Reward/lifting_object: -0.0072
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 2.49s
                      Time elapsed: 00:03:59
                               ETA: 02:03:01

################################################################################
                      [1m Learning iteration 63/2000 [0m                      

                       Computation: 40088 steps/s (collection: 2.294s, learning 0.159s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.1576
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 42.3211
                       Mean reward: 1.12
               Mean episode length: 245.23
    Episode_Reward/reaching_object: 0.2649
     Episode_Reward/lifting_object: -0.0424
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 2.45s
                      Time elapsed: 00:04:02
                               ETA: 02:02:17

################################################################################
                      [1m Learning iteration 64/2000 [0m                      

                       Computation: 39596 steps/s (collection: 2.322s, learning 0.161s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.2843
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 42.3799
                       Mean reward: 1.19
               Mean episode length: 247.54
    Episode_Reward/reaching_object: 0.2675
     Episode_Reward/lifting_object: -0.0104
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 2.48s
                      Time elapsed: 00:04:04
                               ETA: 02:01:34

################################################################################
                      [1m Learning iteration 65/2000 [0m                      

                       Computation: 39924 steps/s (collection: 2.333s, learning 0.130s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.0326
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 42.4864
                       Mean reward: 1.02
               Mean episode length: 247.67
    Episode_Reward/reaching_object: 0.2605
     Episode_Reward/lifting_object: -0.0186
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 2.46s
                      Time elapsed: 00:04:07
                               ETA: 02:00:52

################################################################################
                      [1m Learning iteration 66/2000 [0m                      

                       Computation: 39778 steps/s (collection: 2.258s, learning 0.214s)
             Mean action noise std: 1.25
          Mean value_function loss: 1.2058
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 42.6626
                       Mean reward: -0.28
               Mean episode length: 247.89
    Episode_Reward/reaching_object: 0.2837
     Episode_Reward/lifting_object: -0.1114
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 2.47s
                      Time elapsed: 00:04:09
                               ETA: 02:00:11

################################################################################
                      [1m Learning iteration 67/2000 [0m                      

                       Computation: 39691 steps/s (collection: 2.303s, learning 0.174s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.1581
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 42.7104
                       Mean reward: 1.23
               Mean episode length: 245.52
    Episode_Reward/reaching_object: 0.2823
     Episode_Reward/lifting_object: -0.0447
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 2.48s
                      Time elapsed: 00:04:12
                               ETA: 01:59:32

################################################################################
                      [1m Learning iteration 68/2000 [0m                      

                       Computation: 37027 steps/s (collection: 2.419s, learning 0.236s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.5913
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 42.7877
                       Mean reward: 1.46
               Mean episode length: 245.15
    Episode_Reward/reaching_object: 0.2922
     Episode_Reward/lifting_object: -0.1008
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 2.65s
                      Time elapsed: 00:04:14
                               ETA: 01:58:59

################################################################################
                      [1m Learning iteration 69/2000 [0m                      

                       Computation: 38981 steps/s (collection: 2.354s, learning 0.168s)
             Mean action noise std: 1.26
          Mean value_function loss: 1.6837
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 42.8568
                       Mean reward: -0.10
               Mean episode length: 244.61
    Episode_Reward/reaching_object: 0.2930
     Episode_Reward/lifting_object: -0.1103
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 2.52s
                      Time elapsed: 00:04:17
                               ETA: 01:58:23

################################################################################
                      [1m Learning iteration 70/2000 [0m                      

                       Computation: 41770 steps/s (collection: 2.186s, learning 0.167s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.2476
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 42.9154
                       Mean reward: 1.19
               Mean episode length: 243.30
    Episode_Reward/reaching_object: 0.2964
     Episode_Reward/lifting_object: -0.2515
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 2.35s
                      Time elapsed: 00:04:19
                               ETA: 01:57:43

################################################################################
                      [1m Learning iteration 71/2000 [0m                      

                       Computation: 40026 steps/s (collection: 2.313s, learning 0.143s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0897
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 42.9933
                       Mean reward: 1.47
               Mean episode length: 244.13
    Episode_Reward/reaching_object: 0.3173
     Episode_Reward/lifting_object: -0.0312
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 11.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 2.46s
                      Time elapsed: 00:04:22
                               ETA: 01:57:07

################################################################################
                      [1m Learning iteration 72/2000 [0m                      

                       Computation: 41141 steps/s (collection: 2.246s, learning 0.143s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.0263
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.0619
                       Mean reward: 1.38
               Mean episode length: 244.20
    Episode_Reward/reaching_object: 0.3093
     Episode_Reward/lifting_object: -0.0174
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 2.39s
                      Time elapsed: 00:04:24
                               ETA: 01:56:30

################################################################################
                      [1m Learning iteration 73/2000 [0m                      

                       Computation: 39949 steps/s (collection: 2.296s, learning 0.165s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.0029
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 43.2205
                       Mean reward: 1.44
               Mean episode length: 241.75
    Episode_Reward/reaching_object: 0.3140
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 2.46s
                      Time elapsed: 00:04:27
                               ETA: 01:55:56

################################################################################
                      [1m Learning iteration 74/2000 [0m                      

                       Computation: 39666 steps/s (collection: 2.315s, learning 0.163s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.3928
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 43.2225
                       Mean reward: 1.46
               Mean episode length: 242.17
    Episode_Reward/reaching_object: 0.3198
     Episode_Reward/lifting_object: -0.0476
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 2.48s
                      Time elapsed: 00:04:29
                               ETA: 01:55:24

################################################################################
                      [1m Learning iteration 75/2000 [0m                      

                       Computation: 39576 steps/s (collection: 2.312s, learning 0.172s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.5675
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 43.2575
                       Mean reward: 1.09
               Mean episode length: 241.49
    Episode_Reward/reaching_object: 0.3114
     Episode_Reward/lifting_object: -0.0471
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 2.48s
                      Time elapsed: 00:04:32
                               ETA: 01:54:52

################################################################################
                      [1m Learning iteration 76/2000 [0m                      

                       Computation: 38248 steps/s (collection: 2.336s, learning 0.234s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.2357
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 43.3451
                       Mean reward: 1.60
               Mean episode length: 242.48
    Episode_Reward/reaching_object: 0.3259
     Episode_Reward/lifting_object: -0.0557
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 2.57s
                      Time elapsed: 00:04:34
                               ETA: 01:54:23

################################################################################
                      [1m Learning iteration 77/2000 [0m                      

                       Computation: 36960 steps/s (collection: 2.476s, learning 0.184s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.2657
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 43.4274
                       Mean reward: 1.42
               Mean episode length: 236.42
    Episode_Reward/reaching_object: 0.3087
     Episode_Reward/lifting_object: -0.0749
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 2.66s
                      Time elapsed: 00:04:37
                               ETA: 01:53:57

################################################################################
                      [1m Learning iteration 78/2000 [0m                      

                       Computation: 38198 steps/s (collection: 2.408s, learning 0.165s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0932
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 43.4985
                       Mean reward: 1.55
               Mean episode length: 238.43
    Episode_Reward/reaching_object: 0.3196
     Episode_Reward/lifting_object: -0.0344
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 2.57s
                      Time elapsed: 00:04:39
                               ETA: 01:53:30

################################################################################
                      [1m Learning iteration 79/2000 [0m                      

                       Computation: 38416 steps/s (collection: 2.404s, learning 0.155s)
             Mean action noise std: 1.30
          Mean value_function loss: 1.9046
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 43.6585
                       Mean reward: 0.07
               Mean episode length: 233.17
    Episode_Reward/reaching_object: 0.3201
     Episode_Reward/lifting_object: -0.1762
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 2.56s
                      Time elapsed: 00:04:42
                               ETA: 01:53:02

################################################################################
                      [1m Learning iteration 80/2000 [0m                      

                       Computation: 41364 steps/s (collection: 2.215s, learning 0.162s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0344
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 43.7161
                       Mean reward: 1.34
               Mean episode length: 233.21
    Episode_Reward/reaching_object: 0.3151
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 2.38s
                      Time elapsed: 00:04:44
                               ETA: 01:52:32

################################################################################
                      [1m Learning iteration 81/2000 [0m                      

                       Computation: 40645 steps/s (collection: 2.295s, learning 0.124s)
             Mean action noise std: 1.31
          Mean value_function loss: 0.9012
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 43.8466
                       Mean reward: 0.21
               Mean episode length: 231.68
    Episode_Reward/reaching_object: 0.3117
     Episode_Reward/lifting_object: -0.0472
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 2.42s
                      Time elapsed: 00:04:47
                               ETA: 01:52:02

################################################################################
                      [1m Learning iteration 82/2000 [0m                      

                       Computation: 40615 steps/s (collection: 2.230s, learning 0.190s)
             Mean action noise std: 1.31
          Mean value_function loss: 2.6862
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 43.8912
                       Mean reward: -0.06
               Mean episode length: 233.69
    Episode_Reward/reaching_object: 0.3212
     Episode_Reward/lifting_object: -0.0718
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8333
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 2.42s
                      Time elapsed: 00:04:49
                               ETA: 01:51:34

################################################################################
                      [1m Learning iteration 83/2000 [0m                      

                       Computation: 36403 steps/s (collection: 2.506s, learning 0.195s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.1577
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 43.9700
                       Mean reward: 1.58
               Mean episode length: 227.02
    Episode_Reward/reaching_object: 0.3219
     Episode_Reward/lifting_object: -0.0929
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.0417
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 2.70s
                      Time elapsed: 00:04:52
                               ETA: 01:51:12

################################################################################
                      [1m Learning iteration 84/2000 [0m                      

                       Computation: 32572 steps/s (collection: 2.814s, learning 0.204s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.1152
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 44.0318
                       Mean reward: 1.15
               Mean episode length: 228.12
    Episode_Reward/reaching_object: 0.3236
     Episode_Reward/lifting_object: -0.0504
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.8333
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 3.02s
                      Time elapsed: 00:04:55
                               ETA: 01:50:58

################################################################################
                      [1m Learning iteration 85/2000 [0m                      

                       Computation: 35261 steps/s (collection: 2.609s, learning 0.179s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.1213
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 44.1024
                       Mean reward: 1.52
               Mean episode length: 231.71
    Episode_Reward/reaching_object: 0.3183
     Episode_Reward/lifting_object: -0.0208
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 2.79s
                      Time elapsed: 00:04:58
                               ETA: 01:50:40

################################################################################
                      [1m Learning iteration 86/2000 [0m                      

                       Computation: 38214 steps/s (collection: 2.361s, learning 0.212s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.0464
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 44.2268
                       Mean reward: 1.55
               Mean episode length: 227.42
    Episode_Reward/reaching_object: 0.3246
     Episode_Reward/lifting_object: -0.0353
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 6.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 2.57s
                      Time elapsed: 00:05:00
                               ETA: 01:50:16

################################################################################
                      [1m Learning iteration 87/2000 [0m                      

                       Computation: 38957 steps/s (collection: 2.361s, learning 0.163s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0236
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 44.3784
                       Mean reward: 1.61
               Mean episode length: 221.24
    Episode_Reward/reaching_object: 0.3335
     Episode_Reward/lifting_object: -0.0104
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 2.52s
                      Time elapsed: 00:05:03
                               ETA: 01:49:53

################################################################################
                      [1m Learning iteration 88/2000 [0m                      

                       Computation: 35987 steps/s (collection: 2.558s, learning 0.174s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.3785
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 44.5525
                       Mean reward: 1.50
               Mean episode length: 226.27
    Episode_Reward/reaching_object: 0.3216
     Episode_Reward/lifting_object: -0.0718
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 6.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 2.73s
                      Time elapsed: 00:05:06
                               ETA: 01:49:34

################################################################################
                      [1m Learning iteration 89/2000 [0m                      

                       Computation: 38439 steps/s (collection: 2.328s, learning 0.230s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0238
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 44.5976
                       Mean reward: 0.73
               Mean episode length: 219.62
    Episode_Reward/reaching_object: 0.3234
     Episode_Reward/lifting_object: -0.0606
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.2500
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 2.56s
                      Time elapsed: 00:05:08
                               ETA: 01:49:12

################################################################################
                      [1m Learning iteration 90/2000 [0m                      

                       Computation: 37678 steps/s (collection: 2.426s, learning 0.183s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.0049
               Mean surrogate loss: -0.0083
                 Mean entropy loss: 44.7203
                       Mean reward: 1.53
               Mean episode length: 224.21
    Episode_Reward/reaching_object: 0.3309
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.8333
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 2.61s
                      Time elapsed: 00:05:11
                               ETA: 01:48:51

################################################################################
                      [1m Learning iteration 91/2000 [0m                      

                       Computation: 36852 steps/s (collection: 2.377s, learning 0.291s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0062
               Mean surrogate loss: -0.0071
                 Mean entropy loss: 44.7883
                       Mean reward: 1.50
               Mean episode length: 220.52
    Episode_Reward/reaching_object: 0.3187
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 12.7083
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 2.67s
                      Time elapsed: 00:05:13
                               ETA: 01:48:32

################################################################################
                      [1m Learning iteration 92/2000 [0m                      

                       Computation: 39076 steps/s (collection: 2.388s, learning 0.128s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.0267
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 44.8888
                       Mean reward: 1.53
               Mean episode length: 222.95
    Episode_Reward/reaching_object: 0.3236
     Episode_Reward/lifting_object: -0.0079
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.2500
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 2.52s
                      Time elapsed: 00:05:16
                               ETA: 01:48:10

################################################################################
                      [1m Learning iteration 93/2000 [0m                      

                       Computation: 35522 steps/s (collection: 2.622s, learning 0.145s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.1219
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 44.9620
                       Mean reward: 1.12
               Mean episode length: 216.36
    Episode_Reward/reaching_object: 0.3235
     Episode_Reward/lifting_object: -0.0159
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 2.77s
                      Time elapsed: 00:05:19
                               ETA: 01:47:54

################################################################################
                      [1m Learning iteration 94/2000 [0m                      

                       Computation: 41366 steps/s (collection: 2.227s, learning 0.149s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.6385
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 44.9975
                       Mean reward: 0.54
               Mean episode length: 204.88
    Episode_Reward/reaching_object: 0.3256
     Episode_Reward/lifting_object: -0.0333
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 19.3750
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 2.38s
                      Time elapsed: 00:05:21
                               ETA: 01:47:30

################################################################################
                      [1m Learning iteration 95/2000 [0m                      

                       Computation: 38268 steps/s (collection: 2.397s, learning 0.172s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.1993
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 45.0519
                       Mean reward: 0.92
               Mean episode length: 204.36
    Episode_Reward/reaching_object: 0.3195
     Episode_Reward/lifting_object: -0.0344
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 21.0833
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 2.57s
                      Time elapsed: 00:05:24
                               ETA: 01:47:11

################################################################################
                      [1m Learning iteration 96/2000 [0m                      

                       Computation: 41152 steps/s (collection: 2.204s, learning 0.185s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.1884
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 45.1081
                       Mean reward: 1.39
               Mean episode length: 204.01
    Episode_Reward/reaching_object: 0.3137
     Episode_Reward/lifting_object: -0.0476
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 21.3750
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 2.39s
                      Time elapsed: 00:05:26
                               ETA: 01:46:48

################################################################################
                      [1m Learning iteration 97/2000 [0m                      

                       Computation: 38847 steps/s (collection: 2.346s, learning 0.184s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.8745
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 45.1683
                       Mean reward: 0.85
               Mean episode length: 191.44
    Episode_Reward/reaching_object: 0.3175
     Episode_Reward/lifting_object: -0.0696
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.1667
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 2.53s
                      Time elapsed: 00:05:29
                               ETA: 01:46:28

################################################################################
                      [1m Learning iteration 98/2000 [0m                      

                       Computation: 29896 steps/s (collection: 3.042s, learning 0.246s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.1791
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 45.2188
                       Mean reward: 1.47
               Mean episode length: 185.55
    Episode_Reward/reaching_object: 0.3162
     Episode_Reward/lifting_object: -0.0350
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 19.4583
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 3.29s
                      Time elapsed: 00:05:32
                               ETA: 01:46:24

################################################################################
                      [1m Learning iteration 99/2000 [0m                      

                       Computation: 37482 steps/s (collection: 2.482s, learning 0.141s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0654
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.2941
                       Mean reward: 1.44
               Mean episode length: 190.47
    Episode_Reward/reaching_object: 0.3475
     Episode_Reward/lifting_object: -0.0265
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 2.62s
                      Time elapsed: 00:05:34
                               ETA: 01:46:06

################################################################################
                     [1m Learning iteration 100/2000 [0m                      

                       Computation: 39451 steps/s (collection: 2.342s, learning 0.150s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.5768
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 45.4375
                       Mean reward: 1.80
               Mean episode length: 195.33
    Episode_Reward/reaching_object: 0.3736
     Episode_Reward/lifting_object: -0.0700
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.2083
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 2.49s
                      Time elapsed: 00:05:37
                               ETA: 01:45:47

################################################################################
                     [1m Learning iteration 101/2000 [0m                      

                       Computation: 40653 steps/s (collection: 2.318s, learning 0.101s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0591
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 45.4884
                       Mean reward: 1.97
               Mean episode length: 197.99
    Episode_Reward/reaching_object: 0.3960
     Episode_Reward/lifting_object: -0.0336
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.1250
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 2.42s
                      Time elapsed: 00:05:39
                               ETA: 01:45:26

################################################################################
                     [1m Learning iteration 102/2000 [0m                      

                       Computation: 44039 steps/s (collection: 2.120s, learning 0.113s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.5323
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 45.6038
                       Mean reward: 1.90
               Mean episode length: 205.37
    Episode_Reward/reaching_object: 0.4080
     Episode_Reward/lifting_object: -0.1173
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 2.23s
                      Time elapsed: 00:05:42
                               ETA: 01:45:03

################################################################################
                     [1m Learning iteration 103/2000 [0m                      

                       Computation: 41805 steps/s (collection: 2.194s, learning 0.158s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.0632
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 45.6450
                       Mean reward: 2.19
               Mean episode length: 211.13
    Episode_Reward/reaching_object: 0.4412
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.7083
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 2.35s
                      Time elapsed: 00:05:44
                               ETA: 01:44:42

################################################################################
                     [1m Learning iteration 104/2000 [0m                      

                       Computation: 42325 steps/s (collection: 2.167s, learning 0.156s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.2336
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 45.7525
                       Mean reward: 2.11
               Mean episode length: 220.58
    Episode_Reward/reaching_object: 0.4296
     Episode_Reward/lifting_object: -0.0611
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 2.32s
                      Time elapsed: 00:05:46
                               ETA: 01:44:20

################################################################################
                     [1m Learning iteration 105/2000 [0m                      

                       Computation: 40801 steps/s (collection: 2.219s, learning 0.190s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.4567
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 45.7844
                       Mean reward: 0.66
               Mean episode length: 228.78
    Episode_Reward/reaching_object: 0.4358
     Episode_Reward/lifting_object: -0.0927
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 2.41s
                      Time elapsed: 00:05:49
                               ETA: 01:44:01

################################################################################
                     [1m Learning iteration 106/2000 [0m                      

                       Computation: 41561 steps/s (collection: 2.219s, learning 0.147s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.3240
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 45.8380
                       Mean reward: 1.82
               Mean episode length: 234.48
    Episode_Reward/reaching_object: 0.4557
     Episode_Reward/lifting_object: -0.0267
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 2.37s
                      Time elapsed: 00:05:51
                               ETA: 01:43:41

################################################################################
                     [1m Learning iteration 107/2000 [0m                      

                       Computation: 43743 steps/s (collection: 2.091s, learning 0.156s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.5685
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 45.8859
                       Mean reward: 2.03
               Mean episode length: 233.27
    Episode_Reward/reaching_object: 0.4430
     Episode_Reward/lifting_object: -0.0473
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 2.25s
                      Time elapsed: 00:05:53
                               ETA: 01:43:20

################################################################################
                     [1m Learning iteration 108/2000 [0m                      

                       Computation: 42396 steps/s (collection: 2.182s, learning 0.137s)
             Mean action noise std: 1.42
          Mean value_function loss: 0.1864
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 45.9619
                       Mean reward: 2.05
               Mean episode length: 230.20
    Episode_Reward/reaching_object: 0.4357
     Episode_Reward/lifting_object: -0.0795
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 2.32s
                      Time elapsed: 00:05:56
                               ETA: 01:43:00

################################################################################
                     [1m Learning iteration 109/2000 [0m                      

                       Computation: 41858 steps/s (collection: 2.221s, learning 0.127s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.0330
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 46.1139
                       Mean reward: 1.99
               Mean episode length: 229.90
    Episode_Reward/reaching_object: 0.4228
     Episode_Reward/lifting_object: -0.0314
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 2.35s
                      Time elapsed: 00:05:58
                               ETA: 01:42:41

################################################################################
                     [1m Learning iteration 110/2000 [0m                      

                       Computation: 33637 steps/s (collection: 2.786s, learning 0.137s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.4183
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 46.1837
                       Mean reward: 1.98
               Mean episode length: 229.56
    Episode_Reward/reaching_object: 0.4238
     Episode_Reward/lifting_object: -0.0541
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 2.92s
                      Time elapsed: 00:06:01
                               ETA: 01:42:32

################################################################################
                     [1m Learning iteration 111/2000 [0m                      

                       Computation: 38056 steps/s (collection: 2.450s, learning 0.133s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.0866
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 46.2148
                       Mean reward: 1.88
               Mean episode length: 221.05
    Episode_Reward/reaching_object: 0.4070
     Episode_Reward/lifting_object: -0.0435
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 2.58s
                      Time elapsed: 00:06:03
                               ETA: 01:42:18

################################################################################
                     [1m Learning iteration 112/2000 [0m                      

                       Computation: 43400 steps/s (collection: 2.130s, learning 0.135s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.7640
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 46.2952
                       Mean reward: 1.27
               Mean episode length: 211.68
    Episode_Reward/reaching_object: 0.4191
     Episode_Reward/lifting_object: -0.0712
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 7.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.9583
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 2.27s
                      Time elapsed: 00:06:06
                               ETA: 01:41:58

################################################################################
                     [1m Learning iteration 113/2000 [0m                      

                       Computation: 44221 steps/s (collection: 2.097s, learning 0.126s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.0387
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.3260
                       Mean reward: 2.05
               Mean episode length: 221.83
    Episode_Reward/reaching_object: 0.4144
     Episode_Reward/lifting_object: -0.0397
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7917
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 2.22s
                      Time elapsed: 00:06:08
                               ETA: 01:41:38

################################################################################
                     [1m Learning iteration 114/2000 [0m                      

                       Computation: 43508 steps/s (collection: 2.120s, learning 0.140s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0862
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 46.4309
                       Mean reward: 2.18
               Mean episode length: 230.03
    Episode_Reward/reaching_object: 0.4287
     Episode_Reward/lifting_object: -0.0163
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 8.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3750
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 2.26s
                      Time elapsed: 00:06:10
                               ETA: 01:41:19

################################################################################
                     [1m Learning iteration 115/2000 [0m                      

                       Computation: 43902 steps/s (collection: 2.100s, learning 0.139s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.6367
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 46.5698
                       Mean reward: 0.99
               Mean episode length: 211.26
    Episode_Reward/reaching_object: 0.4468
     Episode_Reward/lifting_object: -0.1516
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 2.24s
                      Time elapsed: 00:06:12
                               ETA: 01:40:59

################################################################################
                     [1m Learning iteration 116/2000 [0m                      

                       Computation: 42359 steps/s (collection: 2.163s, learning 0.158s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.1940
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 46.6008
                       Mean reward: 2.30
               Mean episode length: 225.86
    Episode_Reward/reaching_object: 0.4554
     Episode_Reward/lifting_object: -0.0473
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 2.32s
                      Time elapsed: 00:06:15
                               ETA: 01:40:42

################################################################################
                     [1m Learning iteration 117/2000 [0m                      

                       Computation: 43698 steps/s (collection: 2.116s, learning 0.134s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.4349
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.6641
                       Mean reward: 1.81
               Mean episode length: 220.25
    Episode_Reward/reaching_object: 0.4803
     Episode_Reward/lifting_object: -0.0423
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 2.25s
                      Time elapsed: 00:06:17
                               ETA: 01:40:23

################################################################################
                     [1m Learning iteration 118/2000 [0m                      

                       Computation: 43427 steps/s (collection: 2.112s, learning 0.152s)
             Mean action noise std: 1.46
          Mean value_function loss: 0.0854
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 46.6999
                       Mean reward: 2.33
               Mean episode length: 228.28
    Episode_Reward/reaching_object: 0.5080
     Episode_Reward/lifting_object: -0.0389
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 2.26s
                      Time elapsed: 00:06:19
                               ETA: 01:40:05

################################################################################
                     [1m Learning iteration 119/2000 [0m                      

                       Computation: 43126 steps/s (collection: 2.133s, learning 0.147s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.1535
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 46.8024
                       Mean reward: 2.25
               Mean episode length: 223.20
    Episode_Reward/reaching_object: 0.5217
     Episode_Reward/lifting_object: -0.0251
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 2.28s
                      Time elapsed: 00:06:22
                               ETA: 01:39:48

################################################################################
                     [1m Learning iteration 120/2000 [0m                      

                       Computation: 42655 steps/s (collection: 2.167s, learning 0.137s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.4956
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.9137
                       Mean reward: 2.03
               Mean episode length: 219.95
    Episode_Reward/reaching_object: 0.5037
     Episode_Reward/lifting_object: -0.0769
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 8.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 2.30s
                      Time elapsed: 00:06:24
                               ETA: 01:39:31

################################################################################
                     [1m Learning iteration 121/2000 [0m                      

                       Computation: 42829 steps/s (collection: 2.182s, learning 0.113s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.1580
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 46.9398
                       Mean reward: 2.52
               Mean episode length: 217.50
    Episode_Reward/reaching_object: 0.5354
     Episode_Reward/lifting_object: -0.0217
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 8.9167
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 2.30s
                      Time elapsed: 00:06:26
                               ETA: 01:39:14

################################################################################
                     [1m Learning iteration 122/2000 [0m                      

                       Computation: 41859 steps/s (collection: 2.190s, learning 0.159s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.8593
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 46.9958
                       Mean reward: 2.62
               Mean episode length: 224.74
    Episode_Reward/reaching_object: 0.5410
     Episode_Reward/lifting_object: -0.0759
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 10.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 2.35s
                      Time elapsed: 00:06:28
                               ETA: 01:38:58

################################################################################
                     [1m Learning iteration 123/2000 [0m                      

                       Computation: 42399 steps/s (collection: 2.174s, learning 0.145s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.1634
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.0205
                       Mean reward: 2.44
               Mean episode length: 229.47
    Episode_Reward/reaching_object: 0.5826
     Episode_Reward/lifting_object: -0.1147
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 2.32s
                      Time elapsed: 00:06:31
                               ETA: 01:38:43

################################################################################
                     [1m Learning iteration 124/2000 [0m                      

                       Computation: 43472 steps/s (collection: 2.142s, learning 0.119s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.3418
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 47.0823
                       Mean reward: 2.72
               Mean episode length: 231.08
    Episode_Reward/reaching_object: 0.5927
     Episode_Reward/lifting_object: -0.0435
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 9.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 2.26s
                      Time elapsed: 00:06:33
                               ETA: 01:38:26

################################################################################
                     [1m Learning iteration 125/2000 [0m                      

                       Computation: 41894 steps/s (collection: 2.190s, learning 0.156s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.0088
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 47.0969
                       Mean reward: 2.88
               Mean episode length: 230.87
    Episode_Reward/reaching_object: 0.6084
     Episode_Reward/lifting_object: -0.0196
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 2.35s
                      Time elapsed: 00:06:35
                               ETA: 01:38:11

################################################################################
                     [1m Learning iteration 126/2000 [0m                      

                       Computation: 41343 steps/s (collection: 2.247s, learning 0.131s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.7296
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 47.1168
                       Mean reward: 2.49
               Mean episode length: 226.21
    Episode_Reward/reaching_object: 0.6446
     Episode_Reward/lifting_object: -0.0671
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 2.38s
                      Time elapsed: 00:06:38
                               ETA: 01:37:56

################################################################################
                     [1m Learning iteration 127/2000 [0m                      

                       Computation: 43970 steps/s (collection: 2.116s, learning 0.120s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.2963
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.1387
                       Mean reward: 2.89
               Mean episode length: 233.75
    Episode_Reward/reaching_object: 0.6553
     Episode_Reward/lifting_object: -0.0700
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 2.24s
                      Time elapsed: 00:06:40
                               ETA: 01:37:40

################################################################################
                     [1m Learning iteration 128/2000 [0m                      

                       Computation: 44901 steps/s (collection: 2.059s, learning 0.130s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.0898
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 47.2210
                       Mean reward: 3.12
               Mean episode length: 239.82
    Episode_Reward/reaching_object: 0.6624
     Episode_Reward/lifting_object: -0.0506
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 2.19s
                      Time elapsed: 00:06:42
                               ETA: 01:37:23

################################################################################
                     [1m Learning iteration 129/2000 [0m                      

                       Computation: 45323 steps/s (collection: 2.031s, learning 0.138s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.9476
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 47.2706
                       Mean reward: 1.62
               Mean episode length: 224.16
    Episode_Reward/reaching_object: 0.6587
     Episode_Reward/lifting_object: -0.1082
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 2.17s
                      Time elapsed: 00:06:44
                               ETA: 01:37:07

################################################################################
                     [1m Learning iteration 130/2000 [0m                      

                       Computation: 45011 steps/s (collection: 2.045s, learning 0.139s)
             Mean action noise std: 1.49
          Mean value_function loss: 1.1152
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.2936
                       Mean reward: 1.74
               Mean episode length: 234.20
    Episode_Reward/reaching_object: 0.6545
     Episode_Reward/lifting_object: -0.2247
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 2.18s
                      Time elapsed: 00:06:47
                               ETA: 01:36:50

################################################################################
                     [1m Learning iteration 131/2000 [0m                      

                       Computation: 45979 steps/s (collection: 2.018s, learning 0.120s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.1437
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.3315
                       Mean reward: 3.25
               Mean episode length: 240.00
    Episode_Reward/reaching_object: 0.6744
     Episode_Reward/lifting_object: -0.0392
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 2.14s
                      Time elapsed: 00:06:49
                               ETA: 01:36:33

################################################################################
                     [1m Learning iteration 132/2000 [0m                      

                       Computation: 45094 steps/s (collection: 2.048s, learning 0.132s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.3425
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 47.3926
                       Mean reward: 2.94
               Mean episode length: 238.32
    Episode_Reward/reaching_object: 0.6704
     Episode_Reward/lifting_object: -0.0499
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 2.18s
                      Time elapsed: 00:06:51
                               ETA: 01:36:17

################################################################################
                     [1m Learning iteration 133/2000 [0m                      

                       Computation: 46042 steps/s (collection: 2.026s, learning 0.109s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.1904
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.4121
                       Mean reward: 3.06
               Mean episode length: 227.60
    Episode_Reward/reaching_object: 0.6649
     Episode_Reward/lifting_object: -0.0753
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 2.14s
                      Time elapsed: 00:06:53
                               ETA: 01:36:01

################################################################################
                     [1m Learning iteration 134/2000 [0m                      

                       Computation: 44802 steps/s (collection: 2.059s, learning 0.136s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.2393
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.4804
                       Mean reward: 3.16
               Mean episode length: 223.26
    Episode_Reward/reaching_object: 0.6593
     Episode_Reward/lifting_object: -0.0344
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7500
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 2.19s
                      Time elapsed: 00:06:55
                               ETA: 01:35:45

################################################################################
                     [1m Learning iteration 135/2000 [0m                      

                       Computation: 41099 steps/s (collection: 2.236s, learning 0.156s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.1539
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 47.5619
                       Mean reward: 2.73
               Mean episode length: 224.74
    Episode_Reward/reaching_object: 0.6996
     Episode_Reward/lifting_object: -0.0633
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.2500
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 2.39s
                      Time elapsed: 00:06:58
                               ETA: 01:35:33

################################################################################
                     [1m Learning iteration 136/2000 [0m                      

                       Computation: 39294 steps/s (collection: 2.372s, learning 0.130s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.9496
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 47.6646
                       Mean reward: 2.29
               Mean episode length: 226.12
    Episode_Reward/reaching_object: 0.6940
     Episode_Reward/lifting_object: -0.0801
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 2.50s
                      Time elapsed: 00:07:00
                               ETA: 01:35:22

################################################################################
                     [1m Learning iteration 137/2000 [0m                      

                       Computation: 40473 steps/s (collection: 2.272s, learning 0.157s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.4196
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 47.6878
                       Mean reward: 2.73
               Mean episode length: 228.24
    Episode_Reward/reaching_object: 0.7153
     Episode_Reward/lifting_object: -0.1068
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 2.43s
                      Time elapsed: 00:07:03
                               ETA: 01:35:10

################################################################################
                     [1m Learning iteration 138/2000 [0m                      

                       Computation: 40888 steps/s (collection: 2.256s, learning 0.149s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.4621
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 47.7431
                       Mean reward: 3.07
               Mean episode length: 230.98
    Episode_Reward/reaching_object: 0.7209
     Episode_Reward/lifting_object: -0.1235
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 2.40s
                      Time elapsed: 00:07:05
                               ETA: 01:34:58

################################################################################
                     [1m Learning iteration 139/2000 [0m                      

                       Computation: 42551 steps/s (collection: 2.172s, learning 0.138s)
             Mean action noise std: 1.52
          Mean value_function loss: 1.0988
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 47.7975
                       Mean reward: 2.70
               Mean episode length: 222.95
    Episode_Reward/reaching_object: 0.7463
     Episode_Reward/lifting_object: -0.1380
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 2.31s
                      Time elapsed: 00:07:07
                               ETA: 01:34:45

################################################################################
                     [1m Learning iteration 140/2000 [0m                      

                       Computation: 42672 steps/s (collection: 2.159s, learning 0.145s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.1867
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 47.8237
                       Mean reward: 3.38
               Mean episode length: 240.97
    Episode_Reward/reaching_object: 0.7845
     Episode_Reward/lifting_object: -0.0141
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 2.30s
                      Time elapsed: 00:07:10
                               ETA: 01:34:32

################################################################################
                     [1m Learning iteration 141/2000 [0m                      

                       Computation: 44812 steps/s (collection: 2.061s, learning 0.133s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.0823
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 47.8807
                       Mean reward: 3.54
               Mean episode length: 245.39
    Episode_Reward/reaching_object: 0.8076
     Episode_Reward/lifting_object: -0.0650
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 2.19s
                      Time elapsed: 00:07:12
                               ETA: 01:34:18

################################################################################
                     [1m Learning iteration 142/2000 [0m                      

                       Computation: 42004 steps/s (collection: 2.190s, learning 0.150s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.3917
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 47.9113
                       Mean reward: 3.34
               Mean episode length: 237.72
    Episode_Reward/reaching_object: 0.7707
     Episode_Reward/lifting_object: 0.0373
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 2.34s
                      Time elapsed: 00:07:14
                               ETA: 01:34:06

################################################################################
                     [1m Learning iteration 143/2000 [0m                      

                       Computation: 43299 steps/s (collection: 2.144s, learning 0.126s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.2485
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 47.9475
                       Mean reward: 3.35
               Mean episode length: 231.62
    Episode_Reward/reaching_object: 0.7959
     Episode_Reward/lifting_object: -0.1002
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 2.27s
                      Time elapsed: 00:07:16
                               ETA: 01:33:53

################################################################################
                     [1m Learning iteration 144/2000 [0m                      

                       Computation: 42275 steps/s (collection: 2.185s, learning 0.140s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.2769
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 47.9993
                       Mean reward: 3.05
               Mean episode length: 220.93
    Episode_Reward/reaching_object: 0.7835
     Episode_Reward/lifting_object: -0.0970
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 2.33s
                      Time elapsed: 00:07:19
                               ETA: 01:33:41

################################################################################
                     [1m Learning iteration 145/2000 [0m                      

                       Computation: 42627 steps/s (collection: 2.167s, learning 0.139s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.4893
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 48.0498
                       Mean reward: 3.57
               Mean episode length: 229.90
    Episode_Reward/reaching_object: 0.7536
     Episode_Reward/lifting_object: -0.0691
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 2.31s
                      Time elapsed: 00:07:21
                               ETA: 01:33:29

################################################################################
                     [1m Learning iteration 146/2000 [0m                      

                       Computation: 41923 steps/s (collection: 2.180s, learning 0.165s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.7579
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 48.0998
                       Mean reward: 2.64
               Mean episode length: 220.85
    Episode_Reward/reaching_object: 0.7332
     Episode_Reward/lifting_object: -0.0868
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 2.34s
                      Time elapsed: 00:07:23
                               ETA: 01:33:17

################################################################################
                     [1m Learning iteration 147/2000 [0m                      

                       Computation: 38674 steps/s (collection: 2.413s, learning 0.129s)
             Mean action noise std: 1.54
          Mean value_function loss: 0.6629
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 48.1552
                       Mean reward: 3.19
               Mean episode length: 214.07
    Episode_Reward/reaching_object: 0.7721
     Episode_Reward/lifting_object: -0.0855
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 2.54s
                      Time elapsed: 00:07:26
                               ETA: 01:33:08

################################################################################
                     [1m Learning iteration 148/2000 [0m                      

                       Computation: 43264 steps/s (collection: 2.130s, learning 0.142s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.1739
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 48.2060
                       Mean reward: 3.99
               Mean episode length: 219.52
    Episode_Reward/reaching_object: 0.7599
     Episode_Reward/lifting_object: -0.0022
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 2.27s
                      Time elapsed: 00:07:28
                               ETA: 01:32:56

################################################################################
                     [1m Learning iteration 149/2000 [0m                      

                       Computation: 43909 steps/s (collection: 2.097s, learning 0.142s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.5397
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 48.2877
                       Mean reward: 3.56
               Mean episode length: 221.00
    Episode_Reward/reaching_object: 0.7655
     Episode_Reward/lifting_object: -0.1156
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 2.24s
                      Time elapsed: 00:07:30
                               ETA: 01:32:43

################################################################################
                     [1m Learning iteration 150/2000 [0m                      

                       Computation: 42846 steps/s (collection: 2.171s, learning 0.124s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.1505
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 48.3076
                       Mean reward: 4.05
               Mean episode length: 226.44
    Episode_Reward/reaching_object: 0.8406
     Episode_Reward/lifting_object: -0.0193
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 2.29s
                      Time elapsed: 00:07:33
                               ETA: 01:32:32

################################################################################
                     [1m Learning iteration 151/2000 [0m                      

                       Computation: 41245 steps/s (collection: 2.256s, learning 0.128s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.0725
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 48.3537
                       Mean reward: 4.30
               Mean episode length: 234.49
    Episode_Reward/reaching_object: 0.8163
     Episode_Reward/lifting_object: -0.1053
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 2.38s
                      Time elapsed: 00:07:35
                               ETA: 01:32:21

################################################################################
                     [1m Learning iteration 152/2000 [0m                      

                       Computation: 41068 steps/s (collection: 2.249s, learning 0.145s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.2392
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.4044
                       Mean reward: 4.23
               Mean episode length: 238.32
    Episode_Reward/reaching_object: 0.8350
     Episode_Reward/lifting_object: -0.0636
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 2.39s
                      Time elapsed: 00:07:37
                               ETA: 01:32:11

################################################################################
                     [1m Learning iteration 153/2000 [0m                      

                       Computation: 41960 steps/s (collection: 2.203s, learning 0.140s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.3280
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 48.4333
                       Mean reward: 3.81
               Mean episode length: 227.12
    Episode_Reward/reaching_object: 0.7999
     Episode_Reward/lifting_object: -0.0766
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 2.34s
                      Time elapsed: 00:07:40
                               ETA: 01:32:00

################################################################################
                     [1m Learning iteration 154/2000 [0m                      

                       Computation: 41609 steps/s (collection: 2.220s, learning 0.142s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.8399
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.4868
                       Mean reward: 4.01
               Mean episode length: 223.73
    Episode_Reward/reaching_object: 0.7961
     Episode_Reward/lifting_object: -0.1240
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 2.36s
                      Time elapsed: 00:07:42
                               ETA: 01:31:50

################################################################################
                     [1m Learning iteration 155/2000 [0m                      

                       Computation: 41551 steps/s (collection: 2.230s, learning 0.136s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.1248
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 48.5074
                       Mean reward: 4.33
               Mean episode length: 238.23
    Episode_Reward/reaching_object: 0.8696
     Episode_Reward/lifting_object: -0.0204
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 2.37s
                      Time elapsed: 00:07:45
                               ETA: 01:31:39

################################################################################
                     [1m Learning iteration 156/2000 [0m                      

                       Computation: 42362 steps/s (collection: 2.179s, learning 0.142s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.4658
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 48.5591
                       Mean reward: 4.31
               Mean episode length: 240.57
    Episode_Reward/reaching_object: 0.8652
     Episode_Reward/lifting_object: -0.0845
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 2.32s
                      Time elapsed: 00:07:47
                               ETA: 01:31:29

################################################################################
                     [1m Learning iteration 157/2000 [0m                      

                       Computation: 45094 steps/s (collection: 2.073s, learning 0.107s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.3321
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.5932
                       Mean reward: 4.41
               Mean episode length: 241.87
    Episode_Reward/reaching_object: 0.8589
     Episode_Reward/lifting_object: -0.0147
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 2.18s
                      Time elapsed: 00:07:49
                               ETA: 01:31:16

################################################################################
                     [1m Learning iteration 158/2000 [0m                      

                       Computation: 44734 steps/s (collection: 2.082s, learning 0.116s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.2689
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 48.6598
                       Mean reward: 3.90
               Mean episode length: 236.90
    Episode_Reward/reaching_object: 0.8631
     Episode_Reward/lifting_object: -0.0781
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0194
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 2.20s
                      Time elapsed: 00:07:51
                               ETA: 01:31:04

################################################################################
                     [1m Learning iteration 159/2000 [0m                      

                       Computation: 43437 steps/s (collection: 2.125s, learning 0.138s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.3222
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.7635
                       Mean reward: 3.70
               Mean episode length: 235.29
    Episode_Reward/reaching_object: 0.8479
     Episode_Reward/lifting_object: -0.0794
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 2.26s
                      Time elapsed: 00:07:53
                               ETA: 01:30:53

################################################################################
                     [1m Learning iteration 160/2000 [0m                      

                       Computation: 40951 steps/s (collection: 2.249s, learning 0.151s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.6499
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 48.8117
                       Mean reward: 3.91
               Mean episode length: 229.82
    Episode_Reward/reaching_object: 0.8681
     Episode_Reward/lifting_object: -0.1333
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0191
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 2.40s
                      Time elapsed: 00:07:56
                               ETA: 01:30:44

################################################################################
                     [1m Learning iteration 161/2000 [0m                      

                       Computation: 41721 steps/s (collection: 2.226s, learning 0.130s)
             Mean action noise std: 1.58
          Mean value_function loss: 1.7496
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 48.8268
                       Mean reward: 2.89
               Mean episode length: 235.31
    Episode_Reward/reaching_object: 0.8881
     Episode_Reward/lifting_object: -0.0941
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0192
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 2.36s
                      Time elapsed: 00:07:58
                               ETA: 01:30:34

################################################################################
                     [1m Learning iteration 162/2000 [0m                      

                       Computation: 44661 steps/s (collection: 2.069s, learning 0.132s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.2768
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 48.8555
                       Mean reward: 4.13
               Mean episode length: 241.38
    Episode_Reward/reaching_object: 0.9236
     Episode_Reward/lifting_object: -0.0470
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 2.20s
                      Time elapsed: 00:08:00
                               ETA: 01:30:23

################################################################################
                     [1m Learning iteration 163/2000 [0m                      

                       Computation: 41658 steps/s (collection: 2.219s, learning 0.141s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.0825
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 48.9187
                       Mean reward: 4.37
               Mean episode length: 241.16
    Episode_Reward/reaching_object: 0.9117
     Episode_Reward/lifting_object: -0.1688
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 2.36s
                      Time elapsed: 00:08:03
                               ETA: 01:30:13

################################################################################
                     [1m Learning iteration 164/2000 [0m                      

                       Computation: 42531 steps/s (collection: 2.176s, learning 0.135s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.0502
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 48.9803
                       Mean reward: 4.04
               Mean episode length: 232.58
    Episode_Reward/reaching_object: 0.9146
     Episode_Reward/lifting_object: -0.0265
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 2.31s
                      Time elapsed: 00:08:05
                               ETA: 01:30:03

################################################################################
                     [1m Learning iteration 165/2000 [0m                      

                       Computation: 39805 steps/s (collection: 2.316s, learning 0.154s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.2136
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.0509
                       Mean reward: 4.03
               Mean episode length: 235.24
    Episode_Reward/reaching_object: 0.8851
     Episode_Reward/lifting_object: -0.0031
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 2.47s
                      Time elapsed: 00:08:08
                               ETA: 01:29:55

################################################################################
                     [1m Learning iteration 166/2000 [0m                      

                       Computation: 40727 steps/s (collection: 2.263s, learning 0.151s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.1699
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 49.1272
                       Mean reward: 4.24
               Mean episode length: 234.45
    Episode_Reward/reaching_object: 0.8973
     Episode_Reward/lifting_object: -0.0399
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 2.41s
                      Time elapsed: 00:08:10
                               ETA: 01:29:46

################################################################################
                     [1m Learning iteration 167/2000 [0m                      

                       Computation: 43004 steps/s (collection: 2.181s, learning 0.105s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.9840
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 49.2054
                       Mean reward: 3.06
               Mean episode length: 229.43
    Episode_Reward/reaching_object: 0.8868
     Episode_Reward/lifting_object: -0.0891
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 2.29s
                      Time elapsed: 00:08:12
                               ETA: 01:29:36

################################################################################
                     [1m Learning iteration 168/2000 [0m                      

                       Computation: 36309 steps/s (collection: 2.515s, learning 0.192s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.2722
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.2217
                       Mean reward: 4.17
               Mean episode length: 238.70
    Episode_Reward/reaching_object: 0.9039
     Episode_Reward/lifting_object: -0.1058
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 2.71s
                      Time elapsed: 00:08:15
                               ETA: 01:29:31

################################################################################
                     [1m Learning iteration 169/2000 [0m                      

                       Computation: 37945 steps/s (collection: 2.449s, learning 0.142s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.0596
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 49.2883
                       Mean reward: 4.28
               Mean episode length: 236.40
    Episode_Reward/reaching_object: 0.9244
     Episode_Reward/lifting_object: -0.0130
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 2.59s
                      Time elapsed: 00:08:18
                               ETA: 01:29:24

################################################################################
                     [1m Learning iteration 170/2000 [0m                      

                       Computation: 43207 steps/s (collection: 2.163s, learning 0.112s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.2029
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 49.3625
                       Mean reward: 4.15
               Mean episode length: 234.37
    Episode_Reward/reaching_object: 0.8799
     Episode_Reward/lifting_object: -0.0225
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 2.28s
                      Time elapsed: 00:08:20
                               ETA: 01:29:14

################################################################################
                     [1m Learning iteration 171/2000 [0m                      

                       Computation: 42925 steps/s (collection: 2.150s, learning 0.141s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.1654
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 49.4029
                       Mean reward: 4.27
               Mean episode length: 231.88
    Episode_Reward/reaching_object: 0.9322
     Episode_Reward/lifting_object: -0.0357
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 2.29s
                      Time elapsed: 00:08:22
                               ETA: 01:29:04

################################################################################
                     [1m Learning iteration 172/2000 [0m                      

                       Computation: 43230 steps/s (collection: 2.135s, learning 0.139s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.3176
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 49.4675
                       Mean reward: 4.51
               Mean episode length: 231.99
    Episode_Reward/reaching_object: 0.8919
     Episode_Reward/lifting_object: -0.0660
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 2.27s
                      Time elapsed: 00:08:24
                               ETA: 01:28:55

################################################################################
                     [1m Learning iteration 173/2000 [0m                      

                       Computation: 43056 steps/s (collection: 2.128s, learning 0.155s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.5994
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 49.4881
                       Mean reward: 3.80
               Mean episode length: 229.04
    Episode_Reward/reaching_object: 0.9081
     Episode_Reward/lifting_object: -0.1051
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 2.28s
                      Time elapsed: 00:08:27
                               ETA: 01:28:45

################################################################################
                     [1m Learning iteration 174/2000 [0m                      

                       Computation: 39404 steps/s (collection: 2.361s, learning 0.134s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.2730
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 49.5259
                       Mean reward: 4.62
               Mean episode length: 233.75
    Episode_Reward/reaching_object: 0.9055
     Episode_Reward/lifting_object: -0.0029
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 2.49s
                      Time elapsed: 00:08:29
                               ETA: 01:28:38

################################################################################
                     [1m Learning iteration 175/2000 [0m                      

                       Computation: 41701 steps/s (collection: 2.205s, learning 0.152s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.3408
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 49.5602
                       Mean reward: 4.29
               Mean episode length: 225.13
    Episode_Reward/reaching_object: 0.9419
     Episode_Reward/lifting_object: -0.0892
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 2.36s
                      Time elapsed: 00:08:32
                               ETA: 01:28:29

################################################################################
                     [1m Learning iteration 176/2000 [0m                      

                       Computation: 41955 steps/s (collection: 2.184s, learning 0.159s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.1442
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 49.6130
                       Mean reward: 4.96
               Mean episode length: 242.70
    Episode_Reward/reaching_object: 0.9691
     Episode_Reward/lifting_object: 0.0099
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 2.34s
                      Time elapsed: 00:08:34
                               ETA: 01:28:20

################################################################################
                     [1m Learning iteration 177/2000 [0m                      

                       Computation: 42920 steps/s (collection: 2.141s, learning 0.150s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.2033
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 49.6892
                       Mean reward: 4.48
               Mean episode length: 243.06
    Episode_Reward/reaching_object: 0.9638
     Episode_Reward/lifting_object: -0.0604
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 2.29s
                      Time elapsed: 00:08:36
                               ETA: 01:28:11

################################################################################
                     [1m Learning iteration 178/2000 [0m                      

                       Computation: 40637 steps/s (collection: 2.257s, learning 0.162s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.4057
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.7242
                       Mean reward: 4.28
               Mean episode length: 230.48
    Episode_Reward/reaching_object: 0.9629
     Episode_Reward/lifting_object: -0.0706
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0208
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 2.42s
                      Time elapsed: 00:08:39
                               ETA: 01:28:03

################################################################################
                     [1m Learning iteration 179/2000 [0m                      

                       Computation: 40105 steps/s (collection: 2.296s, learning 0.155s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.2377
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 49.7532
                       Mean reward: 4.88
               Mean episode length: 227.07
    Episode_Reward/reaching_object: 0.9713
     Episode_Reward/lifting_object: -0.0204
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 2.45s
                      Time elapsed: 00:08:41
                               ETA: 01:27:56

################################################################################
                     [1m Learning iteration 180/2000 [0m                      

                       Computation: 42005 steps/s (collection: 2.215s, learning 0.126s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.2376
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 49.8124
                       Mean reward: 4.28
               Mean episode length: 232.15
    Episode_Reward/reaching_object: 0.9026
     Episode_Reward/lifting_object: -0.0142
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0204
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 2.34s
                      Time elapsed: 00:08:43
                               ETA: 01:27:47

################################################################################
                     [1m Learning iteration 181/2000 [0m                      

                       Computation: 42304 steps/s (collection: 2.193s, learning 0.131s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.1152
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 49.8782
                       Mean reward: 4.98
               Mean episode length: 240.74
    Episode_Reward/reaching_object: 0.9509
     Episode_Reward/lifting_object: -0.0395
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 2.32s
                      Time elapsed: 00:08:46
                               ETA: 01:27:39

################################################################################
                     [1m Learning iteration 182/2000 [0m                      

                       Computation: 42548 steps/s (collection: 2.190s, learning 0.120s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.2600
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.9485
                       Mean reward: 4.64
               Mean episode length: 233.76
    Episode_Reward/reaching_object: 0.9593
     Episode_Reward/lifting_object: -0.0521
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0211
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 2.31s
                      Time elapsed: 00:08:48
                               ETA: 01:27:30

################################################################################
                     [1m Learning iteration 183/2000 [0m                      

                       Computation: 43088 steps/s (collection: 2.154s, learning 0.127s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.4402
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 49.9726
                       Mean reward: 4.14
               Mean episode length: 235.69
    Episode_Reward/reaching_object: 0.9506
     Episode_Reward/lifting_object: -0.0557
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 2.28s
                      Time elapsed: 00:08:50
                               ETA: 01:27:21

################################################################################
                     [1m Learning iteration 184/2000 [0m                      

                       Computation: 43098 steps/s (collection: 2.150s, learning 0.131s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.5376
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.0027
                       Mean reward: 5.08
               Mean episode length: 232.94
    Episode_Reward/reaching_object: 0.9200
     Episode_Reward/lifting_object: 0.0018
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0206
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 2.28s
                      Time elapsed: 00:08:53
                               ETA: 01:27:12

################################################################################
                     [1m Learning iteration 185/2000 [0m                      

                       Computation: 44281 steps/s (collection: 2.105s, learning 0.115s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.3696
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 50.0214
                       Mean reward: 4.62
               Mean episode length: 235.95
    Episode_Reward/reaching_object: 0.9587
     Episode_Reward/lifting_object: -0.0229
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 2.22s
                      Time elapsed: 00:08:55
                               ETA: 01:27:03

################################################################################
                     [1m Learning iteration 186/2000 [0m                      

                       Computation: 43875 steps/s (collection: 2.094s, learning 0.146s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.8429
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 50.0491
                       Mean reward: 3.05
               Mean episode length: 227.08
    Episode_Reward/reaching_object: 0.9486
     Episode_Reward/lifting_object: -0.1384
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0212
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 2.24s
                      Time elapsed: 00:08:57
                               ETA: 01:26:54

################################################################################
                     [1m Learning iteration 187/2000 [0m                      

                       Computation: 43788 steps/s (collection: 2.102s, learning 0.143s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.2684
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 50.1144
                       Mean reward: 4.98
               Mean episode length: 231.06
    Episode_Reward/reaching_object: 0.9690
     Episode_Reward/lifting_object: -0.0008
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 2.24s
                      Time elapsed: 00:08:59
                               ETA: 01:26:45

################################################################################
                     [1m Learning iteration 188/2000 [0m                      

                       Computation: 43197 steps/s (collection: 2.141s, learning 0.134s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.3527
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 50.1777
                       Mean reward: 4.73
               Mean episode length: 234.90
    Episode_Reward/reaching_object: 1.0116
     Episode_Reward/lifting_object: -0.0043
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 2.28s
                      Time elapsed: 00:09:02
                               ETA: 01:26:37

################################################################################
                     [1m Learning iteration 189/2000 [0m                      

                       Computation: 43170 steps/s (collection: 2.119s, learning 0.158s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.2475
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 50.2075
                       Mean reward: 4.99
               Mean episode length: 242.72
    Episode_Reward/reaching_object: 1.0109
     Episode_Reward/lifting_object: -0.0413
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 2.28s
                      Time elapsed: 00:09:04
                               ETA: 01:26:28

################################################################################
                     [1m Learning iteration 190/2000 [0m                      

                       Computation: 41624 steps/s (collection: 2.218s, learning 0.143s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.5502
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 50.2522
                       Mean reward: 3.86
               Mean episode length: 237.34
    Episode_Reward/reaching_object: 1.0239
     Episode_Reward/lifting_object: -0.0634
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 2.36s
                      Time elapsed: 00:09:06
                               ETA: 01:26:20

################################################################################
                     [1m Learning iteration 191/2000 [0m                      

                       Computation: 39837 steps/s (collection: 2.331s, learning 0.136s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.2618
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.2671
                       Mean reward: 5.00
               Mean episode length: 235.12
    Episode_Reward/reaching_object: 1.0414
     Episode_Reward/lifting_object: -0.0160
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 2.47s
                      Time elapsed: 00:09:09
                               ETA: 01:26:14

################################################################################
                     [1m Learning iteration 192/2000 [0m                      

                       Computation: 39472 steps/s (collection: 2.353s, learning 0.137s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.2639
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 50.3180
                       Mean reward: 5.07
               Mean episode length: 244.56
    Episode_Reward/reaching_object: 1.0109
     Episode_Reward/lifting_object: 0.0299
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 2.49s
                      Time elapsed: 00:09:11
                               ETA: 01:26:07

################################################################################
                     [1m Learning iteration 193/2000 [0m                      

                       Computation: 41319 steps/s (collection: 2.245s, learning 0.134s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.2612
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.3924
                       Mean reward: 5.16
               Mean episode length: 236.52
    Episode_Reward/reaching_object: 1.0298
     Episode_Reward/lifting_object: -0.0675
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 2.38s
                      Time elapsed: 00:09:14
                               ETA: 01:26:00

################################################################################
                     [1m Learning iteration 194/2000 [0m                      

                       Computation: 38594 steps/s (collection: 2.400s, learning 0.147s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.1835
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 50.4688
                       Mean reward: 5.30
               Mean episode length: 234.91
    Episode_Reward/reaching_object: 1.0158
     Episode_Reward/lifting_object: -0.0655
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 2.55s
                      Time elapsed: 00:09:16
                               ETA: 01:25:54

################################################################################
                     [1m Learning iteration 195/2000 [0m                      

                       Computation: 41793 steps/s (collection: 2.241s, learning 0.111s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.2884
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 50.5488
                       Mean reward: 4.83
               Mean episode length: 243.15
    Episode_Reward/reaching_object: 1.0589
     Episode_Reward/lifting_object: -0.0087
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 2.35s
                      Time elapsed: 00:09:18
                               ETA: 01:25:47

################################################################################
                     [1m Learning iteration 196/2000 [0m                      

                       Computation: 41156 steps/s (collection: 2.247s, learning 0.141s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.7010
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.5997
                       Mean reward: 5.05
               Mean episode length: 239.88
    Episode_Reward/reaching_object: 1.0056
     Episode_Reward/lifting_object: 0.0143
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 20.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 2.39s
                      Time elapsed: 00:09:21
                               ETA: 01:25:40

################################################################################
                     [1m Learning iteration 197/2000 [0m                      

                       Computation: 38426 steps/s (collection: 2.397s, learning 0.162s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.3942
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 50.6666
                       Mean reward: 4.20
               Mean episode length: 231.72
    Episode_Reward/reaching_object: 1.0337
     Episode_Reward/lifting_object: -0.0026
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 18.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 2.56s
                      Time elapsed: 00:09:23
                               ETA: 01:25:34

################################################################################
                     [1m Learning iteration 198/2000 [0m                      

                       Computation: 38737 steps/s (collection: 2.397s, learning 0.141s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.1578
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 50.7132
                       Mean reward: 5.16
               Mean episode length: 237.24
    Episode_Reward/reaching_object: 1.0379
     Episode_Reward/lifting_object: 0.0001
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 2.54s
                      Time elapsed: 00:09:26
                               ETA: 01:25:29

################################################################################
                     [1m Learning iteration 199/2000 [0m                      

                       Computation: 40555 steps/s (collection: 2.285s, learning 0.139s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.1958
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 50.7827
                       Mean reward: 5.00
               Mean episode length: 235.03
    Episode_Reward/reaching_object: 1.0546
     Episode_Reward/lifting_object: 0.0198
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 2.42s
                      Time elapsed: 00:09:28
                               ETA: 01:25:22

################################################################################
                     [1m Learning iteration 200/2000 [0m                      

                       Computation: 42220 steps/s (collection: 2.196s, learning 0.133s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.2178
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.8300
                       Mean reward: 4.35
               Mean episode length: 228.37
    Episode_Reward/reaching_object: 1.0054
     Episode_Reward/lifting_object: -0.0744
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 2.33s
                      Time elapsed: 00:09:31
                               ETA: 01:25:15

################################################################################
                     [1m Learning iteration 201/2000 [0m                      

                       Computation: 36997 steps/s (collection: 2.436s, learning 0.221s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.6252
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 50.8830
                       Mean reward: 4.34
               Mean episode length: 228.91
    Episode_Reward/reaching_object: 1.0021
     Episode_Reward/lifting_object: -0.0362
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 2.66s
                      Time elapsed: 00:09:33
                               ETA: 01:25:10

################################################################################
                     [1m Learning iteration 202/2000 [0m                      

                       Computation: 34171 steps/s (collection: 2.717s, learning 0.160s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.2923
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.9011
                       Mean reward: 3.94
               Mean episode length: 223.36
    Episode_Reward/reaching_object: 0.9962
     Episode_Reward/lifting_object: -0.1529
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 2.88s
                      Time elapsed: 00:09:36
                               ETA: 01:25:08

################################################################################
                     [1m Learning iteration 203/2000 [0m                      

                       Computation: 38677 steps/s (collection: 2.391s, learning 0.151s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.7119
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 50.9365
                       Mean reward: 5.23
               Mean episode length: 237.63
    Episode_Reward/reaching_object: 1.0915
     Episode_Reward/lifting_object: -0.0041
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 2.54s
                      Time elapsed: 00:09:39
                               ETA: 01:25:02

################################################################################
                     [1m Learning iteration 204/2000 [0m                      

                       Computation: 42470 steps/s (collection: 2.164s, learning 0.151s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.1323
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 50.9498
                       Mean reward: 3.83
               Mean episode length: 242.20
    Episode_Reward/reaching_object: 1.0462
     Episode_Reward/lifting_object: -0.0850
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 2.31s
                      Time elapsed: 00:09:41
                               ETA: 01:24:55

################################################################################
                     [1m Learning iteration 205/2000 [0m                      

                       Computation: 42863 steps/s (collection: 2.157s, learning 0.137s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.0806
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.9913
                       Mean reward: 5.75
               Mean episode length: 244.68
    Episode_Reward/reaching_object: 1.1188
     Episode_Reward/lifting_object: 0.0032
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 2.29s
                      Time elapsed: 00:09:43
                               ETA: 01:24:47

################################################################################
                     [1m Learning iteration 206/2000 [0m                      

                       Computation: 42425 steps/s (collection: 2.182s, learning 0.136s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.2885
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 51.0628
                       Mean reward: 5.07
               Mean episode length: 241.03
    Episode_Reward/reaching_object: 1.0938
     Episode_Reward/lifting_object: -0.0527
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 18.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 2.32s
                      Time elapsed: 00:09:46
                               ETA: 01:24:40

################################################################################
                     [1m Learning iteration 207/2000 [0m                      

                       Computation: 42774 steps/s (collection: 2.172s, learning 0.127s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.9568
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.1098
                       Mean reward: 4.70
               Mean episode length: 235.55
    Episode_Reward/reaching_object: 1.0566
     Episode_Reward/lifting_object: -0.0296
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 18.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 2.30s
                      Time elapsed: 00:09:48
                               ETA: 01:24:32

################################################################################
                     [1m Learning iteration 208/2000 [0m                      

                       Computation: 43783 steps/s (collection: 2.124s, learning 0.121s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.2379
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.1218
                       Mean reward: 4.98
               Mean episode length: 236.77
    Episode_Reward/reaching_object: 1.0745
     Episode_Reward/lifting_object: -0.0495
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 2.25s
                      Time elapsed: 00:09:50
                               ETA: 01:24:25

################################################################################
                     [1m Learning iteration 209/2000 [0m                      

                       Computation: 43358 steps/s (collection: 2.139s, learning 0.128s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.2874
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 51.1586
                       Mean reward: 5.34
               Mean episode length: 236.50
    Episode_Reward/reaching_object: 1.0682
     Episode_Reward/lifting_object: -0.0175
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 2.27s
                      Time elapsed: 00:09:52
                               ETA: 01:24:17

################################################################################
                     [1m Learning iteration 210/2000 [0m                      

                       Computation: 38753 steps/s (collection: 2.395s, learning 0.141s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.2211
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 51.1995
                       Mean reward: 5.09
               Mean episode length: 236.75
    Episode_Reward/reaching_object: 1.0664
     Episode_Reward/lifting_object: 0.0023
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 2.54s
                      Time elapsed: 00:09:55
                               ETA: 01:24:12

################################################################################
                     [1m Learning iteration 211/2000 [0m                      

                       Computation: 42759 steps/s (collection: 2.159s, learning 0.140s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.1865
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 51.2572
                       Mean reward: 4.81
               Mean episode length: 240.92
    Episode_Reward/reaching_object: 1.1161
     Episode_Reward/lifting_object: 0.0038
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 2.30s
                      Time elapsed: 00:09:57
                               ETA: 01:24:04

################################################################################
                     [1m Learning iteration 212/2000 [0m                      

                       Computation: 43489 steps/s (collection: 2.143s, learning 0.117s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.2088
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 51.3229
                       Mean reward: 5.00
               Mean episode length: 241.26
    Episode_Reward/reaching_object: 1.0748
     Episode_Reward/lifting_object: 0.0428
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 2.26s
                      Time elapsed: 00:10:00
                               ETA: 01:23:57

################################################################################
                     [1m Learning iteration 213/2000 [0m                      

                       Computation: 43213 steps/s (collection: 2.136s, learning 0.139s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.3914
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 51.3713
                       Mean reward: 4.78
               Mean episode length: 244.54
    Episode_Reward/reaching_object: 1.1013
     Episode_Reward/lifting_object: -0.0382
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 2.27s
                      Time elapsed: 00:10:02
                               ETA: 01:23:50

################################################################################
                     [1m Learning iteration 214/2000 [0m                      

                       Computation: 41552 steps/s (collection: 2.220s, learning 0.146s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.2521
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 51.3843
                       Mean reward: 5.09
               Mean episode length: 232.74
    Episode_Reward/reaching_object: 1.1050
     Episode_Reward/lifting_object: -0.0319
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 2.37s
                      Time elapsed: 00:10:04
                               ETA: 01:23:43

################################################################################
                     [1m Learning iteration 215/2000 [0m                      

                       Computation: 40336 steps/s (collection: 2.308s, learning 0.130s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.1184
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 51.4171
                       Mean reward: 5.28
               Mean episode length: 231.63
    Episode_Reward/reaching_object: 1.1107
     Episode_Reward/lifting_object: 0.0117
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 2.44s
                      Time elapsed: 00:10:07
                               ETA: 01:23:37

################################################################################
                     [1m Learning iteration 216/2000 [0m                      

                       Computation: 40912 steps/s (collection: 2.271s, learning 0.132s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.4022
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 51.4690
                       Mean reward: 5.78
               Mean episode length: 241.07
    Episode_Reward/reaching_object: 1.1360
     Episode_Reward/lifting_object: 0.0144
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 2.40s
                      Time elapsed: 00:10:09
                               ETA: 01:23:31

################################################################################
                     [1m Learning iteration 217/2000 [0m                      

                       Computation: 41286 steps/s (collection: 2.250s, learning 0.131s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.5513
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 51.5187
                       Mean reward: 5.62
               Mean episode length: 241.06
    Episode_Reward/reaching_object: 1.0751
     Episode_Reward/lifting_object: 0.0270
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 18.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 2.38s
                      Time elapsed: 00:10:11
                               ETA: 01:23:25

################################################################################
                     [1m Learning iteration 218/2000 [0m                      

                       Computation: 40789 steps/s (collection: 2.288s, learning 0.122s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.2575
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 51.5722
                       Mean reward: 5.55
               Mean episode length: 237.91
    Episode_Reward/reaching_object: 1.0724
     Episode_Reward/lifting_object: -0.0090
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 18.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 2.41s
                      Time elapsed: 00:10:14
                               ETA: 01:23:19

################################################################################
                     [1m Learning iteration 219/2000 [0m                      

                       Computation: 41756 steps/s (collection: 2.242s, learning 0.112s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.3216
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.6294
                       Mean reward: 5.41
               Mean episode length: 237.28
    Episode_Reward/reaching_object: 1.0768
     Episode_Reward/lifting_object: 0.0200
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 2.35s
                      Time elapsed: 00:10:16
                               ETA: 01:23:12

################################################################################
                     [1m Learning iteration 220/2000 [0m                      

                       Computation: 42017 steps/s (collection: 2.227s, learning 0.113s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.2674
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 51.6792
                       Mean reward: 5.16
               Mean episode length: 240.80
    Episode_Reward/reaching_object: 1.1080
     Episode_Reward/lifting_object: -0.0467
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 2.34s
                      Time elapsed: 00:10:19
                               ETA: 01:23:06

################################################################################
                     [1m Learning iteration 221/2000 [0m                      

                       Computation: 40554 steps/s (collection: 2.299s, learning 0.125s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.3278
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 51.7335
                       Mean reward: 4.84
               Mean episode length: 246.31
    Episode_Reward/reaching_object: 1.0775
     Episode_Reward/lifting_object: -0.0602
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 2.42s
                      Time elapsed: 00:10:21
                               ETA: 01:23:00

################################################################################
                     [1m Learning iteration 222/2000 [0m                      

                       Computation: 39901 steps/s (collection: 2.354s, learning 0.110s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.2334
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 51.7989
                       Mean reward: 5.16
               Mean episode length: 239.76
    Episode_Reward/reaching_object: 1.0822
     Episode_Reward/lifting_object: -0.0339
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 2.46s
                      Time elapsed: 00:10:23
                               ETA: 01:22:54

################################################################################
                     [1m Learning iteration 223/2000 [0m                      

                       Computation: 42272 steps/s (collection: 2.202s, learning 0.123s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.1939
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 51.8526
                       Mean reward: 5.32
               Mean episode length: 239.69
    Episode_Reward/reaching_object: 1.0652
     Episode_Reward/lifting_object: 0.0511
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 2.33s
                      Time elapsed: 00:10:26
                               ETA: 01:22:48

################################################################################
                     [1m Learning iteration 224/2000 [0m                      

                       Computation: 40505 steps/s (collection: 2.300s, learning 0.127s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.3191
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.9004
                       Mean reward: 5.78
               Mean episode length: 238.07
    Episode_Reward/reaching_object: 1.0820
     Episode_Reward/lifting_object: -0.0055
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 2.43s
                      Time elapsed: 00:10:28
                               ETA: 01:22:42

################################################################################
                     [1m Learning iteration 225/2000 [0m                      

                       Computation: 40937 steps/s (collection: 2.285s, learning 0.116s)
             Mean action noise std: 1.79
          Mean value_function loss: 1.2158
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.9321
                       Mean reward: 3.08
               Mean episode length: 243.80
    Episode_Reward/reaching_object: 1.0752
     Episode_Reward/lifting_object: -0.0772
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 2.40s
                      Time elapsed: 00:10:31
                               ETA: 01:22:36

################################################################################
                     [1m Learning iteration 226/2000 [0m                      

                       Computation: 39838 steps/s (collection: 2.345s, learning 0.123s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.7596
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 51.9952
                       Mean reward: 5.38
               Mean episode length: 236.90
    Episode_Reward/reaching_object: 1.1141
     Episode_Reward/lifting_object: 0.0501
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 2.47s
                      Time elapsed: 00:10:33
                               ETA: 01:22:31

################################################################################
                     [1m Learning iteration 227/2000 [0m                      

                       Computation: 38660 steps/s (collection: 2.383s, learning 0.159s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.9709
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 52.0645
                       Mean reward: 5.45
               Mean episode length: 243.19
    Episode_Reward/reaching_object: 1.0818
     Episode_Reward/lifting_object: 0.0170
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 19.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 2.54s
                      Time elapsed: 00:10:36
                               ETA: 01:22:26

################################################################################
                     [1m Learning iteration 228/2000 [0m                      

                       Computation: 40461 steps/s (collection: 2.297s, learning 0.133s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.6294
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.0933
                       Mean reward: 5.49
               Mean episode length: 242.74
    Episode_Reward/reaching_object: 1.1475
     Episode_Reward/lifting_object: 0.0056
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 17.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 2.43s
                      Time elapsed: 00:10:38
                               ETA: 01:22:21

################################################################################
                     [1m Learning iteration 229/2000 [0m                      

                       Computation: 40857 steps/s (collection: 2.267s, learning 0.139s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.4072
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.1473
                       Mean reward: 4.21
               Mean episode length: 235.46
    Episode_Reward/reaching_object: 1.0975
     Episode_Reward/lifting_object: -0.1472
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 2.41s
                      Time elapsed: 00:10:40
                               ETA: 01:22:15

################################################################################
                     [1m Learning iteration 230/2000 [0m                      

                       Computation: 40493 steps/s (collection: 2.292s, learning 0.136s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.3626
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 52.1967
                       Mean reward: 5.67
               Mean episode length: 237.09
    Episode_Reward/reaching_object: 1.1012
     Episode_Reward/lifting_object: 0.0314
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 2.43s
                      Time elapsed: 00:10:43
                               ETA: 01:22:09

################################################################################
                     [1m Learning iteration 231/2000 [0m                      

                       Computation: 39049 steps/s (collection: 2.370s, learning 0.147s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.4454
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 52.2658
                       Mean reward: 5.20
               Mean episode length: 230.42
    Episode_Reward/reaching_object: 1.0897
     Episode_Reward/lifting_object: 0.0413
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 2.52s
                      Time elapsed: 00:10:45
                               ETA: 01:22:04

################################################################################
                     [1m Learning iteration 232/2000 [0m                      

                       Computation: 39935 steps/s (collection: 2.339s, learning 0.122s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.2388
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.2806
                       Mean reward: 4.45
               Mean episode length: 230.80
    Episode_Reward/reaching_object: 1.0523
     Episode_Reward/lifting_object: -0.0787
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 2.46s
                      Time elapsed: 00:10:48
                               ETA: 01:21:59

################################################################################
                     [1m Learning iteration 233/2000 [0m                      

                       Computation: 38422 steps/s (collection: 2.403s, learning 0.156s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.7771
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.3030
                       Mean reward: 5.18
               Mean episode length: 231.63
    Episode_Reward/reaching_object: 1.0682
     Episode_Reward/lifting_object: 0.0161
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 2.56s
                      Time elapsed: 00:10:50
                               ETA: 01:21:55

################################################################################
                     [1m Learning iteration 234/2000 [0m                      

                       Computation: 39455 steps/s (collection: 2.375s, learning 0.117s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.4457
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.3307
                       Mean reward: 4.54
               Mean episode length: 228.67
    Episode_Reward/reaching_object: 1.0629
     Episode_Reward/lifting_object: -0.0256
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 2.49s
                      Time elapsed: 00:10:53
                               ETA: 01:21:50

################################################################################
                     [1m Learning iteration 235/2000 [0m                      

                       Computation: 41696 steps/s (collection: 2.223s, learning 0.135s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.6010
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.3714
                       Mean reward: 5.57
               Mean episode length: 235.16
    Episode_Reward/reaching_object: 1.0609
     Episode_Reward/lifting_object: 0.0297
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 2.36s
                      Time elapsed: 00:10:55
                               ETA: 01:21:44

################################################################################
                     [1m Learning iteration 236/2000 [0m                      

                       Computation: 42552 steps/s (collection: 2.180s, learning 0.131s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.3857
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.3990
                       Mean reward: 3.95
               Mean episode length: 243.14
    Episode_Reward/reaching_object: 1.1011
     Episode_Reward/lifting_object: -0.1224
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 2.31s
                      Time elapsed: 00:10:58
                               ETA: 01:21:38

################################################################################
                     [1m Learning iteration 237/2000 [0m                      

                       Computation: 43839 steps/s (collection: 2.114s, learning 0.129s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.2433
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 52.4386
                       Mean reward: 5.45
               Mean episode length: 244.38
    Episode_Reward/reaching_object: 1.0921
     Episode_Reward/lifting_object: -0.0180
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 2.24s
                      Time elapsed: 00:11:00
                               ETA: 01:21:31

################################################################################
                     [1m Learning iteration 238/2000 [0m                      

                       Computation: 42216 steps/s (collection: 2.207s, learning 0.122s)
             Mean action noise std: 1.82
          Mean value_function loss: 1.3064
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 52.4807
                       Mean reward: 5.19
               Mean episode length: 246.73
    Episode_Reward/reaching_object: 1.0944
     Episode_Reward/lifting_object: 0.0068
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 2.33s
                      Time elapsed: 00:11:02
                               ETA: 01:21:25

################################################################################
                     [1m Learning iteration 239/2000 [0m                      

                       Computation: 39721 steps/s (collection: 2.313s, learning 0.162s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.1861
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.4904
                       Mean reward: 5.46
               Mean episode length: 241.35
    Episode_Reward/reaching_object: 1.1125
     Episode_Reward/lifting_object: -0.0327
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 2.47s
                      Time elapsed: 00:11:05
                               ETA: 01:21:20

################################################################################
                     [1m Learning iteration 240/2000 [0m                      

                       Computation: 41225 steps/s (collection: 2.241s, learning 0.143s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.0996
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 52.5258
                       Mean reward: 5.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0770
     Episode_Reward/lifting_object: 0.0736
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 2.38s
                      Time elapsed: 00:11:07
                               ETA: 01:21:14

################################################################################
                     [1m Learning iteration 241/2000 [0m                      

                       Computation: 42793 steps/s (collection: 2.161s, learning 0.136s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.2812
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.5839
                       Mean reward: 4.86
               Mean episode length: 246.06
    Episode_Reward/reaching_object: 1.1131
     Episode_Reward/lifting_object: -0.0360
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 2.30s
                      Time elapsed: 00:11:09
                               ETA: 01:21:08

################################################################################
                     [1m Learning iteration 242/2000 [0m                      

                       Computation: 39231 steps/s (collection: 2.351s, learning 0.155s)
             Mean action noise std: 1.84
          Mean value_function loss: 0.1867
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 52.6337
                       Mean reward: 5.48
               Mean episode length: 245.42
    Episode_Reward/reaching_object: 1.0539
     Episode_Reward/lifting_object: -0.0518
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 2.51s
                      Time elapsed: 00:11:12
                               ETA: 01:21:03

################################################################################
                     [1m Learning iteration 243/2000 [0m                      

                       Computation: 39298 steps/s (collection: 2.356s, learning 0.145s)
             Mean action noise std: 1.84
          Mean value_function loss: 0.1176
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 52.6854
                       Mean reward: 5.58
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 1.0751
     Episode_Reward/lifting_object: -0.0306
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 2.50s
                      Time elapsed: 00:11:14
                               ETA: 01:20:59

################################################################################
                     [1m Learning iteration 244/2000 [0m                      

                       Computation: 39759 steps/s (collection: 2.320s, learning 0.152s)
             Mean action noise std: 1.84
          Mean value_function loss: 0.2908
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.7365
                       Mean reward: 5.86
               Mean episode length: 246.47
    Episode_Reward/reaching_object: 1.0470
     Episode_Reward/lifting_object: 0.0536
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0288
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 2.47s
                      Time elapsed: 00:11:17
                               ETA: 01:20:54

################################################################################
                     [1m Learning iteration 245/2000 [0m                      

                       Computation: 39560 steps/s (collection: 2.348s, learning 0.137s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.4039
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 52.7689
                       Mean reward: 4.90
               Mean episode length: 247.81
    Episode_Reward/reaching_object: 1.0427
     Episode_Reward/lifting_object: -0.0258
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 2.48s
                      Time elapsed: 00:11:19
                               ETA: 01:20:49

################################################################################
                     [1m Learning iteration 246/2000 [0m                      

                       Computation: 40464 steps/s (collection: 2.282s, learning 0.147s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.0759
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 52.8365
                       Mean reward: 5.38
               Mean episode length: 245.79
    Episode_Reward/reaching_object: 1.0621
     Episode_Reward/lifting_object: 0.0161
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 2.43s
                      Time elapsed: 00:11:22
                               ETA: 01:20:44

################################################################################
                     [1m Learning iteration 247/2000 [0m                      

                       Computation: 38874 steps/s (collection: 2.384s, learning 0.145s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.2030
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 52.9230
                       Mean reward: 5.69
               Mean episode length: 244.03
    Episode_Reward/reaching_object: 1.0610
     Episode_Reward/lifting_object: 0.0543
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0289
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.9583
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 2.53s
                      Time elapsed: 00:11:24
                               ETA: 01:20:39

################################################################################
                     [1m Learning iteration 248/2000 [0m                      

                       Computation: 38730 steps/s (collection: 2.378s, learning 0.160s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.1850
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.9815
                       Mean reward: 5.12
               Mean episode length: 246.04
    Episode_Reward/reaching_object: 1.0683
     Episode_Reward/lifting_object: 0.0467
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 19.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 2.54s
                      Time elapsed: 00:11:27
                               ETA: 01:20:35

################################################################################
                     [1m Learning iteration 249/2000 [0m                      

                       Computation: 36634 steps/s (collection: 2.533s, learning 0.151s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.0728
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 53.0396
                       Mean reward: 5.55
               Mean episode length: 245.10
    Episode_Reward/reaching_object: 1.0780
     Episode_Reward/lifting_object: 0.0531
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 2.68s
                      Time elapsed: 00:11:29
                               ETA: 01:20:32

################################################################################
                     [1m Learning iteration 250/2000 [0m                      

                       Computation: 39687 steps/s (collection: 2.341s, learning 0.136s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.1877
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 53.1155
                       Mean reward: 5.45
               Mean episode length: 243.26
    Episode_Reward/reaching_object: 1.0571
     Episode_Reward/lifting_object: 0.0534
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 2.48s
                      Time elapsed: 00:11:32
                               ETA: 01:20:27

################################################################################
                     [1m Learning iteration 251/2000 [0m                      

                       Computation: 39013 steps/s (collection: 2.359s, learning 0.161s)
             Mean action noise std: 1.88
          Mean value_function loss: 0.2898
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 53.1755
                       Mean reward: 4.98
               Mean episode length: 246.51
    Episode_Reward/reaching_object: 1.0754
     Episode_Reward/lifting_object: -0.0019
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0297
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 2.52s
                      Time elapsed: 00:11:34
                               ETA: 01:20:23

################################################################################
                     [1m Learning iteration 252/2000 [0m                      

                       Computation: 37949 steps/s (collection: 2.484s, learning 0.107s)
             Mean action noise std: 1.88
          Mean value_function loss: 0.4398
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.2317
                       Mean reward: 5.62
               Mean episode length: 246.13
    Episode_Reward/reaching_object: 1.1054
     Episode_Reward/lifting_object: 0.0214
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 2.59s
                      Time elapsed: 00:11:37
                               ETA: 01:20:19

################################################################################
                     [1m Learning iteration 253/2000 [0m                      

                       Computation: 40362 steps/s (collection: 2.288s, learning 0.147s)
             Mean action noise std: 1.88
          Mean value_function loss: 0.1951
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 53.2736
                       Mean reward: 6.06
               Mean episode length: 242.91
    Episode_Reward/reaching_object: 1.1258
     Episode_Reward/lifting_object: 0.0664
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 2.44s
                      Time elapsed: 00:11:39
                               ETA: 01:20:14

################################################################################
                     [1m Learning iteration 254/2000 [0m                      

                       Computation: 42170 steps/s (collection: 2.196s, learning 0.135s)
             Mean action noise std: 1.89
          Mean value_function loss: 0.4155
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 53.3399
                       Mean reward: 5.16
               Mean episode length: 241.93
    Episode_Reward/reaching_object: 1.0727
     Episode_Reward/lifting_object: 0.0206
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0300
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 2.33s
                      Time elapsed: 00:11:42
                               ETA: 01:20:08

################################################################################
                     [1m Learning iteration 255/2000 [0m                      

                       Computation: 43081 steps/s (collection: 2.162s, learning 0.120s)
             Mean action noise std: 1.89
          Mean value_function loss: 0.4458
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 53.3777
                       Mean reward: 5.76
               Mean episode length: 237.46
    Episode_Reward/reaching_object: 1.0827
     Episode_Reward/lifting_object: 0.0465
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0298
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 2.28s
                      Time elapsed: 00:11:44
                               ETA: 01:20:02

################################################################################
                     [1m Learning iteration 256/2000 [0m                      

                       Computation: 43471 steps/s (collection: 2.128s, learning 0.133s)
             Mean action noise std: 1.89
          Mean value_function loss: 1.0428
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 53.4001
                       Mean reward: 5.33
               Mean episode length: 245.19
    Episode_Reward/reaching_object: 1.0795
     Episode_Reward/lifting_object: -0.0228
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 2.26s
                      Time elapsed: 00:11:46
                               ETA: 01:19:56

################################################################################
                     [1m Learning iteration 257/2000 [0m                      

                       Computation: 43683 steps/s (collection: 2.113s, learning 0.137s)
             Mean action noise std: 1.89
          Mean value_function loss: 0.2856
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.4227
                       Mean reward: 4.59
               Mean episode length: 244.23
    Episode_Reward/reaching_object: 1.0995
     Episode_Reward/lifting_object: -0.0164
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 2.25s
                      Time elapsed: 00:11:49
                               ETA: 01:19:50

################################################################################
                     [1m Learning iteration 258/2000 [0m                      

                       Computation: 43571 steps/s (collection: 2.120s, learning 0.136s)
             Mean action noise std: 1.90
          Mean value_function loss: 0.2074
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 53.4860
                       Mean reward: 5.34
               Mean episode length: 245.80
    Episode_Reward/reaching_object: 1.0757
     Episode_Reward/lifting_object: -0.0441
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 2.26s
                      Time elapsed: 00:11:51
                               ETA: 01:19:44

################################################################################
                     [1m Learning iteration 259/2000 [0m                      

                       Computation: 43338 steps/s (collection: 2.135s, learning 0.133s)
             Mean action noise std: 1.90
          Mean value_function loss: 0.5960
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.5583
                       Mean reward: 5.92
               Mean episode length: 236.63
    Episode_Reward/reaching_object: 1.1000
     Episode_Reward/lifting_object: 0.1280
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 2.27s
                      Time elapsed: 00:11:53
                               ETA: 01:19:38

################################################################################
                     [1m Learning iteration 260/2000 [0m                      

                       Computation: 44127 steps/s (collection: 2.124s, learning 0.103s)
             Mean action noise std: 1.91
          Mean value_function loss: 0.2042
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 53.6088
                       Mean reward: 5.35
               Mean episode length: 242.99
    Episode_Reward/reaching_object: 1.0667
     Episode_Reward/lifting_object: 0.0048
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 2.23s
                      Time elapsed: 00:11:55
                               ETA: 01:19:32

################################################################################
                     [1m Learning iteration 261/2000 [0m                      

                       Computation: 43498 steps/s (collection: 2.117s, learning 0.143s)
             Mean action noise std: 1.91
          Mean value_function loss: 1.4005
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 53.6759
                       Mean reward: 4.98
               Mean episode length: 236.69
    Episode_Reward/reaching_object: 1.0790
     Episode_Reward/lifting_object: 0.0004
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 2.26s
                      Time elapsed: 00:11:58
                               ETA: 01:19:26

################################################################################
                     [1m Learning iteration 262/2000 [0m                      

                       Computation: 43065 steps/s (collection: 2.143s, learning 0.140s)
             Mean action noise std: 1.91
          Mean value_function loss: 0.9304
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 53.6913
                       Mean reward: 5.15
               Mean episode length: 236.56
    Episode_Reward/reaching_object: 1.1212
     Episode_Reward/lifting_object: -0.0116
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.28s
                      Time elapsed: 00:12:00
                               ETA: 01:19:20

################################################################################
                     [1m Learning iteration 263/2000 [0m                      

                       Computation: 43808 steps/s (collection: 2.126s, learning 0.118s)
             Mean action noise std: 1.92
          Mean value_function loss: 0.2658
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 53.7327
                       Mean reward: 4.41
               Mean episode length: 230.74
    Episode_Reward/reaching_object: 1.0898
     Episode_Reward/lifting_object: -0.0164
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 2.24s
                      Time elapsed: 00:12:02
                               ETA: 01:19:14

################################################################################
                     [1m Learning iteration 264/2000 [0m                      

                       Computation: 43291 steps/s (collection: 2.126s, learning 0.145s)
             Mean action noise std: 1.92
          Mean value_function loss: 0.4151
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 53.7846
                       Mean reward: 5.23
               Mean episode length: 240.59
    Episode_Reward/reaching_object: 1.1082
     Episode_Reward/lifting_object: 0.0110
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 2.27s
                      Time elapsed: 00:12:04
                               ETA: 01:19:08

################################################################################
                     [1m Learning iteration 265/2000 [0m                      

                       Computation: 42856 steps/s (collection: 2.138s, learning 0.156s)
             Mean action noise std: 1.92
          Mean value_function loss: 0.2188
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.8257
                       Mean reward: 5.87
               Mean episode length: 236.02
    Episode_Reward/reaching_object: 1.0966
     Episode_Reward/lifting_object: 0.0202
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 2.29s
                      Time elapsed: 00:12:07
                               ETA: 01:19:03

################################################################################
                     [1m Learning iteration 266/2000 [0m                      

                       Computation: 42458 steps/s (collection: 2.195s, learning 0.120s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.1818
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 53.8632
                       Mean reward: 5.70
               Mean episode length: 236.68
    Episode_Reward/reaching_object: 1.1466
     Episode_Reward/lifting_object: 0.0579
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 2.32s
                      Time elapsed: 00:12:09
                               ETA: 01:18:57

################################################################################
                     [1m Learning iteration 267/2000 [0m                      

                       Computation: 42215 steps/s (collection: 2.170s, learning 0.159s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.0725
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 53.9399
                       Mean reward: 5.91
               Mean episode length: 234.15
    Episode_Reward/reaching_object: 1.1457
     Episode_Reward/lifting_object: 0.0626
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 2.33s
                      Time elapsed: 00:12:11
                               ETA: 01:18:52

################################################################################
                     [1m Learning iteration 268/2000 [0m                      

                       Computation: 39136 steps/s (collection: 2.370s, learning 0.142s)
             Mean action noise std: 1.94
          Mean value_function loss: 0.2933
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.0229
                       Mean reward: 5.58
               Mean episode length: 239.26
    Episode_Reward/reaching_object: 1.1078
     Episode_Reward/lifting_object: 0.0770
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 2.51s
                      Time elapsed: 00:12:14
                               ETA: 01:18:48

################################################################################
                     [1m Learning iteration 269/2000 [0m                      

                       Computation: 41794 steps/s (collection: 2.225s, learning 0.127s)
             Mean action noise std: 1.94
          Mean value_function loss: 0.2237
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 54.1039
                       Mean reward: 5.50
               Mean episode length: 240.41
    Episode_Reward/reaching_object: 1.1042
     Episode_Reward/lifting_object: 0.0293
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 17.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 2.35s
                      Time elapsed: 00:12:16
                               ETA: 01:18:43

################################################################################
                     [1m Learning iteration 270/2000 [0m                      

                       Computation: 42521 steps/s (collection: 2.172s, learning 0.140s)
             Mean action noise std: 1.95
          Mean value_function loss: 0.2402
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 54.1733
                       Mean reward: 5.57
               Mean episode length: 239.47
    Episode_Reward/reaching_object: 1.1604
     Episode_Reward/lifting_object: -0.0078
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 2.31s
                      Time elapsed: 00:12:19
                               ETA: 01:18:37

################################################################################
                     [1m Learning iteration 271/2000 [0m                      

                       Computation: 39902 steps/s (collection: 2.316s, learning 0.147s)
             Mean action noise std: 1.95
          Mean value_function loss: 0.2730
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 54.2345
                       Mean reward: 5.62
               Mean episode length: 242.42
    Episode_Reward/reaching_object: 1.1546
     Episode_Reward/lifting_object: 0.0506
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 2.46s
                      Time elapsed: 00:12:21
                               ETA: 01:18:33

################################################################################
                     [1m Learning iteration 272/2000 [0m                      

                       Computation: 40289 steps/s (collection: 2.293s, learning 0.147s)
             Mean action noise std: 1.96
          Mean value_function loss: 0.4716
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 54.2923
                       Mean reward: 6.21
               Mean episode length: 245.29
    Episode_Reward/reaching_object: 1.1178
     Episode_Reward/lifting_object: 0.1234
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 2.44s
                      Time elapsed: 00:12:23
                               ETA: 01:18:28

################################################################################
                     [1m Learning iteration 273/2000 [0m                      

                       Computation: 37380 steps/s (collection: 2.469s, learning 0.161s)
             Mean action noise std: 1.96
          Mean value_function loss: 0.8278
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.3312
                       Mean reward: 5.39
               Mean episode length: 243.75
    Episode_Reward/reaching_object: 1.0919
     Episode_Reward/lifting_object: -0.0012
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 2.63s
                      Time elapsed: 00:12:26
                               ETA: 01:18:25

################################################################################
                     [1m Learning iteration 274/2000 [0m                      

                       Computation: 36212 steps/s (collection: 2.555s, learning 0.160s)
             Mean action noise std: 1.97
          Mean value_function loss: 0.2314
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 54.3859
                       Mean reward: 5.57
               Mean episode length: 240.05
    Episode_Reward/reaching_object: 1.1112
     Episode_Reward/lifting_object: 0.0258
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 2.71s
                      Time elapsed: 00:12:29
                               ETA: 01:18:22

################################################################################
                     [1m Learning iteration 275/2000 [0m                      

                       Computation: 39809 steps/s (collection: 2.339s, learning 0.131s)
             Mean action noise std: 1.97
          Mean value_function loss: 0.6182
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 54.4518
                       Mean reward: 5.22
               Mean episode length: 234.48
    Episode_Reward/reaching_object: 1.1502
     Episode_Reward/lifting_object: 0.0293
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 2.47s
                      Time elapsed: 00:12:31
                               ETA: 01:18:18

################################################################################
                     [1m Learning iteration 276/2000 [0m                      

                       Computation: 43163 steps/s (collection: 2.168s, learning 0.109s)
             Mean action noise std: 1.97
          Mean value_function loss: 0.4059
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 54.4857
                       Mean reward: 5.05
               Mean episode length: 231.36
    Episode_Reward/reaching_object: 1.1203
     Episode_Reward/lifting_object: 0.0245
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 2.28s
                      Time elapsed: 00:12:34
                               ETA: 01:18:12

################################################################################
                     [1m Learning iteration 277/2000 [0m                      

                       Computation: 42020 steps/s (collection: 2.191s, learning 0.148s)
             Mean action noise std: 1.98
          Mean value_function loss: 0.3001
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 54.5442
                       Mean reward: 5.82
               Mean episode length: 236.66
    Episode_Reward/reaching_object: 1.1458
     Episode_Reward/lifting_object: 0.0467
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 2.34s
                      Time elapsed: 00:12:36
                               ETA: 01:18:07

################################################################################
                     [1m Learning iteration 278/2000 [0m                      

                       Computation: 38282 steps/s (collection: 2.431s, learning 0.137s)
             Mean action noise std: 1.99
          Mean value_function loss: 0.1429
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.6388
                       Mean reward: 5.95
               Mean episode length: 240.22
    Episode_Reward/reaching_object: 1.1013
     Episode_Reward/lifting_object: 0.1313
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 2.57s
                      Time elapsed: 00:12:38
                               ETA: 01:18:04

################################################################################
                     [1m Learning iteration 279/2000 [0m                      

                       Computation: 40733 steps/s (collection: 2.279s, learning 0.135s)
             Mean action noise std: 1.99
          Mean value_function loss: 0.3783
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 54.7085
                       Mean reward: 5.51
               Mean episode length: 233.01
    Episode_Reward/reaching_object: 1.1125
     Episode_Reward/lifting_object: 0.0727
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 17.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 2.41s
                      Time elapsed: 00:12:41
                               ETA: 01:17:59

################################################################################
                     [1m Learning iteration 280/2000 [0m                      

                       Computation: 41361 steps/s (collection: 2.239s, learning 0.138s)
             Mean action noise std: 1.99
          Mean value_function loss: 0.9764
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 54.7510
                       Mean reward: 5.33
               Mean episode length: 241.50
    Episode_Reward/reaching_object: 1.1207
     Episode_Reward/lifting_object: 0.0700
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 2.38s
                      Time elapsed: 00:12:43
                               ETA: 01:17:54

################################################################################
                     [1m Learning iteration 281/2000 [0m                      

                       Computation: 39916 steps/s (collection: 2.328s, learning 0.135s)
             Mean action noise std: 2.00
          Mean value_function loss: 0.3469
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 54.7916
                       Mean reward: 6.02
               Mean episode length: 228.28
    Episode_Reward/reaching_object: 1.0825
     Episode_Reward/lifting_object: 0.0399
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 2.46s
                      Time elapsed: 00:12:46
                               ETA: 01:17:50

################################################################################
                     [1m Learning iteration 282/2000 [0m                      

                       Computation: 40349 steps/s (collection: 2.296s, learning 0.141s)
             Mean action noise std: 2.00
          Mean value_function loss: 0.6391
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 54.8340
                       Mean reward: 3.84
               Mean episode length: 237.20
    Episode_Reward/reaching_object: 1.0715
     Episode_Reward/lifting_object: 0.0181
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 2.44s
                      Time elapsed: 00:12:48
                               ETA: 01:17:45

################################################################################
                     [1m Learning iteration 283/2000 [0m                      

                       Computation: 38589 steps/s (collection: 2.381s, learning 0.166s)
             Mean action noise std: 2.00
          Mean value_function loss: 0.4196
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 54.8882
                       Mean reward: 6.19
               Mean episode length: 224.59
    Episode_Reward/reaching_object: 1.0748
     Episode_Reward/lifting_object: 0.0505
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 2.55s
                      Time elapsed: 00:12:51
                               ETA: 01:17:42

################################################################################
                     [1m Learning iteration 284/2000 [0m                      

                       Computation: 38672 steps/s (collection: 2.384s, learning 0.158s)
             Mean action noise std: 2.01
          Mean value_function loss: 0.4732
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 54.9653
                       Mean reward: 6.24
               Mean episode length: 238.53
    Episode_Reward/reaching_object: 1.0823
     Episode_Reward/lifting_object: 0.0535
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 2.54s
                      Time elapsed: 00:12:53
                               ETA: 01:17:38

################################################################################
                     [1m Learning iteration 285/2000 [0m                      

                       Computation: 39388 steps/s (collection: 2.347s, learning 0.149s)
             Mean action noise std: 2.02
          Mean value_function loss: 0.3340
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 55.0353
                       Mean reward: 5.67
               Mean episode length: 234.85
    Episode_Reward/reaching_object: 1.1536
     Episode_Reward/lifting_object: 0.1014
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 2.50s
                      Time elapsed: 00:12:56
                               ETA: 01:17:34

################################################################################
                     [1m Learning iteration 286/2000 [0m                      

                       Computation: 41984 steps/s (collection: 2.221s, learning 0.120s)
             Mean action noise std: 2.02
          Mean value_function loss: 0.8507
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.0956
                       Mean reward: 5.10
               Mean episode length: 234.73
    Episode_Reward/reaching_object: 1.1424
     Episode_Reward/lifting_object: -0.0942
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 2.34s
                      Time elapsed: 00:12:58
                               ETA: 01:17:29

################################################################################
                     [1m Learning iteration 287/2000 [0m                      

                       Computation: 40587 steps/s (collection: 2.270s, learning 0.152s)
             Mean action noise std: 2.02
          Mean value_function loss: 0.5112
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 55.1243
                       Mean reward: 4.82
               Mean episode length: 236.25
    Episode_Reward/reaching_object: 1.0670
     Episode_Reward/lifting_object: -0.0145
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 2.42s
                      Time elapsed: 00:13:00
                               ETA: 01:17:25

################################################################################
                     [1m Learning iteration 288/2000 [0m                      

                       Computation: 41435 steps/s (collection: 2.237s, learning 0.135s)
             Mean action noise std: 2.02
          Mean value_function loss: 2.4553
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 55.1707
                       Mean reward: 5.45
               Mean episode length: 234.67
    Episode_Reward/reaching_object: 1.0770
     Episode_Reward/lifting_object: 0.0562
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 2.37s
                      Time elapsed: 00:13:03
                               ETA: 01:17:20

################################################################################
                     [1m Learning iteration 289/2000 [0m                      

                       Computation: 41327 steps/s (collection: 2.264s, learning 0.115s)
             Mean action noise std: 2.03
          Mean value_function loss: 0.2428
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 55.1859
                       Mean reward: 5.60
               Mean episode length: 238.46
    Episode_Reward/reaching_object: 1.1106
     Episode_Reward/lifting_object: 0.0835
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 2.38s
                      Time elapsed: 00:13:05
                               ETA: 01:17:15

################################################################################
                     [1m Learning iteration 290/2000 [0m                      

                       Computation: 41056 steps/s (collection: 2.246s, learning 0.148s)
             Mean action noise std: 2.03
          Mean value_function loss: 0.3748
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 55.2403
                       Mean reward: 6.56
               Mean episode length: 239.64
    Episode_Reward/reaching_object: 1.0988
     Episode_Reward/lifting_object: 0.0790
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 2.39s
                      Time elapsed: 00:13:08
                               ETA: 01:17:11

################################################################################
                     [1m Learning iteration 291/2000 [0m                      

                       Computation: 40839 steps/s (collection: 2.249s, learning 0.158s)
             Mean action noise std: 2.04
          Mean value_function loss: 0.4733
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 55.3101
                       Mean reward: 5.56
               Mean episode length: 243.19
    Episode_Reward/reaching_object: 1.0776
     Episode_Reward/lifting_object: 0.0312
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 2.41s
                      Time elapsed: 00:13:10
                               ETA: 01:17:06

################################################################################
                     [1m Learning iteration 292/2000 [0m                      

                       Computation: 40808 steps/s (collection: 2.295s, learning 0.114s)
             Mean action noise std: 2.04
          Mean value_function loss: 0.8873
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 55.3491
                       Mean reward: 5.19
               Mean episode length: 238.19
    Episode_Reward/reaching_object: 1.0749
     Episode_Reward/lifting_object: 0.0493
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 2.41s
                      Time elapsed: 00:13:12
                               ETA: 01:17:02

################################################################################
                     [1m Learning iteration 293/2000 [0m                      

                       Computation: 42273 steps/s (collection: 2.182s, learning 0.143s)
             Mean action noise std: 2.04
          Mean value_function loss: 0.4684
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 55.3946
                       Mean reward: 6.16
               Mean episode length: 235.73
    Episode_Reward/reaching_object: 1.1108
     Episode_Reward/lifting_object: 0.1935
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 2.33s
                      Time elapsed: 00:13:15
                               ETA: 01:16:57

################################################################################
                     [1m Learning iteration 294/2000 [0m                      

                       Computation: 36990 steps/s (collection: 2.526s, learning 0.132s)
             Mean action noise std: 2.05
          Mean value_function loss: 0.7016
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 55.4401
                       Mean reward: 5.48
               Mean episode length: 236.64
    Episode_Reward/reaching_object: 1.1057
     Episode_Reward/lifting_object: 0.0569
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 2.66s
                      Time elapsed: 00:13:17
                               ETA: 01:16:54

################################################################################
                     [1m Learning iteration 295/2000 [0m                      

                       Computation: 41490 steps/s (collection: 2.215s, learning 0.154s)
             Mean action noise std: 2.05
          Mean value_function loss: 0.4277
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 55.4652
                       Mean reward: 6.33
               Mean episode length: 238.64
    Episode_Reward/reaching_object: 1.0794
     Episode_Reward/lifting_object: 0.1045
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 2.37s
                      Time elapsed: 00:13:20
                               ETA: 01:16:49

################################################################################
                     [1m Learning iteration 296/2000 [0m                      

                       Computation: 42626 steps/s (collection: 2.171s, learning 0.136s)
             Mean action noise std: 2.05
          Mean value_function loss: 0.1932
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 55.5296
                       Mean reward: 5.56
               Mean episode length: 239.08
    Episode_Reward/reaching_object: 1.0651
     Episode_Reward/lifting_object: 0.1017
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 2.31s
                      Time elapsed: 00:13:22
                               ETA: 01:16:44

################################################################################
                     [1m Learning iteration 297/2000 [0m                      

                       Computation: 43324 steps/s (collection: 2.167s, learning 0.102s)
             Mean action noise std: 2.06
          Mean value_function loss: 0.7943
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 55.5878
                       Mean reward: 5.42
               Mean episode length: 235.20
    Episode_Reward/reaching_object: 1.0950
     Episode_Reward/lifting_object: -0.0276
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 2.27s
                      Time elapsed: 00:13:24
                               ETA: 01:16:39

################################################################################
                     [1m Learning iteration 298/2000 [0m                      

                       Computation: 43173 steps/s (collection: 2.142s, learning 0.135s)
             Mean action noise std: 2.06
          Mean value_function loss: 0.5385
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 55.6225
                       Mean reward: 6.30
               Mean episode length: 236.16
    Episode_Reward/reaching_object: 1.0856
     Episode_Reward/lifting_object: 0.1457
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 2.28s
                      Time elapsed: 00:13:27
                               ETA: 01:16:34

################################################################################
                     [1m Learning iteration 299/2000 [0m                      

                       Computation: 42969 steps/s (collection: 2.178s, learning 0.110s)
             Mean action noise std: 2.07
          Mean value_function loss: 0.3512
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 55.6794
                       Mean reward: 6.08
               Mean episode length: 237.02
    Episode_Reward/reaching_object: 1.1065
     Episode_Reward/lifting_object: 0.0811
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 2.29s
                      Time elapsed: 00:13:29
                               ETA: 01:16:29

################################################################################
                     [1m Learning iteration 300/2000 [0m                      

                       Computation: 42254 steps/s (collection: 2.225s, learning 0.101s)
             Mean action noise std: 2.07
          Mean value_function loss: 0.6082
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 55.7394
                       Mean reward: 5.97
               Mean episode length: 231.54
    Episode_Reward/reaching_object: 1.1286
     Episode_Reward/lifting_object: 0.0526
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 2.33s
                      Time elapsed: 00:13:31
                               ETA: 01:16:24

################################################################################
                     [1m Learning iteration 301/2000 [0m                      

                       Computation: 41693 steps/s (collection: 2.231s, learning 0.127s)
             Mean action noise std: 2.07
          Mean value_function loss: 0.7681
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.7993
                       Mean reward: 5.72
               Mean episode length: 234.15
    Episode_Reward/reaching_object: 1.0458
     Episode_Reward/lifting_object: 0.1034
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 2.36s
                      Time elapsed: 00:13:34
                               ETA: 01:16:19

################################################################################
                     [1m Learning iteration 302/2000 [0m                      

                       Computation: 42688 steps/s (collection: 2.169s, learning 0.134s)
             Mean action noise std: 2.08
          Mean value_function loss: 0.5938
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 55.8461
                       Mean reward: 5.92
               Mean episode length: 225.24
    Episode_Reward/reaching_object: 1.0730
     Episode_Reward/lifting_object: 0.1290
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 2.30s
                      Time elapsed: 00:13:36
                               ETA: 01:16:15

################################################################################
                     [1m Learning iteration 303/2000 [0m                      

                       Computation: 42488 steps/s (collection: 2.160s, learning 0.154s)
             Mean action noise std: 2.08
          Mean value_function loss: 0.6030
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 55.9050
                       Mean reward: 7.11
               Mean episode length: 225.71
    Episode_Reward/reaching_object: 1.0637
     Episode_Reward/lifting_object: 0.1484
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 2.31s
                      Time elapsed: 00:13:38
                               ETA: 01:16:10

################################################################################
                     [1m Learning iteration 304/2000 [0m                      

                       Computation: 42846 steps/s (collection: 2.154s, learning 0.140s)
             Mean action noise std: 2.09
          Mean value_function loss: 0.7983
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 55.9480
                       Mean reward: 5.85
               Mean episode length: 232.56
    Episode_Reward/reaching_object: 1.0733
     Episode_Reward/lifting_object: 0.1087
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 2.29s
                      Time elapsed: 00:13:41
                               ETA: 01:16:05

################################################################################
                     [1m Learning iteration 305/2000 [0m                      

                       Computation: 42832 steps/s (collection: 2.145s, learning 0.150s)
             Mean action noise std: 2.09
          Mean value_function loss: 0.4799
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 55.9958
                       Mean reward: 6.24
               Mean episode length: 222.22
    Episode_Reward/reaching_object: 1.0797
     Episode_Reward/lifting_object: 0.2000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 2.30s
                      Time elapsed: 00:13:43
                               ETA: 01:16:00

################################################################################
                     [1m Learning iteration 306/2000 [0m                      

                       Computation: 42911 steps/s (collection: 2.145s, learning 0.146s)
             Mean action noise std: 2.10
          Mean value_function loss: 1.0273
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 56.0576
                       Mean reward: 6.42
               Mean episode length: 229.14
    Episode_Reward/reaching_object: 1.0456
     Episode_Reward/lifting_object: 0.1634
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 2.29s
                      Time elapsed: 00:13:45
                               ETA: 01:15:55

################################################################################
                     [1m Learning iteration 307/2000 [0m                      

                       Computation: 41928 steps/s (collection: 2.229s, learning 0.116s)
             Mean action noise std: 2.10
          Mean value_function loss: 1.2580
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 56.1084
                       Mean reward: 5.99
               Mean episode length: 231.89
    Episode_Reward/reaching_object: 1.0738
     Episode_Reward/lifting_object: 0.1840
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 2.34s
                      Time elapsed: 00:13:47
                               ETA: 01:15:50

################################################################################
                     [1m Learning iteration 308/2000 [0m                      

                       Computation: 42157 steps/s (collection: 2.197s, learning 0.135s)
             Mean action noise std: 2.10
          Mean value_function loss: 0.5575
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 56.1633
                       Mean reward: 5.34
               Mean episode length: 232.98
    Episode_Reward/reaching_object: 1.0547
     Episode_Reward/lifting_object: 0.0698
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 2.33s
                      Time elapsed: 00:13:50
                               ETA: 01:15:46

################################################################################
                     [1m Learning iteration 309/2000 [0m                      

                       Computation: 39612 steps/s (collection: 2.342s, learning 0.140s)
             Mean action noise std: 2.11
          Mean value_function loss: 0.5991
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 56.2156
                       Mean reward: 7.00
               Mean episode length: 234.62
    Episode_Reward/reaching_object: 1.0868
     Episode_Reward/lifting_object: 0.1748
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 2.48s
                      Time elapsed: 00:13:52
                               ETA: 01:15:42

################################################################################
                     [1m Learning iteration 310/2000 [0m                      

                       Computation: 41823 steps/s (collection: 2.225s, learning 0.125s)
             Mean action noise std: 2.12
          Mean value_function loss: 0.3570
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 56.2843
                       Mean reward: 5.40
               Mean episode length: 238.46
    Episode_Reward/reaching_object: 1.0450
     Episode_Reward/lifting_object: 0.1971
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 2.35s
                      Time elapsed: 00:13:55
                               ETA: 01:15:37

################################################################################
                     [1m Learning iteration 311/2000 [0m                      

                       Computation: 41475 steps/s (collection: 2.228s, learning 0.143s)
             Mean action noise std: 2.12
          Mean value_function loss: 0.7217
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.3474
                       Mean reward: 6.42
               Mean episode length: 233.17
    Episode_Reward/reaching_object: 1.0553
     Episode_Reward/lifting_object: 0.1379
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 2.37s
                      Time elapsed: 00:13:57
                               ETA: 01:15:33

################################################################################
                     [1m Learning iteration 312/2000 [0m                      

                       Computation: 41282 steps/s (collection: 2.280s, learning 0.101s)
             Mean action noise std: 2.12
          Mean value_function loss: 0.7799
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 56.3785
                       Mean reward: 5.62
               Mean episode length: 232.09
    Episode_Reward/reaching_object: 1.0823
     Episode_Reward/lifting_object: 0.1227
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.38s
                      Time elapsed: 00:13:59
                               ETA: 01:15:29

################################################################################
                     [1m Learning iteration 313/2000 [0m                      

                       Computation: 40300 steps/s (collection: 2.313s, learning 0.126s)
             Mean action noise std: 2.13
          Mean value_function loss: 1.0502
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 56.4264
                       Mean reward: 6.26
               Mean episode length: 234.28
    Episode_Reward/reaching_object: 1.0991
     Episode_Reward/lifting_object: 0.2332
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 2.44s
                      Time elapsed: 00:14:02
                               ETA: 01:15:25

################################################################################
                     [1m Learning iteration 314/2000 [0m                      

                       Computation: 41791 steps/s (collection: 2.217s, learning 0.135s)
             Mean action noise std: 2.13
          Mean value_function loss: 0.5459
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 56.4596
                       Mean reward: 6.07
               Mean episode length: 227.80
    Episode_Reward/reaching_object: 1.0114
     Episode_Reward/lifting_object: 0.2029
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 2.35s
                      Time elapsed: 00:14:04
                               ETA: 01:15:20

################################################################################
                     [1m Learning iteration 315/2000 [0m                      

                       Computation: 40097 steps/s (collection: 2.335s, learning 0.116s)
             Mean action noise std: 2.13
          Mean value_function loss: 0.4721
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 56.5031
                       Mean reward: 6.15
               Mean episode length: 231.46
    Episode_Reward/reaching_object: 1.0803
     Episode_Reward/lifting_object: 0.1450
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 2.45s
                      Time elapsed: 00:14:07
                               ETA: 01:15:16

################################################################################
                     [1m Learning iteration 316/2000 [0m                      

                       Computation: 36721 steps/s (collection: 2.527s, learning 0.150s)
             Mean action noise std: 2.14
          Mean value_function loss: 0.3769
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 56.5709
                       Mean reward: 7.29
               Mean episode length: 237.77
    Episode_Reward/reaching_object: 1.1062
     Episode_Reward/lifting_object: 0.2553
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.68s
                      Time elapsed: 00:14:09
                               ETA: 01:15:14

################################################################################
                     [1m Learning iteration 317/2000 [0m                      

                       Computation: 41122 steps/s (collection: 2.254s, learning 0.136s)
             Mean action noise std: 2.14
          Mean value_function loss: 1.5920
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.6222
                       Mean reward: 5.51
               Mean episode length: 232.88
    Episode_Reward/reaching_object: 1.0466
     Episode_Reward/lifting_object: 0.1704
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 2.39s
                      Time elapsed: 00:14:12
                               ETA: 01:15:10

################################################################################
                     [1m Learning iteration 318/2000 [0m                      

                       Computation: 40641 steps/s (collection: 2.216s, learning 0.203s)
             Mean action noise std: 2.15
          Mean value_function loss: 0.8546
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 56.6573
                       Mean reward: 6.45
               Mean episode length: 234.27
    Episode_Reward/reaching_object: 1.0687
     Episode_Reward/lifting_object: 0.0853
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.42s
                      Time elapsed: 00:14:14
                               ETA: 01:15:05

################################################################################
                     [1m Learning iteration 319/2000 [0m                      

                       Computation: 41743 steps/s (collection: 2.215s, learning 0.140s)
             Mean action noise std: 2.15
          Mean value_function loss: 0.4886
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 56.7212
                       Mean reward: 5.52
               Mean episode length: 243.11
    Episode_Reward/reaching_object: 1.0530
     Episode_Reward/lifting_object: 0.1109
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 2.35s
                      Time elapsed: 00:14:16
                               ETA: 01:15:01

################################################################################
                     [1m Learning iteration 320/2000 [0m                      

                       Computation: 41289 steps/s (collection: 2.251s, learning 0.130s)
             Mean action noise std: 2.15
          Mean value_function loss: 1.1157
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 56.7638
                       Mean reward: 5.86
               Mean episode length: 235.44
    Episode_Reward/reaching_object: 1.0515
     Episode_Reward/lifting_object: 0.1910
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.38s
                      Time elapsed: 00:14:19
                               ETA: 01:14:57

################################################################################
                     [1m Learning iteration 321/2000 [0m                      

                       Computation: 41206 steps/s (collection: 2.275s, learning 0.111s)
             Mean action noise std: 2.16
          Mean value_function loss: 0.8310
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 56.7918
                       Mean reward: 5.59
               Mean episode length: 232.28
    Episode_Reward/reaching_object: 1.0257
     Episode_Reward/lifting_object: 0.1458
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 2.39s
                      Time elapsed: 00:14:21
                               ETA: 01:14:53

################################################################################
                     [1m Learning iteration 322/2000 [0m                      

                       Computation: 39685 steps/s (collection: 2.342s, learning 0.135s)
             Mean action noise std: 2.16
          Mean value_function loss: 1.2747
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 56.8431
                       Mean reward: 6.35
               Mean episode length: 225.79
    Episode_Reward/reaching_object: 1.0452
     Episode_Reward/lifting_object: 0.1901
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.48s
                      Time elapsed: 00:14:24
                               ETA: 01:14:49

################################################################################
                     [1m Learning iteration 323/2000 [0m                      

                       Computation: 41162 steps/s (collection: 2.252s, learning 0.137s)
             Mean action noise std: 2.16
          Mean value_function loss: 0.7745
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 56.8918
                       Mean reward: 5.97
               Mean episode length: 234.10
    Episode_Reward/reaching_object: 1.0036
     Episode_Reward/lifting_object: 0.2122
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 2.39s
                      Time elapsed: 00:14:26
                               ETA: 01:14:45

################################################################################
                     [1m Learning iteration 324/2000 [0m                      

                       Computation: 39085 steps/s (collection: 2.398s, learning 0.117s)
             Mean action noise std: 2.17
          Mean value_function loss: 0.8754
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 56.9388
                       Mean reward: 6.25
               Mean episode length: 228.76
    Episode_Reward/reaching_object: 1.0337
     Episode_Reward/lifting_object: 0.2266
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.7917
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.52s
                      Time elapsed: 00:14:29
                               ETA: 01:14:41

################################################################################
                     [1m Learning iteration 325/2000 [0m                      

                       Computation: 36757 steps/s (collection: 2.537s, learning 0.138s)
             Mean action noise std: 2.17
          Mean value_function loss: 0.6739
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 56.9959
                       Mean reward: 6.26
               Mean episode length: 226.00
    Episode_Reward/reaching_object: 1.0091
     Episode_Reward/lifting_object: 0.2442
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.67s
                      Time elapsed: 00:14:31
                               ETA: 01:14:39

################################################################################
                     [1m Learning iteration 326/2000 [0m                      

                       Computation: 35694 steps/s (collection: 2.500s, learning 0.254s)
             Mean action noise std: 2.18
          Mean value_function loss: 1.0611
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 57.0352
                       Mean reward: 7.03
               Mean episode length: 214.70
    Episode_Reward/reaching_object: 1.0087
     Episode_Reward/lifting_object: 0.3061
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.75s
                      Time elapsed: 00:14:34
                               ETA: 01:14:36

################################################################################
                     [1m Learning iteration 327/2000 [0m                      

                       Computation: 33732 steps/s (collection: 2.664s, learning 0.250s)
             Mean action noise std: 2.18
          Mean value_function loss: 0.9791
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 57.0824
                       Mean reward: 4.23
               Mean episode length: 218.65
    Episode_Reward/reaching_object: 1.0209
     Episode_Reward/lifting_object: 0.1851
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 2.91s
                      Time elapsed: 00:14:37
                               ETA: 01:14:35

################################################################################
                     [1m Learning iteration 328/2000 [0m                      

                       Computation: 29959 steps/s (collection: 3.063s, learning 0.218s)
             Mean action noise std: 2.19
          Mean value_function loss: 0.9869
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 57.1364
                       Mean reward: 6.65
               Mean episode length: 233.20
    Episode_Reward/reaching_object: 1.0399
     Episode_Reward/lifting_object: 0.2108
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 3.28s
                      Time elapsed: 00:14:40
                               ETA: 01:14:35

################################################################################
                     [1m Learning iteration 329/2000 [0m                      

                       Computation: 32371 steps/s (collection: 2.821s, learning 0.216s)
             Mean action noise std: 2.19
          Mean value_function loss: 0.7768
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 57.1908
                       Mean reward: 6.96
               Mean episode length: 227.25
    Episode_Reward/reaching_object: 1.0223
     Episode_Reward/lifting_object: 0.2825
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 3.04s
                      Time elapsed: 00:14:43
                               ETA: 01:14:34

################################################################################
                     [1m Learning iteration 330/2000 [0m                      

                       Computation: 35781 steps/s (collection: 2.576s, learning 0.171s)
             Mean action noise std: 2.19
          Mean value_function loss: 0.8660
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 57.2396
                       Mean reward: 5.68
               Mean episode length: 222.81
    Episode_Reward/reaching_object: 1.0065
     Episode_Reward/lifting_object: 0.2685
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.75s
                      Time elapsed: 00:14:46
                               ETA: 01:14:32

################################################################################
                     [1m Learning iteration 331/2000 [0m                      

                       Computation: 33108 steps/s (collection: 2.781s, learning 0.188s)
             Mean action noise std: 2.20
          Mean value_function loss: 0.7464
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 57.2882
                       Mean reward: 5.11
               Mean episode length: 232.34
    Episode_Reward/reaching_object: 1.0035
     Episode_Reward/lifting_object: 0.1958
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 2.97s
                      Time elapsed: 00:14:49
                               ETA: 01:14:31

################################################################################
                     [1m Learning iteration 332/2000 [0m                      

                       Computation: 36432 steps/s (collection: 2.570s, learning 0.128s)
             Mean action noise std: 2.20
          Mean value_function loss: 0.7170
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 57.3415
                       Mean reward: 5.78
               Mean episode length: 225.87
    Episode_Reward/reaching_object: 1.0145
     Episode_Reward/lifting_object: 0.3258
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 2.70s
                      Time elapsed: 00:14:52
                               ETA: 01:14:28

################################################################################
                     [1m Learning iteration 333/2000 [0m                      

                       Computation: 16546 steps/s (collection: 5.815s, learning 0.127s)
             Mean action noise std: 2.21
          Mean value_function loss: 0.8792
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 57.3847
                       Mean reward: 7.76
               Mean episode length: 225.66
    Episode_Reward/reaching_object: 1.0149
     Episode_Reward/lifting_object: 0.2885
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 5.94s
                      Time elapsed: 00:14:58
                               ETA: 01:14:42

################################################################################
                     [1m Learning iteration 334/2000 [0m                      

                       Computation: 12913 steps/s (collection: 7.428s, learning 0.185s)
             Mean action noise std: 2.21
          Mean value_function loss: 1.3314
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 57.4438
                       Mean reward: 5.75
               Mean episode length: 222.67
    Episode_Reward/reaching_object: 0.9992
     Episode_Reward/lifting_object: 0.3241
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 7.61s
                      Time elapsed: 00:15:05
                               ETA: 01:15:04

################################################################################
                     [1m Learning iteration 335/2000 [0m                      

                       Computation: 13251 steps/s (collection: 7.265s, learning 0.154s)
             Mean action noise std: 2.21
          Mean value_function loss: 0.9355
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 57.4842
                       Mean reward: 6.84
               Mean episode length: 225.91
    Episode_Reward/reaching_object: 1.0078
     Episode_Reward/lifting_object: 0.4059
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 7.42s
                      Time elapsed: 00:15:13
                               ETA: 01:15:24

################################################################################
                     [1m Learning iteration 336/2000 [0m                      

                       Computation: 13426 steps/s (collection: 7.194s, learning 0.128s)
             Mean action noise std: 2.22
          Mean value_function loss: 1.0962
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 57.5117
                       Mean reward: 5.93
               Mean episode length: 230.05
    Episode_Reward/reaching_object: 0.9726
     Episode_Reward/lifting_object: 0.3100
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 7.32s
                      Time elapsed: 00:15:20
                               ETA: 01:15:44

################################################################################
                     [1m Learning iteration 337/2000 [0m                      

                       Computation: 12621 steps/s (collection: 7.629s, learning 0.159s)
             Mean action noise std: 2.22
          Mean value_function loss: 1.6989
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.5447
                       Mean reward: 7.61
               Mean episode length: 222.65
    Episode_Reward/reaching_object: 0.9644
     Episode_Reward/lifting_object: 0.3754
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 7.79s
                      Time elapsed: 00:15:28
                               ETA: 01:16:07

################################################################################
                     [1m Learning iteration 338/2000 [0m                      

                       Computation: 11260 steps/s (collection: 8.500s, learning 0.231s)
             Mean action noise std: 2.22
          Mean value_function loss: 1.9829
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 57.5714
                       Mean reward: 5.74
               Mean episode length: 227.88
    Episode_Reward/reaching_object: 0.9838
     Episode_Reward/lifting_object: 0.3345
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.1250
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 8.73s
                      Time elapsed: 00:15:36
                               ETA: 01:16:33

################################################################################
                     [1m Learning iteration 339/2000 [0m                      

                       Computation: 10879 steps/s (collection: 8.874s, learning 0.161s)
             Mean action noise std: 2.22
          Mean value_function loss: 2.6830
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 57.6148
                       Mean reward: 6.27
               Mean episode length: 218.05
    Episode_Reward/reaching_object: 0.9593
     Episode_Reward/lifting_object: 0.3614
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 9.04s
                      Time elapsed: 00:15:46
                               ETA: 01:17:01

################################################################################
                     [1m Learning iteration 340/2000 [0m                      

                       Computation: 12724 steps/s (collection: 7.538s, learning 0.187s)
             Mean action noise std: 2.23
          Mean value_function loss: 1.4189
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 57.6411
                       Mean reward: 7.37
               Mean episode length: 221.45
    Episode_Reward/reaching_object: 0.9425
     Episode_Reward/lifting_object: 0.2933
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5417
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 7.73s
                      Time elapsed: 00:15:53
                               ETA: 01:17:22

################################################################################
                     [1m Learning iteration 341/2000 [0m                      

                       Computation: 10507 steps/s (collection: 9.096s, learning 0.259s)
             Mean action noise std: 2.23
          Mean value_function loss: 1.1677
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 57.6965
                       Mean reward: 7.10
               Mean episode length: 225.40
    Episode_Reward/reaching_object: 0.9536
     Episode_Reward/lifting_object: 0.3149
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 9.36s
                      Time elapsed: 00:16:03
                               ETA: 01:17:51

################################################################################
                     [1m Learning iteration 342/2000 [0m                      

                       Computation: 37130 steps/s (collection: 2.453s, learning 0.194s)
             Mean action noise std: 2.24
          Mean value_function loss: 1.3511
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 57.7497
                       Mean reward: 5.30
               Mean episode length: 217.44
    Episode_Reward/reaching_object: 0.9398
     Episode_Reward/lifting_object: 0.3641
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 2.65s
                      Time elapsed: 00:16:05
                               ETA: 01:17:48

################################################################################
                     [1m Learning iteration 343/2000 [0m                      

                       Computation: 34374 steps/s (collection: 2.720s, learning 0.140s)
             Mean action noise std: 2.24
          Mean value_function loss: 0.9937
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 57.7995
                       Mean reward: 7.36
               Mean episode length: 211.85
    Episode_Reward/reaching_object: 0.9158
     Episode_Reward/lifting_object: 0.4067
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 2.86s
                      Time elapsed: 00:16:08
                               ETA: 01:17:45

################################################################################
                     [1m Learning iteration 344/2000 [0m                      

                       Computation: 38419 steps/s (collection: 2.362s, learning 0.197s)
             Mean action noise std: 2.25
          Mean value_function loss: 1.6264
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 57.8489
                       Mean reward: 5.46
               Mean episode length: 208.14
    Episode_Reward/reaching_object: 0.9173
     Episode_Reward/lifting_object: 0.4098
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 11.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 2.56s
                      Time elapsed: 00:16:11
                               ETA: 01:17:41

################################################################################
                     [1m Learning iteration 345/2000 [0m                      

                       Computation: 38826 steps/s (collection: 2.387s, learning 0.145s)
             Mean action noise std: 2.25
          Mean value_function loss: 1.1137
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 57.8777
                       Mean reward: 6.04
               Mean episode length: 214.55
    Episode_Reward/reaching_object: 0.9319
     Episode_Reward/lifting_object: 0.3567
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 2.53s
                      Time elapsed: 00:16:13
                               ETA: 01:17:37

################################################################################
                     [1m Learning iteration 346/2000 [0m                      

                       Computation: 38558 steps/s (collection: 2.420s, learning 0.130s)
             Mean action noise std: 2.25
          Mean value_function loss: 1.3489
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 57.9279
                       Mean reward: 6.58
               Mean episode length: 203.53
    Episode_Reward/reaching_object: 0.9234
     Episode_Reward/lifting_object: 0.4717
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 2.55s
                      Time elapsed: 00:16:16
                               ETA: 01:17:33

################################################################################
                     [1m Learning iteration 347/2000 [0m                      

                       Computation: 40313 steps/s (collection: 2.276s, learning 0.162s)
             Mean action noise std: 2.26
          Mean value_function loss: 1.5102
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 57.9763
                       Mean reward: 6.80
               Mean episode length: 216.19
    Episode_Reward/reaching_object: 0.9778
     Episode_Reward/lifting_object: 0.4455
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 2.44s
                      Time elapsed: 00:16:18
                               ETA: 01:17:28

################################################################################
                     [1m Learning iteration 348/2000 [0m                      

                       Computation: 37049 steps/s (collection: 2.426s, learning 0.227s)
             Mean action noise std: 2.26
          Mean value_function loss: 2.2390
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.0174
                       Mean reward: 6.18
               Mean episode length: 215.49
    Episode_Reward/reaching_object: 0.9542
     Episode_Reward/lifting_object: 0.5333
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 2.65s
                      Time elapsed: 00:16:21
                               ETA: 01:17:25

################################################################################
                     [1m Learning iteration 349/2000 [0m                      

                       Computation: 38361 steps/s (collection: 2.398s, learning 0.164s)
             Mean action noise std: 2.26
          Mean value_function loss: 1.2593
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 58.0412
                       Mean reward: 6.97
               Mean episode length: 217.78
    Episode_Reward/reaching_object: 0.9119
     Episode_Reward/lifting_object: 0.5668
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 2.56s
                      Time elapsed: 00:16:23
                               ETA: 01:17:21

################################################################################
                     [1m Learning iteration 350/2000 [0m                      

                       Computation: 38641 steps/s (collection: 2.421s, learning 0.123s)
             Mean action noise std: 2.27
          Mean value_function loss: 2.3879
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.0763
                       Mean reward: 7.26
               Mean episode length: 215.35
    Episode_Reward/reaching_object: 0.9262
     Episode_Reward/lifting_object: 0.4740
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 2.54s
                      Time elapsed: 00:16:26
                               ETA: 01:17:17

################################################################################
                     [1m Learning iteration 351/2000 [0m                      

                       Computation: 38324 steps/s (collection: 2.400s, learning 0.165s)
             Mean action noise std: 2.27
          Mean value_function loss: 1.9607
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 58.1104
                       Mean reward: 5.81
               Mean episode length: 223.17
    Episode_Reward/reaching_object: 0.9075
     Episode_Reward/lifting_object: 0.1613
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 2.57s
                      Time elapsed: 00:16:28
                               ETA: 01:17:13

################################################################################
                     [1m Learning iteration 352/2000 [0m                      

                       Computation: 40768 steps/s (collection: 2.258s, learning 0.153s)
             Mean action noise std: 2.27
          Mean value_function loss: 1.5143
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 58.1548
                       Mean reward: 7.05
               Mean episode length: 220.16
    Episode_Reward/reaching_object: 0.9210
     Episode_Reward/lifting_object: 0.6199
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 2.41s
                      Time elapsed: 00:16:31
                               ETA: 01:17:08

################################################################################
                     [1m Learning iteration 353/2000 [0m                      

                       Computation: 38736 steps/s (collection: 2.394s, learning 0.144s)
             Mean action noise std: 2.27
          Mean value_function loss: 2.0393
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 58.1792
                       Mean reward: 5.93
               Mean episode length: 213.45
    Episode_Reward/reaching_object: 0.9201
     Episode_Reward/lifting_object: 0.6202
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 2.54s
                      Time elapsed: 00:16:33
                               ETA: 01:17:04

################################################################################
                     [1m Learning iteration 354/2000 [0m                      

                       Computation: 37195 steps/s (collection: 2.401s, learning 0.242s)
             Mean action noise std: 2.28
          Mean value_function loss: 1.5828
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 58.2221
                       Mean reward: 7.80
               Mean episode length: 215.42
    Episode_Reward/reaching_object: 0.9232
     Episode_Reward/lifting_object: 0.6938
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 2.64s
                      Time elapsed: 00:16:36
                               ETA: 01:17:00

################################################################################
                     [1m Learning iteration 355/2000 [0m                      

                       Computation: 39965 steps/s (collection: 2.286s, learning 0.174s)
             Mean action noise std: 2.28
          Mean value_function loss: 1.9029
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 58.2633
                       Mean reward: 7.24
               Mean episode length: 206.43
    Episode_Reward/reaching_object: 0.9257
     Episode_Reward/lifting_object: 0.6221
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 2.46s
                      Time elapsed: 00:16:39
                               ETA: 01:16:56

################################################################################
                     [1m Learning iteration 356/2000 [0m                      

                       Computation: 34894 steps/s (collection: 2.635s, learning 0.183s)
             Mean action noise std: 2.29
          Mean value_function loss: 2.0820
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 58.2945
                       Mean reward: 7.61
               Mean episode length: 206.51
    Episode_Reward/reaching_object: 0.9204
     Episode_Reward/lifting_object: 0.4479
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 2.82s
                      Time elapsed: 00:16:41
                               ETA: 01:16:53

################################################################################
                     [1m Learning iteration 357/2000 [0m                      

                       Computation: 35652 steps/s (collection: 2.447s, learning 0.310s)
             Mean action noise std: 2.29
          Mean value_function loss: 2.0513
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.3309
                       Mean reward: 8.91
               Mean episode length: 217.27
    Episode_Reward/reaching_object: 0.9214
     Episode_Reward/lifting_object: 0.6723
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0375
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 2.76s
                      Time elapsed: 00:16:44
                               ETA: 01:16:50

################################################################################
                     [1m Learning iteration 358/2000 [0m                      

                       Computation: 34910 steps/s (collection: 2.692s, learning 0.124s)
             Mean action noise std: 2.29
          Mean value_function loss: 1.5404
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 58.3553
                       Mean reward: 7.37
               Mean episode length: 215.52
    Episode_Reward/reaching_object: 0.9194
     Episode_Reward/lifting_object: 0.5096
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 2.82s
                      Time elapsed: 00:16:47
                               ETA: 01:16:47

################################################################################
                     [1m Learning iteration 359/2000 [0m                      

                       Computation: 37450 steps/s (collection: 2.432s, learning 0.193s)
             Mean action noise std: 2.30
          Mean value_function loss: 1.2756
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 58.3961
                       Mean reward: 6.89
               Mean episode length: 215.54
    Episode_Reward/reaching_object: 0.9187
     Episode_Reward/lifting_object: 0.6173
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 2.62s
                      Time elapsed: 00:16:50
                               ETA: 01:16:44

################################################################################
                     [1m Learning iteration 360/2000 [0m                      

                       Computation: 37059 steps/s (collection: 2.509s, learning 0.144s)
             Mean action noise std: 2.30
          Mean value_function loss: 1.5877
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 58.4466
                       Mean reward: 7.35
               Mean episode length: 211.67
    Episode_Reward/reaching_object: 0.9208
     Episode_Reward/lifting_object: 0.5828
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 2.65s
                      Time elapsed: 00:16:52
                               ETA: 01:16:40

################################################################################
                     [1m Learning iteration 361/2000 [0m                      

                       Computation: 34230 steps/s (collection: 2.599s, learning 0.273s)
             Mean action noise std: 2.30
          Mean value_function loss: 1.8437
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 58.4819
                       Mean reward: 7.35
               Mean episode length: 218.83
    Episode_Reward/reaching_object: 0.9034
     Episode_Reward/lifting_object: 0.5291
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 2.87s
                      Time elapsed: 00:16:55
                               ETA: 01:16:38

################################################################################
                     [1m Learning iteration 362/2000 [0m                      

                       Computation: 34575 steps/s (collection: 2.663s, learning 0.180s)
             Mean action noise std: 2.31
          Mean value_function loss: 1.8811
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 58.5230
                       Mean reward: 7.47
               Mean episode length: 211.19
    Episode_Reward/reaching_object: 0.9010
     Episode_Reward/lifting_object: 0.7236
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 2.84s
                      Time elapsed: 00:16:58
                               ETA: 01:16:35

################################################################################
                     [1m Learning iteration 363/2000 [0m                      

                       Computation: 33500 steps/s (collection: 2.787s, learning 0.148s)
             Mean action noise std: 2.31
          Mean value_function loss: 1.5173
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 58.5680
                       Mean reward: 8.26
               Mean episode length: 223.43
    Episode_Reward/reaching_object: 0.9122
     Episode_Reward/lifting_object: 0.8015
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 2.93s
                      Time elapsed: 00:17:01
                               ETA: 01:16:33

################################################################################
                     [1m Learning iteration 364/2000 [0m                      

                       Computation: 38609 steps/s (collection: 2.409s, learning 0.137s)
             Mean action noise std: 2.31
          Mean value_function loss: 1.3968
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 58.6142
                       Mean reward: 8.02
               Mean episode length: 215.16
    Episode_Reward/reaching_object: 0.8977
     Episode_Reward/lifting_object: 0.7716
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 2.55s
                      Time elapsed: 00:17:03
                               ETA: 01:16:29

################################################################################
                     [1m Learning iteration 365/2000 [0m                      

                       Computation: 36520 steps/s (collection: 2.518s, learning 0.174s)
             Mean action noise std: 2.32
          Mean value_function loss: 1.8256
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 58.6618
                       Mean reward: 9.81
               Mean episode length: 224.46
    Episode_Reward/reaching_object: 0.8987
     Episode_Reward/lifting_object: 0.8423
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 2.69s
                      Time elapsed: 00:17:06
                               ETA: 01:16:26

################################################################################
                     [1m Learning iteration 366/2000 [0m                      

                       Computation: 36148 steps/s (collection: 2.464s, learning 0.256s)
             Mean action noise std: 2.32
          Mean value_function loss: 1.5425
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 58.6940
                       Mean reward: 9.49
               Mean episode length: 217.32
    Episode_Reward/reaching_object: 0.9080
     Episode_Reward/lifting_object: 0.7753
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 2.72s
                      Time elapsed: 00:17:09
                               ETA: 01:16:22

################################################################################
                     [1m Learning iteration 367/2000 [0m                      

                       Computation: 33268 steps/s (collection: 2.721s, learning 0.234s)
             Mean action noise std: 2.32
          Mean value_function loss: 1.9034
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 58.7244
                       Mean reward: 9.10
               Mean episode length: 214.93
    Episode_Reward/reaching_object: 0.9069
     Episode_Reward/lifting_object: 0.7778
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 2.95s
                      Time elapsed: 00:17:12
                               ETA: 01:16:20

################################################################################
                     [1m Learning iteration 368/2000 [0m                      

                       Computation: 37143 steps/s (collection: 2.464s, learning 0.183s)
             Mean action noise std: 2.33
          Mean value_function loss: 1.5312
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 58.7698
                       Mean reward: 6.86
               Mean episode length: 223.65
    Episode_Reward/reaching_object: 0.8944
     Episode_Reward/lifting_object: 0.6202
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 2.65s
                      Time elapsed: 00:17:14
                               ETA: 01:16:17

################################################################################
                     [1m Learning iteration 369/2000 [0m                      

                       Computation: 37521 steps/s (collection: 2.479s, learning 0.141s)
             Mean action noise std: 2.33
          Mean value_function loss: 1.8840
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 58.8039
                       Mean reward: 8.89
               Mean episode length: 212.31
    Episode_Reward/reaching_object: 0.8912
     Episode_Reward/lifting_object: 0.8371
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 2.62s
                      Time elapsed: 00:17:17
                               ETA: 01:16:13

################################################################################
                     [1m Learning iteration 370/2000 [0m                      

                       Computation: 41095 steps/s (collection: 2.248s, learning 0.144s)
             Mean action noise std: 2.33
          Mean value_function loss: 1.9784
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 58.8369
                       Mean reward: 8.29
               Mean episode length: 214.70
    Episode_Reward/reaching_object: 0.8920
     Episode_Reward/lifting_object: 0.9543
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 2.39s
                      Time elapsed: 00:17:19
                               ETA: 01:16:08

################################################################################
                     [1m Learning iteration 371/2000 [0m                      

                       Computation: 36191 steps/s (collection: 2.598s, learning 0.119s)
             Mean action noise std: 2.34
          Mean value_function loss: 2.1567
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 58.8833
                       Mean reward: 10.50
               Mean episode length: 212.78
    Episode_Reward/reaching_object: 0.8995
     Episode_Reward/lifting_object: 0.8139
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 2.72s
                      Time elapsed: 00:17:22
                               ETA: 01:16:05

################################################################################
                     [1m Learning iteration 372/2000 [0m                      

                       Computation: 38077 steps/s (collection: 2.303s, learning 0.279s)
             Mean action noise std: 2.34
          Mean value_function loss: 2.5073
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 58.9361
                       Mean reward: 7.99
               Mean episode length: 200.93
    Episode_Reward/reaching_object: 0.8636
     Episode_Reward/lifting_object: 0.9936
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 2.58s
                      Time elapsed: 00:17:25
                               ETA: 01:16:02

################################################################################
                     [1m Learning iteration 373/2000 [0m                      

                       Computation: 39168 steps/s (collection: 2.349s, learning 0.161s)
             Mean action noise std: 2.35
          Mean value_function loss: 3.0846
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 58.9886
                       Mean reward: 9.09
               Mean episode length: 203.65
    Episode_Reward/reaching_object: 0.8741
     Episode_Reward/lifting_object: 0.9324
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.3750
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 2.51s
                      Time elapsed: 00:17:27
                               ETA: 01:15:57

################################################################################
                     [1m Learning iteration 374/2000 [0m                      

                       Computation: 38852 steps/s (collection: 2.367s, learning 0.163s)
             Mean action noise std: 2.35
          Mean value_function loss: 2.2835
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 59.0317
                       Mean reward: 6.32
               Mean episode length: 203.80
    Episode_Reward/reaching_object: 0.8656
     Episode_Reward/lifting_object: 0.8493
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 2.53s
                      Time elapsed: 00:17:30
                               ETA: 01:15:53

################################################################################
                     [1m Learning iteration 375/2000 [0m                      

                       Computation: 37277 steps/s (collection: 2.477s, learning 0.160s)
             Mean action noise std: 2.35
          Mean value_function loss: 2.5445
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 59.0566
                       Mean reward: 8.97
               Mean episode length: 207.30
    Episode_Reward/reaching_object: 0.8694
     Episode_Reward/lifting_object: 0.8037
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 2.64s
                      Time elapsed: 00:17:32
                               ETA: 01:15:50

################################################################################
                     [1m Learning iteration 376/2000 [0m                      

                       Computation: 37859 steps/s (collection: 2.412s, learning 0.185s)
             Mean action noise std: 2.35
          Mean value_function loss: 5.4111
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 59.0754
                       Mean reward: 9.19
               Mean episode length: 206.67
    Episode_Reward/reaching_object: 0.8638
     Episode_Reward/lifting_object: 0.8445
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 2.60s
                      Time elapsed: 00:17:35
                               ETA: 01:15:46

################################################################################
                     [1m Learning iteration 377/2000 [0m                      

                       Computation: 35116 steps/s (collection: 2.578s, learning 0.222s)
             Mean action noise std: 2.36
          Mean value_function loss: 3.8243
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 59.0813
                       Mean reward: 9.13
               Mean episode length: 198.29
    Episode_Reward/reaching_object: 0.8528
     Episode_Reward/lifting_object: 0.9875
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 2.80s
                      Time elapsed: 00:17:38
                               ETA: 01:15:43

################################################################################
                     [1m Learning iteration 378/2000 [0m                      

                       Computation: 37641 steps/s (collection: 2.405s, learning 0.207s)
             Mean action noise std: 2.36
          Mean value_function loss: 3.3321
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 59.1023
                       Mean reward: 6.72
               Mean episode length: 198.28
    Episode_Reward/reaching_object: 0.8273
     Episode_Reward/lifting_object: 0.7825
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 2.61s
                      Time elapsed: 00:17:40
                               ETA: 01:15:40

################################################################################
                     [1m Learning iteration 379/2000 [0m                      

                       Computation: 35761 steps/s (collection: 2.574s, learning 0.175s)
             Mean action noise std: 2.36
          Mean value_function loss: 3.1749
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 59.1405
                       Mean reward: 9.41
               Mean episode length: 189.01
    Episode_Reward/reaching_object: 0.8395
     Episode_Reward/lifting_object: 0.8472
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 2.75s
                      Time elapsed: 00:17:43
                               ETA: 01:15:37

################################################################################
                     [1m Learning iteration 380/2000 [0m                      

                       Computation: 39729 steps/s (collection: 2.351s, learning 0.124s)
             Mean action noise std: 2.36
          Mean value_function loss: 2.8588
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 59.1720
                       Mean reward: 10.95
               Mean episode length: 199.78
    Episode_Reward/reaching_object: 0.8139
     Episode_Reward/lifting_object: 0.9265
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 2.47s
                      Time elapsed: 00:17:46
                               ETA: 01:15:33

################################################################################
                     [1m Learning iteration 381/2000 [0m                      

                       Computation: 36355 steps/s (collection: 2.551s, learning 0.153s)
             Mean action noise std: 2.37
          Mean value_function loss: 3.5309
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 59.1959
                       Mean reward: 10.93
               Mean episode length: 190.04
    Episode_Reward/reaching_object: 0.8071
     Episode_Reward/lifting_object: 1.2287
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 12.3333
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 2.70s
                      Time elapsed: 00:17:48
                               ETA: 01:15:29

################################################################################
                     [1m Learning iteration 382/2000 [0m                      

                       Computation: 39894 steps/s (collection: 2.223s, learning 0.241s)
             Mean action noise std: 2.37
          Mean value_function loss: 4.5789
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.2207
                       Mean reward: 9.07
               Mean episode length: 192.95
    Episode_Reward/reaching_object: 0.8129
     Episode_Reward/lifting_object: 1.0849
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 2.46s
                      Time elapsed: 00:17:51
                               ETA: 01:15:25

################################################################################
                     [1m Learning iteration 383/2000 [0m                      

                       Computation: 40969 steps/s (collection: 2.203s, learning 0.197s)
             Mean action noise std: 2.37
          Mean value_function loss: 2.9993
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 59.2401
                       Mean reward: 11.62
               Mean episode length: 178.03
    Episode_Reward/reaching_object: 0.7998
     Episode_Reward/lifting_object: 1.1757
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 12.8750
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 2.40s
                      Time elapsed: 00:17:53
                               ETA: 01:15:21

################################################################################
                     [1m Learning iteration 384/2000 [0m                      

                       Computation: 36112 steps/s (collection: 2.550s, learning 0.173s)
             Mean action noise std: 2.37
          Mean value_function loss: 3.9338
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 59.2678
                       Mean reward: 8.70
               Mean episode length: 189.44
    Episode_Reward/reaching_object: 0.7966
     Episode_Reward/lifting_object: 1.1120
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 2.72s
                      Time elapsed: 00:17:56
                               ETA: 01:15:18

################################################################################
                     [1m Learning iteration 385/2000 [0m                      

                       Computation: 35778 steps/s (collection: 2.591s, learning 0.157s)
             Mean action noise std: 2.38
          Mean value_function loss: 3.1259
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 59.3072
                       Mean reward: 10.25
               Mean episode length: 182.64
    Episode_Reward/reaching_object: 0.7588
     Episode_Reward/lifting_object: 0.9979
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 8.8750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 14.7083
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 2.75s
                      Time elapsed: 00:17:59
                               ETA: 01:15:15

################################################################################
                     [1m Learning iteration 386/2000 [0m                      

                       Computation: 36452 steps/s (collection: 2.567s, learning 0.130s)
             Mean action noise std: 2.38
          Mean value_function loss: 3.6784
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 59.3430
                       Mean reward: 8.80
               Mean episode length: 168.74
    Episode_Reward/reaching_object: 0.7839
     Episode_Reward/lifting_object: 1.1363
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 13.8333
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 2.70s
                      Time elapsed: 00:18:01
                               ETA: 01:15:11

################################################################################
                     [1m Learning iteration 387/2000 [0m                      

                       Computation: 35941 steps/s (collection: 2.523s, learning 0.212s)
             Mean action noise std: 2.38
          Mean value_function loss: 3.2606
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 59.3689
                       Mean reward: 11.11
               Mean episode length: 192.83
    Episode_Reward/reaching_object: 0.8150
     Episode_Reward/lifting_object: 1.2546
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 2.74s
                      Time elapsed: 00:18:04
                               ETA: 01:15:08

################################################################################
                     [1m Learning iteration 388/2000 [0m                      

                       Computation: 37625 steps/s (collection: 2.466s, learning 0.147s)
             Mean action noise std: 2.38
          Mean value_function loss: 2.8762
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 59.3842
                       Mean reward: 9.50
               Mean episode length: 187.96
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 1.1247
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 8.1250
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 12.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 2.61s
                      Time elapsed: 00:18:07
                               ETA: 01:15:05

################################################################################
                     [1m Learning iteration 389/2000 [0m                      

                       Computation: 33152 steps/s (collection: 2.792s, learning 0.174s)
             Mean action noise std: 2.39
          Mean value_function loss: 3.4935
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 59.4077
                       Mean reward: 10.91
               Mean episode length: 205.02
    Episode_Reward/reaching_object: 0.7842
     Episode_Reward/lifting_object: 1.1613
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 2.97s
                      Time elapsed: 00:18:10
                               ETA: 01:15:03

################################################################################
                     [1m Learning iteration 390/2000 [0m                      

                       Computation: 35344 steps/s (collection: 2.633s, learning 0.149s)
             Mean action noise std: 2.39
          Mean value_function loss: 4.1618
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 59.4453
                       Mean reward: 7.92
               Mean episode length: 182.85
    Episode_Reward/reaching_object: 0.8333
     Episode_Reward/lifting_object: 1.1621
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 8.7917
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 2.78s
                      Time elapsed: 00:18:12
                               ETA: 01:15:00

################################################################################
                     [1m Learning iteration 391/2000 [0m                      

                       Computation: 36820 steps/s (collection: 2.465s, learning 0.205s)
             Mean action noise std: 2.39
          Mean value_function loss: 4.3625
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 59.4884
                       Mean reward: 8.71
               Mean episode length: 172.16
    Episode_Reward/reaching_object: 0.7907
     Episode_Reward/lifting_object: 1.2477
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 12.1250
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 2.67s
                      Time elapsed: 00:18:15
                               ETA: 01:14:57

################################################################################
                     [1m Learning iteration 392/2000 [0m                      

                       Computation: 29220 steps/s (collection: 3.037s, learning 0.327s)
             Mean action noise std: 2.40
          Mean value_function loss: 4.9211
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 59.5315
                       Mean reward: 11.13
               Mean episode length: 197.49
    Episode_Reward/reaching_object: 0.8574
     Episode_Reward/lifting_object: 1.4656
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 9.5417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 3.36s
                      Time elapsed: 00:18:19
                               ETA: 01:14:56

################################################################################
                     [1m Learning iteration 393/2000 [0m                      

                       Computation: 30539 steps/s (collection: 3.067s, learning 0.152s)
             Mean action noise std: 2.40
          Mean value_function loss: 5.1038
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 59.5557
                       Mean reward: 9.06
               Mean episode length: 186.63
    Episode_Reward/reaching_object: 0.8154
     Episode_Reward/lifting_object: 1.3784
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 11.4167
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 3.22s
                      Time elapsed: 00:18:22
                               ETA: 01:14:55

################################################################################
                     [1m Learning iteration 394/2000 [0m                      

                       Computation: 33796 steps/s (collection: 2.636s, learning 0.273s)
             Mean action noise std: 2.40
          Mean value_function loss: 3.4744
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 59.5822
                       Mean reward: 11.42
               Mean episode length: 194.69
    Episode_Reward/reaching_object: 0.8384
     Episode_Reward/lifting_object: 1.4227
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 8.6250
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 2.91s
                      Time elapsed: 00:18:25
                               ETA: 01:14:53

################################################################################
                     [1m Learning iteration 395/2000 [0m                      

                       Computation: 31869 steps/s (collection: 2.850s, learning 0.234s)
             Mean action noise std: 2.41
          Mean value_function loss: 4.5220
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 59.6198
                       Mean reward: 11.91
               Mean episode length: 204.63
    Episode_Reward/reaching_object: 0.8710
     Episode_Reward/lifting_object: 1.4574
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 10.1250
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 10.7917
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 3.08s
                      Time elapsed: 00:18:28
                               ETA: 01:14:51

################################################################################
                     [1m Learning iteration 396/2000 [0m                      

                       Computation: 38364 steps/s (collection: 2.450s, learning 0.112s)
             Mean action noise std: 2.41
          Mean value_function loss: 3.6244
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 59.6563
                       Mean reward: 11.20
               Mean episode length: 188.04
    Episode_Reward/reaching_object: 0.8444
     Episode_Reward/lifting_object: 1.4631
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 10.8750
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 2.56s
                      Time elapsed: 00:18:30
                               ETA: 01:14:47

################################################################################
                     [1m Learning iteration 397/2000 [0m                      

                       Computation: 40487 steps/s (collection: 2.276s, learning 0.152s)
             Mean action noise std: 2.41
          Mean value_function loss: 3.5394
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 59.6935
                       Mean reward: 11.37
               Mean episode length: 187.75
    Episode_Reward/reaching_object: 0.8489
     Episode_Reward/lifting_object: 1.4984
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 2.43s
                      Time elapsed: 00:18:33
                               ETA: 01:14:43

################################################################################
                     [1m Learning iteration 398/2000 [0m                      

                       Computation: 41870 steps/s (collection: 2.209s, learning 0.139s)
             Mean action noise std: 2.42
          Mean value_function loss: 4.1812
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 59.7234
                       Mean reward: 12.94
               Mean episode length: 196.40
    Episode_Reward/reaching_object: 0.8105
     Episode_Reward/lifting_object: 1.5283
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 12.2083
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 2.35s
                      Time elapsed: 00:18:35
                               ETA: 01:14:38

################################################################################
                     [1m Learning iteration 399/2000 [0m                      

                       Computation: 39709 steps/s (collection: 2.350s, learning 0.126s)
             Mean action noise std: 2.42
          Mean value_function loss: 4.8308
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 59.7465
                       Mean reward: 11.93
               Mean episode length: 183.05
    Episode_Reward/reaching_object: 0.8056
     Episode_Reward/lifting_object: 1.4013
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 9.6667
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 14.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 2.48s
                      Time elapsed: 00:18:38
                               ETA: 01:14:34

################################################################################
                     [1m Learning iteration 400/2000 [0m                      

                       Computation: 41230 steps/s (collection: 2.247s, learning 0.137s)
             Mean action noise std: 2.42
          Mean value_function loss: 4.2697
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 59.7822
                       Mean reward: 10.50
               Mean episode length: 190.86
    Episode_Reward/reaching_object: 0.8116
     Episode_Reward/lifting_object: 1.6042
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 2.38s
                      Time elapsed: 00:18:40
                               ETA: 01:14:30

################################################################################
                     [1m Learning iteration 401/2000 [0m                      

                       Computation: 41667 steps/s (collection: 2.191s, learning 0.169s)
             Mean action noise std: 2.42
          Mean value_function loss: 5.8439
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 59.8106
                       Mean reward: 12.20
               Mean episode length: 185.67
    Episode_Reward/reaching_object: 0.7729
     Episode_Reward/lifting_object: 1.5586
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 15.0417
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 2.36s
                      Time elapsed: 00:18:42
                               ETA: 01:14:25

################################################################################
                     [1m Learning iteration 402/2000 [0m                      

                       Computation: 37057 steps/s (collection: 2.512s, learning 0.141s)
             Mean action noise std: 2.43
          Mean value_function loss: 6.0755
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 59.8360
                       Mean reward: 10.23
               Mean episode length: 168.89
    Episode_Reward/reaching_object: 0.7757
     Episode_Reward/lifting_object: 1.3619
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 7.9167
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 14.7083
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 2.65s
                      Time elapsed: 00:18:45
                               ETA: 01:14:22

################################################################################
                     [1m Learning iteration 403/2000 [0m                      

                       Computation: 39509 steps/s (collection: 2.379s, learning 0.110s)
             Mean action noise std: 2.43
          Mean value_function loss: 4.3399
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 59.8642
                       Mean reward: 8.34
               Mean episode length: 178.04
    Episode_Reward/reaching_object: 0.7391
     Episode_Reward/lifting_object: 1.3907
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 14.7500
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 2.49s
                      Time elapsed: 00:18:47
                               ETA: 01:14:18

################################################################################
                     [1m Learning iteration 404/2000 [0m                      

                       Computation: 38588 steps/s (collection: 2.400s, learning 0.147s)
             Mean action noise std: 2.43
          Mean value_function loss: 4.9503
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 59.8960
                       Mean reward: 13.88
               Mean episode length: 188.39
    Episode_Reward/reaching_object: 0.7945
     Episode_Reward/lifting_object: 1.5750
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 14.0417
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 2.55s
                      Time elapsed: 00:18:50
                               ETA: 01:14:14

################################################################################
                     [1m Learning iteration 405/2000 [0m                      

                       Computation: 42700 steps/s (collection: 2.153s, learning 0.149s)
             Mean action noise std: 2.44
          Mean value_function loss: 5.9822
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.9267
                       Mean reward: 9.68
               Mean episode length: 184.24
    Episode_Reward/reaching_object: 0.8056
     Episode_Reward/lifting_object: 1.5620
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 13.0417
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 2.30s
                      Time elapsed: 00:18:52
                               ETA: 01:14:10

################################################################################
                     [1m Learning iteration 406/2000 [0m                      

                       Computation: 39506 steps/s (collection: 2.353s, learning 0.135s)
             Mean action noise std: 2.44
          Mean value_function loss: 5.3675
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 59.9552
                       Mean reward: 10.49
               Mean episode length: 177.23
    Episode_Reward/reaching_object: 0.8037
     Episode_Reward/lifting_object: 1.5570
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 2.49s
                      Time elapsed: 00:18:55
                               ETA: 01:14:06

################################################################################
                     [1m Learning iteration 407/2000 [0m                      

                       Computation: 40237 steps/s (collection: 2.293s, learning 0.150s)
             Mean action noise std: 2.44
          Mean value_function loss: 5.0243
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 59.9712
                       Mean reward: 11.95
               Mean episode length: 179.25
    Episode_Reward/reaching_object: 0.7806
     Episode_Reward/lifting_object: 1.7559
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 2.44s
                      Time elapsed: 00:18:57
                               ETA: 01:14:02

################################################################################
                     [1m Learning iteration 408/2000 [0m                      

                       Computation: 42588 steps/s (collection: 2.186s, learning 0.122s)
             Mean action noise std: 2.44
          Mean value_function loss: 6.0660
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 59.9836
                       Mean reward: 12.21
               Mean episode length: 180.35
    Episode_Reward/reaching_object: 0.8018
     Episode_Reward/lifting_object: 1.7740
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 8.1667
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 2.31s
                      Time elapsed: 00:18:59
                               ETA: 01:13:57

################################################################################
                     [1m Learning iteration 409/2000 [0m                      

                       Computation: 40144 steps/s (collection: 2.260s, learning 0.188s)
             Mean action noise std: 2.44
          Mean value_function loss: 8.5997
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 60.0097
                       Mean reward: 13.32
               Mean episode length: 167.23
    Episode_Reward/reaching_object: 0.7723
     Episode_Reward/lifting_object: 1.9166
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 16.2917
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 2.45s
                      Time elapsed: 00:19:02
                               ETA: 01:13:53

################################################################################
                     [1m Learning iteration 410/2000 [0m                      

                       Computation: 40367 steps/s (collection: 2.303s, learning 0.133s)
             Mean action noise std: 2.45
          Mean value_function loss: 6.5453
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 60.0399
                       Mean reward: 16.07
               Mean episode length: 179.37
    Episode_Reward/reaching_object: 0.7995
     Episode_Reward/lifting_object: 2.1096
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 15.8333
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 2.44s
                      Time elapsed: 00:19:04
                               ETA: 01:13:49

################################################################################
                     [1m Learning iteration 411/2000 [0m                      

                       Computation: 40361 steps/s (collection: 2.291s, learning 0.145s)
             Mean action noise std: 2.45
          Mean value_function loss: 5.7749
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 60.0711
                       Mean reward: 14.84
               Mean episode length: 184.63
    Episode_Reward/reaching_object: 0.7921
     Episode_Reward/lifting_object: 1.9800
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 15.0417
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 2.44s
                      Time elapsed: 00:19:07
                               ETA: 01:13:44

################################################################################
                     [1m Learning iteration 412/2000 [0m                      

                       Computation: 42799 steps/s (collection: 2.149s, learning 0.147s)
             Mean action noise std: 2.45
          Mean value_function loss: 5.4947
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 60.1008
                       Mean reward: 10.63
               Mean episode length: 181.05
    Episode_Reward/reaching_object: 0.7867
     Episode_Reward/lifting_object: 2.2964
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 7.7083
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 16.7917
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 2.30s
                      Time elapsed: 00:19:09
                               ETA: 01:13:40

################################################################################
                     [1m Learning iteration 413/2000 [0m                      

                       Computation: 43610 steps/s (collection: 2.140s, learning 0.115s)
             Mean action noise std: 2.45
          Mean value_function loss: 8.8895
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 60.1117
                       Mean reward: 15.89
               Mean episode length: 164.39
    Episode_Reward/reaching_object: 0.7653
     Episode_Reward/lifting_object: 2.2412
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 17.3333
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 2.25s
                      Time elapsed: 00:19:11
                               ETA: 01:13:35

################################################################################
                     [1m Learning iteration 414/2000 [0m                      

                       Computation: 38657 steps/s (collection: 2.390s, learning 0.153s)
             Mean action noise std: 2.45
          Mean value_function loss: 5.9732
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 60.1188
                       Mean reward: 12.40
               Mean episode length: 176.28
    Episode_Reward/reaching_object: 0.7429
     Episode_Reward/lifting_object: 2.1043
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 18.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 2.54s
                      Time elapsed: 00:19:14
                               ETA: 01:13:31

################################################################################
                     [1m Learning iteration 415/2000 [0m                      

                       Computation: 41937 steps/s (collection: 2.202s, learning 0.142s)
             Mean action noise std: 2.46
          Mean value_function loss: 9.5733
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 60.1315
                       Mean reward: 15.09
               Mean episode length: 162.02
    Episode_Reward/reaching_object: 0.7381
     Episode_Reward/lifting_object: 1.9838
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0386
      Episode_Termination/time_out: 5.6250
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 17.1667
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 2.34s
                      Time elapsed: 00:19:16
                               ETA: 01:13:27

################################################################################
                     [1m Learning iteration 416/2000 [0m                      

                       Computation: 43888 steps/s (collection: 2.124s, learning 0.116s)
             Mean action noise std: 2.46
          Mean value_function loss: 6.3753
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 60.1561
                       Mean reward: 15.04
               Mean episode length: 162.40
    Episode_Reward/reaching_object: 0.7104
     Episode_Reward/lifting_object: 2.0490
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 17.8333
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 2.24s
                      Time elapsed: 00:19:18
                               ETA: 01:13:22

################################################################################
                     [1m Learning iteration 417/2000 [0m                      

                       Computation: 41064 steps/s (collection: 2.253s, learning 0.141s)
             Mean action noise std: 2.46
          Mean value_function loss: 5.8234
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 60.1873
                       Mean reward: 12.87
               Mean episode length: 164.15
    Episode_Reward/reaching_object: 0.7336
     Episode_Reward/lifting_object: 2.2453
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 5.7083
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 17.3750
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 2.39s
                      Time elapsed: 00:19:21
                               ETA: 01:13:18

################################################################################
                     [1m Learning iteration 418/2000 [0m                      

                       Computation: 42653 steps/s (collection: 2.155s, learning 0.150s)
             Mean action noise std: 2.46
          Mean value_function loss: 6.9821
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 60.2183
                       Mean reward: 14.49
               Mean episode length: 167.64
    Episode_Reward/reaching_object: 0.7236
     Episode_Reward/lifting_object: 1.9442
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 6.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 16.7083
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 2.30s
                      Time elapsed: 00:19:23
                               ETA: 01:13:13

################################################################################
                     [1m Learning iteration 419/2000 [0m                      

                       Computation: 35749 steps/s (collection: 2.581s, learning 0.169s)
             Mean action noise std: 2.47
          Mean value_function loss: 6.4581
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 60.2420
                       Mean reward: 13.56
               Mean episode length: 154.99
    Episode_Reward/reaching_object: 0.7015
     Episode_Reward/lifting_object: 2.0246
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 19.5000
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 2.75s
                      Time elapsed: 00:19:26
                               ETA: 01:13:10

################################################################################
                     [1m Learning iteration 420/2000 [0m                      

                       Computation: 36327 steps/s (collection: 2.557s, learning 0.149s)
             Mean action noise std: 2.47
          Mean value_function loss: 7.4021
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 60.2587
                       Mean reward: 10.99
               Mean episode length: 164.99
    Episode_Reward/reaching_object: 0.7459
     Episode_Reward/lifting_object: 1.9068
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 19.6667
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 2.71s
                      Time elapsed: 00:19:29
                               ETA: 01:13:07

################################################################################
                     [1m Learning iteration 421/2000 [0m                      

                       Computation: 37841 steps/s (collection: 2.404s, learning 0.194s)
             Mean action noise std: 2.47
          Mean value_function loss: 7.3222
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 60.2844
                       Mean reward: 13.32
               Mean episode length: 162.17
    Episode_Reward/reaching_object: 0.7272
     Episode_Reward/lifting_object: 2.3151
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 19.0417
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 2.60s
                      Time elapsed: 00:19:31
                               ETA: 01:13:04

################################################################################
                     [1m Learning iteration 422/2000 [0m                      

                       Computation: 35028 steps/s (collection: 2.580s, learning 0.226s)
             Mean action noise std: 2.47
          Mean value_function loss: 12.3700
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.3079
                       Mean reward: 16.00
               Mean episode length: 165.10
    Episode_Reward/reaching_object: 0.6983
     Episode_Reward/lifting_object: 2.1212
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 18.4583
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 2.81s
                      Time elapsed: 00:19:34
                               ETA: 01:13:01

################################################################################
                     [1m Learning iteration 423/2000 [0m                      

                       Computation: 37367 steps/s (collection: 2.481s, learning 0.150s)
             Mean action noise std: 2.47
          Mean value_function loss: 8.6993
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 60.3276
                       Mean reward: 12.13
               Mean episode length: 159.90
    Episode_Reward/reaching_object: 0.7116
     Episode_Reward/lifting_object: 1.9287
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 19.9167
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 2.63s
                      Time elapsed: 00:19:37
                               ETA: 01:12:58

################################################################################
                     [1m Learning iteration 424/2000 [0m                      

                       Computation: 37132 steps/s (collection: 2.444s, learning 0.203s)
             Mean action noise std: 2.48
          Mean value_function loss: 8.0631
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 60.3486
                       Mean reward: 12.94
               Mean episode length: 152.55
    Episode_Reward/reaching_object: 0.6843
     Episode_Reward/lifting_object: 2.1838
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 20.7500
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 2.65s
                      Time elapsed: 00:19:39
                               ETA: 01:12:55

################################################################################
                     [1m Learning iteration 425/2000 [0m                      

                       Computation: 36106 steps/s (collection: 2.471s, learning 0.252s)
             Mean action noise std: 2.48
          Mean value_function loss: 8.6551
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 60.3764
                       Mean reward: 12.99
               Mean episode length: 138.84
    Episode_Reward/reaching_object: 0.6499
     Episode_Reward/lifting_object: 2.0079
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 20.5000
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 2.72s
                      Time elapsed: 00:19:42
                               ETA: 01:12:52

################################################################################
                     [1m Learning iteration 426/2000 [0m                      

                       Computation: 36431 steps/s (collection: 2.547s, learning 0.151s)
             Mean action noise std: 2.48
          Mean value_function loss: 7.2724
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 60.3898
                       Mean reward: 16.80
               Mean episode length: 161.42
    Episode_Reward/reaching_object: 0.6686
     Episode_Reward/lifting_object: 2.1669
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 22.7083
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 2.70s
                      Time elapsed: 00:19:45
                               ETA: 01:12:49

################################################################################
                     [1m Learning iteration 427/2000 [0m                      

                       Computation: 32647 steps/s (collection: 2.785s, learning 0.226s)
             Mean action noise std: 2.48
          Mean value_function loss: 9.9696
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 60.3996
                       Mean reward: 13.52
               Mean episode length: 165.00
    Episode_Reward/reaching_object: 0.6802
     Episode_Reward/lifting_object: 2.2917
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 20.4583
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 3.01s
                      Time elapsed: 00:19:48
                               ETA: 01:12:47

################################################################################
                     [1m Learning iteration 428/2000 [0m                      

                       Computation: 34345 steps/s (collection: 2.686s, learning 0.177s)
             Mean action noise std: 2.48
          Mean value_function loss: 8.4209
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 60.4164
                       Mean reward: 14.90
               Mean episode length: 143.76
    Episode_Reward/reaching_object: 0.6671
     Episode_Reward/lifting_object: 2.1452
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 21.9167
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 2.86s
                      Time elapsed: 00:19:51
                               ETA: 01:12:44

################################################################################
                     [1m Learning iteration 429/2000 [0m                      

                       Computation: 33116 steps/s (collection: 2.714s, learning 0.255s)
             Mean action noise std: 2.49
          Mean value_function loss: 9.3671
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 60.4365
                       Mean reward: 16.37
               Mean episode length: 154.40
    Episode_Reward/reaching_object: 0.6560
     Episode_Reward/lifting_object: 2.4144
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 20.4583
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 2.97s
                      Time elapsed: 00:19:54
                               ETA: 01:12:42

################################################################################
                     [1m Learning iteration 430/2000 [0m                      

                       Computation: 34802 steps/s (collection: 2.656s, learning 0.169s)
             Mean action noise std: 2.49
          Mean value_function loss: 9.8422
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 60.4568
                       Mean reward: 18.23
               Mean episode length: 156.25
    Episode_Reward/reaching_object: 0.6576
     Episode_Reward/lifting_object: 2.4588
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0365
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.82s
                      Time elapsed: 00:19:56
                               ETA: 01:12:40

################################################################################
                     [1m Learning iteration 431/2000 [0m                      

                       Computation: 34410 steps/s (collection: 2.594s, learning 0.263s)
             Mean action noise std: 2.49
          Mean value_function loss: 10.5086
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 60.4740
                       Mean reward: 14.08
               Mean episode length: 145.65
    Episode_Reward/reaching_object: 0.6444
     Episode_Reward/lifting_object: 2.3432
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 23.5417
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 2.86s
                      Time elapsed: 00:19:59
                               ETA: 01:12:37

################################################################################
                     [1m Learning iteration 432/2000 [0m                      

                       Computation: 36647 steps/s (collection: 2.482s, learning 0.200s)
             Mean action noise std: 2.49
          Mean value_function loss: 10.3468
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 60.4865
                       Mean reward: 17.02
               Mean episode length: 152.08
    Episode_Reward/reaching_object: 0.6398
     Episode_Reward/lifting_object: 2.4983
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 24.7083
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 2.68s
                      Time elapsed: 00:20:02
                               ETA: 01:12:34

################################################################################
                     [1m Learning iteration 433/2000 [0m                      

                       Computation: 34103 steps/s (collection: 2.582s, learning 0.300s)
             Mean action noise std: 2.49
          Mean value_function loss: 12.3613
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.5020
                       Mean reward: 14.37
               Mean episode length: 141.64
    Episode_Reward/reaching_object: 0.6298
     Episode_Reward/lifting_object: 2.1444
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 23.9583
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 2.88s
                      Time elapsed: 00:20:05
                               ETA: 01:12:32

################################################################################
                     [1m Learning iteration 434/2000 [0m                      

                       Computation: 35633 steps/s (collection: 2.579s, learning 0.180s)
             Mean action noise std: 2.49
          Mean value_function loss: 8.0847
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 60.5166
                       Mean reward: 14.59
               Mean episode length: 143.08
    Episode_Reward/reaching_object: 0.6125
     Episode_Reward/lifting_object: 2.1979
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 23.2500
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 2.76s
                      Time elapsed: 00:20:08
                               ETA: 01:12:29

################################################################################
                     [1m Learning iteration 435/2000 [0m                      

                       Computation: 36792 steps/s (collection: 2.489s, learning 0.183s)
             Mean action noise std: 2.49
          Mean value_function loss: 10.2340
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 60.5354
                       Mean reward: 14.81
               Mean episode length: 142.63
    Episode_Reward/reaching_object: 0.6424
     Episode_Reward/lifting_object: 2.5878
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 24.7083
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 2.67s
                      Time elapsed: 00:20:10
                               ETA: 01:12:26

################################################################################
                     [1m Learning iteration 436/2000 [0m                      

                       Computation: 38949 steps/s (collection: 2.319s, learning 0.205s)
             Mean action noise std: 2.50
          Mean value_function loss: 8.8912
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 60.5477
                       Mean reward: 15.28
               Mean episode length: 147.68
    Episode_Reward/reaching_object: 0.6117
     Episode_Reward/lifting_object: 2.5841
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 24.9583
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 2.52s
                      Time elapsed: 00:20:13
                               ETA: 01:12:22

################################################################################
                     [1m Learning iteration 437/2000 [0m                      

                       Computation: 36736 steps/s (collection: 2.436s, learning 0.240s)
             Mean action noise std: 2.50
          Mean value_function loss: 14.7957
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 60.5541
                       Mean reward: 15.31
               Mean episode length: 150.26
    Episode_Reward/reaching_object: 0.6357
     Episode_Reward/lifting_object: 2.7345
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 24.7500
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 2.68s
                      Time elapsed: 00:20:15
                               ETA: 01:12:19

################################################################################
                     [1m Learning iteration 438/2000 [0m                      

                       Computation: 30608 steps/s (collection: 3.017s, learning 0.195s)
             Mean action noise std: 2.50
          Mean value_function loss: 9.7793
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 60.5628
                       Mean reward: 14.75
               Mean episode length: 123.67
    Episode_Reward/reaching_object: 0.6187
     Episode_Reward/lifting_object: 2.5886
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 25.4167
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 3.21s
                      Time elapsed: 00:20:19
                               ETA: 01:12:17

################################################################################
                     [1m Learning iteration 439/2000 [0m                      

                       Computation: 29943 steps/s (collection: 3.017s, learning 0.266s)
             Mean action noise std: 2.50
          Mean value_function loss: 10.5974
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 60.5744
                       Mean reward: 17.07
               Mean episode length: 148.38
    Episode_Reward/reaching_object: 0.6096
     Episode_Reward/lifting_object: 2.8160
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0236
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 24.3750
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 3.28s
                      Time elapsed: 00:20:22
                               ETA: 01:12:16

################################################################################
                     [1m Learning iteration 440/2000 [0m                      

                       Computation: 30362 steps/s (collection: 2.969s, learning 0.268s)
             Mean action noise std: 2.50
          Mean value_function loss: 11.8654
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 60.5892
                       Mean reward: 16.10
               Mean episode length: 128.65
    Episode_Reward/reaching_object: 0.6182
     Episode_Reward/lifting_object: 2.7381
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 25.9583
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 3.24s
                      Time elapsed: 00:20:25
                               ETA: 01:12:15

################################################################################
                     [1m Learning iteration 441/2000 [0m                      

                       Computation: 31368 steps/s (collection: 2.920s, learning 0.214s)
             Mean action noise std: 2.50
          Mean value_function loss: 11.3653
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.6089
                       Mean reward: 18.99
               Mean episode length: 139.49
    Episode_Reward/reaching_object: 0.6237
     Episode_Reward/lifting_object: 2.8799
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 24.2083
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 3.13s
                      Time elapsed: 00:20:28
                               ETA: 01:12:14

################################################################################
                     [1m Learning iteration 442/2000 [0m                      

                       Computation: 25729 steps/s (collection: 3.564s, learning 0.257s)
             Mean action noise std: 2.50
          Mean value_function loss: 11.2041
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 60.6221
                       Mean reward: 18.55
               Mean episode length: 140.51
    Episode_Reward/reaching_object: 0.6278
     Episode_Reward/lifting_object: 3.1160
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 23.4167
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 3.82s
                      Time elapsed: 00:20:32
                               ETA: 01:12:15

################################################################################
                     [1m Learning iteration 443/2000 [0m                      

                       Computation: 29307 steps/s (collection: 3.076s, learning 0.278s)
             Mean action noise std: 2.50
          Mean value_function loss: 12.1381
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 60.6293
                       Mean reward: 21.11
               Mean episode length: 145.58
    Episode_Reward/reaching_object: 0.6360
     Episode_Reward/lifting_object: 3.0731
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 23.7500
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 3.35s
                      Time elapsed: 00:20:36
                               ETA: 01:12:14

################################################################################
                     [1m Learning iteration 444/2000 [0m                      

                       Computation: 31632 steps/s (collection: 2.851s, learning 0.257s)
             Mean action noise std: 2.51
          Mean value_function loss: 10.0066
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 60.6479
                       Mean reward: 16.83
               Mean episode length: 138.58
    Episode_Reward/reaching_object: 0.6128
     Episode_Reward/lifting_object: 2.8118
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 25.7917
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 3.11s
                      Time elapsed: 00:20:39
                               ETA: 01:12:12

################################################################################
                     [1m Learning iteration 445/2000 [0m                      

                       Computation: 31214 steps/s (collection: 2.934s, learning 0.215s)
             Mean action noise std: 2.51
          Mean value_function loss: 9.5769
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 60.6704
                       Mean reward: 19.31
               Mean episode length: 143.63
    Episode_Reward/reaching_object: 0.6265
     Episode_Reward/lifting_object: 3.1315
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 23.4167
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 3.15s
                      Time elapsed: 00:20:42
                               ETA: 01:12:11

################################################################################
                     [1m Learning iteration 446/2000 [0m                      

                       Computation: 32910 steps/s (collection: 2.749s, learning 0.238s)
             Mean action noise std: 2.51
          Mean value_function loss: 11.5936
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 60.6848
                       Mean reward: 20.79
               Mean episode length: 141.14
    Episode_Reward/reaching_object: 0.6401
     Episode_Reward/lifting_object: 3.2743
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 4.0000
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 24.6250
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 2.99s
                      Time elapsed: 00:20:45
                               ETA: 01:12:09

################################################################################
                     [1m Learning iteration 447/2000 [0m                      

                       Computation: 32769 steps/s (collection: 2.843s, learning 0.157s)
             Mean action noise std: 2.51
          Mean value_function loss: 11.4421
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 60.6992
                       Mean reward: 18.10
               Mean episode length: 149.45
    Episode_Reward/reaching_object: 0.6327
     Episode_Reward/lifting_object: 3.0527
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 3.5000
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 25.8750
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 3.00s
                      Time elapsed: 00:20:48
                               ETA: 01:12:07

################################################################################
                     [1m Learning iteration 448/2000 [0m                      

                       Computation: 31432 steps/s (collection: 2.920s, learning 0.207s)
             Mean action noise std: 2.51
          Mean value_function loss: 16.4712
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 60.7202
                       Mean reward: 19.74
               Mean episode length: 132.78
    Episode_Reward/reaching_object: 0.6215
     Episode_Reward/lifting_object: 2.8362
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0360
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 24.9583
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 3.13s
                      Time elapsed: 00:20:51
                               ETA: 01:12:05

################################################################################
                     [1m Learning iteration 449/2000 [0m                      

                       Computation: 31359 steps/s (collection: 2.925s, learning 0.209s)
             Mean action noise std: 2.51
          Mean value_function loss: 15.6641
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 60.7383
                       Mean reward: 21.25
               Mean episode length: 150.39
    Episode_Reward/reaching_object: 0.6431
     Episode_Reward/lifting_object: 3.2005
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 25.0417
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 3.13s
                      Time elapsed: 00:20:54
                               ETA: 01:12:03

################################################################################
                     [1m Learning iteration 450/2000 [0m                      

                       Computation: 31232 steps/s (collection: 2.907s, learning 0.241s)
             Mean action noise std: 2.52
          Mean value_function loss: 10.5528
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 60.7502
                       Mean reward: 18.72
               Mean episode length: 133.55
    Episode_Reward/reaching_object: 0.6092
     Episode_Reward/lifting_object: 3.1485
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 27.5417
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 3.15s
                      Time elapsed: 00:20:57
                               ETA: 01:12:02

################################################################################
                     [1m Learning iteration 451/2000 [0m                      

                       Computation: 31463 steps/s (collection: 2.945s, learning 0.179s)
             Mean action noise std: 2.52
          Mean value_function loss: 13.4562
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 60.7588
                       Mean reward: 15.80
               Mean episode length: 128.36
    Episode_Reward/reaching_object: 0.5989
     Episode_Reward/lifting_object: 3.1027
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 26.9167
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 3.12s
                      Time elapsed: 00:21:00
                               ETA: 01:12:00

################################################################################
                     [1m Learning iteration 452/2000 [0m                      

                       Computation: 39361 steps/s (collection: 2.351s, learning 0.146s)
             Mean action noise std: 2.52
          Mean value_function loss: 15.3117
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 60.7741
                       Mean reward: 23.47
               Mean episode length: 135.10
    Episode_Reward/reaching_object: 0.5825
     Episode_Reward/lifting_object: 3.2895
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 27.4583
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.50s
                      Time elapsed: 00:21:03
                               ETA: 01:11:56

################################################################################
                     [1m Learning iteration 453/2000 [0m                      

                       Computation: 38843 steps/s (collection: 2.347s, learning 0.184s)
             Mean action noise std: 2.52
          Mean value_function loss: 11.4080
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 60.7914
                       Mean reward: 19.45
               Mean episode length: 124.81
    Episode_Reward/reaching_object: 0.5756
     Episode_Reward/lifting_object: 3.2407
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 27.8750
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 2.53s
                      Time elapsed: 00:21:05
                               ETA: 01:11:53

################################################################################
                     [1m Learning iteration 454/2000 [0m                      

                       Computation: 40177 steps/s (collection: 2.272s, learning 0.175s)
             Mean action noise std: 2.52
          Mean value_function loss: 17.7255
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 60.7989
                       Mean reward: 19.68
               Mean episode length: 125.90
    Episode_Reward/reaching_object: 0.5790
     Episode_Reward/lifting_object: 3.3138
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 26.9167
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 2.45s
                      Time elapsed: 00:21:08
                               ETA: 01:11:49

################################################################################
                     [1m Learning iteration 455/2000 [0m                      

                       Computation: 40356 steps/s (collection: 2.253s, learning 0.183s)
             Mean action noise std: 2.52
          Mean value_function loss: 11.6778
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 60.8095
                       Mean reward: 18.86
               Mean episode length: 128.91
    Episode_Reward/reaching_object: 0.5661
     Episode_Reward/lifting_object: 3.0872
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 27.5833
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 2.44s
                      Time elapsed: 00:21:10
                               ETA: 01:11:45

################################################################################
                     [1m Learning iteration 456/2000 [0m                      

                       Computation: 42456 steps/s (collection: 2.177s, learning 0.138s)
             Mean action noise std: 2.52
          Mean value_function loss: 16.0244
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 60.8234
                       Mean reward: 18.29
               Mean episode length: 122.11
    Episode_Reward/reaching_object: 0.5635
     Episode_Reward/lifting_object: 3.4158
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 28.9167
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 2.32s
                      Time elapsed: 00:21:13
                               ETA: 01:11:40

################################################################################
                     [1m Learning iteration 457/2000 [0m                      

                       Computation: 42271 steps/s (collection: 2.207s, learning 0.119s)
             Mean action noise std: 2.52
          Mean value_function loss: 12.1382
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.8327
                       Mean reward: 20.66
               Mean episode length: 137.17
    Episode_Reward/reaching_object: 0.5954
     Episode_Reward/lifting_object: 3.7703
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 3.5417
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 27.7500
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 2.33s
                      Time elapsed: 00:21:15
                               ETA: 01:11:36

################################################################################
                     [1m Learning iteration 458/2000 [0m                      

                       Computation: 37459 steps/s (collection: 2.459s, learning 0.166s)
             Mean action noise std: 2.53
          Mean value_function loss: 14.9636
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 60.8430
                       Mean reward: 21.14
               Mean episode length: 118.54
    Episode_Reward/reaching_object: 0.5550
     Episode_Reward/lifting_object: 3.3663
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 28.8333
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 2.62s
                      Time elapsed: 00:21:17
                               ETA: 01:11:33

################################################################################
                     [1m Learning iteration 459/2000 [0m                      

                       Computation: 40080 steps/s (collection: 2.314s, learning 0.139s)
             Mean action noise std: 2.53
          Mean value_function loss: 14.7171
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 60.8574
                       Mean reward: 20.23
               Mean episode length: 137.49
    Episode_Reward/reaching_object: 0.5572
     Episode_Reward/lifting_object: 3.5715
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 2.45s
                      Time elapsed: 00:21:20
                               ETA: 01:11:29

################################################################################
                     [1m Learning iteration 460/2000 [0m                      

                       Computation: 41404 steps/s (collection: 2.256s, learning 0.118s)
             Mean action noise std: 2.53
          Mean value_function loss: 15.9743
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 60.8665
                       Mean reward: 21.42
               Mean episode length: 120.02
    Episode_Reward/reaching_object: 0.5508
     Episode_Reward/lifting_object: 3.2781
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 32.0417
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.37s
                      Time elapsed: 00:21:22
                               ETA: 01:11:25

################################################################################
                     [1m Learning iteration 461/2000 [0m                      

                       Computation: 39974 steps/s (collection: 2.345s, learning 0.115s)
             Mean action noise std: 2.53
          Mean value_function loss: 14.6809
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 60.8689
                       Mean reward: 21.75
               Mean episode length: 134.67
    Episode_Reward/reaching_object: 0.5467
     Episode_Reward/lifting_object: 3.4478
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 30.1667
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 2.46s
                      Time elapsed: 00:21:25
                               ETA: 01:11:21

################################################################################
                     [1m Learning iteration 462/2000 [0m                      

                       Computation: 40814 steps/s (collection: 2.227s, learning 0.182s)
             Mean action noise std: 2.53
          Mean value_function loss: 13.8024
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.8715
                       Mean reward: 25.72
               Mean episode length: 133.12
    Episode_Reward/reaching_object: 0.5580
     Episode_Reward/lifting_object: 3.7472
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 29.2500
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 2.41s
                      Time elapsed: 00:21:27
                               ETA: 01:11:17

################################################################################
                     [1m Learning iteration 463/2000 [0m                      

                       Computation: 39509 steps/s (collection: 2.326s, learning 0.163s)
             Mean action noise std: 2.53
          Mean value_function loss: 13.3464
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 60.8779
                       Mean reward: 22.70
               Mean episode length: 130.45
    Episode_Reward/reaching_object: 0.5457
     Episode_Reward/lifting_object: 3.7581
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 29.9583
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 2.49s
                      Time elapsed: 00:21:30
                               ETA: 01:11:13

################################################################################
                     [1m Learning iteration 464/2000 [0m                      

                       Computation: 38891 steps/s (collection: 2.405s, learning 0.123s)
             Mean action noise std: 2.53
          Mean value_function loss: 17.5542
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 60.8914
                       Mean reward: 19.83
               Mean episode length: 119.25
    Episode_Reward/reaching_object: 0.5349
     Episode_Reward/lifting_object: 3.3420
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 31.4167
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.53s
                      Time elapsed: 00:21:32
                               ETA: 01:11:10

################################################################################
                     [1m Learning iteration 465/2000 [0m                      

                       Computation: 38821 steps/s (collection: 2.380s, learning 0.152s)
             Mean action noise std: 2.53
          Mean value_function loss: 14.6543
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 60.9093
                       Mean reward: 20.54
               Mean episode length: 123.77
    Episode_Reward/reaching_object: 0.5340
     Episode_Reward/lifting_object: 3.3400
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 31.5417
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 2.53s
                      Time elapsed: 00:21:35
                               ETA: 01:11:06

################################################################################
                     [1m Learning iteration 466/2000 [0m                      

                       Computation: 37690 steps/s (collection: 2.374s, learning 0.234s)
             Mean action noise std: 2.53
          Mean value_function loss: 19.1243
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 60.9212
                       Mean reward: 19.60
               Mean episode length: 108.85
    Episode_Reward/reaching_object: 0.5361
     Episode_Reward/lifting_object: 3.2136
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 30.7917
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.61s
                      Time elapsed: 00:21:37
                               ETA: 01:11:03

################################################################################
                     [1m Learning iteration 467/2000 [0m                      

                       Computation: 37390 steps/s (collection: 2.417s, learning 0.213s)
             Mean action noise std: 2.53
          Mean value_function loss: 16.9188
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 60.9335
                       Mean reward: 22.92
               Mean episode length: 124.78
    Episode_Reward/reaching_object: 0.5475
     Episode_Reward/lifting_object: 3.6213
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 30.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 2.63s
                      Time elapsed: 00:21:40
                               ETA: 01:10:59

################################################################################
                     [1m Learning iteration 468/2000 [0m                      

                       Computation: 36395 steps/s (collection: 2.560s, learning 0.141s)
             Mean action noise std: 2.54
          Mean value_function loss: 15.4508
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 60.9419
                       Mean reward: 22.03
               Mean episode length: 122.22
    Episode_Reward/reaching_object: 0.5468
     Episode_Reward/lifting_object: 3.9284
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 36.1250
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 2.70s
                      Time elapsed: 00:21:43
                               ETA: 01:10:56

################################################################################
                     [1m Learning iteration 469/2000 [0m                      

                       Computation: 36186 steps/s (collection: 2.532s, learning 0.185s)
             Mean action noise std: 2.54
          Mean value_function loss: 14.1240
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 60.9467
                       Mean reward: 17.38
               Mean episode length: 117.63
    Episode_Reward/reaching_object: 0.5185
     Episode_Reward/lifting_object: 3.5125
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 32.3750
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 2.72s
                      Time elapsed: 00:21:45
                               ETA: 01:10:53

################################################################################
                     [1m Learning iteration 470/2000 [0m                      

                       Computation: 37228 steps/s (collection: 2.477s, learning 0.164s)
             Mean action noise std: 2.54
          Mean value_function loss: 16.0253
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 60.9581
                       Mean reward: 19.93
               Mean episode length: 115.77
    Episode_Reward/reaching_object: 0.5189
     Episode_Reward/lifting_object: 3.4582
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 33.7500
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 2.64s
                      Time elapsed: 00:21:48
                               ETA: 01:10:50

################################################################################
                     [1m Learning iteration 471/2000 [0m                      

                       Computation: 37748 steps/s (collection: 2.482s, learning 0.122s)
             Mean action noise std: 2.54
          Mean value_function loss: 14.9804
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 60.9718
                       Mean reward: 17.61
               Mean episode length: 111.75
    Episode_Reward/reaching_object: 0.5077
     Episode_Reward/lifting_object: 3.6973
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 34.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 2.60s
                      Time elapsed: 00:21:51
                               ETA: 01:10:47

################################################################################
                     [1m Learning iteration 472/2000 [0m                      

                       Computation: 34874 steps/s (collection: 2.626s, learning 0.193s)
             Mean action noise std: 2.54
          Mean value_function loss: 16.7925
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 60.9831
                       Mean reward: 19.13
               Mean episode length: 102.78
    Episode_Reward/reaching_object: 0.5092
     Episode_Reward/lifting_object: 3.7764
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 34.7083
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.82s
                      Time elapsed: 00:21:53
                               ETA: 01:10:44

################################################################################
                     [1m Learning iteration 473/2000 [0m                      

                       Computation: 35355 steps/s (collection: 2.548s, learning 0.232s)
             Mean action noise std: 2.54
          Mean value_function loss: 22.5571
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 60.9934
                       Mean reward: 19.72
               Mean episode length: 104.14
    Episode_Reward/reaching_object: 0.4980
     Episode_Reward/lifting_object: 3.8154
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 36.6667
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 2.78s
                      Time elapsed: 00:21:56
                               ETA: 01:10:41

################################################################################
                     [1m Learning iteration 474/2000 [0m                      

                       Computation: 31371 steps/s (collection: 2.877s, learning 0.257s)
             Mean action noise std: 2.54
          Mean value_function loss: 15.5274
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 60.9974
                       Mean reward: 19.18
               Mean episode length: 106.00
    Episode_Reward/reaching_object: 0.5001
     Episode_Reward/lifting_object: 3.7425
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 36.2083
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 3.13s
                      Time elapsed: 00:21:59
                               ETA: 01:10:40

################################################################################
                     [1m Learning iteration 475/2000 [0m                      

                       Computation: 34627 steps/s (collection: 2.628s, learning 0.211s)
             Mean action noise std: 2.54
          Mean value_function loss: 18.2117
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 61.0047
                       Mean reward: 18.63
               Mean episode length: 105.23
    Episode_Reward/reaching_object: 0.4700
     Episode_Reward/lifting_object: 3.9264
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 33.5833
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 2.84s
                      Time elapsed: 00:22:02
                               ETA: 01:10:37

################################################################################
                     [1m Learning iteration 476/2000 [0m                      

                       Computation: 31529 steps/s (collection: 2.926s, learning 0.192s)
             Mean action noise std: 2.54
          Mean value_function loss: 19.1523
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 61.0136
                       Mean reward: 21.85
               Mean episode length: 110.02
    Episode_Reward/reaching_object: 0.4848
     Episode_Reward/lifting_object: 3.9113
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0302
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 34.8333
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 3.12s
                      Time elapsed: 00:22:05
                               ETA: 01:10:35

################################################################################
                     [1m Learning iteration 477/2000 [0m                      

                       Computation: 37701 steps/s (collection: 2.490s, learning 0.118s)
             Mean action noise std: 2.54
          Mean value_function loss: 20.0203
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 61.0235
                       Mean reward: 26.99
               Mean episode length: 111.82
    Episode_Reward/reaching_object: 0.4881
     Episode_Reward/lifting_object: 4.0566
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 33.9583
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 2.61s
                      Time elapsed: 00:22:08
                               ETA: 01:10:32

################################################################################
                     [1m Learning iteration 478/2000 [0m                      

                       Computation: 36819 steps/s (collection: 2.491s, learning 0.179s)
             Mean action noise std: 2.55
          Mean value_function loss: 24.0983
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 61.0400
                       Mean reward: 20.14
               Mean episode length: 100.94
    Episode_Reward/reaching_object: 0.4741
     Episode_Reward/lifting_object: 3.9508
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 36.7917
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.67s
                      Time elapsed: 00:22:11
                               ETA: 01:10:29

################################################################################
                     [1m Learning iteration 479/2000 [0m                      

                       Computation: 31708 steps/s (collection: 2.838s, learning 0.262s)
             Mean action noise std: 2.55
          Mean value_function loss: 25.7772
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 61.0568
                       Mean reward: 24.12
               Mean episode length: 104.38
    Episode_Reward/reaching_object: 0.5099
     Episode_Reward/lifting_object: 4.0958
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 36.2083
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 3.10s
                      Time elapsed: 00:22:14
                               ETA: 01:10:27

################################################################################
                     [1m Learning iteration 480/2000 [0m                      

                       Computation: 30153 steps/s (collection: 3.103s, learning 0.157s)
             Mean action noise std: 2.55
          Mean value_function loss: 25.8261
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 61.0693
                       Mean reward: 10.21
               Mean episode length: 102.23
    Episode_Reward/reaching_object: 0.4916
     Episode_Reward/lifting_object: 3.7473
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 37.8333
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 3.26s
                      Time elapsed: 00:22:17
                               ETA: 01:10:26

################################################################################
                     [1m Learning iteration 481/2000 [0m                      

                       Computation: 30949 steps/s (collection: 3.002s, learning 0.174s)
             Mean action noise std: 2.55
          Mean value_function loss: 19.1807
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 61.0763
                       Mean reward: 20.49
               Mean episode length: 95.67
    Episode_Reward/reaching_object: 0.4918
     Episode_Reward/lifting_object: 4.2407
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 38.6667
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 3.18s
                      Time elapsed: 00:22:20
                               ETA: 01:10:24

################################################################################
                     [1m Learning iteration 482/2000 [0m                      

                       Computation: 34874 steps/s (collection: 2.644s, learning 0.175s)
             Mean action noise std: 2.55
          Mean value_function loss: 27.8983
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.0845
                       Mean reward: 26.03
               Mean episode length: 104.63
    Episode_Reward/reaching_object: 0.4823
     Episode_Reward/lifting_object: 4.2352
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0304
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 38.6667
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.82s
                      Time elapsed: 00:22:23
                               ETA: 01:10:22

################################################################################
                     [1m Learning iteration 483/2000 [0m                      

                       Computation: 37811 steps/s (collection: 2.478s, learning 0.122s)
             Mean action noise std: 2.55
          Mean value_function loss: 26.5635
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 61.0968
                       Mean reward: 23.24
               Mean episode length: 104.69
    Episode_Reward/reaching_object: 0.4680
     Episode_Reward/lifting_object: 4.1362
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 38.8750
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 2.60s
                      Time elapsed: 00:22:26
                               ETA: 01:10:18

################################################################################
                     [1m Learning iteration 484/2000 [0m                      

                       Computation: 37615 steps/s (collection: 2.460s, learning 0.153s)
             Mean action noise std: 2.55
          Mean value_function loss: 17.8669
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 61.1100
                       Mean reward: 21.30
               Mean episode length: 100.37
    Episode_Reward/reaching_object: 0.4591
     Episode_Reward/lifting_object: 4.0483
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 40.0417
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.61s
                      Time elapsed: 00:22:28
                               ETA: 01:10:15

################################################################################
                     [1m Learning iteration 485/2000 [0m                      

                       Computation: 35407 steps/s (collection: 2.575s, learning 0.201s)
             Mean action noise std: 2.55
          Mean value_function loss: 20.5381
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 61.1159
                       Mean reward: 19.20
               Mean episode length: 96.92
    Episode_Reward/reaching_object: 0.4628
     Episode_Reward/lifting_object: 4.2802
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 39.0417
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 2.78s
                      Time elapsed: 00:22:31
                               ETA: 01:10:12

################################################################################
                     [1m Learning iteration 486/2000 [0m                      

                       Computation: 35973 steps/s (collection: 2.527s, learning 0.206s)
             Mean action noise std: 2.55
          Mean value_function loss: 27.0499
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 61.1229
                       Mean reward: 25.01
               Mean episode length: 102.62
    Episode_Reward/reaching_object: 0.4517
     Episode_Reward/lifting_object: 4.2063
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 42.2500
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.73s
                      Time elapsed: 00:22:34
                               ETA: 01:10:09

################################################################################
                     [1m Learning iteration 487/2000 [0m                      

                       Computation: 38922 steps/s (collection: 2.361s, learning 0.164s)
             Mean action noise std: 2.56
          Mean value_function loss: 24.6810
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 61.1347
                       Mean reward: 18.28
               Mean episode length: 96.45
    Episode_Reward/reaching_object: 0.4500
     Episode_Reward/lifting_object: 4.1633
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 2.5000
     Episode_Termination/robot_out: 40.6250
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 2.53s
                      Time elapsed: 00:22:36
                               ETA: 01:10:06

################################################################################
                     [1m Learning iteration 488/2000 [0m                      

                       Computation: 38717 steps/s (collection: 2.407s, learning 0.132s)
             Mean action noise std: 2.56
          Mean value_function loss: 33.5787
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 61.1466
                       Mean reward: 20.87
               Mean episode length: 97.41
    Episode_Reward/reaching_object: 0.4511
     Episode_Reward/lifting_object: 4.3735
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 40.9583
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.54s
                      Time elapsed: 00:22:39
                               ETA: 01:10:02

################################################################################
                     [1m Learning iteration 489/2000 [0m                      

                       Computation: 39244 steps/s (collection: 2.357s, learning 0.148s)
             Mean action noise std: 2.56
          Mean value_function loss: 32.1597
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 61.1559
                       Mean reward: 23.32
               Mean episode length: 98.07
    Episode_Reward/reaching_object: 0.4463
     Episode_Reward/lifting_object: 3.8951
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 43.4167
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 2.50s
                      Time elapsed: 00:22:41
                               ETA: 01:09:59

################################################################################
                     [1m Learning iteration 490/2000 [0m                      

                       Computation: 40227 steps/s (collection: 2.288s, learning 0.156s)
             Mean action noise std: 2.56
          Mean value_function loss: 29.0666
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 61.1724
                       Mean reward: 21.25
               Mean episode length: 97.43
    Episode_Reward/reaching_object: 0.4368
     Episode_Reward/lifting_object: 4.1166
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 40.2500
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.44s
                      Time elapsed: 00:22:44
                               ETA: 01:09:55

################################################################################
                     [1m Learning iteration 491/2000 [0m                      

                       Computation: 41851 steps/s (collection: 2.240s, learning 0.109s)
             Mean action noise std: 2.56
          Mean value_function loss: 24.8148
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 61.1878
                       Mean reward: 27.66
               Mean episode length: 103.42
    Episode_Reward/reaching_object: 0.4477
     Episode_Reward/lifting_object: 4.3506
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 44.9167
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 2.35s
                      Time elapsed: 00:22:46
                               ETA: 01:09:51

################################################################################
                     [1m Learning iteration 492/2000 [0m                      

                       Computation: 39765 steps/s (collection: 2.243s, learning 0.229s)
             Mean action noise std: 2.56
          Mean value_function loss: 23.1028
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 61.1956
                       Mean reward: 21.12
               Mean episode length: 98.35
    Episode_Reward/reaching_object: 0.4284
     Episode_Reward/lifting_object: 4.0789
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 41.8333
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.47s
                      Time elapsed: 00:22:48
                               ETA: 01:09:47

################################################################################
                     [1m Learning iteration 493/2000 [0m                      

                       Computation: 41355 steps/s (collection: 2.240s, learning 0.138s)
             Mean action noise std: 2.56
          Mean value_function loss: 24.9094
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 61.2044
                       Mean reward: 27.32
               Mean episode length: 95.76
    Episode_Reward/reaching_object: 0.4350
     Episode_Reward/lifting_object: 4.3532
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 40.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 2.38s
                      Time elapsed: 00:22:51
                               ETA: 01:09:43

################################################################################
                     [1m Learning iteration 494/2000 [0m                      

                       Computation: 40826 steps/s (collection: 2.248s, learning 0.160s)
             Mean action noise std: 2.56
          Mean value_function loss: 25.1019
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 61.2130
                       Mean reward: 23.21
               Mean episode length: 91.06
    Episode_Reward/reaching_object: 0.4255
     Episode_Reward/lifting_object: 4.0649
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 43.7083
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.41s
                      Time elapsed: 00:22:53
                               ETA: 01:09:39

################################################################################
                     [1m Learning iteration 495/2000 [0m                      

                       Computation: 39474 steps/s (collection: 2.313s, learning 0.178s)
             Mean action noise std: 2.57
          Mean value_function loss: 27.7380
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.2220
                       Mean reward: 22.38
               Mean episode length: 84.77
    Episode_Reward/reaching_object: 0.4280
     Episode_Reward/lifting_object: 4.2595
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 44.5000
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 2.49s
                      Time elapsed: 00:22:56
                               ETA: 01:09:35

################################################################################
                     [1m Learning iteration 496/2000 [0m                      

                       Computation: 39846 steps/s (collection: 2.326s, learning 0.141s)
             Mean action noise std: 2.57
          Mean value_function loss: 27.3549
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 61.2387
                       Mean reward: 27.11
               Mean episode length: 95.54
    Episode_Reward/reaching_object: 0.4248
     Episode_Reward/lifting_object: 4.3888
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 44.6250
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.47s
                      Time elapsed: 00:22:58
                               ETA: 01:09:32

################################################################################
                     [1m Learning iteration 497/2000 [0m                      

                       Computation: 38464 steps/s (collection: 2.377s, learning 0.179s)
             Mean action noise std: 2.57
          Mean value_function loss: 30.2386
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 61.2532
                       Mean reward: 22.12
               Mean episode length: 86.91
    Episode_Reward/reaching_object: 0.4186
     Episode_Reward/lifting_object: 4.3036
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 44.1667
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 2.56s
                      Time elapsed: 00:23:01
                               ETA: 01:09:28

################################################################################
                     [1m Learning iteration 498/2000 [0m                      

                       Computation: 34722 steps/s (collection: 2.556s, learning 0.276s)
             Mean action noise std: 2.57
          Mean value_function loss: 29.9834
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 61.2672
                       Mean reward: 25.73
               Mean episode length: 95.42
    Episode_Reward/reaching_object: 0.4316
     Episode_Reward/lifting_object: 4.5033
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 46.5833
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 2.83s
                      Time elapsed: 00:23:04
                               ETA: 01:09:26

################################################################################
                     [1m Learning iteration 499/2000 [0m                      

                       Computation: 36381 steps/s (collection: 2.576s, learning 0.126s)
             Mean action noise std: 2.57
          Mean value_function loss: 25.6274
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.2774
                       Mean reward: 20.33
               Mean episode length: 88.27
    Episode_Reward/reaching_object: 0.4010
     Episode_Reward/lifting_object: 4.2686
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 44.2083
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 2.70s
                      Time elapsed: 00:23:06
                               ETA: 01:09:23

################################################################################
                     [1m Learning iteration 500/2000 [0m                      

                       Computation: 40380 steps/s (collection: 2.318s, learning 0.117s)
             Mean action noise std: 2.57
          Mean value_function loss: 48.0833
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 61.2897
                       Mean reward: 21.33
               Mean episode length: 91.49
    Episode_Reward/reaching_object: 0.4141
     Episode_Reward/lifting_object: 4.3696
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 46.6667
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.43s
                      Time elapsed: 00:23:09
                               ETA: 01:09:19

################################################################################
                     [1m Learning iteration 501/2000 [0m                      

                       Computation: 36411 steps/s (collection: 2.493s, learning 0.207s)
             Mean action noise std: 2.57
          Mean value_function loss: 33.9511
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 61.3025
                       Mean reward: 26.85
               Mean episode length: 88.11
    Episode_Reward/reaching_object: 0.4084
     Episode_Reward/lifting_object: 4.2730
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 45.5417
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 2.70s
                      Time elapsed: 00:23:11
                               ETA: 01:09:16

################################################################################
                     [1m Learning iteration 502/2000 [0m                      

                       Computation: 33076 steps/s (collection: 2.811s, learning 0.161s)
             Mean action noise std: 2.57
          Mean value_function loss: 29.2550
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 61.3108
                       Mean reward: 27.90
               Mean episode length: 88.96
    Episode_Reward/reaching_object: 0.4005
     Episode_Reward/lifting_object: 4.4309
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 46.6250
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.97s
                      Time elapsed: 00:23:14
                               ETA: 01:09:14

################################################################################
                     [1m Learning iteration 503/2000 [0m                      

                       Computation: 36668 steps/s (collection: 2.495s, learning 0.186s)
             Mean action noise std: 2.57
          Mean value_function loss: 34.2635
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 61.3144
                       Mean reward: 24.25
               Mean episode length: 86.36
    Episode_Reward/reaching_object: 0.4046
     Episode_Reward/lifting_object: 4.5092
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 48.2917
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 2.68s
                      Time elapsed: 00:23:17
                               ETA: 01:09:11

################################################################################
                     [1m Learning iteration 504/2000 [0m                      

                       Computation: 33662 steps/s (collection: 2.707s, learning 0.213s)
             Mean action noise std: 2.57
          Mean value_function loss: 40.9522
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 61.3150
                       Mean reward: 16.06
               Mean episode length: 77.17
    Episode_Reward/reaching_object: 0.3975
     Episode_Reward/lifting_object: 4.4887
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 45.2083
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 2.92s
                      Time elapsed: 00:23:20
                               ETA: 01:09:08

################################################################################
                     [1m Learning iteration 505/2000 [0m                      

                       Computation: 33766 steps/s (collection: 2.745s, learning 0.167s)
             Mean action noise std: 2.57
          Mean value_function loss: 36.5211
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 61.3167
                       Mean reward: 20.92
               Mean episode length: 86.69
    Episode_Reward/reaching_object: 0.3828
     Episode_Reward/lifting_object: 4.3952
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 48.4167
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 2.91s
                      Time elapsed: 00:23:23
                               ETA: 01:09:06

################################################################################
                     [1m Learning iteration 506/2000 [0m                      

                       Computation: 34630 steps/s (collection: 2.699s, learning 0.140s)
             Mean action noise std: 2.58
          Mean value_function loss: 32.8222
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 61.3213
                       Mean reward: 23.02
               Mean episode length: 79.83
    Episode_Reward/reaching_object: 0.3822
     Episode_Reward/lifting_object: 4.5070
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 48.9167
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 2.84s
                      Time elapsed: 00:23:26
                               ETA: 01:09:03

################################################################################
                     [1m Learning iteration 507/2000 [0m                      

                       Computation: 35994 steps/s (collection: 2.530s, learning 0.201s)
             Mean action noise std: 2.58
          Mean value_function loss: 45.1337
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 61.3267
                       Mean reward: 23.20
               Mean episode length: 86.12
    Episode_Reward/reaching_object: 0.3883
     Episode_Reward/lifting_object: 4.5629
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 48.2083
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 2.73s
                      Time elapsed: 00:23:29
                               ETA: 01:09:01

################################################################################
                     [1m Learning iteration 508/2000 [0m                      

                       Computation: 34876 steps/s (collection: 2.582s, learning 0.237s)
             Mean action noise std: 2.58
          Mean value_function loss: 41.3544
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 61.3287
                       Mean reward: 25.94
               Mean episode length: 85.56
    Episode_Reward/reaching_object: 0.3815
     Episode_Reward/lifting_object: 4.4750
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 47.7500
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.82s
                      Time elapsed: 00:23:31
                               ETA: 01:08:58

################################################################################
                     [1m Learning iteration 509/2000 [0m                      

                       Computation: 35057 steps/s (collection: 2.556s, learning 0.248s)
             Mean action noise std: 2.58
          Mean value_function loss: 45.7226
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 61.3298
                       Mean reward: 24.29
               Mean episode length: 81.10
    Episode_Reward/reaching_object: 0.3656
     Episode_Reward/lifting_object: 4.4902
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 47.2917
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 2.80s
                      Time elapsed: 00:23:34
                               ETA: 01:08:55

################################################################################
                     [1m Learning iteration 510/2000 [0m                      

                       Computation: 35450 steps/s (collection: 2.571s, learning 0.202s)
             Mean action noise std: 2.58
          Mean value_function loss: 32.5415
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 61.3326
                       Mean reward: 24.70
               Mean episode length: 78.96
    Episode_Reward/reaching_object: 0.3757
     Episode_Reward/lifting_object: 4.6913
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 2.3333
     Episode_Termination/robot_out: 46.0417
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 2.77s
                      Time elapsed: 00:23:37
                               ETA: 01:08:52

################################################################################
                     [1m Learning iteration 511/2000 [0m                      

                       Computation: 35716 steps/s (collection: 2.500s, learning 0.253s)
             Mean action noise std: 2.58
          Mean value_function loss: 32.5628
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 61.3364
                       Mean reward: 24.83
               Mean episode length: 77.93
    Episode_Reward/reaching_object: 0.3750
     Episode_Reward/lifting_object: 4.6184
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 49.2917
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 2.75s
                      Time elapsed: 00:23:40
                               ETA: 01:08:50

################################################################################
                     [1m Learning iteration 512/2000 [0m                      

                       Computation: 35242 steps/s (collection: 2.582s, learning 0.207s)
             Mean action noise std: 2.58
          Mean value_function loss: 34.5673
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 61.3419
                       Mean reward: 22.41
               Mean episode length: 87.48
    Episode_Reward/reaching_object: 0.3782
     Episode_Reward/lifting_object: 4.7409
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 48.5833
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 2.79s
                      Time elapsed: 00:23:42
                               ETA: 01:08:47

################################################################################
                     [1m Learning iteration 513/2000 [0m                      

                       Computation: 34097 steps/s (collection: 2.649s, learning 0.234s)
             Mean action noise std: 2.58
          Mean value_function loss: 29.1198
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.3488
                       Mean reward: 18.37
               Mean episode length: 76.08
    Episode_Reward/reaching_object: 0.3692
     Episode_Reward/lifting_object: 4.4571
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0264
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 2.5417
     Episode_Termination/robot_out: 47.3750
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 2.88s
                      Time elapsed: 00:23:45
                               ETA: 01:08:44

################################################################################
                     [1m Learning iteration 514/2000 [0m                      

                       Computation: 33714 steps/s (collection: 2.794s, learning 0.122s)
             Mean action noise std: 2.58
          Mean value_function loss: 31.3318
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 61.3534
                       Mean reward: 23.60
               Mean episode length: 76.25
    Episode_Reward/reaching_object: 0.3720
     Episode_Reward/lifting_object: 4.7183
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 46.4583
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.92s
                      Time elapsed: 00:23:48
                               ETA: 01:08:42

################################################################################
                     [1m Learning iteration 515/2000 [0m                      

                       Computation: 36970 steps/s (collection: 2.498s, learning 0.161s)
             Mean action noise std: 2.58
          Mean value_function loss: 30.6252
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.3608
                       Mean reward: 22.70
               Mean episode length: 76.91
    Episode_Reward/reaching_object: 0.3636
     Episode_Reward/lifting_object: 4.7267
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 3.0417
     Episode_Termination/robot_out: 50.6250
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 2.66s
                      Time elapsed: 00:23:51
                               ETA: 01:08:39

################################################################################
                     [1m Learning iteration 516/2000 [0m                      

                       Computation: 37681 steps/s (collection: 2.424s, learning 0.185s)
             Mean action noise std: 2.58
          Mean value_function loss: 34.5346
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 61.3677
                       Mean reward: 29.27
               Mean episode length: 80.49
    Episode_Reward/reaching_object: 0.3698
     Episode_Reward/lifting_object: 4.9348
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 49.5000
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 2.61s
                      Time elapsed: 00:23:54
                               ETA: 01:08:36

################################################################################
                     [1m Learning iteration 517/2000 [0m                      

                       Computation: 33991 steps/s (collection: 2.680s, learning 0.212s)
             Mean action noise std: 2.58
          Mean value_function loss: 31.3754
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 61.3721
                       Mean reward: 24.86
               Mean episode length: 80.65
    Episode_Reward/reaching_object: 0.3588
     Episode_Reward/lifting_object: 4.6914
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 2.8333
     Episode_Termination/robot_out: 50.1667
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 2.89s
                      Time elapsed: 00:23:56
                               ETA: 01:08:33

################################################################################
                     [1m Learning iteration 518/2000 [0m                      

                       Computation: 38410 steps/s (collection: 2.380s, learning 0.179s)
             Mean action noise std: 2.58
          Mean value_function loss: 43.1094
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 61.3781
                       Mean reward: 25.92
               Mean episode length: 82.24
    Episode_Reward/reaching_object: 0.3587
     Episode_Reward/lifting_object: 4.6604
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 2.4583
     Episode_Termination/robot_out: 49.5833
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.56s
                      Time elapsed: 00:23:59
                               ETA: 01:08:30

################################################################################
                     [1m Learning iteration 519/2000 [0m                      

                       Computation: 38391 steps/s (collection: 2.373s, learning 0.188s)
             Mean action noise std: 2.58
          Mean value_function loss: 46.3237
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 61.3843
                       Mean reward: 26.38
               Mean episode length: 75.93
    Episode_Reward/reaching_object: 0.3458
     Episode_Reward/lifting_object: 4.5359
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 49.5000
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 2.56s
                      Time elapsed: 00:24:02
                               ETA: 01:08:26

################################################################################
                     [1m Learning iteration 520/2000 [0m                      

                       Computation: 33756 steps/s (collection: 2.660s, learning 0.253s)
             Mean action noise std: 2.58
          Mean value_function loss: 31.8451
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 61.3884
                       Mean reward: 30.75
               Mean episode length: 89.69
    Episode_Reward/reaching_object: 0.3727
     Episode_Reward/lifting_object: 5.0327
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 49.5833
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 2.91s
                      Time elapsed: 00:24:04
                               ETA: 01:08:24

################################################################################
                     [1m Learning iteration 521/2000 [0m                      

                       Computation: 30770 steps/s (collection: 3.000s, learning 0.195s)
             Mean action noise std: 2.58
          Mean value_function loss: 30.5878
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 61.3944
                       Mean reward: 26.53
               Mean episode length: 85.96
    Episode_Reward/reaching_object: 0.3593
     Episode_Reward/lifting_object: 4.9954
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 49.2083
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 3.19s
                      Time elapsed: 00:24:08
                               ETA: 01:08:23

################################################################################
                     [1m Learning iteration 522/2000 [0m                      

                       Computation: 34452 steps/s (collection: 2.642s, learning 0.211s)
             Mean action noise std: 2.58
          Mean value_function loss: 31.4861
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 61.4000
                       Mean reward: 29.20
               Mean episode length: 84.75
    Episode_Reward/reaching_object: 0.3609
     Episode_Reward/lifting_object: 5.0330
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 2.6667
     Episode_Termination/robot_out: 50.0417
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.85s
                      Time elapsed: 00:24:10
                               ETA: 01:08:20

################################################################################
                     [1m Learning iteration 523/2000 [0m                      

                       Computation: 32979 steps/s (collection: 2.786s, learning 0.194s)
             Mean action noise std: 2.58
          Mean value_function loss: 29.8170
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 61.4043
                       Mean reward: 28.76
               Mean episode length: 82.57
    Episode_Reward/reaching_object: 0.3771
     Episode_Reward/lifting_object: 5.1716
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 48.9583
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 2.98s
                      Time elapsed: 00:24:13
                               ETA: 01:08:18

################################################################################
                     [1m Learning iteration 524/2000 [0m                      

                       Computation: 36438 steps/s (collection: 2.477s, learning 0.221s)
             Mean action noise std: 2.58
          Mean value_function loss: 33.9404
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 61.4105
                       Mean reward: 30.29
               Mean episode length: 84.53
    Episode_Reward/reaching_object: 0.3651
     Episode_Reward/lifting_object: 5.1124
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 50.2500
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 2.70s
                      Time elapsed: 00:24:16
                               ETA: 01:08:15

################################################################################
                     [1m Learning iteration 525/2000 [0m                      

                       Computation: 37277 steps/s (collection: 2.453s, learning 0.184s)
             Mean action noise std: 2.59
          Mean value_function loss: 31.9489
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 61.4168
                       Mean reward: 25.74
               Mean episode length: 83.75
    Episode_Reward/reaching_object: 0.3601
     Episode_Reward/lifting_object: 4.9739
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.2500
     Episode_Termination/robot_out: 51.0417
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 2.64s
                      Time elapsed: 00:24:19
                               ETA: 01:08:12

################################################################################
                     [1m Learning iteration 526/2000 [0m                      

                       Computation: 36281 steps/s (collection: 2.559s, learning 0.150s)
             Mean action noise std: 2.59
          Mean value_function loss: 30.0818
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 61.4223
                       Mean reward: 27.80
               Mean episode length: 79.00
    Episode_Reward/reaching_object: 0.3549
     Episode_Reward/lifting_object: 5.1467
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 2.5000
     Episode_Termination/robot_out: 51.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.71s
                      Time elapsed: 00:24:22
                               ETA: 01:08:09

################################################################################
                     [1m Learning iteration 527/2000 [0m                      

                       Computation: 35966 steps/s (collection: 2.550s, learning 0.183s)
             Mean action noise std: 2.59
          Mean value_function loss: 40.8198
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 61.4268
                       Mean reward: 26.21
               Mean episode length: 74.33
    Episode_Reward/reaching_object: 0.3566
     Episode_Reward/lifting_object: 5.2591
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 2.3333
     Episode_Termination/robot_out: 51.5417
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 2.73s
                      Time elapsed: 00:24:24
                               ETA: 01:08:06

################################################################################
                     [1m Learning iteration 528/2000 [0m                      

                       Computation: 32837 steps/s (collection: 2.817s, learning 0.177s)
             Mean action noise std: 2.59
          Mean value_function loss: 45.7528
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 61.4335
                       Mean reward: 21.47
               Mean episode length: 72.39
    Episode_Reward/reaching_object: 0.3400
     Episode_Reward/lifting_object: 4.7182
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.7917
     Episode_Termination/robot_out: 50.1667
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 2.99s
                      Time elapsed: 00:24:27
                               ETA: 01:08:04

################################################################################
                     [1m Learning iteration 529/2000 [0m                      

                       Computation: 33054 steps/s (collection: 2.755s, learning 0.219s)
             Mean action noise std: 2.59
          Mean value_function loss: 34.0629
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 61.4415
                       Mean reward: 25.61
               Mean episode length: 74.12
    Episode_Reward/reaching_object: 0.3483
     Episode_Reward/lifting_object: 5.1260
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 52.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 2.97s
                      Time elapsed: 00:24:30
                               ETA: 01:08:01

################################################################################
                     [1m Learning iteration 530/2000 [0m                      

                       Computation: 32615 steps/s (collection: 2.853s, learning 0.161s)
             Mean action noise std: 2.59
          Mean value_function loss: 36.7839
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 61.4484
                       Mean reward: 25.18
               Mean episode length: 75.06
    Episode_Reward/reaching_object: 0.3500
     Episode_Reward/lifting_object: 5.1965
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 52.1667
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 3.01s
                      Time elapsed: 00:24:33
                               ETA: 01:07:59

################################################################################
                     [1m Learning iteration 531/2000 [0m                      

                       Computation: 35786 steps/s (collection: 2.568s, learning 0.179s)
             Mean action noise std: 2.59
          Mean value_function loss: 34.1290
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 61.4540
                       Mean reward: 28.50
               Mean episode length: 81.49
    Episode_Reward/reaching_object: 0.3555
     Episode_Reward/lifting_object: 5.3058
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0244
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 52.7917
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 2.75s
                      Time elapsed: 00:24:36
                               ETA: 01:07:56

################################################################################
                     [1m Learning iteration 532/2000 [0m                      

                       Computation: 35836 steps/s (collection: 2.538s, learning 0.205s)
             Mean action noise std: 2.59
          Mean value_function loss: 44.0298
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 61.4609
                       Mean reward: 27.30
               Mean episode length: 73.77
    Episode_Reward/reaching_object: 0.3572
     Episode_Reward/lifting_object: 5.4230
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 51.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.74s
                      Time elapsed: 00:24:39
                               ETA: 01:07:54

################################################################################
                     [1m Learning iteration 533/2000 [0m                      

                       Computation: 38059 steps/s (collection: 2.425s, learning 0.158s)
             Mean action noise std: 2.59
          Mean value_function loss: 38.6229
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 61.4710
                       Mean reward: 28.62
               Mean episode length: 71.55
    Episode_Reward/reaching_object: 0.3561
     Episode_Reward/lifting_object: 5.1797
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 2.6250
     Episode_Termination/robot_out: 53.5000
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 2.58s
                      Time elapsed: 00:24:41
                               ETA: 01:07:50

################################################################################
                     [1m Learning iteration 534/2000 [0m                      

                       Computation: 37839 steps/s (collection: 2.439s, learning 0.159s)
             Mean action noise std: 2.59
          Mean value_function loss: 46.6685
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.4792
                       Mean reward: 27.71
               Mean episode length: 78.59
    Episode_Reward/reaching_object: 0.3474
     Episode_Reward/lifting_object: 5.1487
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 53.1667
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 2.60s
                      Time elapsed: 00:24:44
                               ETA: 01:07:47

################################################################################
                     [1m Learning iteration 535/2000 [0m                      

                       Computation: 36226 steps/s (collection: 2.518s, learning 0.196s)
             Mean action noise std: 2.59
          Mean value_function loss: 48.5154
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 61.4854
                       Mean reward: 29.16
               Mean episode length: 73.83
    Episode_Reward/reaching_object: 0.3449
     Episode_Reward/lifting_object: 5.1810
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 51.8750
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 2.71s
                      Time elapsed: 00:24:47
                               ETA: 01:07:44

################################################################################
                     [1m Learning iteration 536/2000 [0m                      

                       Computation: 38199 steps/s (collection: 2.401s, learning 0.173s)
             Mean action noise std: 2.59
          Mean value_function loss: 42.4576
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 61.4943
                       Mean reward: 31.31
               Mean episode length: 72.23
    Episode_Reward/reaching_object: 0.3447
     Episode_Reward/lifting_object: 5.2168
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 2.7083
     Episode_Termination/robot_out: 55.4583
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.57s
                      Time elapsed: 00:24:49
                               ETA: 01:07:41

################################################################################
                     [1m Learning iteration 537/2000 [0m                      

                       Computation: 33403 steps/s (collection: 2.774s, learning 0.169s)
             Mean action noise std: 2.59
          Mean value_function loss: 42.9175
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 61.5014
                       Mean reward: 27.43
               Mean episode length: 71.31
    Episode_Reward/reaching_object: 0.3435
     Episode_Reward/lifting_object: 5.4472
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 53.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 2.94s
                      Time elapsed: 00:24:52
                               ETA: 01:07:38

################################################################################
                     [1m Learning iteration 538/2000 [0m                      

                       Computation: 37386 steps/s (collection: 2.441s, learning 0.189s)
             Mean action noise std: 2.60
          Mean value_function loss: 39.4296
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 61.5114
                       Mean reward: 30.83
               Mean episode length: 77.97
    Episode_Reward/reaching_object: 0.3439
     Episode_Reward/lifting_object: 5.3424
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.5000
     Episode_Termination/robot_out: 53.1667
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.63s
                      Time elapsed: 00:24:55
                               ETA: 01:07:35

################################################################################
                     [1m Learning iteration 539/2000 [0m                      

                       Computation: 38488 steps/s (collection: 2.411s, learning 0.143s)
             Mean action noise std: 2.60
          Mean value_function loss: 41.5256
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 61.5197
                       Mean reward: 28.68
               Mean episode length: 78.09
    Episode_Reward/reaching_object: 0.3539
     Episode_Reward/lifting_object: 5.7015
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 54.2500
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 2.55s
                      Time elapsed: 00:24:57
                               ETA: 01:07:32

################################################################################
                     [1m Learning iteration 540/2000 [0m                      

                       Computation: 37777 steps/s (collection: 2.414s, learning 0.188s)
             Mean action noise std: 2.60
          Mean value_function loss: 41.6940
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 61.5280
                       Mean reward: 28.82
               Mean episode length: 72.45
    Episode_Reward/reaching_object: 0.3390
     Episode_Reward/lifting_object: 5.5755
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 55.2500
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.60s
                      Time elapsed: 00:25:00
                               ETA: 01:07:29

################################################################################
                     [1m Learning iteration 541/2000 [0m                      

                       Computation: 36843 steps/s (collection: 2.474s, learning 0.194s)
             Mean action noise std: 2.60
          Mean value_function loss: 40.9057
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 61.5353
                       Mean reward: 30.27
               Mean episode length: 74.25
    Episode_Reward/reaching_object: 0.3403
     Episode_Reward/lifting_object: 5.3645
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.7083
     Episode_Termination/robot_out: 56.0833
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 2.67s
                      Time elapsed: 00:25:03
                               ETA: 01:07:26

################################################################################
                     [1m Learning iteration 542/2000 [0m                      

                       Computation: 36621 steps/s (collection: 2.510s, learning 0.174s)
             Mean action noise std: 2.60
          Mean value_function loss: 49.8703
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 61.5431
                       Mean reward: 27.54
               Mean episode length: 67.88
    Episode_Reward/reaching_object: 0.3463
     Episode_Reward/lifting_object: 5.8223
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 54.2083
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.68s
                      Time elapsed: 00:25:05
                               ETA: 01:07:23

################################################################################
                     [1m Learning iteration 543/2000 [0m                      

                       Computation: 39196 steps/s (collection: 2.359s, learning 0.149s)
             Mean action noise std: 2.60
          Mean value_function loss: 45.8008
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 61.5490
                       Mean reward: 34.36
               Mean episode length: 72.05
    Episode_Reward/reaching_object: 0.3381
     Episode_Reward/lifting_object: 5.3820
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 55.5000
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 2.51s
                      Time elapsed: 00:25:08
                               ETA: 01:07:19

################################################################################
                     [1m Learning iteration 544/2000 [0m                      

                       Computation: 38732 steps/s (collection: 2.397s, learning 0.141s)
             Mean action noise std: 2.60
          Mean value_function loss: 46.4337
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 61.5527
                       Mean reward: 30.38
               Mean episode length: 73.90
    Episode_Reward/reaching_object: 0.3441
     Episode_Reward/lifting_object: 5.6931
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 54.8333
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.54s
                      Time elapsed: 00:25:10
                               ETA: 01:07:16

################################################################################
                     [1m Learning iteration 545/2000 [0m                      

                       Computation: 39038 steps/s (collection: 2.343s, learning 0.175s)
             Mean action noise std: 2.60
          Mean value_function loss: 48.4196
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 61.5545
                       Mean reward: 32.18
               Mean episode length: 75.26
    Episode_Reward/reaching_object: 0.3338
     Episode_Reward/lifting_object: 5.4021
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 57.0417
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 2.52s
                      Time elapsed: 00:25:13
                               ETA: 01:07:12

################################################################################
                     [1m Learning iteration 546/2000 [0m                      

                       Computation: 40368 steps/s (collection: 2.304s, learning 0.132s)
             Mean action noise std: 2.60
          Mean value_function loss: 39.6577
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 61.5564
                       Mean reward: 34.49
               Mean episode length: 73.74
    Episode_Reward/reaching_object: 0.3415
     Episode_Reward/lifting_object: 5.7798
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 54.7500
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.44s
                      Time elapsed: 00:25:15
                               ETA: 01:07:09

################################################################################
                     [1m Learning iteration 547/2000 [0m                      

                       Computation: 38687 steps/s (collection: 2.382s, learning 0.159s)
             Mean action noise std: 2.60
          Mean value_function loss: 48.8257
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 61.5581
                       Mean reward: 32.84
               Mean episode length: 73.66
    Episode_Reward/reaching_object: 0.3416
     Episode_Reward/lifting_object: 5.8024
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 56.9167
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 2.54s
                      Time elapsed: 00:25:18
                               ETA: 01:07:05

################################################################################
                     [1m Learning iteration 548/2000 [0m                      

                       Computation: 38501 steps/s (collection: 2.419s, learning 0.134s)
             Mean action noise std: 2.60
          Mean value_function loss: 53.5629
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 61.5591
                       Mean reward: 31.05
               Mean episode length: 72.69
    Episode_Reward/reaching_object: 0.3447
     Episode_Reward/lifting_object: 5.9008
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 54.3333
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 2.55s
                      Time elapsed: 00:25:20
                               ETA: 01:07:02

################################################################################
                     [1m Learning iteration 549/2000 [0m                      

                       Computation: 37985 steps/s (collection: 2.428s, learning 0.160s)
             Mean action noise std: 2.60
          Mean value_function loss: 55.7719
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 61.5596
                       Mean reward: 28.05
               Mean episode length: 67.19
    Episode_Reward/reaching_object: 0.3402
     Episode_Reward/lifting_object: 5.6835
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 55.5833
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.59s
                      Time elapsed: 00:25:23
                               ETA: 01:06:59

################################################################################
                     [1m Learning iteration 550/2000 [0m                      

                       Computation: 39647 steps/s (collection: 2.343s, learning 0.136s)
             Mean action noise std: 2.60
          Mean value_function loss: 49.5982
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 61.5598
                       Mean reward: 33.73
               Mean episode length: 74.03
    Episode_Reward/reaching_object: 0.3405
     Episode_Reward/lifting_object: 5.8229
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 56.5000
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.48s
                      Time elapsed: 00:25:25
                               ETA: 01:06:55

################################################################################
                     [1m Learning iteration 551/2000 [0m                      

                       Computation: 38088 steps/s (collection: 2.461s, learning 0.120s)
             Mean action noise std: 2.60
          Mean value_function loss: 55.3519
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 61.5599
                       Mean reward: 29.75
               Mean episode length: 71.51
    Episode_Reward/reaching_object: 0.3523
     Episode_Reward/lifting_object: 5.9464
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 56.2083
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 2.58s
                      Time elapsed: 00:25:28
                               ETA: 01:06:52

################################################################################
                     [1m Learning iteration 552/2000 [0m                      

                       Computation: 36817 steps/s (collection: 2.468s, learning 0.202s)
             Mean action noise std: 2.60
          Mean value_function loss: 46.9494
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 61.5601
                       Mean reward: 29.69
               Mean episode length: 69.00
    Episode_Reward/reaching_object: 0.3436
     Episode_Reward/lifting_object: 5.8286
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 56.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.67s
                      Time elapsed: 00:25:31
                               ETA: 01:06:49

################################################################################
                     [1m Learning iteration 553/2000 [0m                      

                       Computation: 36455 steps/s (collection: 2.561s, learning 0.135s)
             Mean action noise std: 2.60
          Mean value_function loss: 44.1431
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 61.5603
                       Mean reward: 27.22
               Mean episode length: 68.01
    Episode_Reward/reaching_object: 0.3375
     Episode_Reward/lifting_object: 5.7665
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 57.0417
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.70s
                      Time elapsed: 00:25:33
                               ETA: 01:06:46

################################################################################
                     [1m Learning iteration 554/2000 [0m                      

                       Computation: 37941 steps/s (collection: 2.404s, learning 0.187s)
             Mean action noise std: 2.60
          Mean value_function loss: 49.7310
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 61.5605
                       Mean reward: 33.04
               Mean episode length: 71.99
    Episode_Reward/reaching_object: 0.3357
     Episode_Reward/lifting_object: 5.6012
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 55.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.59s
                      Time elapsed: 00:25:36
                               ETA: 01:06:43

################################################################################
                     [1m Learning iteration 555/2000 [0m                      

                       Computation: 38254 steps/s (collection: 2.396s, learning 0.174s)
             Mean action noise std: 2.60
          Mean value_function loss: 46.5757
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 61.5607
                       Mean reward: 29.17
               Mean episode length: 66.91
    Episode_Reward/reaching_object: 0.3457
     Episode_Reward/lifting_object: 5.8635
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 56.5833
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 2.57s
                      Time elapsed: 00:25:39
                               ETA: 01:06:39

################################################################################
                     [1m Learning iteration 556/2000 [0m                      

                       Computation: 39801 steps/s (collection: 2.360s, learning 0.110s)
             Mean action noise std: 2.60
          Mean value_function loss: 51.2378
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 61.5610
                       Mean reward: 29.52
               Mean episode length: 69.29
    Episode_Reward/reaching_object: 0.3328
     Episode_Reward/lifting_object: 5.7733
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 57.4167
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 2.47s
                      Time elapsed: 00:25:41
                               ETA: 01:06:36

################################################################################
                     [1m Learning iteration 557/2000 [0m                      

                       Computation: 38871 steps/s (collection: 2.409s, learning 0.120s)
             Mean action noise std: 2.60
          Mean value_function loss: 48.7290
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 61.5612
                       Mean reward: 29.84
               Mean episode length: 70.13
    Episode_Reward/reaching_object: 0.3318
     Episode_Reward/lifting_object: 5.5632
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 56.2083
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 2.53s
                      Time elapsed: 00:25:44
                               ETA: 01:06:32

################################################################################
                     [1m Learning iteration 558/2000 [0m                      

                       Computation: 36774 steps/s (collection: 2.495s, learning 0.178s)
             Mean action noise std: 2.60
          Mean value_function loss: 50.6517
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 61.5614
                       Mean reward: 30.88
               Mean episode length: 74.09
    Episode_Reward/reaching_object: 0.3363
     Episode_Reward/lifting_object: 5.7568
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 56.5000
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 2.67s
                      Time elapsed: 00:25:46
                               ETA: 01:06:29

################################################################################
                     [1m Learning iteration 559/2000 [0m                      

                       Computation: 37744 steps/s (collection: 2.434s, learning 0.170s)
             Mean action noise std: 2.60
          Mean value_function loss: 46.2991
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 61.5615
                       Mean reward: 32.87
               Mean episode length: 73.35
    Episode_Reward/reaching_object: 0.3332
     Episode_Reward/lifting_object: 5.9445
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 57.4583
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 2.60s
                      Time elapsed: 00:25:49
                               ETA: 01:06:26

################################################################################
                     [1m Learning iteration 560/2000 [0m                      

                       Computation: 37768 steps/s (collection: 2.465s, learning 0.138s)
             Mean action noise std: 2.60
          Mean value_function loss: 44.3792
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 61.5617
                       Mean reward: 29.09
               Mean episode length: 71.34
    Episode_Reward/reaching_object: 0.3332
     Episode_Reward/lifting_object: 5.7647
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 56.3750
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 2.60s
                      Time elapsed: 00:25:51
                               ETA: 01:06:23

################################################################################
                     [1m Learning iteration 561/2000 [0m                      

                       Computation: 38465 steps/s (collection: 2.411s, learning 0.145s)
             Mean action noise std: 2.60
          Mean value_function loss: 46.5491
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 61.5618
                       Mean reward: 32.45
               Mean episode length: 72.17
    Episode_Reward/reaching_object: 0.3344
     Episode_Reward/lifting_object: 5.9244
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 59.2500
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 2.56s
                      Time elapsed: 00:25:54
                               ETA: 01:06:20

################################################################################
                     [1m Learning iteration 562/2000 [0m                      

                       Computation: 39344 steps/s (collection: 2.347s, learning 0.152s)
             Mean action noise std: 2.60
          Mean value_function loss: 40.9655
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 61.5620
                       Mean reward: 31.04
               Mean episode length: 70.33
    Episode_Reward/reaching_object: 0.3338
     Episode_Reward/lifting_object: 6.0157
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 56.9167
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 2.50s
                      Time elapsed: 00:25:56
                               ETA: 01:06:16

################################################################################
                     [1m Learning iteration 563/2000 [0m                      

                       Computation: 37742 steps/s (collection: 2.447s, learning 0.158s)
             Mean action noise std: 2.60
          Mean value_function loss: 52.5416
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 61.5622
                       Mean reward: 28.26
               Mean episode length: 63.29
    Episode_Reward/reaching_object: 0.3223
     Episode_Reward/lifting_object: 5.6639
      Episode_Reward/object_height: 0.0008
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 57.4583
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 2.60s
                      Time elapsed: 00:25:59
                               ETA: 01:06:13

################################################################################
                     [1m Learning iteration 564/2000 [0m                      

                       Computation: 39570 steps/s (collection: 2.341s, learning 0.144s)
             Mean action noise std: 2.60
          Mean value_function loss: 50.6062
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 61.5623
                       Mean reward: 27.54
               Mean episode length: 68.95
    Episode_Reward/reaching_object: 0.3240
     Episode_Reward/lifting_object: 5.6622
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 56.6667
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 2.48s
                      Time elapsed: 00:26:02
                               ETA: 01:06:10

################################################################################
                     [1m Learning iteration 565/2000 [0m                      

                       Computation: 38822 steps/s (collection: 2.400s, learning 0.133s)
             Mean action noise std: 2.60
          Mean value_function loss: 42.7321
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 61.5625
                       Mean reward: 35.40
               Mean episode length: 74.96
    Episode_Reward/reaching_object: 0.3345
     Episode_Reward/lifting_object: 5.9914
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 59.6667
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 2.53s
                      Time elapsed: 00:26:04
                               ETA: 01:06:06

################################################################################
                     [1m Learning iteration 566/2000 [0m                      

                       Computation: 38436 steps/s (collection: 2.387s, learning 0.171s)
             Mean action noise std: 2.60
          Mean value_function loss: 49.3881
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 61.5627
                       Mean reward: 28.82
               Mean episode length: 68.17
    Episode_Reward/reaching_object: 0.3221
     Episode_Reward/lifting_object: 5.8240
      Episode_Reward/object_height: 0.0008
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0218
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 58.2083
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 2.56s
                      Time elapsed: 00:26:07
                               ETA: 01:06:03

################################################################################
                     [1m Learning iteration 567/2000 [0m                      

                       Computation: 39363 steps/s (collection: 2.356s, learning 0.142s)
             Mean action noise std: 2.60
          Mean value_function loss: 45.9186
               Mean surrogate loss: 0.0076
                 Mean entropy loss: 61.5628
                       Mean reward: 30.87
               Mean episode length: 69.20
    Episode_Reward/reaching_object: 0.3238
     Episode_Reward/lifting_object: 5.7897
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3333
     Episode_Termination/robot_out: 58.3750
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 2.50s
                      Time elapsed: 00:26:09
                               ETA: 01:06:00

################################################################################
                     [1m Learning iteration 568/2000 [0m                      

                       Computation: 38150 steps/s (collection: 2.433s, learning 0.144s)
             Mean action noise std: 2.60
          Mean value_function loss: 52.0790
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 61.5630
                       Mean reward: 28.53
               Mean episode length: 64.34
    Episode_Reward/reaching_object: 0.3274
     Episode_Reward/lifting_object: 5.9407
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 59.5417
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 2.58s
                      Time elapsed: 00:26:12
                               ETA: 01:05:56

################################################################################
                     [1m Learning iteration 569/2000 [0m                      

                       Computation: 37342 steps/s (collection: 2.436s, learning 0.196s)
             Mean action noise std: 2.60
          Mean value_function loss: 39.2703
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 61.5633
                       Mean reward: 30.25
               Mean episode length: 70.28
    Episode_Reward/reaching_object: 0.3167
     Episode_Reward/lifting_object: 5.7300
      Episode_Reward/object_height: 0.0008
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 57.7083
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 2.63s
                      Time elapsed: 00:26:14
                               ETA: 01:05:53

################################################################################
                     [1m Learning iteration 570/2000 [0m                      

                       Computation: 37440 steps/s (collection: 2.463s, learning 0.163s)
             Mean action noise std: 2.60
          Mean value_function loss: 48.5661
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 61.5640
                       Mean reward: 31.03
               Mean episode length: 65.89
    Episode_Reward/reaching_object: 0.3195
     Episode_Reward/lifting_object: 6.0552
      Episode_Reward/object_height: 0.0008
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 2.2083
     Episode_Termination/robot_out: 59.0833
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.63s
                      Time elapsed: 00:26:17
                               ETA: 01:05:50

################################################################################
                     [1m Learning iteration 571/2000 [0m                      

                       Computation: 39290 steps/s (collection: 2.305s, learning 0.197s)
             Mean action noise std: 2.60
          Mean value_function loss: 61.1095
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 61.5642
                       Mean reward: 27.87
               Mean episode length: 67.30
    Episode_Reward/reaching_object: 0.3208
     Episode_Reward/lifting_object: 5.5626
      Episode_Reward/object_height: 0.0008
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 59.0833
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 2.50s
                      Time elapsed: 00:26:19
                               ETA: 01:05:47

################################################################################
                     [1m Learning iteration 572/2000 [0m                      

                       Computation: 36943 steps/s (collection: 2.490s, learning 0.171s)
             Mean action noise std: 2.60
          Mean value_function loss: 56.6106
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 61.5644
                       Mean reward: 29.27
               Mean episode length: 71.98
    Episode_Reward/reaching_object: 0.3198
     Episode_Reward/lifting_object: 5.7344
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 57.6667
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 2.66s
                      Time elapsed: 00:26:22
                               ETA: 01:05:44

################################################################################
                     [1m Learning iteration 573/2000 [0m                      

                       Computation: 38894 steps/s (collection: 2.352s, learning 0.175s)
             Mean action noise std: 2.60
          Mean value_function loss: 49.8682
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 61.5648
                       Mean reward: 32.82
               Mean episode length: 70.00
    Episode_Reward/reaching_object: 0.3182
     Episode_Reward/lifting_object: 5.7107
      Episode_Reward/object_height: 0.0008
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 60.5833
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 2.53s
                      Time elapsed: 00:26:25
                               ETA: 01:05:40

################################################################################
                     [1m Learning iteration 574/2000 [0m                      

                       Computation: 34976 steps/s (collection: 2.615s, learning 0.196s)
             Mean action noise std: 2.60
          Mean value_function loss: 58.1066
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 61.5651
                       Mean reward: 28.85
               Mean episode length: 67.72
    Episode_Reward/reaching_object: 0.3126
     Episode_Reward/lifting_object: 5.6665
      Episode_Reward/object_height: 0.0008
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 58.6667
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.81s
                      Time elapsed: 00:26:27
                               ETA: 01:05:38

################################################################################
                     [1m Learning iteration 575/2000 [0m                      

                       Computation: 33707 steps/s (collection: 2.726s, learning 0.191s)
             Mean action noise std: 2.60
          Mean value_function loss: 56.0196
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 61.5654
                       Mean reward: 23.46
               Mean episode length: 61.42
    Episode_Reward/reaching_object: 0.3144
     Episode_Reward/lifting_object: 5.4706
      Episode_Reward/object_height: 0.0008
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 58.9583
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 2.92s
                      Time elapsed: 00:26:30
                               ETA: 01:05:35

################################################################################
                     [1m Learning iteration 576/2000 [0m                      

                       Computation: 35385 steps/s (collection: 2.667s, learning 0.111s)
             Mean action noise std: 2.60
          Mean value_function loss: 56.3148
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 61.5659
                       Mean reward: 32.41
               Mean episode length: 70.43
    Episode_Reward/reaching_object: 0.3190
     Episode_Reward/lifting_object: 5.8547
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.3333
     Episode_Termination/robot_out: 57.2500
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.78s
                      Time elapsed: 00:26:33
                               ETA: 01:05:33

################################################################################
                     [1m Learning iteration 577/2000 [0m                      

                       Computation: 38266 steps/s (collection: 2.435s, learning 0.134s)
             Mean action noise std: 2.60
          Mean value_function loss: 61.2682
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 61.5672
                       Mean reward: 28.46
               Mean episode length: 66.30
    Episode_Reward/reaching_object: 0.3167
     Episode_Reward/lifting_object: 5.8637
      Episode_Reward/object_height: 0.0008
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 58.8333
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 2.57s
                      Time elapsed: 00:26:36
                               ETA: 01:05:29

################################################################################
                     [1m Learning iteration 578/2000 [0m                      

                       Computation: 38615 steps/s (collection: 2.369s, learning 0.177s)
             Mean action noise std: 2.60
          Mean value_function loss: 45.3743
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 61.5702
                       Mean reward: 38.34
               Mean episode length: 72.09
    Episode_Reward/reaching_object: 0.3213
     Episode_Reward/lifting_object: 6.0629
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 2.3333
     Episode_Termination/robot_out: 59.2083
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 2.55s
                      Time elapsed: 00:26:38
                               ETA: 01:05:26

################################################################################
                     [1m Learning iteration 579/2000 [0m                      

                       Computation: 37346 steps/s (collection: 2.448s, learning 0.184s)
             Mean action noise std: 2.60
          Mean value_function loss: 70.4800
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 61.5755
                       Mean reward: 33.09
               Mean episode length: 68.43
    Episode_Reward/reaching_object: 0.3249
     Episode_Reward/lifting_object: 6.2742
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 56.2917
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.63s
                      Time elapsed: 00:26:41
                               ETA: 01:05:23

################################################################################
                     [1m Learning iteration 580/2000 [0m                      

                       Computation: 32354 steps/s (collection: 2.833s, learning 0.205s)
             Mean action noise std: 2.60
          Mean value_function loss: 82.8187
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 61.5837
                       Mean reward: 25.20
               Mean episode length: 67.42
    Episode_Reward/reaching_object: 0.3139
     Episode_Reward/lifting_object: 5.7441
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.7500
     Episode_Termination/robot_out: 58.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 3.04s
                      Time elapsed: 00:26:44
                               ETA: 01:05:21

################################################################################
                     [1m Learning iteration 581/2000 [0m                      

                       Computation: 33822 steps/s (collection: 2.709s, learning 0.197s)
             Mean action noise std: 2.60
          Mean value_function loss: 52.7658
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 61.5892
                       Mean reward: 33.24
               Mean episode length: 70.22
    Episode_Reward/reaching_object: 0.3246
     Episode_Reward/lifting_object: 6.0243
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 58.2500
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 2.91s
                      Time elapsed: 00:26:47
                               ETA: 01:05:18

################################################################################
                     [1m Learning iteration 582/2000 [0m                      

                       Computation: 35556 steps/s (collection: 2.627s, learning 0.138s)
             Mean action noise std: 2.60
          Mean value_function loss: 58.7209
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 61.5919
                       Mean reward: 34.26
               Mean episode length: 69.73
    Episode_Reward/reaching_object: 0.3223
     Episode_Reward/lifting_object: 6.2827
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4167
     Episode_Termination/robot_out: 59.0417
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 2.76s
                      Time elapsed: 00:26:50
                               ETA: 01:05:16

################################################################################
                     [1m Learning iteration 583/2000 [0m                      

                       Computation: 35202 steps/s (collection: 2.586s, learning 0.206s)
             Mean action noise std: 2.60
          Mean value_function loss: 47.4307
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 61.5968
                       Mean reward: 34.80
               Mean episode length: 68.92
    Episode_Reward/reaching_object: 0.3263
     Episode_Reward/lifting_object: 6.2959
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 59.2917
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 2.79s
                      Time elapsed: 00:26:52
                               ETA: 01:05:13

################################################################################
                     [1m Learning iteration 584/2000 [0m                      

                       Computation: 36018 steps/s (collection: 2.537s, learning 0.192s)
             Mean action noise std: 2.61
          Mean value_function loss: 65.5804
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 61.6044
                       Mean reward: 30.27
               Mean episode length: 62.79
    Episode_Reward/reaching_object: 0.3083
     Episode_Reward/lifting_object: 6.1203
      Episode_Reward/object_height: 0.0008
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 57.0000
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.73s
                      Time elapsed: 00:26:55
                               ETA: 01:05:10

################################################################################
                     [1m Learning iteration 585/2000 [0m                      

                       Computation: 33181 steps/s (collection: 2.734s, learning 0.229s)
             Mean action noise std: 2.61
          Mean value_function loss: 52.9595
               Mean surrogate loss: 0.0114
                 Mean entropy loss: 61.6147
                       Mean reward: 32.97
               Mean episode length: 66.55
    Episode_Reward/reaching_object: 0.3203
     Episode_Reward/lifting_object: 6.4230
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 57.4167
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.96s
                      Time elapsed: 00:26:58
                               ETA: 01:05:08

################################################################################
                     [1m Learning iteration 586/2000 [0m                      

                       Computation: 32450 steps/s (collection: 2.864s, learning 0.165s)
             Mean action noise std: 2.61
          Mean value_function loss: 63.2204
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 61.6162
                       Mean reward: 35.02
               Mean episode length: 70.95
    Episode_Reward/reaching_object: 0.3189
     Episode_Reward/lifting_object: 6.3823
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 58.6667
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 3.03s
                      Time elapsed: 00:27:01
                               ETA: 01:05:06

################################################################################
                     [1m Learning iteration 587/2000 [0m                      

                       Computation: 33573 steps/s (collection: 2.763s, learning 0.165s)
             Mean action noise std: 2.61
          Mean value_function loss: 57.9601
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 61.6188
                       Mean reward: 28.62
               Mean episode length: 63.70
    Episode_Reward/reaching_object: 0.3169
     Episode_Reward/lifting_object: 6.4456
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.6250
     Episode_Termination/robot_out: 57.8333
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 2.93s
                      Time elapsed: 00:27:04
                               ETA: 01:05:03

################################################################################
                     [1m Learning iteration 588/2000 [0m                      

                       Computation: 32473 steps/s (collection: 2.792s, learning 0.236s)
             Mean action noise std: 2.61
          Mean value_function loss: 47.6770
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 61.6226
                       Mean reward: 36.23
               Mean episode length: 62.90
    Episode_Reward/reaching_object: 0.3162
     Episode_Reward/lifting_object: 6.4229
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.5417
     Episode_Termination/robot_out: 57.7083
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 3.03s
                      Time elapsed: 00:27:07
                               ETA: 01:05:01

################################################################################
                     [1m Learning iteration 589/2000 [0m                      

                       Computation: 32412 steps/s (collection: 2.777s, learning 0.256s)
             Mean action noise std: 2.61
          Mean value_function loss: 55.9070
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 61.6274
                       Mean reward: 29.52
               Mean episode length: 69.15
    Episode_Reward/reaching_object: 0.3260
     Episode_Reward/lifting_object: 6.5896
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.1667
     Episode_Termination/robot_out: 57.8750
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 3.03s
                      Time elapsed: 00:27:10
                               ETA: 01:04:59

################################################################################
                     [1m Learning iteration 590/2000 [0m                      

                       Computation: 32064 steps/s (collection: 2.806s, learning 0.260s)
             Mean action noise std: 2.61
          Mean value_function loss: 79.3433
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 61.6339
                       Mean reward: 31.03
               Mean episode length: 67.66
    Episode_Reward/reaching_object: 0.3217
     Episode_Reward/lifting_object: 6.4153
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0233
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 56.1250
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 3.07s
                      Time elapsed: 00:27:13
                               ETA: 01:04:57

################################################################################
                     [1m Learning iteration 591/2000 [0m                      

                       Computation: 33458 steps/s (collection: 2.664s, learning 0.274s)
             Mean action noise std: 2.61
          Mean value_function loss: 70.5794
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 61.6442
                       Mean reward: 40.21
               Mean episode length: 74.13
    Episode_Reward/reaching_object: 0.3176
     Episode_Reward/lifting_object: 6.5438
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 59.2083
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.94s
                      Time elapsed: 00:27:16
                               ETA: 01:04:55

################################################################################
                     [1m Learning iteration 592/2000 [0m                      

                       Computation: 33609 steps/s (collection: 2.686s, learning 0.239s)
             Mean action noise std: 2.61
          Mean value_function loss: 69.5319
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 61.6549
                       Mean reward: 34.51
               Mean episode length: 66.69
    Episode_Reward/reaching_object: 0.3301
     Episode_Reward/lifting_object: 6.7721
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 59.7917
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.92s
                      Time elapsed: 00:27:19
                               ETA: 01:04:52

################################################################################
                     [1m Learning iteration 593/2000 [0m                      

                       Computation: 31469 steps/s (collection: 2.885s, learning 0.239s)
             Mean action noise std: 2.61
          Mean value_function loss: 52.1314
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 61.6624
                       Mean reward: 36.38
               Mean episode length: 72.47
    Episode_Reward/reaching_object: 0.3262
     Episode_Reward/lifting_object: 6.8006
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 59.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 3.12s
                      Time elapsed: 00:27:22
                               ETA: 01:04:51

################################################################################
                     [1m Learning iteration 594/2000 [0m                      

                       Computation: 33056 steps/s (collection: 2.679s, learning 0.295s)
             Mean action noise std: 2.61
          Mean value_function loss: 64.9503
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 61.6738
                       Mean reward: 32.46
               Mean episode length: 62.91
    Episode_Reward/reaching_object: 0.3238
     Episode_Reward/lifting_object: 6.9179
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 58.1667
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.97s
                      Time elapsed: 00:27:25
                               ETA: 01:04:48

################################################################################
                     [1m Learning iteration 595/2000 [0m                      

                       Computation: 36088 steps/s (collection: 2.560s, learning 0.164s)
             Mean action noise std: 2.61
          Mean value_function loss: 71.0137
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 61.6835
                       Mean reward: 40.06
               Mean episode length: 74.40
    Episode_Reward/reaching_object: 0.3192
     Episode_Reward/lifting_object: 6.4741
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 57.4583
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.72s
                      Time elapsed: 00:27:28
                               ETA: 01:04:45

################################################################################
                     [1m Learning iteration 596/2000 [0m                      

                       Computation: 37533 steps/s (collection: 2.483s, learning 0.137s)
             Mean action noise std: 2.61
          Mean value_function loss: 59.8572
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 61.6899
                       Mean reward: 36.53
               Mean episode length: 61.90
    Episode_Reward/reaching_object: 0.3163
     Episode_Reward/lifting_object: 6.5515
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 59.8333
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.62s
                      Time elapsed: 00:27:31
                               ETA: 01:04:42

################################################################################
                     [1m Learning iteration 597/2000 [0m                      

                       Computation: 37766 steps/s (collection: 2.407s, learning 0.196s)
             Mean action noise std: 2.62
          Mean value_function loss: 71.8781
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 61.7011
                       Mean reward: 38.10
               Mean episode length: 74.37
    Episode_Reward/reaching_object: 0.3252
     Episode_Reward/lifting_object: 6.7767
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 58.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 2.60s
                      Time elapsed: 00:27:33
                               ETA: 01:04:39

################################################################################
                     [1m Learning iteration 598/2000 [0m                      

                       Computation: 34166 steps/s (collection: 2.704s, learning 0.173s)
             Mean action noise std: 2.62
          Mean value_function loss: 66.0027
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 61.7126
                       Mean reward: 34.97
               Mean episode length: 67.54
    Episode_Reward/reaching_object: 0.3229
     Episode_Reward/lifting_object: 6.8736
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.1250
     Episode_Termination/robot_out: 59.8333
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 2.88s
                      Time elapsed: 00:27:36
                               ETA: 01:04:37

################################################################################
                     [1m Learning iteration 599/2000 [0m                      

                       Computation: 37225 steps/s (collection: 2.444s, learning 0.197s)
             Mean action noise std: 2.62
          Mean value_function loss: 54.7608
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 61.7196
                       Mean reward: 31.59
               Mean episode length: 65.29
    Episode_Reward/reaching_object: 0.3239
     Episode_Reward/lifting_object: 6.8041
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 59.3333
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.64s
                      Time elapsed: 00:27:39
                               ETA: 01:04:34

################################################################################
                     [1m Learning iteration 600/2000 [0m                      

                       Computation: 37508 steps/s (collection: 2.448s, learning 0.173s)
             Mean action noise std: 2.62
          Mean value_function loss: 60.1395
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.7266
                       Mean reward: 35.24
               Mean episode length: 65.28
    Episode_Reward/reaching_object: 0.3149
     Episode_Reward/lifting_object: 6.6071
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 60.5833
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.62s
                      Time elapsed: 00:27:41
                               ETA: 01:04:30

################################################################################
                     [1m Learning iteration 601/2000 [0m                      

                       Computation: 38197 steps/s (collection: 2.453s, learning 0.121s)
             Mean action noise std: 2.62
          Mean value_function loss: 50.2669
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 61.7343
                       Mean reward: 36.37
               Mean episode length: 65.54
    Episode_Reward/reaching_object: 0.3201
     Episode_Reward/lifting_object: 7.0116
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0222
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 56.9167
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.57s
                      Time elapsed: 00:27:44
                               ETA: 01:04:27

################################################################################
                     [1m Learning iteration 602/2000 [0m                      

                       Computation: 37560 steps/s (collection: 2.457s, learning 0.160s)
             Mean action noise std: 2.62
          Mean value_function loss: 68.0514
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 61.7392
                       Mean reward: 28.25
               Mean episode length: 65.63
    Episode_Reward/reaching_object: 0.3151
     Episode_Reward/lifting_object: 6.8193
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 60.6250
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 2.62s
                      Time elapsed: 00:27:46
                               ETA: 01:04:24

################################################################################
                     [1m Learning iteration 603/2000 [0m                      

                       Computation: 35776 steps/s (collection: 2.566s, learning 0.182s)
             Mean action noise std: 2.62
          Mean value_function loss: 77.8174
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 61.7416
                       Mean reward: 28.83
               Mean episode length: 64.30
    Episode_Reward/reaching_object: 0.3235
     Episode_Reward/lifting_object: 6.8643
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 59.7917
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 2.75s
                      Time elapsed: 00:27:49
                               ETA: 01:04:21

################################################################################
                     [1m Learning iteration 604/2000 [0m                      

                       Computation: 36115 steps/s (collection: 2.576s, learning 0.146s)
             Mean action noise std: 2.62
          Mean value_function loss: 61.4537
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 61.7453
                       Mean reward: 36.18
               Mean episode length: 64.28
    Episode_Reward/reaching_object: 0.3275
     Episode_Reward/lifting_object: 7.0595
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 60.1250
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 2.72s
                      Time elapsed: 00:27:52
                               ETA: 01:04:18

################################################################################
                     [1m Learning iteration 605/2000 [0m                      

                       Computation: 33537 steps/s (collection: 2.774s, learning 0.157s)
             Mean action noise std: 2.62
          Mean value_function loss: 59.7587
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 61.7528
                       Mean reward: 37.87
               Mean episode length: 68.84
    Episode_Reward/reaching_object: 0.3206
     Episode_Reward/lifting_object: 7.0260
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 60.9167
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 2.93s
                      Time elapsed: 00:27:55
                               ETA: 01:04:16

################################################################################
                     [1m Learning iteration 606/2000 [0m                      

                       Computation: 35200 steps/s (collection: 2.634s, learning 0.159s)
             Mean action noise std: 2.62
          Mean value_function loss: 57.0352
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 61.7585
                       Mean reward: 40.57
               Mean episode length: 71.85
    Episode_Reward/reaching_object: 0.3307
     Episode_Reward/lifting_object: 7.0473
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 60.4583
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.79s
                      Time elapsed: 00:27:58
                               ETA: 01:04:13

################################################################################
                     [1m Learning iteration 607/2000 [0m                      

                       Computation: 36511 steps/s (collection: 2.527s, learning 0.165s)
             Mean action noise std: 2.62
          Mean value_function loss: 80.5796
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 61.7657
                       Mean reward: 39.26
               Mean episode length: 68.83
    Episode_Reward/reaching_object: 0.3253
     Episode_Reward/lifting_object: 6.9324
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 61.3333
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.69s
                      Time elapsed: 00:28:00
                               ETA: 01:04:10

################################################################################
                     [1m Learning iteration 608/2000 [0m                      

                       Computation: 36077 steps/s (collection: 2.550s, learning 0.175s)
             Mean action noise std: 2.62
          Mean value_function loss: 102.0649
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 61.7743
                       Mean reward: 37.33
               Mean episode length: 67.44
    Episode_Reward/reaching_object: 0.3257
     Episode_Reward/lifting_object: 7.1498
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 59.2083
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.72s
                      Time elapsed: 00:28:03
                               ETA: 01:04:08

################################################################################
                     [1m Learning iteration 609/2000 [0m                      

                       Computation: 37543 steps/s (collection: 2.479s, learning 0.139s)
             Mean action noise std: 2.62
          Mean value_function loss: 74.8294
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 61.7836
                       Mean reward: 38.29
               Mean episode length: 68.86
    Episode_Reward/reaching_object: 0.3277
     Episode_Reward/lifting_object: 7.0005
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 60.6250
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 2.62s
                      Time elapsed: 00:28:06
                               ETA: 01:04:05

################################################################################
                     [1m Learning iteration 610/2000 [0m                      

                       Computation: 38635 steps/s (collection: 2.379s, learning 0.165s)
             Mean action noise std: 2.63
          Mean value_function loss: 65.6593
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 61.7900
                       Mean reward: 37.09
               Mean episode length: 68.23
    Episode_Reward/reaching_object: 0.3288
     Episode_Reward/lifting_object: 7.3132
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 59.7083
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.54s
                      Time elapsed: 00:28:08
                               ETA: 01:04:01

################################################################################
                     [1m Learning iteration 611/2000 [0m                      

                       Computation: 36975 steps/s (collection: 2.475s, learning 0.184s)
             Mean action noise std: 2.63
          Mean value_function loss: 65.5308
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 61.7942
                       Mean reward: 38.39
               Mean episode length: 64.72
    Episode_Reward/reaching_object: 0.3262
     Episode_Reward/lifting_object: 7.4166
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 62.3750
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 2.66s
                      Time elapsed: 00:28:11
                               ETA: 01:03:58

################################################################################
                     [1m Learning iteration 612/2000 [0m                      

                       Computation: 35613 steps/s (collection: 2.613s, learning 0.148s)
             Mean action noise std: 2.63
          Mean value_function loss: 65.0710
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 61.8006
                       Mean reward: 44.09
               Mean episode length: 73.36
    Episode_Reward/reaching_object: 0.3215
     Episode_Reward/lifting_object: 7.3729
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 60.4583
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.76s
                      Time elapsed: 00:28:14
                               ETA: 01:03:55

################################################################################
                     [1m Learning iteration 613/2000 [0m                      

                       Computation: 34527 steps/s (collection: 2.711s, learning 0.137s)
             Mean action noise std: 2.63
          Mean value_function loss: 75.3912
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 61.8046
                       Mean reward: 29.06
               Mean episode length: 60.55
    Episode_Reward/reaching_object: 0.3100
     Episode_Reward/lifting_object: 6.9516
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 61.8333
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 2.85s
                      Time elapsed: 00:28:16
                               ETA: 01:03:53

################################################################################
                     [1m Learning iteration 614/2000 [0m                      

                       Computation: 35388 steps/s (collection: 2.566s, learning 0.212s)
             Mean action noise std: 2.63
          Mean value_function loss: 63.1304
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 61.8105
                       Mean reward: 41.21
               Mean episode length: 69.17
    Episode_Reward/reaching_object: 0.3279
     Episode_Reward/lifting_object: 7.4595
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 65.1250
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.78s
                      Time elapsed: 00:28:19
                               ETA: 01:03:50

################################################################################
                     [1m Learning iteration 615/2000 [0m                      

                       Computation: 35801 steps/s (collection: 2.549s, learning 0.197s)
             Mean action noise std: 2.63
          Mean value_function loss: 56.8005
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 61.8175
                       Mean reward: 38.60
               Mean episode length: 65.04
    Episode_Reward/reaching_object: 0.3083
     Episode_Reward/lifting_object: 7.1124
      Episode_Reward/object_height: 0.0008
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 64.6250
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 2.75s
                      Time elapsed: 00:28:22
                               ETA: 01:03:47

################################################################################
                     [1m Learning iteration 616/2000 [0m                      

                       Computation: 37091 steps/s (collection: 2.450s, learning 0.200s)
             Mean action noise std: 2.63
          Mean value_function loss: 72.6259
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 61.8208
                       Mean reward: 33.32
               Mean episode length: 57.03
    Episode_Reward/reaching_object: 0.2985
     Episode_Reward/lifting_object: 6.9805
      Episode_Reward/object_height: 0.0008
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0214
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 63.2500
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.65s
                      Time elapsed: 00:28:25
                               ETA: 01:03:44

################################################################################
                     [1m Learning iteration 617/2000 [0m                      

                       Computation: 34674 steps/s (collection: 2.668s, learning 0.167s)
             Mean action noise std: 2.63
          Mean value_function loss: 80.3053
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 61.8242
                       Mean reward: 32.81
               Mean episode length: 63.61
    Episode_Reward/reaching_object: 0.3157
     Episode_Reward/lifting_object: 7.2598
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 64.7083
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.84s
                      Time elapsed: 00:28:27
                               ETA: 01:03:42

################################################################################
                     [1m Learning iteration 618/2000 [0m                      

                       Computation: 38046 steps/s (collection: 2.396s, learning 0.188s)
             Mean action noise std: 2.63
          Mean value_function loss: 72.2325
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 61.8297
                       Mean reward: 36.52
               Mean episode length: 58.17
    Episode_Reward/reaching_object: 0.3046
     Episode_Reward/lifting_object: 7.2630
      Episode_Reward/object_height: 0.0008
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 62.9167
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.58s
                      Time elapsed: 00:28:30
                               ETA: 01:03:39

################################################################################
                     [1m Learning iteration 619/2000 [0m                      

                       Computation: 28635 steps/s (collection: 3.115s, learning 0.318s)
             Mean action noise std: 2.63
          Mean value_function loss: 62.8365
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 61.8371
                       Mean reward: 37.04
               Mean episode length: 59.90
    Episode_Reward/reaching_object: 0.3082
     Episode_Reward/lifting_object: 7.2899
      Episode_Reward/object_height: 0.0008
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 62.2083
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 3.43s
                      Time elapsed: 00:28:33
                               ETA: 01:03:37

################################################################################
                     [1m Learning iteration 620/2000 [0m                      

                       Computation: 26350 steps/s (collection: 3.481s, learning 0.249s)
             Mean action noise std: 2.63
          Mean value_function loss: 67.4440
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 61.8405
                       Mean reward: 39.04
               Mean episode length: 62.32
    Episode_Reward/reaching_object: 0.3066
     Episode_Reward/lifting_object: 7.4115
      Episode_Reward/object_height: 0.0008
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 64.5833
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 3.73s
                      Time elapsed: 00:28:37
                               ETA: 01:03:37

################################################################################
                     [1m Learning iteration 621/2000 [0m                      

                       Computation: 31329 steps/s (collection: 2.984s, learning 0.154s)
             Mean action noise std: 2.63
          Mean value_function loss: 65.5483
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 61.8435
                       Mean reward: 39.48
               Mean episode length: 59.89
    Episode_Reward/reaching_object: 0.3116
     Episode_Reward/lifting_object: 7.4695
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 63.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 3.14s
                      Time elapsed: 00:28:40
                               ETA: 01:03:35

################################################################################
                     [1m Learning iteration 622/2000 [0m                      

                       Computation: 37482 steps/s (collection: 2.486s, learning 0.137s)
             Mean action noise std: 2.63
          Mean value_function loss: 87.8199
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 61.8474
                       Mean reward: 34.26
               Mean episode length: 57.09
    Episode_Reward/reaching_object: 0.3093
     Episode_Reward/lifting_object: 7.6144
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.4583
     Episode_Termination/robot_out: 63.9167
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 2.62s
                      Time elapsed: 00:28:43
                               ETA: 01:03:32

################################################################################
                     [1m Learning iteration 623/2000 [0m                      

                       Computation: 36213 steps/s (collection: 2.601s, learning 0.113s)
             Mean action noise std: 2.63
          Mean value_function loss: 77.3200
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 61.8528
                       Mean reward: 36.24
               Mean episode length: 63.25
    Episode_Reward/reaching_object: 0.3079
     Episode_Reward/lifting_object: 7.5491
      Episode_Reward/object_height: 0.0008
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0219
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 62.6250
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 2.71s
                      Time elapsed: 00:28:46
                               ETA: 01:03:29

################################################################################
                     [1m Learning iteration 624/2000 [0m                      

                       Computation: 38219 steps/s (collection: 2.440s, learning 0.133s)
             Mean action noise std: 2.63
          Mean value_function loss: 77.2983
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 61.8581
                       Mean reward: 40.76
               Mean episode length: 61.84
    Episode_Reward/reaching_object: 0.3122
     Episode_Reward/lifting_object: 7.7413
      Episode_Reward/object_height: 0.0008
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0223
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 62.7083
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.57s
                      Time elapsed: 00:28:48
                               ETA: 01:03:26

################################################################################
                     [1m Learning iteration 625/2000 [0m                      

                       Computation: 37433 steps/s (collection: 2.480s, learning 0.146s)
             Mean action noise std: 2.63
          Mean value_function loss: 75.8071
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 61.8649
                       Mean reward: 39.80
               Mean episode length: 60.07
    Episode_Reward/reaching_object: 0.3112
     Episode_Reward/lifting_object: 7.6967
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 64.7500
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 2.63s
                      Time elapsed: 00:28:51
                               ETA: 01:03:23

################################################################################
                     [1m Learning iteration 626/2000 [0m                      

                       Computation: 33561 steps/s (collection: 2.756s, learning 0.173s)
             Mean action noise std: 2.63
          Mean value_function loss: 68.9688
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 61.8767
                       Mean reward: 39.58
               Mean episode length: 60.80
    Episode_Reward/reaching_object: 0.3064
     Episode_Reward/lifting_object: 7.7337
      Episode_Reward/object_height: 0.0008
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 61.4583
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.93s
                      Time elapsed: 00:28:54
                               ETA: 01:03:20

################################################################################
                     [1m Learning iteration 627/2000 [0m                      

                       Computation: 31914 steps/s (collection: 2.897s, learning 0.184s)
             Mean action noise std: 2.64
          Mean value_function loss: 79.0024
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 61.8835
                       Mean reward: 39.26
               Mean episode length: 62.33
    Episode_Reward/reaching_object: 0.3188
     Episode_Reward/lifting_object: 7.8974
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 64.1250
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 3.08s
                      Time elapsed: 00:28:57
                               ETA: 01:03:18

################################################################################
                     [1m Learning iteration 628/2000 [0m                      

                       Computation: 33105 steps/s (collection: 2.843s, learning 0.127s)
             Mean action noise std: 2.64
          Mean value_function loss: 77.9613
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 61.8898
                       Mean reward: 32.73
               Mean episode length: 60.35
    Episode_Reward/reaching_object: 0.3090
     Episode_Reward/lifting_object: 7.6593
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 64.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.97s
                      Time elapsed: 00:29:00
                               ETA: 01:03:16

################################################################################
                     [1m Learning iteration 629/2000 [0m                      

                       Computation: 33358 steps/s (collection: 2.783s, learning 0.164s)
             Mean action noise std: 2.64
          Mean value_function loss: 102.1919
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 61.8950
                       Mean reward: 41.14
               Mean episode length: 62.99
    Episode_Reward/reaching_object: 0.3218
     Episode_Reward/lifting_object: 8.1557
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 62.3333
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 2.95s
                      Time elapsed: 00:29:03
                               ETA: 01:03:13

################################################################################
                     [1m Learning iteration 630/2000 [0m                      

                       Computation: 36888 steps/s (collection: 2.490s, learning 0.175s)
             Mean action noise std: 2.64
          Mean value_function loss: 93.2261
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 61.8989
                       Mean reward: 40.87
               Mean episode length: 61.89
    Episode_Reward/reaching_object: 0.3116
     Episode_Reward/lifting_object: 8.0242
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 62.9583
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.66s
                      Time elapsed: 00:29:05
                               ETA: 01:03:10

################################################################################
                     [1m Learning iteration 631/2000 [0m                      

                       Computation: 35243 steps/s (collection: 2.634s, learning 0.155s)
             Mean action noise std: 2.64
          Mean value_function loss: 73.1689
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 61.9047
                       Mean reward: 38.79
               Mean episode length: 59.29
    Episode_Reward/reaching_object: 0.3080
     Episode_Reward/lifting_object: 7.7613
      Episode_Reward/object_height: 0.0008
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0220
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 63.7083
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.79s
                      Time elapsed: 00:29:08
                               ETA: 01:03:08

################################################################################
                     [1m Learning iteration 632/2000 [0m                      

                       Computation: 38605 steps/s (collection: 2.366s, learning 0.180s)
             Mean action noise std: 2.64
          Mean value_function loss: 72.1973
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 61.9074
                       Mean reward: 43.11
               Mean episode length: 62.19
    Episode_Reward/reaching_object: 0.3169
     Episode_Reward/lifting_object: 8.4311
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 62.0417
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.55s
                      Time elapsed: 00:29:11
                               ETA: 01:03:04

################################################################################
                     [1m Learning iteration 633/2000 [0m                      

                       Computation: 38031 steps/s (collection: 2.427s, learning 0.158s)
             Mean action noise std: 2.64
          Mean value_function loss: 81.7246
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 61.9105
                       Mean reward: 39.81
               Mean episode length: 62.18
    Episode_Reward/reaching_object: 0.3097
     Episode_Reward/lifting_object: 8.1792
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 62.4167
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 2.58s
                      Time elapsed: 00:29:13
                               ETA: 01:03:01

################################################################################
                     [1m Learning iteration 634/2000 [0m                      

                       Computation: 36408 steps/s (collection: 2.540s, learning 0.160s)
             Mean action noise std: 2.64
          Mean value_function loss: 79.9294
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 61.9163
                       Mean reward: 50.24
               Mean episode length: 69.43
    Episode_Reward/reaching_object: 0.3179
     Episode_Reward/lifting_object: 8.3926
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 64.0833
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.70s
                      Time elapsed: 00:29:16
                               ETA: 01:02:58

################################################################################
                     [1m Learning iteration 635/2000 [0m                      

                       Computation: 33592 steps/s (collection: 2.756s, learning 0.170s)
             Mean action noise std: 2.64
          Mean value_function loss: 69.7146
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 61.9235
                       Mean reward: 45.33
               Mean episode length: 70.71
    Episode_Reward/reaching_object: 0.3209
     Episode_Reward/lifting_object: 8.5728
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0234
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 62.7917
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 2.93s
                      Time elapsed: 00:29:19
                               ETA: 01:02:56

################################################################################
                     [1m Learning iteration 636/2000 [0m                      

                       Computation: 37192 steps/s (collection: 2.494s, learning 0.149s)
             Mean action noise std: 2.64
          Mean value_function loss: 67.3317
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 61.9290
                       Mean reward: 41.20
               Mean episode length: 61.47
    Episode_Reward/reaching_object: 0.3118
     Episode_Reward/lifting_object: 8.4533
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 61.9167
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.64s
                      Time elapsed: 00:29:22
                               ETA: 01:02:53

################################################################################
                     [1m Learning iteration 637/2000 [0m                      

                       Computation: 35526 steps/s (collection: 2.509s, learning 0.258s)
             Mean action noise std: 2.64
          Mean value_function loss: 85.5274
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.9325
                       Mean reward: 41.89
               Mean episode length: 67.68
    Episode_Reward/reaching_object: 0.3126
     Episode_Reward/lifting_object: 8.4456
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0230
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 62.2917
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.77s
                      Time elapsed: 00:29:24
                               ETA: 01:02:50

################################################################################
                     [1m Learning iteration 638/2000 [0m                      

                       Computation: 30883 steps/s (collection: 3.026s, learning 0.157s)
             Mean action noise std: 2.64
          Mean value_function loss: 82.4190
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 61.9346
                       Mean reward: 46.19
               Mean episode length: 70.19
    Episode_Reward/reaching_object: 0.3112
     Episode_Reward/lifting_object: 8.4602
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 65.2500
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 3.18s
                      Time elapsed: 00:29:28
                               ETA: 01:02:48

################################################################################
                     [1m Learning iteration 639/2000 [0m                      

                       Computation: 32679 steps/s (collection: 2.833s, learning 0.175s)
             Mean action noise std: 2.64
          Mean value_function loss: 96.4476
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 61.9379
                       Mean reward: 46.09
               Mean episode length: 63.51
    Episode_Reward/reaching_object: 0.3101
     Episode_Reward/lifting_object: 8.7948
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 65.3750
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 3.01s
                      Time elapsed: 00:29:31
                               ETA: 01:02:46

################################################################################
                     [1m Learning iteration 640/2000 [0m                      

                       Computation: 36708 steps/s (collection: 2.491s, learning 0.187s)
             Mean action noise std: 2.64
          Mean value_function loss: 95.1312
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.9437
                       Mean reward: 33.91
               Mean episode length: 55.76
    Episode_Reward/reaching_object: 0.3087
     Episode_Reward/lifting_object: 8.1714
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 63.6250
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.68s
                      Time elapsed: 00:29:33
                               ETA: 01:02:43

################################################################################
                     [1m Learning iteration 641/2000 [0m                      

                       Computation: 36114 steps/s (collection: 2.574s, learning 0.148s)
             Mean action noise std: 2.64
          Mean value_function loss: 77.3349
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 61.9490
                       Mean reward: 42.91
               Mean episode length: 63.67
    Episode_Reward/reaching_object: 0.3086
     Episode_Reward/lifting_object: 8.5325
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 63.7500
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.72s
                      Time elapsed: 00:29:36
                               ETA: 01:02:40

################################################################################
                     [1m Learning iteration 642/2000 [0m                      

                       Computation: 38098 steps/s (collection: 2.418s, learning 0.162s)
             Mean action noise std: 2.64
          Mean value_function loss: 92.1165
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 61.9508
                       Mean reward: 45.78
               Mean episode length: 66.10
    Episode_Reward/reaching_object: 0.3113
     Episode_Reward/lifting_object: 8.7976
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0231
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 64.1250
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.58s
                      Time elapsed: 00:29:39
                               ETA: 01:02:37

################################################################################
                     [1m Learning iteration 643/2000 [0m                      

                       Computation: 36143 steps/s (collection: 2.542s, learning 0.178s)
             Mean action noise std: 2.64
          Mean value_function loss: 82.6117
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 61.9521
                       Mean reward: 43.29
               Mean episode length: 61.23
    Episode_Reward/reaching_object: 0.3088
     Episode_Reward/lifting_object: 8.4668
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 64.2083
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 2.72s
                      Time elapsed: 00:29:41
                               ETA: 01:02:34

################################################################################
                     [1m Learning iteration 644/2000 [0m                      

                       Computation: 33715 steps/s (collection: 2.752s, learning 0.164s)
             Mean action noise std: 2.64
          Mean value_function loss: 71.7044
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 61.9564
                       Mean reward: 45.85
               Mean episode length: 62.69
    Episode_Reward/reaching_object: 0.3125
     Episode_Reward/lifting_object: 8.6964
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.3750
     Episode_Termination/robot_out: 62.9583
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.92s
                      Time elapsed: 00:29:44
                               ETA: 01:02:32

################################################################################
                     [1m Learning iteration 645/2000 [0m                      

                       Computation: 33454 steps/s (collection: 2.770s, learning 0.169s)
             Mean action noise std: 2.64
          Mean value_function loss: 89.6967
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 61.9595
                       Mean reward: 43.91
               Mean episode length: 60.19
    Episode_Reward/reaching_object: 0.3167
     Episode_Reward/lifting_object: 9.0314
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 65.5000
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.94s
                      Time elapsed: 00:29:47
                               ETA: 01:02:29

################################################################################
                     [1m Learning iteration 646/2000 [0m                      

                       Computation: 36295 steps/s (collection: 2.542s, learning 0.167s)
             Mean action noise std: 2.64
          Mean value_function loss: 117.9098
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 61.9616
                       Mean reward: 48.95
               Mean episode length: 61.07
    Episode_Reward/reaching_object: 0.3122
     Episode_Reward/lifting_object: 8.6207
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 64.4167
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.71s
                      Time elapsed: 00:29:50
                               ETA: 01:02:26

################################################################################
                     [1m Learning iteration 647/2000 [0m                      

                       Computation: 32162 steps/s (collection: 2.843s, learning 0.213s)
             Mean action noise std: 2.64
          Mean value_function loss: 101.1565
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 61.9633
                       Mean reward: 49.81
               Mean episode length: 67.22
    Episode_Reward/reaching_object: 0.3151
     Episode_Reward/lifting_object: 8.7011
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0228
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 65.7083
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 3.06s
                      Time elapsed: 00:29:53
                               ETA: 01:02:24

################################################################################
                     [1m Learning iteration 648/2000 [0m                      

                       Computation: 33870 steps/s (collection: 2.773s, learning 0.129s)
             Mean action noise std: 2.64
          Mean value_function loss: 82.6918
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 61.9649
                       Mean reward: 49.35
               Mean episode length: 63.45
    Episode_Reward/reaching_object: 0.3091
     Episode_Reward/lifting_object: 8.8891
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 64.5000
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.90s
                      Time elapsed: 00:29:56
                               ETA: 01:02:22

################################################################################
                     [1m Learning iteration 649/2000 [0m                      

                       Computation: 35460 steps/s (collection: 2.613s, learning 0.160s)
             Mean action noise std: 2.64
          Mean value_function loss: 96.3371
               Mean surrogate loss: 0.0096
                 Mean entropy loss: 61.9656
                       Mean reward: 38.85
               Mean episode length: 61.25
    Episode_Reward/reaching_object: 0.3120
     Episode_Reward/lifting_object: 8.7208
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 63.7083
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 2.77s
                      Time elapsed: 00:29:59
                               ETA: 01:02:19

################################################################################
                     [1m Learning iteration 650/2000 [0m                      

                       Computation: 36233 steps/s (collection: 2.533s, learning 0.180s)
             Mean action noise std: 2.64
          Mean value_function loss: 87.6180
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 61.9661
                       Mean reward: 49.19
               Mean episode length: 65.09
    Episode_Reward/reaching_object: 0.3114
     Episode_Reward/lifting_object: 8.7303
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 65.1667
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.71s
                      Time elapsed: 00:30:01
                               ETA: 01:02:16

################################################################################
                     [1m Learning iteration 651/2000 [0m                      

                       Computation: 33498 steps/s (collection: 2.786s, learning 0.148s)
             Mean action noise std: 2.64
          Mean value_function loss: 92.6200
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 61.9672
                       Mean reward: 38.80
               Mean episode length: 60.95
    Episode_Reward/reaching_object: 0.3121
     Episode_Reward/lifting_object: 8.9688
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0226
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 66.6250
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.93s
                      Time elapsed: 00:30:04
                               ETA: 01:02:14

################################################################################
                     [1m Learning iteration 652/2000 [0m                      

                       Computation: 37277 steps/s (collection: 2.467s, learning 0.170s)
             Mean action noise std: 2.64
          Mean value_function loss: 91.3937
               Mean surrogate loss: 0.0056
                 Mean entropy loss: 61.9682
                       Mean reward: 40.56
               Mean episode length: 60.51
    Episode_Reward/reaching_object: 0.3113
     Episode_Reward/lifting_object: 8.6703
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0232
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 65.4583
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.64s
                      Time elapsed: 00:30:07
                               ETA: 01:02:11

################################################################################
                     [1m Learning iteration 653/2000 [0m                      

                       Computation: 37560 steps/s (collection: 2.415s, learning 0.202s)
             Mean action noise std: 2.65
          Mean value_function loss: 89.9259
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 61.9695
                       Mean reward: 48.23
               Mean episode length: 64.27
    Episode_Reward/reaching_object: 0.3117
     Episode_Reward/lifting_object: 8.5311
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0236
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 66.1667
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 2.62s
                      Time elapsed: 00:30:10
                               ETA: 01:02:08

################################################################################
                     [1m Learning iteration 654/2000 [0m                      

                       Computation: 37651 steps/s (collection: 2.459s, learning 0.152s)
             Mean action noise std: 2.65
          Mean value_function loss: 110.3687
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 61.9725
                       Mean reward: 43.00
               Mean episode length: 60.64
    Episode_Reward/reaching_object: 0.3109
     Episode_Reward/lifting_object: 8.6535
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0229
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 65.3333
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.61s
                      Time elapsed: 00:30:12
                               ETA: 01:02:04

################################################################################
                     [1m Learning iteration 655/2000 [0m                      

                       Computation: 32130 steps/s (collection: 2.838s, learning 0.221s)
             Mean action noise std: 2.65
          Mean value_function loss: 95.8647
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 61.9771
                       Mean reward: 46.91
               Mean episode length: 61.12
    Episode_Reward/reaching_object: 0.3163
     Episode_Reward/lifting_object: 8.5373
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 64.5000
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 3.06s
                      Time elapsed: 00:30:15
                               ETA: 01:02:02

################################################################################
                     [1m Learning iteration 656/2000 [0m                      

                       Computation: 32043 steps/s (collection: 2.874s, learning 0.194s)
             Mean action noise std: 2.65
          Mean value_function loss: 96.6864
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 61.9809
                       Mean reward: 47.61
               Mean episode length: 61.79
    Episode_Reward/reaching_object: 0.3192
     Episode_Reward/lifting_object: 8.8073
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 64.4583
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 3.07s
                      Time elapsed: 00:30:18
                               ETA: 01:02:00

################################################################################
                     [1m Learning iteration 657/2000 [0m                      

                       Computation: 29441 steps/s (collection: 3.079s, learning 0.260s)
             Mean action noise std: 2.65
          Mean value_function loss: 124.1829
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 61.9851
                       Mean reward: 53.38
               Mean episode length: 64.71
    Episode_Reward/reaching_object: 0.3192
     Episode_Reward/lifting_object: 8.8201
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 65.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 3.34s
                      Time elapsed: 00:30:22
                               ETA: 01:01:58

################################################################################
                     [1m Learning iteration 658/2000 [0m                      

                       Computation: 31545 steps/s (collection: 2.915s, learning 0.201s)
             Mean action noise std: 2.65
          Mean value_function loss: 112.4370
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 61.9916
                       Mean reward: 47.53
               Mean episode length: 63.30
    Episode_Reward/reaching_object: 0.3193
     Episode_Reward/lifting_object: 8.7391
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 63.9583
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 3.12s
                      Time elapsed: 00:30:25
                               ETA: 01:01:56

################################################################################
                     [1m Learning iteration 659/2000 [0m                      

                       Computation: 30870 steps/s (collection: 2.972s, learning 0.212s)
             Mean action noise std: 2.65
          Mean value_function loss: 112.2422
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 61.9950
                       Mean reward: 41.94
               Mean episode length: 64.78
    Episode_Reward/reaching_object: 0.3267
     Episode_Reward/lifting_object: 9.1453
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 64.6250
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 3.18s
                      Time elapsed: 00:30:28
                               ETA: 01:01:55

################################################################################
                     [1m Learning iteration 660/2000 [0m                      

                       Computation: 33097 steps/s (collection: 2.779s, learning 0.191s)
             Mean action noise std: 2.65
          Mean value_function loss: 119.8573
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 62.0009
                       Mean reward: 46.42
               Mean episode length: 62.08
    Episode_Reward/reaching_object: 0.3189
     Episode_Reward/lifting_object: 9.0353
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 64.2500
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 2.97s
                      Time elapsed: 00:30:31
                               ETA: 01:01:52

################################################################################
                     [1m Learning iteration 661/2000 [0m                      

                       Computation: 29285 steps/s (collection: 3.141s, learning 0.216s)
             Mean action noise std: 2.65
          Mean value_function loss: 109.3083
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 62.0077
                       Mean reward: 47.49
               Mean episode length: 64.63
    Episode_Reward/reaching_object: 0.3232
     Episode_Reward/lifting_object: 9.3637
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 64.2083
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 3.36s
                      Time elapsed: 00:30:34
                               ETA: 01:01:51

################################################################################
                     [1m Learning iteration 662/2000 [0m                      

                       Computation: 30018 steps/s (collection: 3.000s, learning 0.274s)
             Mean action noise std: 2.65
          Mean value_function loss: 107.9042
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 62.0129
                       Mean reward: 46.51
               Mean episode length: 62.54
    Episode_Reward/reaching_object: 0.3152
     Episode_Reward/lifting_object: 9.0150
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 63.1667
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 3.27s
                      Time elapsed: 00:30:38
                               ETA: 01:01:49

################################################################################
                     [1m Learning iteration 663/2000 [0m                      

                       Computation: 29718 steps/s (collection: 3.028s, learning 0.280s)
             Mean action noise std: 2.65
          Mean value_function loss: 106.1382
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 62.0184
                       Mean reward: 48.60
               Mean episode length: 62.85
    Episode_Reward/reaching_object: 0.3280
     Episode_Reward/lifting_object: 9.1861
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 63.7500
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 3.31s
                      Time elapsed: 00:30:41
                               ETA: 01:01:47

################################################################################
                     [1m Learning iteration 664/2000 [0m                      

                       Computation: 27915 steps/s (collection: 3.293s, learning 0.228s)
             Mean action noise std: 2.65
          Mean value_function loss: 89.2299
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 62.0220
                       Mean reward: 53.87
               Mean episode length: 65.52
    Episode_Reward/reaching_object: 0.3249
     Episode_Reward/lifting_object: 9.6694
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 64.6250
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 3.52s
                      Time elapsed: 00:30:44
                               ETA: 01:01:46

################################################################################
                     [1m Learning iteration 665/2000 [0m                      

                       Computation: 28177 steps/s (collection: 3.251s, learning 0.238s)
             Mean action noise std: 2.65
          Mean value_function loss: 107.4744
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 62.0272
                       Mean reward: 45.20
               Mean episode length: 62.39
    Episode_Reward/reaching_object: 0.3239
     Episode_Reward/lifting_object: 9.7045
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 62.5833
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 3.49s
                      Time elapsed: 00:30:48
                               ETA: 01:01:44

################################################################################
                     [1m Learning iteration 666/2000 [0m                      

                       Computation: 20048 steps/s (collection: 4.671s, learning 0.233s)
             Mean action noise std: 2.65
          Mean value_function loss: 108.2345
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 62.0325
                       Mean reward: 54.37
               Mean episode length: 66.43
    Episode_Reward/reaching_object: 0.3289
     Episode_Reward/lifting_object: 9.7147
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 64.2500
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 4.90s
                      Time elapsed: 00:30:53
                               ETA: 01:01:46

################################################################################
                     [1m Learning iteration 667/2000 [0m                      

                       Computation: 12291 steps/s (collection: 7.852s, learning 0.145s)
             Mean action noise std: 2.65
          Mean value_function loss: 110.2405
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 62.0372
                       Mean reward: 45.82
               Mean episode length: 62.23
    Episode_Reward/reaching_object: 0.3271
     Episode_Reward/lifting_object: 9.8827
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 62.8333
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 8.00s
                      Time elapsed: 00:31:01
                               ETA: 01:01:54

################################################################################
                     [1m Learning iteration 668/2000 [0m                      

                       Computation: 12870 steps/s (collection: 7.493s, learning 0.145s)
             Mean action noise std: 2.65
          Mean value_function loss: 132.7108
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 62.0400
                       Mean reward: 47.34
               Mean episode length: 57.92
    Episode_Reward/reaching_object: 0.3150
     Episode_Reward/lifting_object: 9.5987
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 66.7917
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 7.64s
                      Time elapsed: 00:31:08
                               ETA: 01:02:00

################################################################################
                     [1m Learning iteration 669/2000 [0m                      

                       Computation: 12756 steps/s (collection: 7.569s, learning 0.137s)
             Mean action noise std: 2.65
          Mean value_function loss: 144.2273
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.0433
                       Mean reward: 43.92
               Mean episode length: 59.64
    Episode_Reward/reaching_object: 0.3174
     Episode_Reward/lifting_object: 9.5483
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 65.6250
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 7.71s
                      Time elapsed: 00:31:16
                               ETA: 01:02:07

################################################################################
                     [1m Learning iteration 670/2000 [0m                      

                       Computation: 12683 steps/s (collection: 7.581s, learning 0.169s)
             Mean action noise std: 2.65
          Mean value_function loss: 132.5223
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 62.0480
                       Mean reward: 51.56
               Mean episode length: 63.83
    Episode_Reward/reaching_object: 0.3180
     Episode_Reward/lifting_object: 9.8403
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 64.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 7.75s
                      Time elapsed: 00:31:24
                               ETA: 01:02:14

################################################################################
                     [1m Learning iteration 671/2000 [0m                      

                       Computation: 12723 steps/s (collection: 7.578s, learning 0.149s)
             Mean action noise std: 2.65
          Mean value_function loss: 106.9653
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 62.0507
                       Mean reward: 41.26
               Mean episode length: 59.67
    Episode_Reward/reaching_object: 0.3202
     Episode_Reward/lifting_object: 9.8391
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0248
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 66.4583
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 7.73s
                      Time elapsed: 00:31:32
                               ETA: 01:02:21

################################################################################
                     [1m Learning iteration 672/2000 [0m                      

                       Computation: 12878 steps/s (collection: 7.473s, learning 0.161s)
             Mean action noise std: 2.65
          Mean value_function loss: 121.9485
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 62.0533
                       Mean reward: 46.06
               Mean episode length: 62.18
    Episode_Reward/reaching_object: 0.3144
     Episode_Reward/lifting_object: 9.6707
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 65.2083
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 7.63s
                      Time elapsed: 00:31:39
                               ETA: 01:02:28

################################################################################
                     [1m Learning iteration 673/2000 [0m                      

                       Computation: 11068 steps/s (collection: 8.651s, learning 0.230s)
             Mean action noise std: 2.66
          Mean value_function loss: 111.3813
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 62.0566
                       Mean reward: 52.92
               Mean episode length: 63.01
    Episode_Reward/reaching_object: 0.3181
     Episode_Reward/lifting_object: 9.8413
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 65.3750
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 8.88s
                      Time elapsed: 00:31:48
                               ETA: 01:02:37

################################################################################
                     [1m Learning iteration 674/2000 [0m                      

                       Computation: 10638 steps/s (collection: 9.104s, learning 0.137s)
             Mean action noise std: 2.66
          Mean value_function loss: 123.9428
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 62.0622
                       Mean reward: 45.57
               Mean episode length: 60.98
    Episode_Reward/reaching_object: 0.3181
     Episode_Reward/lifting_object: 9.4954
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 66.1667
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 9.24s
                      Time elapsed: 00:31:57
                               ETA: 01:02:47

################################################################################
                     [1m Learning iteration 675/2000 [0m                      

                       Computation: 13382 steps/s (collection: 7.138s, learning 0.208s)
             Mean action noise std: 2.66
          Mean value_function loss: 134.2143
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 62.0676
                       Mean reward: 53.18
               Mean episode length: 60.31
    Episode_Reward/reaching_object: 0.3169
     Episode_Reward/lifting_object: 9.8616
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0238
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 66.8333
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 7.35s
                      Time elapsed: 00:32:05
                               ETA: 01:02:53

################################################################################
                     [1m Learning iteration 676/2000 [0m                      

                       Computation: 29017 steps/s (collection: 3.243s, learning 0.145s)
             Mean action noise std: 2.66
          Mean value_function loss: 108.2072
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 62.0706
                       Mean reward: 54.51
               Mean episode length: 63.28
    Episode_Reward/reaching_object: 0.3144
     Episode_Reward/lifting_object: 9.8536
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0240
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 65.4583
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 3.39s
                      Time elapsed: 00:32:08
                               ETA: 01:02:51

################################################################################
                     [1m Learning iteration 677/2000 [0m                      

                       Computation: 25302 steps/s (collection: 3.558s, learning 0.328s)
             Mean action noise std: 2.66
          Mean value_function loss: 123.6642
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 62.0731
                       Mean reward: 53.87
               Mean episode length: 61.75
    Episode_Reward/reaching_object: 0.3168
     Episode_Reward/lifting_object: 9.9945
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0237
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 64.8750
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 3.89s
                      Time elapsed: 00:32:12
                               ETA: 01:02:50

################################################################################
                     [1m Learning iteration 678/2000 [0m                      

                       Computation: 28459 steps/s (collection: 3.246s, learning 0.209s)
             Mean action noise std: 2.66
          Mean value_function loss: 128.1488
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 62.0759
                       Mean reward: 55.91
               Mean episode length: 62.73
    Episode_Reward/reaching_object: 0.3157
     Episode_Reward/lifting_object: 9.6097
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 65.8333
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 3.45s
                      Time elapsed: 00:32:15
                               ETA: 01:02:49

################################################################################
                     [1m Learning iteration 679/2000 [0m                      

                       Computation: 28838 steps/s (collection: 3.161s, learning 0.248s)
             Mean action noise std: 2.66
          Mean value_function loss: 142.1772
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 62.0805
                       Mean reward: 48.30
               Mean episode length: 58.73
    Episode_Reward/reaching_object: 0.3174
     Episode_Reward/lifting_object: 10.0180
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 65.1667
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 3.41s
                      Time elapsed: 00:32:19
                               ETA: 01:02:47

################################################################################
                     [1m Learning iteration 680/2000 [0m                      

                       Computation: 29373 steps/s (collection: 3.126s, learning 0.221s)
             Mean action noise std: 2.66
          Mean value_function loss: 139.6622
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 62.0856
                       Mean reward: 62.18
               Mean episode length: 70.32
    Episode_Reward/reaching_object: 0.3310
     Episode_Reward/lifting_object: 10.1391
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 65.1250
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 3.35s
                      Time elapsed: 00:32:22
                               ETA: 01:02:45

################################################################################
                     [1m Learning iteration 681/2000 [0m                      

                       Computation: 32555 steps/s (collection: 2.771s, learning 0.249s)
             Mean action noise std: 2.66
          Mean value_function loss: 123.3185
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 62.0898
                       Mean reward: 52.10
               Mean episode length: 61.44
    Episode_Reward/reaching_object: 0.3319
     Episode_Reward/lifting_object: 10.5014
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 63.3333
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 3.02s
                      Time elapsed: 00:32:25
                               ETA: 01:02:42

################################################################################
                     [1m Learning iteration 682/2000 [0m                      

                       Computation: 32020 steps/s (collection: 2.861s, learning 0.209s)
             Mean action noise std: 2.66
          Mean value_function loss: 125.5770
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 62.0935
                       Mean reward: 50.71
               Mean episode length: 62.44
    Episode_Reward/reaching_object: 0.3305
     Episode_Reward/lifting_object: 10.6458
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0249
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 63.3750
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 3.07s
                      Time elapsed: 00:32:28
                               ETA: 01:02:40

################################################################################
                     [1m Learning iteration 683/2000 [0m                      

                       Computation: 31318 steps/s (collection: 2.930s, learning 0.209s)
             Mean action noise std: 2.66
          Mean value_function loss: 134.6292
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 62.0963
                       Mean reward: 60.26
               Mean episode length: 65.58
    Episode_Reward/reaching_object: 0.3300
     Episode_Reward/lifting_object: 10.6906
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 65.6667
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 3.14s
                      Time elapsed: 00:32:31
                               ETA: 01:02:38

################################################################################
                     [1m Learning iteration 684/2000 [0m                      

                       Computation: 30882 steps/s (collection: 2.956s, learning 0.227s)
             Mean action noise std: 2.66
          Mean value_function loss: 144.4963
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 62.1001
                       Mean reward: 52.64
               Mean episode length: 62.48
    Episode_Reward/reaching_object: 0.3297
     Episode_Reward/lifting_object: 10.5393
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 65.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 3.18s
                      Time elapsed: 00:32:35
                               ETA: 01:02:35

################################################################################
                     [1m Learning iteration 685/2000 [0m                      

                       Computation: 29025 steps/s (collection: 3.170s, learning 0.217s)
             Mean action noise std: 2.66
          Mean value_function loss: 140.6413
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 62.1045
                       Mean reward: 52.35
               Mean episode length: 61.82
    Episode_Reward/reaching_object: 0.3229
     Episode_Reward/lifting_object: 10.2739
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 65.2917
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 3.39s
                      Time elapsed: 00:32:38
                               ETA: 01:02:34

################################################################################
                     [1m Learning iteration 686/2000 [0m                      

                       Computation: 32679 steps/s (collection: 2.837s, learning 0.171s)
             Mean action noise std: 2.66
          Mean value_function loss: 138.0008
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 62.1070
                       Mean reward: 61.24
               Mean episode length: 65.97
    Episode_Reward/reaching_object: 0.3267
     Episode_Reward/lifting_object: 10.2469
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 63.8333
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 3.01s
                      Time elapsed: 00:32:41
                               ETA: 01:02:31

################################################################################
                     [1m Learning iteration 687/2000 [0m                      

                       Computation: 39495 steps/s (collection: 2.355s, learning 0.134s)
             Mean action noise std: 2.66
          Mean value_function loss: 109.2219
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 62.1103
                       Mean reward: 55.19
               Mean episode length: 65.25
    Episode_Reward/reaching_object: 0.3214
     Episode_Reward/lifting_object: 10.3744
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0242
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 63.8333
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 2.49s
                      Time elapsed: 00:32:43
                               ETA: 01:02:28

################################################################################
                     [1m Learning iteration 688/2000 [0m                      

                       Computation: 39304 steps/s (collection: 2.363s, learning 0.138s)
             Mean action noise std: 2.66
          Mean value_function loss: 132.0259
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 62.1149
                       Mean reward: 53.80
               Mean episode length: 62.85
    Episode_Reward/reaching_object: 0.3271
     Episode_Reward/lifting_object: 10.9593
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0243
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 62.7083
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 2.50s
                      Time elapsed: 00:32:46
                               ETA: 01:02:24

################################################################################
                     [1m Learning iteration 689/2000 [0m                      

                       Computation: 37738 steps/s (collection: 2.441s, learning 0.164s)
             Mean action noise std: 2.66
          Mean value_function loss: 117.0978
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.1177
                       Mean reward: 56.13
               Mean episode length: 62.23
    Episode_Reward/reaching_object: 0.3335
     Episode_Reward/lifting_object: 10.9244
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0251
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 62.2500
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 2.60s
                      Time elapsed: 00:32:49
                               ETA: 01:02:21

################################################################################
                     [1m Learning iteration 690/2000 [0m                      

                       Computation: 40183 steps/s (collection: 2.300s, learning 0.146s)
             Mean action noise std: 2.66
          Mean value_function loss: 106.8955
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 62.1194
                       Mean reward: 57.84
               Mean episode length: 66.30
    Episode_Reward/reaching_object: 0.3342
     Episode_Reward/lifting_object: 11.2105
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 60.6667
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 2.45s
                      Time elapsed: 00:32:51
                               ETA: 01:02:17

################################################################################
                     [1m Learning iteration 691/2000 [0m                      

                       Computation: 41619 steps/s (collection: 2.246s, learning 0.116s)
             Mean action noise std: 2.66
          Mean value_function loss: 121.6760
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 62.1222
                       Mean reward: 56.25
               Mean episode length: 67.56
    Episode_Reward/reaching_object: 0.3414
     Episode_Reward/lifting_object: 11.5702
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 63.0833
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 2.36s
                      Time elapsed: 00:32:53
                               ETA: 01:02:13

################################################################################
                     [1m Learning iteration 692/2000 [0m                      

                       Computation: 40957 steps/s (collection: 2.271s, learning 0.130s)
             Mean action noise std: 2.66
          Mean value_function loss: 113.1022
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 62.1241
                       Mean reward: 66.11
               Mean episode length: 67.91
    Episode_Reward/reaching_object: 0.3378
     Episode_Reward/lifting_object: 11.5714
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 58.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 2.40s
                      Time elapsed: 00:32:56
                               ETA: 01:02:10

################################################################################
                     [1m Learning iteration 693/2000 [0m                      

                       Computation: 41256 steps/s (collection: 2.247s, learning 0.136s)
             Mean action noise std: 2.66
          Mean value_function loss: 126.4633
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 62.1256
                       Mean reward: 61.82
               Mean episode length: 70.48
    Episode_Reward/reaching_object: 0.3353
     Episode_Reward/lifting_object: 11.2273
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 61.2500
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 2.38s
                      Time elapsed: 00:32:58
                               ETA: 01:02:06

################################################################################
                     [1m Learning iteration 694/2000 [0m                      

                       Computation: 40855 steps/s (collection: 2.269s, learning 0.138s)
             Mean action noise std: 2.66
          Mean value_function loss: 128.9339
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 62.1303
                       Mean reward: 56.73
               Mean episode length: 63.51
    Episode_Reward/reaching_object: 0.3452
     Episode_Reward/lifting_object: 11.9877
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 60.7917
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 2.41s
                      Time elapsed: 00:33:01
                               ETA: 01:02:02

################################################################################
                     [1m Learning iteration 695/2000 [0m                      

                       Computation: 41794 steps/s (collection: 2.233s, learning 0.119s)
             Mean action noise std: 2.67
          Mean value_function loss: 128.8856
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 62.1386
                       Mean reward: 58.32
               Mean episode length: 62.79
    Episode_Reward/reaching_object: 0.3363
     Episode_Reward/lifting_object: 11.4840
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 61.9167
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 2.35s
                      Time elapsed: 00:33:03
                               ETA: 01:01:58

################################################################################
                     [1m Learning iteration 696/2000 [0m                      

                       Computation: 41260 steps/s (collection: 2.244s, learning 0.139s)
             Mean action noise std: 2.67
          Mean value_function loss: 148.4745
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 62.1432
                       Mean reward: 54.37
               Mean episode length: 69.72
    Episode_Reward/reaching_object: 0.3407
     Episode_Reward/lifting_object: 11.4354
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 61.6250
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 2.38s
                      Time elapsed: 00:33:05
                               ETA: 01:01:55

################################################################################
                     [1m Learning iteration 697/2000 [0m                      

                       Computation: 39849 steps/s (collection: 2.346s, learning 0.121s)
             Mean action noise std: 2.67
          Mean value_function loss: 138.2271
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 62.1490
                       Mean reward: 55.02
               Mean episode length: 63.32
    Episode_Reward/reaching_object: 0.3351
     Episode_Reward/lifting_object: 11.2772
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 61.4167
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 2.47s
                      Time elapsed: 00:33:08
                               ETA: 01:01:51

################################################################################
                     [1m Learning iteration 698/2000 [0m                      

                       Computation: 37491 steps/s (collection: 2.481s, learning 0.141s)
             Mean action noise std: 2.67
          Mean value_function loss: 119.2956
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 62.1532
                       Mean reward: 58.75
               Mean episode length: 67.90
    Episode_Reward/reaching_object: 0.3371
     Episode_Reward/lifting_object: 11.7037
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0833
     Episode_Termination/robot_out: 61.1667
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 2.62s
                      Time elapsed: 00:33:10
                               ETA: 01:01:48

################################################################################
                     [1m Learning iteration 699/2000 [0m                      

                       Computation: 39039 steps/s (collection: 2.384s, learning 0.134s)
             Mean action noise std: 2.67
          Mean value_function loss: 128.8869
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 62.1565
                       Mean reward: 57.69
               Mean episode length: 60.72
    Episode_Reward/reaching_object: 0.3371
     Episode_Reward/lifting_object: 11.9600
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 64.2500
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 2.52s
                      Time elapsed: 00:33:13
                               ETA: 01:01:44

################################################################################
                     [1m Learning iteration 700/2000 [0m                      

                       Computation: 40213 steps/s (collection: 2.304s, learning 0.140s)
             Mean action noise std: 2.67
          Mean value_function loss: 123.0469
               Mean surrogate loss: 0.0081
                 Mean entropy loss: 62.1589
                       Mean reward: 54.77
               Mean episode length: 60.64
    Episode_Reward/reaching_object: 0.3338
     Episode_Reward/lifting_object: 11.4473
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 63.2083
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 2.44s
                      Time elapsed: 00:33:15
                               ETA: 01:01:41

################################################################################
                     [1m Learning iteration 701/2000 [0m                      

                       Computation: 40197 steps/s (collection: 2.325s, learning 0.121s)
             Mean action noise std: 2.67
          Mean value_function loss: 141.9672
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 62.1595
                       Mean reward: 56.64
               Mean episode length: 62.10
    Episode_Reward/reaching_object: 0.3326
     Episode_Reward/lifting_object: 11.7407
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 63.0417
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 2.45s
                      Time elapsed: 00:33:18
                               ETA: 01:01:37

################################################################################
                     [1m Learning iteration 702/2000 [0m                      

                       Computation: 40593 steps/s (collection: 2.287s, learning 0.135s)
             Mean action noise std: 2.67
          Mean value_function loss: 124.2257
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 62.1598
                       Mean reward: 52.51
               Mean episode length: 60.05
    Episode_Reward/reaching_object: 0.3341
     Episode_Reward/lifting_object: 11.8767
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 63.3750
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 2.42s
                      Time elapsed: 00:33:20
                               ETA: 01:01:34

################################################################################
                     [1m Learning iteration 703/2000 [0m                      

                       Computation: 41190 steps/s (collection: 2.245s, learning 0.142s)
             Mean action noise std: 2.67
          Mean value_function loss: 140.6447
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 62.1601
                       Mean reward: 60.52
               Mean episode length: 67.53
    Episode_Reward/reaching_object: 0.3317
     Episode_Reward/lifting_object: 12.1269
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 60.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 2.39s
                      Time elapsed: 00:33:23
                               ETA: 01:01:30

################################################################################
                     [1m Learning iteration 704/2000 [0m                      

                       Computation: 40883 steps/s (collection: 2.274s, learning 0.131s)
             Mean action noise std: 2.67
          Mean value_function loss: 124.9851
               Mean surrogate loss: 0.0107
                 Mean entropy loss: 62.1603
                       Mean reward: 66.44
               Mean episode length: 68.22
    Episode_Reward/reaching_object: 0.3412
     Episode_Reward/lifting_object: 12.1648
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 62.7917
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 2.40s
                      Time elapsed: 00:33:25
                               ETA: 01:01:26

################################################################################
                     [1m Learning iteration 705/2000 [0m                      

                       Computation: 41617 steps/s (collection: 2.254s, learning 0.109s)
             Mean action noise std: 2.67
          Mean value_function loss: 138.4552
               Mean surrogate loss: 0.0139
                 Mean entropy loss: 62.1605
                       Mean reward: 63.02
               Mean episode length: 64.78
    Episode_Reward/reaching_object: 0.3385
     Episode_Reward/lifting_object: 12.1946
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 63.3750
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 2.36s
                      Time elapsed: 00:33:27
                               ETA: 01:01:22

################################################################################
                     [1m Learning iteration 706/2000 [0m                      

                       Computation: 41602 steps/s (collection: 2.247s, learning 0.116s)
             Mean action noise std: 2.67
          Mean value_function loss: 127.3828
               Mean surrogate loss: 0.0109
                 Mean entropy loss: 62.1607
                       Mean reward: 65.99
               Mean episode length: 65.59
    Episode_Reward/reaching_object: 0.3370
     Episode_Reward/lifting_object: 12.2204
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 62.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 2.36s
                      Time elapsed: 00:33:30
                               ETA: 01:01:19

################################################################################
                     [1m Learning iteration 707/2000 [0m                      

                       Computation: 40671 steps/s (collection: 2.279s, learning 0.138s)
             Mean action noise std: 2.67
          Mean value_function loss: 118.8507
               Mean surrogate loss: 0.0079
                 Mean entropy loss: 62.1608
                       Mean reward: 70.34
               Mean episode length: 70.23
    Episode_Reward/reaching_object: 0.3325
     Episode_Reward/lifting_object: 11.9766
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0253
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 62.4167
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 2.42s
                      Time elapsed: 00:33:32
                               ETA: 01:01:15

################################################################################
                     [1m Learning iteration 708/2000 [0m                      

                       Computation: 41808 steps/s (collection: 2.245s, learning 0.106s)
             Mean action noise std: 2.67
          Mean value_function loss: 152.5325
               Mean surrogate loss: 0.0120
                 Mean entropy loss: 62.1608
                       Mean reward: 61.07
               Mean episode length: 64.03
    Episode_Reward/reaching_object: 0.3302
     Episode_Reward/lifting_object: 11.8630
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 60.7500
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 2.35s
                      Time elapsed: 00:33:34
                               ETA: 01:01:11

################################################################################
                     [1m Learning iteration 709/2000 [0m                      

                       Computation: 41162 steps/s (collection: 2.256s, learning 0.133s)
             Mean action noise std: 2.67
          Mean value_function loss: 142.2014
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 62.1609
                       Mean reward: 61.72
               Mean episode length: 62.96
    Episode_Reward/reaching_object: 0.3450
     Episode_Reward/lifting_object: 11.9891
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 63.3333
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 2.39s
                      Time elapsed: 00:33:37
                               ETA: 01:01:08

################################################################################
                     [1m Learning iteration 710/2000 [0m                      

                       Computation: 41395 steps/s (collection: 2.236s, learning 0.139s)
             Mean action noise std: 2.67
          Mean value_function loss: 113.9244
               Mean surrogate loss: 0.0100
                 Mean entropy loss: 62.1610
                       Mean reward: 62.48
               Mean episode length: 66.10
    Episode_Reward/reaching_object: 0.3430
     Episode_Reward/lifting_object: 12.2708
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 62.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 2.37s
                      Time elapsed: 00:33:39
                               ETA: 01:01:04

################################################################################
                     [1m Learning iteration 711/2000 [0m                      

                       Computation: 41082 steps/s (collection: 2.260s, learning 0.133s)
             Mean action noise std: 2.67
          Mean value_function loss: 113.4869
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 62.1611
                       Mean reward: 69.26
               Mean episode length: 63.97
    Episode_Reward/reaching_object: 0.3349
     Episode_Reward/lifting_object: 12.1806
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7917
     Episode_Termination/robot_out: 60.5833
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 2.39s
                      Time elapsed: 00:33:42
                               ETA: 01:01:00

################################################################################
                     [1m Learning iteration 712/2000 [0m                      

                       Computation: 40531 steps/s (collection: 2.297s, learning 0.128s)
             Mean action noise std: 2.67
          Mean value_function loss: 122.0780
               Mean surrogate loss: 0.0104
                 Mean entropy loss: 62.1612
                       Mean reward: 63.25
               Mean episode length: 64.10
    Episode_Reward/reaching_object: 0.3379
     Episode_Reward/lifting_object: 12.2482
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 62.3750
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 2.43s
                      Time elapsed: 00:33:44
                               ETA: 01:00:57

################################################################################
                     [1m Learning iteration 713/2000 [0m                      

                       Computation: 41002 steps/s (collection: 2.262s, learning 0.135s)
             Mean action noise std: 2.67
          Mean value_function loss: 126.2747
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 62.1614
                       Mean reward: 65.83
               Mean episode length: 64.01
    Episode_Reward/reaching_object: 0.3395
     Episode_Reward/lifting_object: 12.5987
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 2.2917
     Episode_Termination/robot_out: 60.1250
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 2.40s
                      Time elapsed: 00:33:46
                               ETA: 01:00:53

################################################################################
                     [1m Learning iteration 714/2000 [0m                      

                       Computation: 41399 steps/s (collection: 2.237s, learning 0.137s)
             Mean action noise std: 2.67
          Mean value_function loss: 129.3897
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.1622
                       Mean reward: 62.08
               Mean episode length: 64.02
    Episode_Reward/reaching_object: 0.3393
     Episode_Reward/lifting_object: 12.4426
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 63.6250
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 2.37s
                      Time elapsed: 00:33:49
                               ETA: 01:00:49

################################################################################
                     [1m Learning iteration 715/2000 [0m                      

                       Computation: 40482 steps/s (collection: 2.281s, learning 0.147s)
             Mean action noise std: 2.67
          Mean value_function loss: 122.8331
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 62.1641
                       Mean reward: 62.15
               Mean episode length: 60.05
    Episode_Reward/reaching_object: 0.3396
     Episode_Reward/lifting_object: 12.7686
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 61.6250
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 2.43s
                      Time elapsed: 00:33:51
                               ETA: 01:00:46

################################################################################
                     [1m Learning iteration 716/2000 [0m                      

                       Computation: 40691 steps/s (collection: 2.277s, learning 0.139s)
             Mean action noise std: 2.67
          Mean value_function loss: 127.2623
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 62.1692
                       Mean reward: 60.47
               Mean episode length: 65.93
    Episode_Reward/reaching_object: 0.3426
     Episode_Reward/lifting_object: 12.7862
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 60.8750
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 2.42s
                      Time elapsed: 00:33:54
                               ETA: 01:00:42

################################################################################
                     [1m Learning iteration 717/2000 [0m                      

                       Computation: 41010 steps/s (collection: 2.293s, learning 0.104s)
             Mean action noise std: 2.67
          Mean value_function loss: 112.8730
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 62.1729
                       Mean reward: 64.57
               Mean episode length: 64.68
    Episode_Reward/reaching_object: 0.3438
     Episode_Reward/lifting_object: 12.8994
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0256
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 61.3333
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 2.40s
                      Time elapsed: 00:33:56
                               ETA: 01:00:39

################################################################################
                     [1m Learning iteration 718/2000 [0m                      

                       Computation: 42325 steps/s (collection: 2.203s, learning 0.119s)
             Mean action noise std: 2.67
          Mean value_function loss: 134.1645
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 62.1740
                       Mean reward: 69.57
               Mean episode length: 67.23
    Episode_Reward/reaching_object: 0.3454
     Episode_Reward/lifting_object: 12.8983
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0258
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 61.8750
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 2.32s
                      Time elapsed: 00:33:58
                               ETA: 01:00:35

################################################################################
                     [1m Learning iteration 719/2000 [0m                      

                       Computation: 42040 steps/s (collection: 2.233s, learning 0.105s)
             Mean action noise std: 2.67
          Mean value_function loss: 140.4066
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 62.1757
                       Mean reward: 58.27
               Mean episode length: 62.90
    Episode_Reward/reaching_object: 0.3483
     Episode_Reward/lifting_object: 12.7437
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 63.7083
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 2.34s
                      Time elapsed: 00:34:01
                               ETA: 01:00:31

################################################################################
                     [1m Learning iteration 720/2000 [0m                      

                       Computation: 40175 steps/s (collection: 2.329s, learning 0.118s)
             Mean action noise std: 2.67
          Mean value_function loss: 147.3934
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 62.1768
                       Mean reward: 71.38
               Mean episode length: 67.80
    Episode_Reward/reaching_object: 0.3426
     Episode_Reward/lifting_object: 12.6865
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0257
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 63.2917
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 2.45s
                      Time elapsed: 00:34:03
                               ETA: 01:00:28

################################################################################
                     [1m Learning iteration 721/2000 [0m                      

                       Computation: 39832 steps/s (collection: 2.364s, learning 0.104s)
             Mean action noise std: 2.67
          Mean value_function loss: 142.8983
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 62.1793
                       Mean reward: 69.99
               Mean episode length: 64.14
    Episode_Reward/reaching_object: 0.3308
     Episode_Reward/lifting_object: 12.4134
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.8750
     Episode_Termination/robot_out: 65.7083
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 2.47s
                      Time elapsed: 00:34:06
                               ETA: 01:00:24

################################################################################
                     [1m Learning iteration 722/2000 [0m                      

                       Computation: 41688 steps/s (collection: 2.251s, learning 0.107s)
             Mean action noise std: 2.67
          Mean value_function loss: 129.7832
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 62.1824
                       Mean reward: 56.88
               Mean episode length: 60.50
    Episode_Reward/reaching_object: 0.3271
     Episode_Reward/lifting_object: 12.1986
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 63.4583
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 2.36s
                      Time elapsed: 00:34:08
                               ETA: 01:00:21

################################################################################
                     [1m Learning iteration 723/2000 [0m                      

                       Computation: 41557 steps/s (collection: 2.254s, learning 0.111s)
             Mean action noise std: 2.67
          Mean value_function loss: 138.7953
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 62.1839
                       Mean reward: 62.01
               Mean episode length: 61.52
    Episode_Reward/reaching_object: 0.3245
     Episode_Reward/lifting_object: 12.0991
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0417
     Episode_Termination/robot_out: 66.0833
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 2.37s
                      Time elapsed: 00:34:10
                               ETA: 01:00:17

################################################################################
                     [1m Learning iteration 724/2000 [0m                      

                       Computation: 40874 steps/s (collection: 2.275s, learning 0.130s)
             Mean action noise std: 2.67
          Mean value_function loss: 152.0320
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 62.1848
                       Mean reward: 59.55
               Mean episode length: 58.49
    Episode_Reward/reaching_object: 0.3227
     Episode_Reward/lifting_object: 12.2388
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 64.4167
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 2.41s
                      Time elapsed: 00:34:13
                               ETA: 01:00:13

################################################################################
                     [1m Learning iteration 725/2000 [0m                      

                       Computation: 41033 steps/s (collection: 2.266s, learning 0.130s)
             Mean action noise std: 2.67
          Mean value_function loss: 157.4193
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 62.1857
                       Mean reward: 72.83
               Mean episode length: 68.21
    Episode_Reward/reaching_object: 0.3266
     Episode_Reward/lifting_object: 12.2519
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 66.5417
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 2.40s
                      Time elapsed: 00:34:15
                               ETA: 01:00:10

################################################################################
                     [1m Learning iteration 726/2000 [0m                      

                       Computation: 41387 steps/s (collection: 2.261s, learning 0.115s)
             Mean action noise std: 2.67
          Mean value_function loss: 130.3452
               Mean surrogate loss: 0.0107
                 Mean entropy loss: 62.1878
                       Mean reward: 67.11
               Mean episode length: 63.04
    Episode_Reward/reaching_object: 0.3193
     Episode_Reward/lifting_object: 11.7309
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 67.3333
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 2.38s
                      Time elapsed: 00:34:18
                               ETA: 01:00:06

################################################################################
                     [1m Learning iteration 727/2000 [0m                      

                       Computation: 41932 steps/s (collection: 2.234s, learning 0.111s)
             Mean action noise std: 2.67
          Mean value_function loss: 146.5994
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 62.1884
                       Mean reward: 57.86
               Mean episode length: 56.93
    Episode_Reward/reaching_object: 0.3131
     Episode_Reward/lifting_object: 12.0398
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 66.5417
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 2.34s
                      Time elapsed: 00:34:20
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 728/2000 [0m                      

                       Computation: 40901 steps/s (collection: 2.290s, learning 0.114s)
             Mean action noise std: 2.67
          Mean value_function loss: 140.8508
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 62.1886
                       Mean reward: 65.10
               Mean episode length: 62.82
    Episode_Reward/reaching_object: 0.3134
     Episode_Reward/lifting_object: 11.7235
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 66.4583
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 2.40s
                      Time elapsed: 00:34:22
                               ETA: 00:59:59

################################################################################
                     [1m Learning iteration 729/2000 [0m                      

                       Computation: 42178 steps/s (collection: 2.229s, learning 0.102s)
             Mean action noise std: 2.67
          Mean value_function loss: 164.0814
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 62.1896
                       Mean reward: 64.21
               Mean episode length: 59.03
    Episode_Reward/reaching_object: 0.3145
     Episode_Reward/lifting_object: 11.7704
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0241
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 66.6667
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 2.33s
                      Time elapsed: 00:34:25
                               ETA: 00:59:55

################################################################################
                     [1m Learning iteration 730/2000 [0m                      

                       Computation: 41402 steps/s (collection: 2.246s, learning 0.128s)
             Mean action noise std: 2.67
          Mean value_function loss: 143.6418
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 62.1903
                       Mean reward: 62.14
               Mean episode length: 60.77
    Episode_Reward/reaching_object: 0.3186
     Episode_Reward/lifting_object: 11.8870
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0245
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 66.0000
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 2.37s
                      Time elapsed: 00:34:27
                               ETA: 00:59:51

################################################################################
                     [1m Learning iteration 731/2000 [0m                      

                       Computation: 38282 steps/s (collection: 2.423s, learning 0.145s)
             Mean action noise std: 2.67
          Mean value_function loss: 135.9449
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 62.1907
                       Mean reward: 67.41
               Mean episode length: 62.48
    Episode_Reward/reaching_object: 0.3221
     Episode_Reward/lifting_object: 12.7333
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0247
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 63.3750
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 2.57s
                      Time elapsed: 00:34:30
                               ETA: 00:59:48

################################################################################
                     [1m Learning iteration 732/2000 [0m                      

                       Computation: 39237 steps/s (collection: 2.397s, learning 0.108s)
             Mean action noise std: 2.67
          Mean value_function loss: 159.6558
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.1922
                       Mean reward: 67.49
               Mean episode length: 62.34
    Episode_Reward/reaching_object: 0.3180
     Episode_Reward/lifting_object: 12.5035
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0246
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 64.0417
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 2.51s
                      Time elapsed: 00:34:32
                               ETA: 00:59:45

################################################################################
                     [1m Learning iteration 733/2000 [0m                      

                       Computation: 39020 steps/s (collection: 2.362s, learning 0.158s)
             Mean action noise std: 2.67
          Mean value_function loss: 198.9345
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 62.1951
                       Mean reward: 70.06
               Mean episode length: 63.39
    Episode_Reward/reaching_object: 0.3336
     Episode_Reward/lifting_object: 12.7315
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0262
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 66.0833
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 2.52s
                      Time elapsed: 00:34:35
                               ETA: 00:59:41

################################################################################
                     [1m Learning iteration 734/2000 [0m                      

                       Computation: 37285 steps/s (collection: 2.507s, learning 0.129s)
             Mean action noise std: 2.67
          Mean value_function loss: 158.6675
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 62.2007
                       Mean reward: 68.19
               Mean episode length: 62.00
    Episode_Reward/reaching_object: 0.3299
     Episode_Reward/lifting_object: 12.7203
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0259
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 65.1250
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 2.64s
                      Time elapsed: 00:34:37
                               ETA: 00:59:38

################################################################################
                     [1m Learning iteration 735/2000 [0m                      

                       Computation: 33844 steps/s (collection: 2.654s, learning 0.251s)
             Mean action noise std: 2.67
          Mean value_function loss: 153.9508
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 62.2078
                       Mean reward: 66.90
               Mean episode length: 67.59
    Episode_Reward/reaching_object: 0.3245
     Episode_Reward/lifting_object: 12.2807
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0255
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 64.4583
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 2.90s
                      Time elapsed: 00:34:40
                               ETA: 00:59:36

################################################################################
                     [1m Learning iteration 736/2000 [0m                      

                       Computation: 29980 steps/s (collection: 3.137s, learning 0.142s)
             Mean action noise std: 2.67
          Mean value_function loss: 165.1818
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 62.2094
                       Mean reward: 68.54
               Mean episode length: 60.43
    Episode_Reward/reaching_object: 0.3335
     Episode_Reward/lifting_object: 13.3486
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0260
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 62.7083
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 3.28s
                      Time elapsed: 00:34:43
                               ETA: 00:59:34

################################################################################
                     [1m Learning iteration 737/2000 [0m                      

                       Computation: 37481 steps/s (collection: 2.491s, learning 0.132s)
             Mean action noise std: 2.67
          Mean value_function loss: 187.4300
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 62.2129
                       Mean reward: 72.10
               Mean episode length: 66.17
    Episode_Reward/reaching_object: 0.3352
     Episode_Reward/lifting_object: 13.1491
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 63.4167
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 2.62s
                      Time elapsed: 00:34:46
                               ETA: 00:59:30

################################################################################
                     [1m Learning iteration 738/2000 [0m                      

                       Computation: 37917 steps/s (collection: 2.441s, learning 0.152s)
             Mean action noise std: 2.67
          Mean value_function loss: 156.4077
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 62.2164
                       Mean reward: 69.97
               Mean episode length: 66.51
    Episode_Reward/reaching_object: 0.3390
     Episode_Reward/lifting_object: 13.2977
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 60.8750
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 2.59s
                      Time elapsed: 00:34:49
                               ETA: 00:59:27

################################################################################
                     [1m Learning iteration 739/2000 [0m                      

                       Computation: 39836 steps/s (collection: 2.360s, learning 0.108s)
             Mean action noise std: 2.67
          Mean value_function loss: 175.4125
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 62.2195
                       Mean reward: 65.35
               Mean episode length: 57.64
    Episode_Reward/reaching_object: 0.3448
     Episode_Reward/lifting_object: 13.7652
      Episode_Reward/object_height: 0.0010
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 60.4583
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 2.47s
                      Time elapsed: 00:34:51
                               ETA: 00:59:24

################################################################################
                     [1m Learning iteration 740/2000 [0m                      

                       Computation: 37254 steps/s (collection: 2.513s, learning 0.126s)
             Mean action noise std: 2.68
          Mean value_function loss: 150.8371
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.2219
                       Mean reward: 69.02
               Mean episode length: 65.34
    Episode_Reward/reaching_object: 0.3547
     Episode_Reward/lifting_object: 13.8769
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 64.7917
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 2.64s
                      Time elapsed: 00:34:54
                               ETA: 00:59:21

################################################################################
                     [1m Learning iteration 741/2000 [0m                      

                       Computation: 38580 steps/s (collection: 2.430s, learning 0.118s)
             Mean action noise std: 2.68
          Mean value_function loss: 185.7235
               Mean surrogate loss: 0.0077
                 Mean entropy loss: 62.2241
                       Mean reward: 76.43
               Mean episode length: 64.58
    Episode_Reward/reaching_object: 0.3587
     Episode_Reward/lifting_object: 14.2072
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 58.7083
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 2.55s
                      Time elapsed: 00:34:56
                               ETA: 00:59:17

################################################################################
                     [1m Learning iteration 742/2000 [0m                      

                       Computation: 37923 steps/s (collection: 2.471s, learning 0.121s)
             Mean action noise std: 2.68
          Mean value_function loss: 161.8995
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 62.2249
                       Mean reward: 74.39
               Mean episode length: 64.30
    Episode_Reward/reaching_object: 0.3559
     Episode_Reward/lifting_object: 14.1548
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9167
     Episode_Termination/robot_out: 59.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 2.59s
                      Time elapsed: 00:34:59
                               ETA: 00:59:14

################################################################################
                     [1m Learning iteration 743/2000 [0m                      

                       Computation: 37931 steps/s (collection: 2.477s, learning 0.114s)
             Mean action noise std: 2.68
          Mean value_function loss: 172.2877
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 62.2275
                       Mean reward: 74.26
               Mean episode length: 65.50
    Episode_Reward/reaching_object: 0.3530
     Episode_Reward/lifting_object: 13.9364
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 60.5833
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 2.59s
                      Time elapsed: 00:35:01
                               ETA: 00:59:11

################################################################################
                     [1m Learning iteration 744/2000 [0m                      

                       Computation: 41435 steps/s (collection: 2.262s, learning 0.110s)
             Mean action noise std: 2.68
          Mean value_function loss: 171.3816
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 62.2322
                       Mean reward: 70.83
               Mean episode length: 65.50
    Episode_Reward/reaching_object: 0.3597
     Episode_Reward/lifting_object: 14.2022
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 61.6667
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 2.37s
                      Time elapsed: 00:35:04
                               ETA: 00:59:07

################################################################################
                     [1m Learning iteration 745/2000 [0m                      

                       Computation: 39667 steps/s (collection: 2.372s, learning 0.107s)
             Mean action noise std: 2.68
          Mean value_function loss: 187.5264
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 62.2340
                       Mean reward: 61.47
               Mean episode length: 61.16
    Episode_Reward/reaching_object: 0.3659
     Episode_Reward/lifting_object: 14.4274
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0287
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 57.6250
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 2.48s
                      Time elapsed: 00:35:06
                               ETA: 00:59:04

################################################################################
                     [1m Learning iteration 746/2000 [0m                      

                       Computation: 39050 steps/s (collection: 2.406s, learning 0.111s)
             Mean action noise std: 2.68
          Mean value_function loss: 162.9584
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 62.2369
                       Mean reward: 78.09
               Mean episode length: 70.31
    Episode_Reward/reaching_object: 0.3661
     Episode_Reward/lifting_object: 14.7298
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 62.3333
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 2.52s
                      Time elapsed: 00:35:09
                               ETA: 00:59:00

################################################################################
                     [1m Learning iteration 747/2000 [0m                      

                       Computation: 41150 steps/s (collection: 2.283s, learning 0.106s)
             Mean action noise std: 2.68
          Mean value_function loss: 188.3217
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 62.2404
                       Mean reward: 71.35
               Mean episode length: 63.29
    Episode_Reward/reaching_object: 0.3573
     Episode_Reward/lifting_object: 14.3119
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 61.1250
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 2.39s
                      Time elapsed: 00:35:11
                               ETA: 00:58:57

################################################################################
                     [1m Learning iteration 748/2000 [0m                      

                       Computation: 38760 steps/s (collection: 2.340s, learning 0.196s)
             Mean action noise std: 2.68
          Mean value_function loss: 182.4296
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 62.2439
                       Mean reward: 66.82
               Mean episode length: 60.27
    Episode_Reward/reaching_object: 0.3539
     Episode_Reward/lifting_object: 14.4066
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 63.5000
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 2.54s
                      Time elapsed: 00:35:14
                               ETA: 00:58:54

################################################################################
                     [1m Learning iteration 749/2000 [0m                      

                       Computation: 36839 steps/s (collection: 2.478s, learning 0.190s)
             Mean action noise std: 2.68
          Mean value_function loss: 179.6922
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 62.2458
                       Mean reward: 77.18
               Mean episode length: 67.59
    Episode_Reward/reaching_object: 0.3568
     Episode_Reward/lifting_object: 14.8307
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 60.0833
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 2.67s
                      Time elapsed: 00:35:16
                               ETA: 00:58:51

################################################################################
                     [1m Learning iteration 750/2000 [0m                      

                       Computation: 36632 steps/s (collection: 2.532s, learning 0.151s)
             Mean action noise std: 2.68
          Mean value_function loss: 161.4475
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 62.2484
                       Mean reward: 71.50
               Mean episode length: 62.55
    Episode_Reward/reaching_object: 0.3530
     Episode_Reward/lifting_object: 14.1976
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 61.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 2.68s
                      Time elapsed: 00:35:19
                               ETA: 00:58:47

################################################################################
                     [1m Learning iteration 751/2000 [0m                      

                       Computation: 38662 steps/s (collection: 2.388s, learning 0.155s)
             Mean action noise std: 2.68
          Mean value_function loss: 173.1056
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 62.2542
                       Mean reward: 74.90
               Mean episode length: 65.13
    Episode_Reward/reaching_object: 0.3605
     Episode_Reward/lifting_object: 15.0492
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0279
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 59.6250
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 2.54s
                      Time elapsed: 00:35:22
                               ETA: 00:58:44

################################################################################
                     [1m Learning iteration 752/2000 [0m                      

                       Computation: 39316 steps/s (collection: 2.386s, learning 0.114s)
             Mean action noise std: 2.68
          Mean value_function loss: 177.5833
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 62.2589
                       Mean reward: 78.62
               Mean episode length: 67.20
    Episode_Reward/reaching_object: 0.3615
     Episode_Reward/lifting_object: 14.8515
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0293
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 60.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 2.50s
                      Time elapsed: 00:35:24
                               ETA: 00:58:41

################################################################################
                     [1m Learning iteration 753/2000 [0m                      

                       Computation: 39853 steps/s (collection: 2.325s, learning 0.142s)
             Mean action noise std: 2.68
          Mean value_function loss: 200.5521
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 62.2629
                       Mean reward: 75.21
               Mean episode length: 65.08
    Episode_Reward/reaching_object: 0.3489
     Episode_Reward/lifting_object: 14.3178
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 59.2083
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 2.47s
                      Time elapsed: 00:35:27
                               ETA: 00:58:37

################################################################################
                     [1m Learning iteration 754/2000 [0m                      

                       Computation: 39812 steps/s (collection: 2.325s, learning 0.144s)
             Mean action noise std: 2.68
          Mean value_function loss: 187.5704
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 62.2664
                       Mean reward: 87.89
               Mean episode length: 69.42
    Episode_Reward/reaching_object: 0.3684
     Episode_Reward/lifting_object: 15.5742
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 62.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 2.47s
                      Time elapsed: 00:35:29
                               ETA: 00:58:34

################################################################################
                     [1m Learning iteration 755/2000 [0m                      

                       Computation: 40157 steps/s (collection: 2.287s, learning 0.161s)
             Mean action noise std: 2.68
          Mean value_function loss: 168.5158
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 62.2683
                       Mean reward: 74.69
               Mean episode length: 64.70
    Episode_Reward/reaching_object: 0.3567
     Episode_Reward/lifting_object: 14.9964
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 61.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 2.45s
                      Time elapsed: 00:35:32
                               ETA: 00:58:31

################################################################################
                     [1m Learning iteration 756/2000 [0m                      

                       Computation: 38388 steps/s (collection: 2.423s, learning 0.137s)
             Mean action noise std: 2.68
          Mean value_function loss: 178.1532
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 62.2690
                       Mean reward: 79.08
               Mean episode length: 64.74
    Episode_Reward/reaching_object: 0.3567
     Episode_Reward/lifting_object: 14.8871
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 57.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 2.56s
                      Time elapsed: 00:35:34
                               ETA: 00:58:27

################################################################################
                     [1m Learning iteration 757/2000 [0m                      

                       Computation: 39076 steps/s (collection: 2.364s, learning 0.152s)
             Mean action noise std: 2.68
          Mean value_function loss: 211.2767
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 62.2694
                       Mean reward: 75.31
               Mean episode length: 64.80
    Episode_Reward/reaching_object: 0.3598
     Episode_Reward/lifting_object: 14.9827
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0286
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 63.2083
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 2.52s
                      Time elapsed: 00:35:37
                               ETA: 00:58:24

################################################################################
                     [1m Learning iteration 758/2000 [0m                      

                       Computation: 38338 steps/s (collection: 2.437s, learning 0.127s)
             Mean action noise std: 2.68
          Mean value_function loss: 193.0438
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 62.2698
                       Mean reward: 73.22
               Mean episode length: 62.65
    Episode_Reward/reaching_object: 0.3606
     Episode_Reward/lifting_object: 15.0439
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 62.1250
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 2.56s
                      Time elapsed: 00:35:39
                               ETA: 00:58:21

################################################################################
                     [1m Learning iteration 759/2000 [0m                      

                       Computation: 38292 steps/s (collection: 2.415s, learning 0.153s)
             Mean action noise std: 2.68
          Mean value_function loss: 177.3912
               Mean surrogate loss: 0.0134
                 Mean entropy loss: 62.2701
                       Mean reward: 77.99
               Mean episode length: 67.87
    Episode_Reward/reaching_object: 0.3545
     Episode_Reward/lifting_object: 14.9416
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 60.5833
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 2.57s
                      Time elapsed: 00:35:42
                               ETA: 00:58:18

################################################################################
                     [1m Learning iteration 760/2000 [0m                      

                       Computation: 39101 steps/s (collection: 2.391s, learning 0.123s)
             Mean action noise std: 2.68
          Mean value_function loss: 180.0478
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 62.2703
                       Mean reward: 77.54
               Mean episode length: 65.50
    Episode_Reward/reaching_object: 0.3568
     Episode_Reward/lifting_object: 15.2515
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0280
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 65.6250
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 2.51s
                      Time elapsed: 00:35:44
                               ETA: 00:58:14

################################################################################
                     [1m Learning iteration 761/2000 [0m                      

                       Computation: 38383 steps/s (collection: 2.406s, learning 0.155s)
             Mean action noise std: 2.68
          Mean value_function loss: 195.6245
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 62.2708
                       Mean reward: 68.91
               Mean episode length: 64.37
    Episode_Reward/reaching_object: 0.3543
     Episode_Reward/lifting_object: 14.9608
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0276
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 62.0417
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 2.56s
                      Time elapsed: 00:35:47
                               ETA: 00:58:11

################################################################################
                     [1m Learning iteration 762/2000 [0m                      

                       Computation: 36997 steps/s (collection: 2.519s, learning 0.138s)
             Mean action noise std: 2.68
          Mean value_function loss: 174.6193
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 62.2716
                       Mean reward: 77.26
               Mean episode length: 64.81
    Episode_Reward/reaching_object: 0.3499
     Episode_Reward/lifting_object: 15.2658
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 62.9167
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 2.66s
                      Time elapsed: 00:35:49
                               ETA: 00:58:08

################################################################################
                     [1m Learning iteration 763/2000 [0m                      

                       Computation: 37991 steps/s (collection: 2.445s, learning 0.142s)
             Mean action noise std: 2.68
          Mean value_function loss: 188.4361
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 62.2719
                       Mean reward: 83.53
               Mean episode length: 68.61
    Episode_Reward/reaching_object: 0.3491
     Episode_Reward/lifting_object: 15.1444
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 63.7500
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 2.59s
                      Time elapsed: 00:35:52
                               ETA: 00:58:05

################################################################################
                     [1m Learning iteration 764/2000 [0m                      

                       Computation: 38987 steps/s (collection: 2.383s, learning 0.138s)
             Mean action noise std: 2.68
          Mean value_function loss: 203.1908
               Mean surrogate loss: 0.0124
                 Mean entropy loss: 62.2721
                       Mean reward: 74.10
               Mean episode length: 61.96
    Episode_Reward/reaching_object: 0.3516
     Episode_Reward/lifting_object: 15.1923
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 62.2917
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 2.52s
                      Time elapsed: 00:35:55
                               ETA: 00:58:01

################################################################################
                     [1m Learning iteration 765/2000 [0m                      

                       Computation: 39707 steps/s (collection: 2.353s, learning 0.123s)
             Mean action noise std: 2.68
          Mean value_function loss: 184.0978
               Mean surrogate loss: 0.0159
                 Mean entropy loss: 62.2722
                       Mean reward: 77.89
               Mean episode length: 64.84
    Episode_Reward/reaching_object: 0.3538
     Episode_Reward/lifting_object: 15.2387
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 59.5000
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 2.48s
                      Time elapsed: 00:35:57
                               ETA: 00:57:58

################################################################################
                     [1m Learning iteration 766/2000 [0m                      

                       Computation: 38513 steps/s (collection: 2.408s, learning 0.145s)
             Mean action noise std: 2.68
          Mean value_function loss: 191.8569
               Mean surrogate loss: 0.0126
                 Mean entropy loss: 62.2723
                       Mean reward: 81.91
               Mean episode length: 68.56
    Episode_Reward/reaching_object: 0.3560
     Episode_Reward/lifting_object: 15.0649
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 63.1667
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 2.55s
                      Time elapsed: 00:36:00
                               ETA: 00:57:55

################################################################################
                     [1m Learning iteration 767/2000 [0m                      

                       Computation: 36915 steps/s (collection: 2.527s, learning 0.136s)
             Mean action noise std: 2.68
          Mean value_function loss: 170.8777
               Mean surrogate loss: 0.0108
                 Mean entropy loss: 62.2724
                       Mean reward: 76.74
               Mean episode length: 63.55
    Episode_Reward/reaching_object: 0.3559
     Episode_Reward/lifting_object: 15.4228
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 62.9583
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 2.66s
                      Time elapsed: 00:36:02
                               ETA: 00:57:52

################################################################################
                     [1m Learning iteration 768/2000 [0m                      

                       Computation: 38490 steps/s (collection: 2.402s, learning 0.152s)
             Mean action noise std: 2.68
          Mean value_function loss: 175.4731
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 62.2729
                       Mean reward: 87.13
               Mean episode length: 64.24
    Episode_Reward/reaching_object: 0.3508
     Episode_Reward/lifting_object: 15.1733
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 60.5417
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 2.55s
                      Time elapsed: 00:36:05
                               ETA: 00:57:49

################################################################################
                     [1m Learning iteration 769/2000 [0m                      

                       Computation: 39778 steps/s (collection: 2.349s, learning 0.122s)
             Mean action noise std: 2.68
          Mean value_function loss: 168.1125
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 62.2749
                       Mean reward: 91.93
               Mean episode length: 70.16
    Episode_Reward/reaching_object: 0.3602
     Episode_Reward/lifting_object: 15.8505
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0284
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 65.4167
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 2.47s
                      Time elapsed: 00:36:07
                               ETA: 00:57:45

################################################################################
                     [1m Learning iteration 770/2000 [0m                      

                       Computation: 40304 steps/s (collection: 2.306s, learning 0.133s)
             Mean action noise std: 2.68
          Mean value_function loss: 185.8560
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 62.2789
                       Mean reward: 77.75
               Mean episode length: 64.86
    Episode_Reward/reaching_object: 0.3494
     Episode_Reward/lifting_object: 15.0446
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 2.0000
     Episode_Termination/robot_out: 63.7083
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 2.44s
                      Time elapsed: 00:36:10
                               ETA: 00:57:42

################################################################################
                     [1m Learning iteration 771/2000 [0m                      

                       Computation: 40136 steps/s (collection: 2.305s, learning 0.144s)
             Mean action noise std: 2.68
          Mean value_function loss: 179.8444
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 62.2844
                       Mean reward: 81.46
               Mean episode length: 63.96
    Episode_Reward/reaching_object: 0.3521
     Episode_Reward/lifting_object: 15.6531
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 61.9583
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 2.45s
                      Time elapsed: 00:36:12
                               ETA: 00:57:38

################################################################################
                     [1m Learning iteration 772/2000 [0m                      

                       Computation: 39807 steps/s (collection: 2.336s, learning 0.133s)
             Mean action noise std: 2.68
          Mean value_function loss: 185.9350
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 62.2907
                       Mean reward: 88.94
               Mean episode length: 66.70
    Episode_Reward/reaching_object: 0.3477
     Episode_Reward/lifting_object: 15.0925
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0272
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5833
     Episode_Termination/robot_out: 62.3333
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 2.47s
                      Time elapsed: 00:36:15
                               ETA: 00:57:35

################################################################################
                     [1m Learning iteration 773/2000 [0m                      

                       Computation: 40946 steps/s (collection: 2.285s, learning 0.115s)
             Mean action noise std: 2.68
          Mean value_function loss: 200.1189
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 62.2950
                       Mean reward: 75.75
               Mean episode length: 62.48
    Episode_Reward/reaching_object: 0.3473
     Episode_Reward/lifting_object: 15.0045
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 61.1250
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 2.40s
                      Time elapsed: 00:36:17
                               ETA: 00:57:32

################################################################################
                     [1m Learning iteration 774/2000 [0m                      

                       Computation: 41621 steps/s (collection: 2.239s, learning 0.123s)
             Mean action noise std: 2.68
          Mean value_function loss: 189.5273
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 62.2995
                       Mean reward: 72.61
               Mean episode length: 64.40
    Episode_Reward/reaching_object: 0.3610
     Episode_Reward/lifting_object: 16.1761
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 61.3750
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 2.36s
                      Time elapsed: 00:36:19
                               ETA: 00:57:28

################################################################################
                     [1m Learning iteration 775/2000 [0m                      

                       Computation: 39649 steps/s (collection: 2.335s, learning 0.144s)
             Mean action noise std: 2.69
          Mean value_function loss: 184.2882
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 62.3054
                       Mean reward: 79.54
               Mean episode length: 64.16
    Episode_Reward/reaching_object: 0.3617
     Episode_Reward/lifting_object: 16.3038
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0285
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 62.5000
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 2.48s
                      Time elapsed: 00:36:22
                               ETA: 00:57:25

################################################################################
                     [1m Learning iteration 776/2000 [0m                      

                       Computation: 36371 steps/s (collection: 2.556s, learning 0.147s)
             Mean action noise std: 2.69
          Mean value_function loss: 190.5091
               Mean surrogate loss: 0.0075
                 Mean entropy loss: 62.3074
                       Mean reward: 77.56
               Mean episode length: 65.30
    Episode_Reward/reaching_object: 0.3627
     Episode_Reward/lifting_object: 16.3218
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 61.9583
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 2.70s
                      Time elapsed: 00:36:25
                               ETA: 00:57:22

################################################################################
                     [1m Learning iteration 777/2000 [0m                      

                       Computation: 38272 steps/s (collection: 2.442s, learning 0.126s)
             Mean action noise std: 2.69
          Mean value_function loss: 207.9881
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 62.3086
                       Mean reward: 81.40
               Mean episode length: 67.82
    Episode_Reward/reaching_object: 0.3571
     Episode_Reward/lifting_object: 16.1346
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 65.0833
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 2.57s
                      Time elapsed: 00:36:27
                               ETA: 00:57:18

################################################################################
                     [1m Learning iteration 778/2000 [0m                      

                       Computation: 39483 steps/s (collection: 2.350s, learning 0.140s)
             Mean action noise std: 2.69
          Mean value_function loss: 203.2337
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 62.3117
                       Mean reward: 74.50
               Mean episode length: 62.41
    Episode_Reward/reaching_object: 0.3505
     Episode_Reward/lifting_object: 15.6047
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0274
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 61.5417
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 2.49s
                      Time elapsed: 00:36:30
                               ETA: 00:57:15

################################################################################
                     [1m Learning iteration 779/2000 [0m                      

                       Computation: 37021 steps/s (collection: 2.514s, learning 0.141s)
             Mean action noise std: 2.69
          Mean value_function loss: 214.1182
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 62.3162
                       Mean reward: 74.91
               Mean episode length: 60.12
    Episode_Reward/reaching_object: 0.3494
     Episode_Reward/lifting_object: 15.9030
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 68.0417
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 2.66s
                      Time elapsed: 00:36:32
                               ETA: 00:57:12

################################################################################
                     [1m Learning iteration 780/2000 [0m                      

                       Computation: 36049 steps/s (collection: 2.601s, learning 0.126s)
             Mean action noise std: 2.69
          Mean value_function loss: 208.0653
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 62.3199
                       Mean reward: 75.20
               Mean episode length: 59.20
    Episode_Reward/reaching_object: 0.3436
     Episode_Reward/lifting_object: 15.2470
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 64.3333
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 2.73s
                      Time elapsed: 00:36:35
                               ETA: 00:57:09

################################################################################
                     [1m Learning iteration 781/2000 [0m                      

                       Computation: 40399 steps/s (collection: 2.326s, learning 0.108s)
             Mean action noise std: 2.69
          Mean value_function loss: 249.2851
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 62.3224
                       Mean reward: 73.46
               Mean episode length: 60.95
    Episode_Reward/reaching_object: 0.3522
     Episode_Reward/lifting_object: 15.7971
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 62.6667
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 2.43s
                      Time elapsed: 00:36:37
                               ETA: 00:57:06

################################################################################
                     [1m Learning iteration 782/2000 [0m                      

                       Computation: 39760 steps/s (collection: 2.336s, learning 0.136s)
             Mean action noise std: 2.69
          Mean value_function loss: 194.3786
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 62.3243
                       Mean reward: 91.27
               Mean episode length: 62.57
    Episode_Reward/reaching_object: 0.3530
     Episode_Reward/lifting_object: 16.2414
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 63.5417
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 2.47s
                      Time elapsed: 00:36:40
                               ETA: 00:57:02

################################################################################
                     [1m Learning iteration 783/2000 [0m                      

                       Computation: 38174 steps/s (collection: 2.456s, learning 0.119s)
             Mean action noise std: 2.69
          Mean value_function loss: 202.5825
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 62.3250
                       Mean reward: 77.71
               Mean episode length: 62.45
    Episode_Reward/reaching_object: 0.3611
     Episode_Reward/lifting_object: 16.2805
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0270
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 64.3750
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 2.58s
                      Time elapsed: 00:36:43
                               ETA: 00:56:59

################################################################################
                     [1m Learning iteration 784/2000 [0m                      

                       Computation: 39793 steps/s (collection: 2.328s, learning 0.142s)
             Mean action noise std: 2.69
          Mean value_function loss: 194.3607
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 62.3254
                       Mean reward: 84.30
               Mean episode length: 62.30
    Episode_Reward/reaching_object: 0.3532
     Episode_Reward/lifting_object: 16.4869
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 62.4167
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 2.47s
                      Time elapsed: 00:36:45
                               ETA: 00:56:56

################################################################################
                     [1m Learning iteration 785/2000 [0m                      

                       Computation: 36971 steps/s (collection: 2.513s, learning 0.146s)
             Mean action noise std: 2.69
          Mean value_function loss: 193.2299
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 62.3260
                       Mean reward: 83.19
               Mean episode length: 66.39
    Episode_Reward/reaching_object: 0.3499
     Episode_Reward/lifting_object: 15.8730
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 62.0833
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 2.66s
                      Time elapsed: 00:36:48
                               ETA: 00:56:53

################################################################################
                     [1m Learning iteration 786/2000 [0m                      

                       Computation: 38448 steps/s (collection: 2.424s, learning 0.133s)
             Mean action noise std: 2.69
          Mean value_function loss: 197.8548
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 62.3299
                       Mean reward: 81.71
               Mean episode length: 62.43
    Episode_Reward/reaching_object: 0.3617
     Episode_Reward/lifting_object: 16.4212
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0275
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 60.1250
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 2.56s
                      Time elapsed: 00:36:50
                               ETA: 00:56:50

################################################################################
                     [1m Learning iteration 787/2000 [0m                      

                       Computation: 39630 steps/s (collection: 2.339s, learning 0.141s)
             Mean action noise std: 2.69
          Mean value_function loss: 213.2208
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 62.3343
                       Mean reward: 90.01
               Mean episode length: 65.63
    Episode_Reward/reaching_object: 0.3640
     Episode_Reward/lifting_object: 16.7905
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 64.6667
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 2.48s
                      Time elapsed: 00:36:53
                               ETA: 00:56:46

################################################################################
                     [1m Learning iteration 788/2000 [0m                      

                       Computation: 38688 steps/s (collection: 2.388s, learning 0.153s)
             Mean action noise std: 2.69
          Mean value_function loss: 180.8697
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 62.3366
                       Mean reward: 94.59
               Mean episode length: 69.04
    Episode_Reward/reaching_object: 0.3708
     Episode_Reward/lifting_object: 17.1311
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0281
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 57.5417
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 2.54s
                      Time elapsed: 00:36:55
                               ETA: 00:56:43

################################################################################
                     [1m Learning iteration 789/2000 [0m                      

                       Computation: 38031 steps/s (collection: 2.448s, learning 0.136s)
             Mean action noise std: 2.69
          Mean value_function loss: 191.0571
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 62.3390
                       Mean reward: 84.36
               Mean episode length: 64.52
    Episode_Reward/reaching_object: 0.3626
     Episode_Reward/lifting_object: 16.7592
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0273
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 59.5833
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 2.58s
                      Time elapsed: 00:36:58
                               ETA: 00:56:40

################################################################################
                     [1m Learning iteration 790/2000 [0m                      

                       Computation: 39287 steps/s (collection: 2.374s, learning 0.129s)
             Mean action noise std: 2.69
          Mean value_function loss: 196.5768
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 62.3421
                       Mean reward: 91.86
               Mean episode length: 70.61
    Episode_Reward/reaching_object: 0.3893
     Episode_Reward/lifting_object: 18.0490
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 59.5000
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 2.50s
                      Time elapsed: 00:37:00
                               ETA: 00:56:37

################################################################################
                     [1m Learning iteration 791/2000 [0m                      

                       Computation: 38360 steps/s (collection: 2.419s, learning 0.144s)
             Mean action noise std: 2.69
          Mean value_function loss: 188.1629
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 62.3429
                       Mean reward: 94.63
               Mean episode length: 70.22
    Episode_Reward/reaching_object: 0.3790
     Episode_Reward/lifting_object: 17.5276
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 59.6250
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 2.56s
                      Time elapsed: 00:37:03
                               ETA: 00:56:34

################################################################################
                     [1m Learning iteration 792/2000 [0m                      

                       Computation: 38027 steps/s (collection: 2.446s, learning 0.139s)
             Mean action noise std: 2.69
          Mean value_function loss: 191.6953
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 62.3441
                       Mean reward: 96.87
               Mean episode length: 72.46
    Episode_Reward/reaching_object: 0.3817
     Episode_Reward/lifting_object: 17.6739
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 59.3333
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 2.59s
                      Time elapsed: 00:37:05
                               ETA: 00:56:30

################################################################################
                     [1m Learning iteration 793/2000 [0m                      

                       Computation: 35122 steps/s (collection: 2.611s, learning 0.188s)
             Mean action noise std: 2.69
          Mean value_function loss: 216.8974
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 62.3461
                       Mean reward: 88.67
               Mean episode length: 71.81
    Episode_Reward/reaching_object: 0.3902
     Episode_Reward/lifting_object: 18.0442
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 54.8750
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 2.80s
                      Time elapsed: 00:37:08
                               ETA: 00:56:28

################################################################################
                     [1m Learning iteration 794/2000 [0m                      

                       Computation: 37735 steps/s (collection: 2.465s, learning 0.140s)
             Mean action noise std: 2.69
          Mean value_function loss: 203.9892
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 62.3491
                       Mean reward: 90.53
               Mean episode length: 65.78
    Episode_Reward/reaching_object: 0.3856
     Episode_Reward/lifting_object: 18.2848
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 61.0833
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 2.61s
                      Time elapsed: 00:37:11
                               ETA: 00:56:24

################################################################################
                     [1m Learning iteration 795/2000 [0m                      

                       Computation: 36979 steps/s (collection: 2.543s, learning 0.115s)
             Mean action noise std: 2.69
          Mean value_function loss: 228.9124
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 62.3528
                       Mean reward: 98.95
               Mean episode length: 71.23
    Episode_Reward/reaching_object: 0.3945
     Episode_Reward/lifting_object: 18.4909
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0311
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 58.7083
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 2.66s
                      Time elapsed: 00:37:14
                               ETA: 00:56:21

################################################################################
                     [1m Learning iteration 796/2000 [0m                      

                       Computation: 38704 steps/s (collection: 2.399s, learning 0.141s)
             Mean action noise std: 2.69
          Mean value_function loss: 244.7630
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 62.3551
                       Mean reward: 91.11
               Mean episode length: 66.86
    Episode_Reward/reaching_object: 0.3845
     Episode_Reward/lifting_object: 17.9171
      Episode_Reward/object_height: 0.0012
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 57.3750
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 2.54s
                      Time elapsed: 00:37:16
                               ETA: 00:56:18

################################################################################
                     [1m Learning iteration 797/2000 [0m                      

                       Computation: 37933 steps/s (collection: 2.470s, learning 0.121s)
             Mean action noise std: 2.69
          Mean value_function loss: 208.6850
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 62.3568
                       Mean reward: 103.97
               Mean episode length: 75.36
    Episode_Reward/reaching_object: 0.3968
     Episode_Reward/lifting_object: 18.2344
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 56.7917
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 2.59s
                      Time elapsed: 00:37:19
                               ETA: 00:56:15

################################################################################
                     [1m Learning iteration 798/2000 [0m                      

                       Computation: 37519 steps/s (collection: 2.502s, learning 0.118s)
             Mean action noise std: 2.69
          Mean value_function loss: 210.3383
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 62.3586
                       Mean reward: 94.54
               Mean episode length: 72.54
    Episode_Reward/reaching_object: 0.3990
     Episode_Reward/lifting_object: 18.9976
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 59.1667
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 2.62s
                      Time elapsed: 00:37:21
                               ETA: 00:56:12

################################################################################
                     [1m Learning iteration 799/2000 [0m                      

                       Computation: 35583 steps/s (collection: 2.565s, learning 0.198s)
             Mean action noise std: 2.69
          Mean value_function loss: 230.7324
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 62.3608
                       Mean reward: 97.18
               Mean episode length: 65.93
    Episode_Reward/reaching_object: 0.3970
     Episode_Reward/lifting_object: 19.1159
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 57.9583
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 2.76s
                      Time elapsed: 00:37:24
                               ETA: 00:56:09

################################################################################
                     [1m Learning iteration 800/2000 [0m                      

                       Computation: 37970 steps/s (collection: 2.449s, learning 0.140s)
             Mean action noise std: 2.69
          Mean value_function loss: 214.5471
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 62.3614
                       Mean reward: 100.29
               Mean episode length: 68.36
    Episode_Reward/reaching_object: 0.3926
     Episode_Reward/lifting_object: 18.8154
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 57.5000
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 2.59s
                      Time elapsed: 00:37:27
                               ETA: 00:56:06

################################################################################
                     [1m Learning iteration 801/2000 [0m                      

                       Computation: 37645 steps/s (collection: 2.425s, learning 0.187s)
             Mean action noise std: 2.69
          Mean value_function loss: 207.9916
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 62.3619
                       Mean reward: 101.46
               Mean episode length: 74.76
    Episode_Reward/reaching_object: 0.3928
     Episode_Reward/lifting_object: 18.7874
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 58.2083
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 2.61s
                      Time elapsed: 00:37:29
                               ETA: 00:56:03

################################################################################
                     [1m Learning iteration 802/2000 [0m                      

                       Computation: 38964 steps/s (collection: 2.408s, learning 0.115s)
             Mean action noise std: 2.69
          Mean value_function loss: 224.1808
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 62.3648
                       Mean reward: 97.08
               Mean episode length: 69.14
    Episode_Reward/reaching_object: 0.3984
     Episode_Reward/lifting_object: 19.3057
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 57.5000
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 2.52s
                      Time elapsed: 00:37:32
                               ETA: 00:56:00

################################################################################
                     [1m Learning iteration 803/2000 [0m                      

                       Computation: 34078 steps/s (collection: 2.764s, learning 0.120s)
             Mean action noise std: 2.69
          Mean value_function loss: 202.7804
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 62.3692
                       Mean reward: 98.55
               Mean episode length: 70.97
    Episode_Reward/reaching_object: 0.4003
     Episode_Reward/lifting_object: 19.0326
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 54.8333
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 2.88s
                      Time elapsed: 00:37:35
                               ETA: 00:55:57

################################################################################
                     [1m Learning iteration 804/2000 [0m                      

                       Computation: 37188 steps/s (collection: 2.539s, learning 0.105s)
             Mean action noise std: 2.69
          Mean value_function loss: 203.2326
               Mean surrogate loss: 0.0098
                 Mean entropy loss: 62.3715
                       Mean reward: 106.59
               Mean episode length: 74.86
    Episode_Reward/reaching_object: 0.4075
     Episode_Reward/lifting_object: 20.0726
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 53.9167
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 2.64s
                      Time elapsed: 00:37:37
                               ETA: 00:55:54

################################################################################
                     [1m Learning iteration 805/2000 [0m                      

                       Computation: 39188 steps/s (collection: 2.360s, learning 0.148s)
             Mean action noise std: 2.69
          Mean value_function loss: 211.4130
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 62.3717
                       Mean reward: 109.84
               Mean episode length: 76.97
    Episode_Reward/reaching_object: 0.4103
     Episode_Reward/lifting_object: 19.4849
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 56.2500
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 2.51s
                      Time elapsed: 00:37:40
                               ETA: 00:55:51

################################################################################
                     [1m Learning iteration 806/2000 [0m                      

                       Computation: 27931 steps/s (collection: 3.197s, learning 0.323s)
             Mean action noise std: 2.69
          Mean value_function loss: 209.6306
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 62.3723
                       Mean reward: 100.10
               Mean episode length: 73.78
    Episode_Reward/reaching_object: 0.4137
     Episode_Reward/lifting_object: 19.3017
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 53.5000
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 3.52s
                      Time elapsed: 00:37:43
                               ETA: 00:55:49

################################################################################
                     [1m Learning iteration 807/2000 [0m                      

                       Computation: 29138 steps/s (collection: 3.147s, learning 0.227s)
             Mean action noise std: 2.69
          Mean value_function loss: 218.7081
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 62.3745
                       Mean reward: 95.88
               Mean episode length: 77.23
    Episode_Reward/reaching_object: 0.4263
     Episode_Reward/lifting_object: 19.7197
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 50.4583
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 3.37s
                      Time elapsed: 00:37:47
                               ETA: 00:55:47

################################################################################
                     [1m Learning iteration 808/2000 [0m                      

                       Computation: 32516 steps/s (collection: 2.886s, learning 0.137s)
             Mean action noise std: 2.69
          Mean value_function loss: 219.0285
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 62.3773
                       Mean reward: 97.31
               Mean episode length: 74.26
    Episode_Reward/reaching_object: 0.4221
     Episode_Reward/lifting_object: 19.9539
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 51.8750
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 3.02s
                      Time elapsed: 00:37:50
                               ETA: 00:55:44

################################################################################
                     [1m Learning iteration 809/2000 [0m                      

                       Computation: 32148 steps/s (collection: 2.883s, learning 0.175s)
             Mean action noise std: 2.70
          Mean value_function loss: 222.1939
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 62.3807
                       Mean reward: 101.13
               Mean episode length: 82.40
    Episode_Reward/reaching_object: 0.4327
     Episode_Reward/lifting_object: 19.7798
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 52.5417
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 3.06s
                      Time elapsed: 00:37:53
                               ETA: 00:55:42

################################################################################
                     [1m Learning iteration 810/2000 [0m                      

                       Computation: 28629 steps/s (collection: 3.175s, learning 0.259s)
             Mean action noise std: 2.70
          Mean value_function loss: 193.1428
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 62.3844
                       Mean reward: 97.61
               Mean episode length: 74.86
    Episode_Reward/reaching_object: 0.4376
     Episode_Reward/lifting_object: 20.1520
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0364
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 52.3750
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 3.43s
                      Time elapsed: 00:37:56
                               ETA: 00:55:40

################################################################################
                     [1m Learning iteration 811/2000 [0m                      

                       Computation: 28762 steps/s (collection: 3.252s, learning 0.166s)
             Mean action noise std: 2.70
          Mean value_function loss: 182.3480
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.3877
                       Mean reward: 99.53
               Mean episode length: 73.78
    Episode_Reward/reaching_object: 0.4224
     Episode_Reward/lifting_object: 19.4994
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 50.7500
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 3.42s
                      Time elapsed: 00:38:00
                               ETA: 00:55:38

################################################################################
                     [1m Learning iteration 812/2000 [0m                      

                       Computation: 30519 steps/s (collection: 3.011s, learning 0.210s)
             Mean action noise std: 2.70
          Mean value_function loss: 192.3569
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 62.3901
                       Mean reward: 102.12
               Mean episode length: 77.82
    Episode_Reward/reaching_object: 0.4344
     Episode_Reward/lifting_object: 20.3434
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0362
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 50.3750
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 3.22s
                      Time elapsed: 00:38:03
                               ETA: 00:55:36

################################################################################
                     [1m Learning iteration 813/2000 [0m                      

                       Computation: 33116 steps/s (collection: 2.842s, learning 0.126s)
             Mean action noise std: 2.70
          Mean value_function loss: 207.3437
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 62.3912
                       Mean reward: 108.48
               Mean episode length: 76.98
    Episode_Reward/reaching_object: 0.4384
     Episode_Reward/lifting_object: 20.3783
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 50.8333
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 2.97s
                      Time elapsed: 00:38:06
                               ETA: 00:55:33

################################################################################
                     [1m Learning iteration 814/2000 [0m                      

                       Computation: 32388 steps/s (collection: 2.815s, learning 0.220s)
             Mean action noise std: 2.70
          Mean value_function loss: 197.0959
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 62.3926
                       Mean reward: 100.67
               Mean episode length: 78.85
    Episode_Reward/reaching_object: 0.4352
     Episode_Reward/lifting_object: 19.8442
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 50.3333
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 3.04s
                      Time elapsed: 00:38:09
                               ETA: 00:55:31

################################################################################
                     [1m Learning iteration 815/2000 [0m                      

                       Computation: 27504 steps/s (collection: 3.286s, learning 0.288s)
             Mean action noise std: 2.70
          Mean value_function loss: 195.4922
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 62.3949
                       Mean reward: 109.54
               Mean episode length: 86.70
    Episode_Reward/reaching_object: 0.4455
     Episode_Reward/lifting_object: 20.7063
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0368
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 52.6250
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 3.57s
                      Time elapsed: 00:38:12
                               ETA: 00:55:29

################################################################################
                     [1m Learning iteration 816/2000 [0m                      

                       Computation: 30311 steps/s (collection: 3.107s, learning 0.137s)
             Mean action noise std: 2.70
          Mean value_function loss: 225.1954
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 62.3990
                       Mean reward: 105.50
               Mean episode length: 82.36
    Episode_Reward/reaching_object: 0.4392
     Episode_Reward/lifting_object: 20.7348
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 53.8750
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 3.24s
                      Time elapsed: 00:38:16
                               ETA: 00:55:27

################################################################################
                     [1m Learning iteration 817/2000 [0m                      

                       Computation: 27825 steps/s (collection: 3.260s, learning 0.273s)
             Mean action noise std: 2.70
          Mean value_function loss: 209.3235
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 62.4020
                       Mean reward: 100.94
               Mean episode length: 78.63
    Episode_Reward/reaching_object: 0.4263
     Episode_Reward/lifting_object: 20.0038
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 51.1667
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 3.53s
                      Time elapsed: 00:38:19
                               ETA: 00:55:25

################################################################################
                     [1m Learning iteration 818/2000 [0m                      

                       Computation: 31720 steps/s (collection: 2.868s, learning 0.231s)
             Mean action noise std: 2.70
          Mean value_function loss: 231.6898
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 62.4036
                       Mean reward: 101.14
               Mean episode length: 74.46
    Episode_Reward/reaching_object: 0.4264
     Episode_Reward/lifting_object: 20.5497
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 55.3750
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 3.10s
                      Time elapsed: 00:38:22
                               ETA: 00:55:23

################################################################################
                     [1m Learning iteration 819/2000 [0m                      

                       Computation: 31514 steps/s (collection: 2.815s, learning 0.305s)
             Mean action noise std: 2.70
          Mean value_function loss: 213.8254
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 62.4071
                       Mean reward: 103.26
               Mean episode length: 75.17
    Episode_Reward/reaching_object: 0.4232
     Episode_Reward/lifting_object: 20.2263
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 53.7917
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 3.12s
                      Time elapsed: 00:38:25
                               ETA: 00:55:21

################################################################################
                     [1m Learning iteration 820/2000 [0m                      

                       Computation: 31505 steps/s (collection: 2.867s, learning 0.253s)
             Mean action noise std: 2.70
          Mean value_function loss: 222.7084
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 62.4100
                       Mean reward: 102.27
               Mean episode length: 76.86
    Episode_Reward/reaching_object: 0.4103
     Episode_Reward/lifting_object: 19.5692
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 51.5833
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 3.12s
                      Time elapsed: 00:38:29
                               ETA: 00:55:18

################################################################################
                     [1m Learning iteration 821/2000 [0m                      

                       Computation: 34258 steps/s (collection: 2.649s, learning 0.221s)
             Mean action noise std: 2.70
          Mean value_function loss: 206.8293
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 62.4134
                       Mean reward: 104.45
               Mean episode length: 75.30
    Episode_Reward/reaching_object: 0.4258
     Episode_Reward/lifting_object: 20.5828
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 55.0833
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 2.87s
                      Time elapsed: 00:38:31
                               ETA: 00:55:15

################################################################################
                     [1m Learning iteration 822/2000 [0m                      

                       Computation: 36591 steps/s (collection: 2.454s, learning 0.233s)
             Mean action noise std: 2.70
          Mean value_function loss: 215.5060
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 62.4167
                       Mean reward: 109.27
               Mean episode length: 78.46
    Episode_Reward/reaching_object: 0.4145
     Episode_Reward/lifting_object: 20.2918
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 51.2917
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 2.69s
                      Time elapsed: 00:38:34
                               ETA: 00:55:12

################################################################################
                     [1m Learning iteration 823/2000 [0m                      

                       Computation: 37682 steps/s (collection: 2.506s, learning 0.103s)
             Mean action noise std: 2.70
          Mean value_function loss: 232.3847
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.4190
                       Mean reward: 105.93
               Mean episode length: 74.79
    Episode_Reward/reaching_object: 0.4320
     Episode_Reward/lifting_object: 21.0169
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 52.2500
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 2.61s
                      Time elapsed: 00:38:37
                               ETA: 00:55:09

################################################################################
                     [1m Learning iteration 824/2000 [0m                      

                       Computation: 31581 steps/s (collection: 2.928s, learning 0.185s)
             Mean action noise std: 2.70
          Mean value_function loss: 203.1552
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 62.4225
                       Mean reward: 109.88
               Mean episode length: 80.10
    Episode_Reward/reaching_object: 0.4278
     Episode_Reward/lifting_object: 20.7191
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 52.5833
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 3.11s
                      Time elapsed: 00:38:40
                               ETA: 00:55:07

################################################################################
                     [1m Learning iteration 825/2000 [0m                      

                       Computation: 38049 steps/s (collection: 2.369s, learning 0.215s)
             Mean action noise std: 2.70
          Mean value_function loss: 233.4396
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 62.4248
                       Mean reward: 106.47
               Mean episode length: 73.95
    Episode_Reward/reaching_object: 0.4298
     Episode_Reward/lifting_object: 21.4496
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 53.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 2.58s
                      Time elapsed: 00:38:42
                               ETA: 00:55:04

################################################################################
                     [1m Learning iteration 826/2000 [0m                      

                       Computation: 37945 steps/s (collection: 2.451s, learning 0.140s)
             Mean action noise std: 2.70
          Mean value_function loss: 219.6560
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 62.4254
                       Mean reward: 112.89
               Mean episode length: 79.69
    Episode_Reward/reaching_object: 0.4311
     Episode_Reward/lifting_object: 21.2714
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 54.2500
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 2.59s
                      Time elapsed: 00:38:45
                               ETA: 00:55:01

################################################################################
                     [1m Learning iteration 827/2000 [0m                      

                       Computation: 39291 steps/s (collection: 2.365s, learning 0.137s)
             Mean action noise std: 2.70
          Mean value_function loss: 225.3644
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 62.4254
                       Mean reward: 105.84
               Mean episode length: 74.68
    Episode_Reward/reaching_object: 0.4305
     Episode_Reward/lifting_object: 21.7696
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 53.1667
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 2.50s
                      Time elapsed: 00:38:47
                               ETA: 00:54:57

################################################################################
                     [1m Learning iteration 828/2000 [0m                      

                       Computation: 39120 steps/s (collection: 2.384s, learning 0.129s)
             Mean action noise std: 2.70
          Mean value_function loss: 230.8362
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 62.4261
                       Mean reward: 98.45
               Mean episode length: 69.61
    Episode_Reward/reaching_object: 0.4278
     Episode_Reward/lifting_object: 21.2175
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 53.5833
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 2.51s
                      Time elapsed: 00:38:50
                               ETA: 00:54:54

################################################################################
                     [1m Learning iteration 829/2000 [0m                      

                       Computation: 40656 steps/s (collection: 2.286s, learning 0.132s)
             Mean action noise std: 2.70
          Mean value_function loss: 218.8232
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 62.4304
                       Mean reward: 121.01
               Mean episode length: 77.24
    Episode_Reward/reaching_object: 0.4207
     Episode_Reward/lifting_object: 21.7080
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 52.4583
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 2.42s
                      Time elapsed: 00:38:52
                               ETA: 00:54:51

################################################################################
                     [1m Learning iteration 830/2000 [0m                      

                       Computation: 40021 steps/s (collection: 2.332s, learning 0.125s)
             Mean action noise std: 2.70
          Mean value_function loss: 233.9074
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 62.4349
                       Mean reward: 110.82
               Mean episode length: 77.08
    Episode_Reward/reaching_object: 0.4332
     Episode_Reward/lifting_object: 22.3318
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 54.7083
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 2.46s
                      Time elapsed: 00:38:55
                               ETA: 00:54:48

################################################################################
                     [1m Learning iteration 831/2000 [0m                      

                       Computation: 39945 steps/s (collection: 2.321s, learning 0.140s)
             Mean action noise std: 2.70
          Mean value_function loss: 226.1766
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.4379
                       Mean reward: 106.52
               Mean episode length: 72.58
    Episode_Reward/reaching_object: 0.4233
     Episode_Reward/lifting_object: 21.7253
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 53.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 2.46s
                      Time elapsed: 00:38:57
                               ETA: 00:54:44

################################################################################
                     [1m Learning iteration 832/2000 [0m                      

                       Computation: 40279 steps/s (collection: 2.311s, learning 0.130s)
             Mean action noise std: 2.70
          Mean value_function loss: 243.6373
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 62.4412
                       Mean reward: 111.76
               Mean episode length: 75.98
    Episode_Reward/reaching_object: 0.4236
     Episode_Reward/lifting_object: 21.7902
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 54.4583
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 2.44s
                      Time elapsed: 00:39:00
                               ETA: 00:54:41

################################################################################
                     [1m Learning iteration 833/2000 [0m                      

                       Computation: 40998 steps/s (collection: 2.271s, learning 0.127s)
             Mean action noise std: 2.70
          Mean value_function loss: 246.1933
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 62.4433
                       Mean reward: 111.44
               Mean episode length: 77.54
    Episode_Reward/reaching_object: 0.4276
     Episode_Reward/lifting_object: 22.1818
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 52.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 2.40s
                      Time elapsed: 00:39:02
                               ETA: 00:54:38

################################################################################
                     [1m Learning iteration 834/2000 [0m                      

                       Computation: 40829 steps/s (collection: 2.255s, learning 0.153s)
             Mean action noise std: 2.70
          Mean value_function loss: 222.3976
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.4459
                       Mean reward: 103.67
               Mean episode length: 73.40
    Episode_Reward/reaching_object: 0.4259
     Episode_Reward/lifting_object: 22.1268
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 54.3750
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 2.41s
                      Time elapsed: 00:39:05
                               ETA: 00:54:34

################################################################################
                     [1m Learning iteration 835/2000 [0m                      

                       Computation: 41123 steps/s (collection: 2.291s, learning 0.100s)
             Mean action noise std: 2.70
          Mean value_function loss: 248.9442
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 62.4473
                       Mean reward: 112.83
               Mean episode length: 75.33
    Episode_Reward/reaching_object: 0.4366
     Episode_Reward/lifting_object: 22.9393
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 53.7500
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 2.39s
                      Time elapsed: 00:39:07
                               ETA: 00:54:31

################################################################################
                     [1m Learning iteration 836/2000 [0m                      

                       Computation: 40503 steps/s (collection: 2.285s, learning 0.142s)
             Mean action noise std: 2.70
          Mean value_function loss: 276.6661
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 62.4499
                       Mean reward: 106.17
               Mean episode length: 74.34
    Episode_Reward/reaching_object: 0.4226
     Episode_Reward/lifting_object: 22.0108
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 51.7500
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 2.43s
                      Time elapsed: 00:39:09
                               ETA: 00:54:27

################################################################################
                     [1m Learning iteration 837/2000 [0m                      

                       Computation: 40755 steps/s (collection: 2.273s, learning 0.139s)
             Mean action noise std: 2.70
          Mean value_function loss: 238.4252
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 62.4514
                       Mean reward: 112.13
               Mean episode length: 73.75
    Episode_Reward/reaching_object: 0.4294
     Episode_Reward/lifting_object: 22.4516
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 55.5000
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 2.41s
                      Time elapsed: 00:39:12
                               ETA: 00:54:24

################################################################################
                     [1m Learning iteration 838/2000 [0m                      

                       Computation: 39561 steps/s (collection: 2.349s, learning 0.136s)
             Mean action noise std: 2.70
          Mean value_function loss: 239.3082
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 62.4521
                       Mean reward: 120.80
               Mean episode length: 77.63
    Episode_Reward/reaching_object: 0.4359
     Episode_Reward/lifting_object: 23.2531
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 54.7083
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 2.48s
                      Time elapsed: 00:39:14
                               ETA: 00:54:21

################################################################################
                     [1m Learning iteration 839/2000 [0m                      

                       Computation: 40847 steps/s (collection: 2.286s, learning 0.121s)
             Mean action noise std: 2.70
          Mean value_function loss: 246.4758
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 62.4521
                       Mean reward: 115.98
               Mean episode length: 74.32
    Episode_Reward/reaching_object: 0.4185
     Episode_Reward/lifting_object: 22.4353
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 58.3750
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 2.41s
                      Time elapsed: 00:39:17
                               ETA: 00:54:18

################################################################################
                     [1m Learning iteration 840/2000 [0m                      

                       Computation: 40630 steps/s (collection: 2.283s, learning 0.137s)
             Mean action noise std: 2.70
          Mean value_function loss: 249.8871
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 62.4529
                       Mean reward: 117.60
               Mean episode length: 72.29
    Episode_Reward/reaching_object: 0.4272
     Episode_Reward/lifting_object: 23.1959
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 56.6250
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 2.42s
                      Time elapsed: 00:39:19
                               ETA: 00:54:14

################################################################################
                     [1m Learning iteration 841/2000 [0m                      

                       Computation: 37362 steps/s (collection: 2.482s, learning 0.150s)
             Mean action noise std: 2.70
          Mean value_function loss: 270.5494
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 62.4546
                       Mean reward: 114.20
               Mean episode length: 68.58
    Episode_Reward/reaching_object: 0.4177
     Episode_Reward/lifting_object: 22.4387
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 51.8750
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 2.63s
                      Time elapsed: 00:39:22
                               ETA: 00:54:11

################################################################################
                     [1m Learning iteration 842/2000 [0m                      

                       Computation: 35786 steps/s (collection: 2.593s, learning 0.154s)
             Mean action noise std: 2.70
          Mean value_function loss: 250.4202
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 62.4553
                       Mean reward: 122.60
               Mean episode length: 77.93
    Episode_Reward/reaching_object: 0.4171
     Episode_Reward/lifting_object: 22.2917
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0328
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 55.0417
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 2.75s
                      Time elapsed: 00:39:25
                               ETA: 00:54:08

################################################################################
                     [1m Learning iteration 843/2000 [0m                      

                       Computation: 31840 steps/s (collection: 2.859s, learning 0.229s)
             Mean action noise std: 2.70
          Mean value_function loss: 259.4504
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 62.4563
                       Mean reward: 118.59
               Mean episode length: 76.28
    Episode_Reward/reaching_object: 0.4247
     Episode_Reward/lifting_object: 23.1818
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 55.4583
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 3.09s
                      Time elapsed: 00:39:28
                               ETA: 00:54:06

################################################################################
                     [1m Learning iteration 844/2000 [0m                      

                       Computation: 35715 steps/s (collection: 2.638s, learning 0.114s)
             Mean action noise std: 2.71
          Mean value_function loss: 241.0727
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 62.4594
                       Mean reward: 119.88
               Mean episode length: 76.65
    Episode_Reward/reaching_object: 0.4303
     Episode_Reward/lifting_object: 23.1614
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 55.9167
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 2.75s
                      Time elapsed: 00:39:30
                               ETA: 00:54:03

################################################################################
                     [1m Learning iteration 845/2000 [0m                      

                       Computation: 39435 steps/s (collection: 2.373s, learning 0.120s)
             Mean action noise std: 2.71
          Mean value_function loss: 239.5371
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 62.4621
                       Mean reward: 120.33
               Mean episode length: 74.36
    Episode_Reward/reaching_object: 0.4225
     Episode_Reward/lifting_object: 22.8495
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 54.9583
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 2.49s
                      Time elapsed: 00:39:33
                               ETA: 00:54:00

################################################################################
                     [1m Learning iteration 846/2000 [0m                      

                       Computation: 34419 steps/s (collection: 2.716s, learning 0.140s)
             Mean action noise std: 2.71
          Mean value_function loss: 259.3102
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 62.4658
                       Mean reward: 104.96
               Mean episode length: 70.06
    Episode_Reward/reaching_object: 0.4189
     Episode_Reward/lifting_object: 23.0379
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 53.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 2.86s
                      Time elapsed: 00:39:36
                               ETA: 00:53:57

################################################################################
                     [1m Learning iteration 847/2000 [0m                      

                       Computation: 39196 steps/s (collection: 2.397s, learning 0.111s)
             Mean action noise std: 2.71
          Mean value_function loss: 252.0726
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 62.4688
                       Mean reward: 124.23
               Mean episode length: 73.28
    Episode_Reward/reaching_object: 0.4324
     Episode_Reward/lifting_object: 23.8425
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 55.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 2.51s
                      Time elapsed: 00:39:38
                               ETA: 00:53:54

################################################################################
                     [1m Learning iteration 848/2000 [0m                      

                       Computation: 40678 steps/s (collection: 2.316s, learning 0.101s)
             Mean action noise std: 2.71
          Mean value_function loss: 233.9131
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 62.4714
                       Mean reward: 127.87
               Mean episode length: 77.94
    Episode_Reward/reaching_object: 0.4263
     Episode_Reward/lifting_object: 23.2844
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 54.6250
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 2.42s
                      Time elapsed: 00:39:41
                               ETA: 00:53:50

################################################################################
                     [1m Learning iteration 849/2000 [0m                      

                       Computation: 38653 steps/s (collection: 2.415s, learning 0.128s)
             Mean action noise std: 2.71
          Mean value_function loss: 244.0866
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 62.4718
                       Mean reward: 105.64
               Mean episode length: 70.46
    Episode_Reward/reaching_object: 0.4275
     Episode_Reward/lifting_object: 23.3602
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 52.2917
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 2.54s
                      Time elapsed: 00:39:43
                               ETA: 00:53:47

################################################################################
                     [1m Learning iteration 850/2000 [0m                      

                       Computation: 39275 steps/s (collection: 2.311s, learning 0.192s)
             Mean action noise std: 2.71
          Mean value_function loss: 261.0500
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 62.4717
                       Mean reward: 122.46
               Mean episode length: 75.97
    Episode_Reward/reaching_object: 0.4330
     Episode_Reward/lifting_object: 24.0811
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 52.9583
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 2.50s
                      Time elapsed: 00:39:46
                               ETA: 00:53:44

################################################################################
                     [1m Learning iteration 851/2000 [0m                      

                       Computation: 39354 steps/s (collection: 2.383s, learning 0.115s)
             Mean action noise std: 2.71
          Mean value_function loss: 252.2988
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.4735
                       Mean reward: 128.36
               Mean episode length: 79.65
    Episode_Reward/reaching_object: 0.4384
     Episode_Reward/lifting_object: 24.0606
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 53.6250
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 2.50s
                      Time elapsed: 00:39:48
                               ETA: 00:53:41

################################################################################
                     [1m Learning iteration 852/2000 [0m                      

                       Computation: 40245 steps/s (collection: 2.328s, learning 0.115s)
             Mean action noise std: 2.71
          Mean value_function loss: 275.8056
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 62.4752
                       Mean reward: 121.91
               Mean episode length: 71.96
    Episode_Reward/reaching_object: 0.4420
     Episode_Reward/lifting_object: 24.4526
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 56.0417
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 2.44s
                      Time elapsed: 00:39:51
                               ETA: 00:53:38

################################################################################
                     [1m Learning iteration 853/2000 [0m                      

                       Computation: 39227 steps/s (collection: 2.392s, learning 0.114s)
             Mean action noise std: 2.71
          Mean value_function loss: 281.9851
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 62.4769
                       Mean reward: 118.69
               Mean episode length: 75.10
    Episode_Reward/reaching_object: 0.4284
     Episode_Reward/lifting_object: 23.2723
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 56.7500
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 2.51s
                      Time elapsed: 00:39:53
                               ETA: 00:53:34

################################################################################
                     [1m Learning iteration 854/2000 [0m                      

                       Computation: 35651 steps/s (collection: 2.532s, learning 0.225s)
             Mean action noise std: 2.71
          Mean value_function loss: 281.0595
               Mean surrogate loss: 0.0066
                 Mean entropy loss: 62.4781
                       Mean reward: 116.74
               Mean episode length: 71.95
    Episode_Reward/reaching_object: 0.4269
     Episode_Reward/lifting_object: 23.4251
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 55.4167
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 2.76s
                      Time elapsed: 00:39:56
                               ETA: 00:53:31

################################################################################
                     [1m Learning iteration 855/2000 [0m                      

                       Computation: 32919 steps/s (collection: 2.771s, learning 0.215s)
             Mean action noise std: 2.71
          Mean value_function loss: 252.7883
               Mean surrogate loss: 0.0069
                 Mean entropy loss: 62.4785
                       Mean reward: 120.61
               Mean episode length: 73.11
    Episode_Reward/reaching_object: 0.4307
     Episode_Reward/lifting_object: 23.7631
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 52.3333
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 2.99s
                      Time elapsed: 00:39:59
                               ETA: 00:53:29

################################################################################
                     [1m Learning iteration 856/2000 [0m                      

                       Computation: 35626 steps/s (collection: 2.591s, learning 0.169s)
             Mean action noise std: 2.71
          Mean value_function loss: 260.4430
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 62.4788
                       Mean reward: 117.77
               Mean episode length: 69.90
    Episode_Reward/reaching_object: 0.4272
     Episode_Reward/lifting_object: 24.0350
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 56.1667
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.76s
                      Time elapsed: 00:40:02
                               ETA: 00:53:26

################################################################################
                     [1m Learning iteration 857/2000 [0m                      

                       Computation: 32928 steps/s (collection: 2.767s, learning 0.219s)
             Mean action noise std: 2.71
          Mean value_function loss: 249.2668
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.4795
                       Mean reward: 118.33
               Mean episode length: 74.40
    Episode_Reward/reaching_object: 0.4335
     Episode_Reward/lifting_object: 24.2750
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 54.9167
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 2.99s
                      Time elapsed: 00:40:05
                               ETA: 00:53:23

################################################################################
                     [1m Learning iteration 858/2000 [0m                      

                       Computation: 38252 steps/s (collection: 2.430s, learning 0.140s)
             Mean action noise std: 2.71
          Mean value_function loss: 275.2694
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 62.4822
                       Mean reward: 115.63
               Mean episode length: 72.24
    Episode_Reward/reaching_object: 0.4342
     Episode_Reward/lifting_object: 24.4713
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 50.4167
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 2.57s
                      Time elapsed: 00:40:07
                               ETA: 00:53:20

################################################################################
                     [1m Learning iteration 859/2000 [0m                      

                       Computation: 37601 steps/s (collection: 2.438s, learning 0.176s)
             Mean action noise std: 2.71
          Mean value_function loss: 278.5593
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.4867
                       Mean reward: 126.56
               Mean episode length: 73.94
    Episode_Reward/reaching_object: 0.4198
     Episode_Reward/lifting_object: 23.3972
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 55.0417
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 2.61s
                      Time elapsed: 00:40:10
                               ETA: 00:53:17

################################################################################
                     [1m Learning iteration 860/2000 [0m                      

                       Computation: 35462 steps/s (collection: 2.607s, learning 0.165s)
             Mean action noise std: 2.71
          Mean value_function loss: 276.2311
               Mean surrogate loss: 0.0067
                 Mean entropy loss: 62.4908
                       Mean reward: 111.91
               Mean episode length: 72.68
    Episode_Reward/reaching_object: 0.4343
     Episode_Reward/lifting_object: 23.6854
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 55.9583
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 2.77s
                      Time elapsed: 00:40:13
                               ETA: 00:53:14

################################################################################
                     [1m Learning iteration 861/2000 [0m                      

                       Computation: 37843 steps/s (collection: 2.469s, learning 0.129s)
             Mean action noise std: 2.71
          Mean value_function loss: 264.2802
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 62.4917
                       Mean reward: 117.43
               Mean episode length: 73.19
    Episode_Reward/reaching_object: 0.4271
     Episode_Reward/lifting_object: 23.6921
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 57.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 2.60s
                      Time elapsed: 00:40:15
                               ETA: 00:53:11

################################################################################
                     [1m Learning iteration 862/2000 [0m                      

                       Computation: 38868 steps/s (collection: 2.388s, learning 0.141s)
             Mean action noise std: 2.71
          Mean value_function loss: 274.6219
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 62.4932
                       Mean reward: 125.49
               Mean episode length: 69.68
    Episode_Reward/reaching_object: 0.4213
     Episode_Reward/lifting_object: 24.0378
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 55.2917
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 2.53s
                      Time elapsed: 00:40:18
                               ETA: 00:53:08

################################################################################
                     [1m Learning iteration 863/2000 [0m                      

                       Computation: 36978 steps/s (collection: 2.540s, learning 0.119s)
             Mean action noise std: 2.71
          Mean value_function loss: 267.4069
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 62.4955
                       Mean reward: 126.53
               Mean episode length: 74.77
    Episode_Reward/reaching_object: 0.4258
     Episode_Reward/lifting_object: 24.2622
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 56.4583
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 2.66s
                      Time elapsed: 00:40:20
                               ETA: 00:53:05

################################################################################
                     [1m Learning iteration 864/2000 [0m                      

                       Computation: 35638 steps/s (collection: 2.610s, learning 0.149s)
             Mean action noise std: 2.71
          Mean value_function loss: 278.1678
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 62.4982
                       Mean reward: 119.88
               Mean episode length: 70.28
    Episode_Reward/reaching_object: 0.4175
     Episode_Reward/lifting_object: 23.7319
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 55.4167
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 2.76s
                      Time elapsed: 00:40:23
                               ETA: 00:53:02

################################################################################
                     [1m Learning iteration 865/2000 [0m                      

                       Computation: 35217 steps/s (collection: 2.640s, learning 0.151s)
             Mean action noise std: 2.71
          Mean value_function loss: 283.3734
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 62.5003
                       Mean reward: 115.80
               Mean episode length: 74.81
    Episode_Reward/reaching_object: 0.4190
     Episode_Reward/lifting_object: 23.7704
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 53.9583
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 2.79s
                      Time elapsed: 00:40:26
                               ETA: 00:53:00

################################################################################
                     [1m Learning iteration 866/2000 [0m                      

                       Computation: 39596 steps/s (collection: 2.372s, learning 0.111s)
             Mean action noise std: 2.71
          Mean value_function loss: 269.4411
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 62.5016
                       Mean reward: 127.60
               Mean episode length: 72.28
    Episode_Reward/reaching_object: 0.4217
     Episode_Reward/lifting_object: 24.5881
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 54.0833
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 2.48s
                      Time elapsed: 00:40:28
                               ETA: 00:52:56

################################################################################
                     [1m Learning iteration 867/2000 [0m                      

                       Computation: 38434 steps/s (collection: 2.407s, learning 0.151s)
             Mean action noise std: 2.71
          Mean value_function loss: 305.0581
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 62.5015
                       Mean reward: 113.54
               Mean episode length: 70.44
    Episode_Reward/reaching_object: 0.4325
     Episode_Reward/lifting_object: 25.1291
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 56.4583
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 2.56s
                      Time elapsed: 00:40:31
                               ETA: 00:52:53

################################################################################
                     [1m Learning iteration 868/2000 [0m                      

                       Computation: 37909 steps/s (collection: 2.455s, learning 0.138s)
             Mean action noise std: 2.71
          Mean value_function loss: 276.2409
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 62.5030
                       Mean reward: 126.81
               Mean episode length: 73.60
    Episode_Reward/reaching_object: 0.4301
     Episode_Reward/lifting_object: 24.6935
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 54.5417
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 2.59s
                      Time elapsed: 00:40:34
                               ETA: 00:52:50

################################################################################
                     [1m Learning iteration 869/2000 [0m                      

                       Computation: 37618 steps/s (collection: 2.479s, learning 0.134s)
             Mean action noise std: 2.71
          Mean value_function loss: 272.9697
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 62.5036
                       Mean reward: 126.09
               Mean episode length: 74.47
    Episode_Reward/reaching_object: 0.4323
     Episode_Reward/lifting_object: 25.1877
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0352
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 55.0417
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 2.61s
                      Time elapsed: 00:40:36
                               ETA: 00:52:47

################################################################################
                     [1m Learning iteration 870/2000 [0m                      

                       Computation: 37655 steps/s (collection: 2.467s, learning 0.144s)
             Mean action noise std: 2.71
          Mean value_function loss: 292.8885
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 62.5040
                       Mean reward: 124.52
               Mean episode length: 71.44
    Episode_Reward/reaching_object: 0.4331
     Episode_Reward/lifting_object: 25.8316
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 55.6250
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 2.61s
                      Time elapsed: 00:40:39
                               ETA: 00:52:44

################################################################################
                     [1m Learning iteration 871/2000 [0m                      

                       Computation: 38051 steps/s (collection: 2.468s, learning 0.115s)
             Mean action noise std: 2.71
          Mean value_function loss: 310.7427
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 62.5044
                       Mean reward: 133.23
               Mean episode length: 75.49
    Episode_Reward/reaching_object: 0.4310
     Episode_Reward/lifting_object: 25.7087
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 53.9167
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 2.58s
                      Time elapsed: 00:40:41
                               ETA: 00:52:41

################################################################################
                     [1m Learning iteration 872/2000 [0m                      

                       Computation: 38652 steps/s (collection: 2.427s, learning 0.116s)
             Mean action noise std: 2.71
          Mean value_function loss: 296.1953
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 62.5056
                       Mean reward: 134.52
               Mean episode length: 76.43
    Episode_Reward/reaching_object: 0.4302
     Episode_Reward/lifting_object: 25.5868
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 54.1250
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 2.54s
                      Time elapsed: 00:40:44
                               ETA: 00:52:38

################################################################################
                     [1m Learning iteration 873/2000 [0m                      

                       Computation: 33737 steps/s (collection: 2.668s, learning 0.246s)
             Mean action noise std: 2.71
          Mean value_function loss: 291.7487
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 62.5082
                       Mean reward: 130.86
               Mean episode length: 72.01
    Episode_Reward/reaching_object: 0.4278
     Episode_Reward/lifting_object: 25.7591
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 55.9167
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 2.91s
                      Time elapsed: 00:40:47
                               ETA: 00:52:35

################################################################################
                     [1m Learning iteration 874/2000 [0m                      

                       Computation: 29750 steps/s (collection: 3.118s, learning 0.186s)
             Mean action noise std: 2.71
          Mean value_function loss: 297.9260
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 62.5097
                       Mean reward: 140.27
               Mean episode length: 78.23
    Episode_Reward/reaching_object: 0.4248
     Episode_Reward/lifting_object: 25.2887
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 56.9583
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 3.30s
                      Time elapsed: 00:40:50
                               ETA: 00:52:33

################################################################################
                     [1m Learning iteration 875/2000 [0m                      

                       Computation: 29098 steps/s (collection: 3.141s, learning 0.237s)
             Mean action noise std: 2.71
          Mean value_function loss: 297.7971
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 62.5114
                       Mean reward: 123.73
               Mean episode length: 79.62
    Episode_Reward/reaching_object: 0.4374
     Episode_Reward/lifting_object: 26.3422
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 52.5833
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 3.38s
                      Time elapsed: 00:40:53
                               ETA: 00:52:31

################################################################################
                     [1m Learning iteration 876/2000 [0m                      

                       Computation: 32969 steps/s (collection: 2.818s, learning 0.164s)
             Mean action noise std: 2.71
          Mean value_function loss: 298.8589
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 62.5151
                       Mean reward: 138.74
               Mean episode length: 76.29
    Episode_Reward/reaching_object: 0.4311
     Episode_Reward/lifting_object: 25.4168
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 53.9167
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 2.98s
                      Time elapsed: 00:40:56
                               ETA: 00:52:28

################################################################################
                     [1m Learning iteration 877/2000 [0m                      

                       Computation: 32276 steps/s (collection: 2.901s, learning 0.145s)
             Mean action noise std: 2.71
          Mean value_function loss: 327.5939
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 62.5170
                       Mean reward: 132.60
               Mean episode length: 74.06
    Episode_Reward/reaching_object: 0.4342
     Episode_Reward/lifting_object: 26.0968
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 56.7083
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 3.05s
                      Time elapsed: 00:41:00
                               ETA: 00:52:26

################################################################################
                     [1m Learning iteration 878/2000 [0m                      

                       Computation: 28809 steps/s (collection: 3.056s, learning 0.356s)
             Mean action noise std: 2.71
          Mean value_function loss: 286.7458
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 62.5192
                       Mean reward: 129.89
               Mean episode length: 71.83
    Episode_Reward/reaching_object: 0.4351
     Episode_Reward/lifting_object: 26.2303
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 55.0417
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 3.41s
                      Time elapsed: 00:41:03
                               ETA: 00:52:24

################################################################################
                     [1m Learning iteration 879/2000 [0m                      

                       Computation: 30501 steps/s (collection: 2.904s, learning 0.319s)
             Mean action noise std: 2.71
          Mean value_function loss: 307.1360
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 62.5232
                       Mean reward: 135.22
               Mean episode length: 74.83
    Episode_Reward/reaching_object: 0.4325
     Episode_Reward/lifting_object: 26.7438
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 54.9583
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 3.22s
                      Time elapsed: 00:41:06
                               ETA: 00:52:22

################################################################################
                     [1m Learning iteration 880/2000 [0m                      

                       Computation: 33360 steps/s (collection: 2.822s, learning 0.125s)
             Mean action noise std: 2.71
          Mean value_function loss: 286.1766
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 62.5262
                       Mean reward: 129.69
               Mean episode length: 70.25
    Episode_Reward/reaching_object: 0.4289
     Episode_Reward/lifting_object: 26.5340
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 56.9583
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 2.95s
                      Time elapsed: 00:41:09
                               ETA: 00:52:19

################################################################################
                     [1m Learning iteration 881/2000 [0m                      

                       Computation: 34546 steps/s (collection: 2.580s, learning 0.266s)
             Mean action noise std: 2.71
          Mean value_function loss: 298.0903
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 62.5293
                       Mean reward: 123.29
               Mean episode length: 70.26
    Episode_Reward/reaching_object: 0.4222
     Episode_Reward/lifting_object: 26.1705
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 55.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 2.85s
                      Time elapsed: 00:41:12
                               ETA: 00:52:16

################################################################################
                     [1m Learning iteration 882/2000 [0m                      

                       Computation: 31338 steps/s (collection: 2.841s, learning 0.296s)
             Mean action noise std: 2.72
          Mean value_function loss: 314.7999
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 62.5319
                       Mean reward: 129.19
               Mean episode length: 73.56
    Episode_Reward/reaching_object: 0.4230
     Episode_Reward/lifting_object: 26.2106
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 57.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 3.14s
                      Time elapsed: 00:41:15
                               ETA: 00:52:14

################################################################################
                     [1m Learning iteration 883/2000 [0m                      

                       Computation: 32437 steps/s (collection: 2.872s, learning 0.158s)
             Mean action noise std: 2.72
          Mean value_function loss: 321.8214
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 62.5349
                       Mean reward: 129.24
               Mean episode length: 70.07
    Episode_Reward/reaching_object: 0.4192
     Episode_Reward/lifting_object: 25.8931
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 57.3750
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 3.03s
                      Time elapsed: 00:41:18
                               ETA: 00:52:11

################################################################################
                     [1m Learning iteration 884/2000 [0m                      

                       Computation: 36875 steps/s (collection: 2.517s, learning 0.149s)
             Mean action noise std: 2.72
          Mean value_function loss: 336.8431
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 62.5387
                       Mean reward: 128.03
               Mean episode length: 72.00
    Episode_Reward/reaching_object: 0.4131
     Episode_Reward/lifting_object: 25.9562
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 56.3750
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 2.67s
                      Time elapsed: 00:41:21
                               ETA: 00:52:08

################################################################################
                     [1m Learning iteration 885/2000 [0m                      

                       Computation: 35393 steps/s (collection: 2.634s, learning 0.143s)
             Mean action noise std: 2.72
          Mean value_function loss: 354.6074
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.5407
                       Mean reward: 127.77
               Mean episode length: 72.80
    Episode_Reward/reaching_object: 0.4241
     Episode_Reward/lifting_object: 26.5971
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 57.3750
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 2.78s
                      Time elapsed: 00:41:24
                               ETA: 00:52:06

################################################################################
                     [1m Learning iteration 886/2000 [0m                      

                       Computation: 36225 steps/s (collection: 2.540s, learning 0.174s)
             Mean action noise std: 2.72
          Mean value_function loss: 323.6279
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 62.5425
                       Mean reward: 137.80
               Mean episode length: 73.81
    Episode_Reward/reaching_object: 0.4198
     Episode_Reward/lifting_object: 26.6867
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 57.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 2.71s
                      Time elapsed: 00:41:26
                               ETA: 00:52:03

################################################################################
                     [1m Learning iteration 887/2000 [0m                      

                       Computation: 32681 steps/s (collection: 2.814s, learning 0.194s)
             Mean action noise std: 2.72
          Mean value_function loss: 343.4389
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 62.5443
                       Mean reward: 139.14
               Mean episode length: 71.14
    Episode_Reward/reaching_object: 0.4221
     Episode_Reward/lifting_object: 27.5503
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 56.5833
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 3.01s
                      Time elapsed: 00:41:29
                               ETA: 00:52:00

################################################################################
                     [1m Learning iteration 888/2000 [0m                      

                       Computation: 36115 steps/s (collection: 2.525s, learning 0.197s)
             Mean action noise std: 2.72
          Mean value_function loss: 322.5038
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 62.5458
                       Mean reward: 136.81
               Mean episode length: 70.55
    Episode_Reward/reaching_object: 0.4292
     Episode_Reward/lifting_object: 27.6366
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 56.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 2.72s
                      Time elapsed: 00:41:32
                               ETA: 00:51:57

################################################################################
                     [1m Learning iteration 889/2000 [0m                      

                       Computation: 37096 steps/s (collection: 2.523s, learning 0.127s)
             Mean action noise std: 2.72
          Mean value_function loss: 327.4111
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 62.5462
                       Mean reward: 151.87
               Mean episode length: 76.74
    Episode_Reward/reaching_object: 0.4174
     Episode_Reward/lifting_object: 27.1537
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 56.5833
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 2.65s
                      Time elapsed: 00:41:35
                               ETA: 00:51:54

################################################################################
                     [1m Learning iteration 890/2000 [0m                      

                       Computation: 34547 steps/s (collection: 2.640s, learning 0.205s)
             Mean action noise std: 2.72
          Mean value_function loss: 335.2571
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 62.5468
                       Mean reward: 153.52
               Mean episode length: 76.58
    Episode_Reward/reaching_object: 0.4298
     Episode_Reward/lifting_object: 27.5777
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 55.4583
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 2.85s
                      Time elapsed: 00:41:37
                               ETA: 00:51:51

################################################################################
                     [1m Learning iteration 891/2000 [0m                      

                       Computation: 34062 steps/s (collection: 2.733s, learning 0.153s)
             Mean action noise std: 2.72
          Mean value_function loss: 311.6876
               Mean surrogate loss: 0.0121
                 Mean entropy loss: 62.5489
                       Mean reward: 148.68
               Mean episode length: 70.59
    Episode_Reward/reaching_object: 0.4222
     Episode_Reward/lifting_object: 27.7198
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 55.2500
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 2.89s
                      Time elapsed: 00:41:40
                               ETA: 00:51:49

################################################################################
                     [1m Learning iteration 892/2000 [0m                      

                       Computation: 37086 steps/s (collection: 2.478s, learning 0.173s)
             Mean action noise std: 2.72
          Mean value_function loss: 328.1841
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 62.5496
                       Mean reward: 160.54
               Mean episode length: 80.44
    Episode_Reward/reaching_object: 0.4315
     Episode_Reward/lifting_object: 28.3861
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 53.7917
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 2.65s
                      Time elapsed: 00:41:43
                               ETA: 00:51:46

################################################################################
                     [1m Learning iteration 893/2000 [0m                      

                       Computation: 28618 steps/s (collection: 3.227s, learning 0.208s)
             Mean action noise std: 2.72
          Mean value_function loss: 324.0467
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 62.5504
                       Mean reward: 136.62
               Mean episode length: 72.39
    Episode_Reward/reaching_object: 0.4421
     Episode_Reward/lifting_object: 28.9624
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 58.1667
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 3.43s
                      Time elapsed: 00:41:46
                               ETA: 00:51:44

################################################################################
                     [1m Learning iteration 894/2000 [0m                      

                       Computation: 31781 steps/s (collection: 2.973s, learning 0.121s)
             Mean action noise std: 2.72
          Mean value_function loss: 338.2148
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 62.5514
                       Mean reward: 142.47
               Mean episode length: 74.00
    Episode_Reward/reaching_object: 0.4337
     Episode_Reward/lifting_object: 28.4997
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 54.5000
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 3.09s
                      Time elapsed: 00:41:50
                               ETA: 00:51:41

################################################################################
                     [1m Learning iteration 895/2000 [0m                      

                       Computation: 35645 steps/s (collection: 2.607s, learning 0.151s)
             Mean action noise std: 2.72
          Mean value_function loss: 330.6011
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 62.5531
                       Mean reward: 133.97
               Mean episode length: 70.72
    Episode_Reward/reaching_object: 0.4237
     Episode_Reward/lifting_object: 27.5006
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 57.6667
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 2.76s
                      Time elapsed: 00:41:52
                               ETA: 00:51:38

################################################################################
                     [1m Learning iteration 896/2000 [0m                      

                       Computation: 34880 steps/s (collection: 2.653s, learning 0.165s)
             Mean action noise std: 2.72
          Mean value_function loss: 323.5265
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 62.5541
                       Mean reward: 147.88
               Mean episode length: 73.90
    Episode_Reward/reaching_object: 0.4220
     Episode_Reward/lifting_object: 27.7681
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 54.6667
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 2.82s
                      Time elapsed: 00:41:55
                               ETA: 00:51:36

################################################################################
                     [1m Learning iteration 897/2000 [0m                      

                       Computation: 40944 steps/s (collection: 2.276s, learning 0.125s)
             Mean action noise std: 2.72
          Mean value_function loss: 344.3505
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 62.5543
                       Mean reward: 133.93
               Mean episode length: 68.82
    Episode_Reward/reaching_object: 0.4209
     Episode_Reward/lifting_object: 27.9778
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 57.7500
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.40s
                      Time elapsed: 00:41:58
                               ETA: 00:51:32

################################################################################
                     [1m Learning iteration 898/2000 [0m                      

                       Computation: 36763 steps/s (collection: 2.471s, learning 0.203s)
             Mean action noise std: 2.72
          Mean value_function loss: 350.7357
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 62.5545
                       Mean reward: 140.10
               Mean episode length: 69.69
    Episode_Reward/reaching_object: 0.4233
     Episode_Reward/lifting_object: 28.2015
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 55.4167
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 2.67s
                      Time elapsed: 00:42:00
                               ETA: 00:51:29

################################################################################
                     [1m Learning iteration 899/2000 [0m                      

                       Computation: 29499 steps/s (collection: 3.044s, learning 0.288s)
             Mean action noise std: 2.72
          Mean value_function loss: 358.9988
               Mean surrogate loss: 0.0109
                 Mean entropy loss: 62.5547
                       Mean reward: 157.57
               Mean episode length: 76.04
    Episode_Reward/reaching_object: 0.4365
     Episode_Reward/lifting_object: 29.6889
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 55.4583
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 3.33s
                      Time elapsed: 00:42:04
                               ETA: 00:51:27

################################################################################
                     [1m Learning iteration 900/2000 [0m                      

                       Computation: 35132 steps/s (collection: 2.652s, learning 0.146s)
             Mean action noise std: 2.72
          Mean value_function loss: 341.8387
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 62.5548
                       Mean reward: 148.62
               Mean episode length: 72.38
    Episode_Reward/reaching_object: 0.4312
     Episode_Reward/lifting_object: 29.1500
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 54.7083
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 2.80s
                      Time elapsed: 00:42:06
                               ETA: 00:51:24

################################################################################
                     [1m Learning iteration 901/2000 [0m                      

                       Computation: 36078 steps/s (collection: 2.573s, learning 0.152s)
             Mean action noise std: 2.72
          Mean value_function loss: 348.9221
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 62.5552
                       Mean reward: 147.87
               Mean episode length: 72.00
    Episode_Reward/reaching_object: 0.4394
     Episode_Reward/lifting_object: 30.0098
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0332
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 54.0417
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 2.72s
                      Time elapsed: 00:42:09
                               ETA: 00:51:22

################################################################################
                     [1m Learning iteration 902/2000 [0m                      

                       Computation: 37947 steps/s (collection: 2.466s, learning 0.124s)
             Mean action noise std: 2.72
          Mean value_function loss: 372.3666
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 62.5556
                       Mean reward: 116.44
               Mean episode length: 64.43
    Episode_Reward/reaching_object: 0.4307
     Episode_Reward/lifting_object: 29.1986
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 56.3333
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 2.59s
                      Time elapsed: 00:42:12
                               ETA: 00:51:18

################################################################################
                     [1m Learning iteration 903/2000 [0m                      

                       Computation: 37103 steps/s (collection: 2.498s, learning 0.151s)
             Mean action noise std: 2.72
          Mean value_function loss: 374.4778
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 62.5560
                       Mean reward: 142.73
               Mean episode length: 68.72
    Episode_Reward/reaching_object: 0.4387
     Episode_Reward/lifting_object: 29.9733
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 59.0417
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 2.65s
                      Time elapsed: 00:42:14
                               ETA: 00:51:15

################################################################################
                     [1m Learning iteration 904/2000 [0m                      

                       Computation: 37624 steps/s (collection: 2.476s, learning 0.137s)
             Mean action noise std: 2.72
          Mean value_function loss: 393.1676
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 62.5570
                       Mean reward: 145.41
               Mean episode length: 71.13
    Episode_Reward/reaching_object: 0.4223
     Episode_Reward/lifting_object: 29.1674
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 57.1250
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 2.61s
                      Time elapsed: 00:42:17
                               ETA: 00:51:12

################################################################################
                     [1m Learning iteration 905/2000 [0m                      

                       Computation: 39501 steps/s (collection: 2.355s, learning 0.134s)
             Mean action noise std: 2.72
          Mean value_function loss: 388.7562
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 62.5589
                       Mean reward: 134.74
               Mean episode length: 66.92
    Episode_Reward/reaching_object: 0.3989
     Episode_Reward/lifting_object: 26.9697
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 58.6667
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.49s
                      Time elapsed: 00:42:19
                               ETA: 00:51:09

################################################################################
                     [1m Learning iteration 906/2000 [0m                      

                       Computation: 37351 steps/s (collection: 2.495s, learning 0.137s)
             Mean action noise std: 2.72
          Mean value_function loss: 395.2990
               Mean surrogate loss: 0.0201
                 Mean entropy loss: 62.5596
                       Mean reward: 150.70
               Mean episode length: 71.12
    Episode_Reward/reaching_object: 0.4113
     Episode_Reward/lifting_object: 28.5570
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 55.9583
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 2.63s
                      Time elapsed: 00:42:22
                               ETA: 00:51:06

################################################################################
                     [1m Learning iteration 907/2000 [0m                      

                       Computation: 38539 steps/s (collection: 2.406s, learning 0.145s)
             Mean action noise std: 2.72
          Mean value_function loss: 381.3942
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 62.5598
                       Mean reward: 147.99
               Mean episode length: 69.64
    Episode_Reward/reaching_object: 0.4278
     Episode_Reward/lifting_object: 29.9736
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 53.8333
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.55s
                      Time elapsed: 00:42:25
                               ETA: 00:51:03

################################################################################
                     [1m Learning iteration 908/2000 [0m                      

                       Computation: 38538 steps/s (collection: 2.410s, learning 0.141s)
             Mean action noise std: 2.72
          Mean value_function loss: 381.3843
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 62.5605
                       Mean reward: 146.18
               Mean episode length: 69.62
    Episode_Reward/reaching_object: 0.4351
     Episode_Reward/lifting_object: 30.7026
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 54.7083
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.55s
                      Time elapsed: 00:42:27
                               ETA: 00:51:00

################################################################################
                     [1m Learning iteration 909/2000 [0m                      

                       Computation: 36767 steps/s (collection: 2.520s, learning 0.154s)
             Mean action noise std: 2.72
          Mean value_function loss: 395.5722
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 62.5616
                       Mean reward: 155.66
               Mean episode length: 72.73
    Episode_Reward/reaching_object: 0.4378
     Episode_Reward/lifting_object: 31.5382
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 53.2917
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.67s
                      Time elapsed: 00:42:30
                               ETA: 00:50:57

################################################################################
                     [1m Learning iteration 910/2000 [0m                      

                       Computation: 38401 steps/s (collection: 2.414s, learning 0.146s)
             Mean action noise std: 2.72
          Mean value_function loss: 366.1114
               Mean surrogate loss: 0.0114
                 Mean entropy loss: 62.5627
                       Mean reward: 166.42
               Mean episode length: 74.35
    Episode_Reward/reaching_object: 0.4567
     Episode_Reward/lifting_object: 32.6525
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 52.3750
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.56s
                      Time elapsed: 00:42:32
                               ETA: 00:50:54

################################################################################
                     [1m Learning iteration 911/2000 [0m                      

                       Computation: 17995 steps/s (collection: 5.128s, learning 0.335s)
             Mean action noise std: 2.72
          Mean value_function loss: 392.1782
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 62.5628
                       Mean reward: 151.73
               Mean episode length: 70.80
    Episode_Reward/reaching_object: 0.4524
     Episode_Reward/lifting_object: 32.5531
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 55.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 5.46s
                      Time elapsed: 00:42:38
                               ETA: 00:50:54

################################################################################
                     [1m Learning iteration 912/2000 [0m                      

                       Computation: 18163 steps/s (collection: 4.953s, learning 0.459s)
             Mean action noise std: 2.72
          Mean value_function loss: 370.1209
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 62.5628
                       Mean reward: 174.45
               Mean episode length: 76.66
    Episode_Reward/reaching_object: 0.4508
     Episode_Reward/lifting_object: 32.6588
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 53.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 5.41s
                      Time elapsed: 00:42:43
                               ETA: 00:50:55

################################################################################
                     [1m Learning iteration 913/2000 [0m                      

                       Computation: 32759 steps/s (collection: 2.857s, learning 0.144s)
             Mean action noise std: 2.72
          Mean value_function loss: 380.9246
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 62.5633
                       Mean reward: 176.64
               Mean episode length: 81.40
    Episode_Reward/reaching_object: 0.4552
     Episode_Reward/lifting_object: 32.7096
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 49.7917
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 3.00s
                      Time elapsed: 00:42:46
                               ETA: 00:50:52

################################################################################
                     [1m Learning iteration 914/2000 [0m                      

                       Computation: 39888 steps/s (collection: 2.344s, learning 0.121s)
             Mean action noise std: 2.72
          Mean value_function loss: 375.1608
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 62.5646
                       Mean reward: 174.80
               Mean episode length: 77.76
    Episode_Reward/reaching_object: 0.4565
     Episode_Reward/lifting_object: 33.4470
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 50.6250
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 2.46s
                      Time elapsed: 00:42:49
                               ETA: 00:50:49

################################################################################
                     [1m Learning iteration 915/2000 [0m                      

                       Computation: 38873 steps/s (collection: 2.376s, learning 0.153s)
             Mean action noise std: 2.72
          Mean value_function loss: 418.8503
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.5655
                       Mean reward: 170.34
               Mean episode length: 77.71
    Episode_Reward/reaching_object: 0.4601
     Episode_Reward/lifting_object: 34.1770
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 50.9583
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.53s
                      Time elapsed: 00:42:51
                               ETA: 00:50:46

################################################################################
                     [1m Learning iteration 916/2000 [0m                      

                       Computation: 39606 steps/s (collection: 2.371s, learning 0.111s)
             Mean action noise std: 2.72
          Mean value_function loss: 403.7858
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 62.5656
                       Mean reward: 192.97
               Mean episode length: 83.03
    Episode_Reward/reaching_object: 0.4683
     Episode_Reward/lifting_object: 34.3705
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 53.2083
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.48s
                      Time elapsed: 00:42:54
                               ETA: 00:50:43

################################################################################
                     [1m Learning iteration 917/2000 [0m                      

                       Computation: 40503 steps/s (collection: 2.289s, learning 0.138s)
             Mean action noise std: 2.72
          Mean value_function loss: 393.0647
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 62.5655
                       Mean reward: 190.43
               Mean episode length: 79.90
    Episode_Reward/reaching_object: 0.4816
     Episode_Reward/lifting_object: 36.4992
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0334
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 48.6667
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.43s
                      Time elapsed: 00:42:56
                               ETA: 00:50:39

################################################################################
                     [1m Learning iteration 918/2000 [0m                      

                       Computation: 40008 steps/s (collection: 2.315s, learning 0.142s)
             Mean action noise std: 2.72
          Mean value_function loss: 423.6066
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 62.5652
                       Mean reward: 187.69
               Mean episode length: 82.93
    Episode_Reward/reaching_object: 0.4766
     Episode_Reward/lifting_object: 35.9767
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 48.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.46s
                      Time elapsed: 00:42:59
                               ETA: 00:50:36

################################################################################
                     [1m Learning iteration 919/2000 [0m                      

                       Computation: 41601 steps/s (collection: 2.240s, learning 0.123s)
             Mean action noise std: 2.72
          Mean value_function loss: 1190898.9000
               Mean surrogate loss: 0.0121
                 Mean entropy loss: 62.5659
                       Mean reward: 209.61
               Mean episode length: 87.54
    Episode_Reward/reaching_object: 0.4984
     Episode_Reward/lifting_object: 38.2647
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 49.4583
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.36s
                      Time elapsed: 00:43:01
                               ETA: 00:50:33

################################################################################
                     [1m Learning iteration 920/2000 [0m                      

                       Computation: 41086 steps/s (collection: 2.262s, learning 0.130s)
             Mean action noise std: 2.72
          Mean value_function loss: 485250352904404992.0000
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 62.5663
                       Mean reward: 172.74
               Mean episode length: 76.85
    Episode_Reward/reaching_object: 0.4856
     Episode_Reward/lifting_object: 36.5688
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -34222.5391
          Episode_Reward/joint_vel: -14443115.0000
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 51.6667
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 2.39s
                      Time elapsed: 00:43:03
                               ETA: 00:50:29

################################################################################
                     [1m Learning iteration 921/2000 [0m                      

                       Computation: 41102 steps/s (collection: 2.251s, learning 0.141s)
             Mean action noise std: 2.72
          Mean value_function loss: 423.8412
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.5670
                       Mean reward: 170.34
               Mean episode length: 77.24
    Episode_Reward/reaching_object: 0.4583
     Episode_Reward/lifting_object: 34.3345
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0367
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 54.2917
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 2.39s
                      Time elapsed: 00:43:06
                               ETA: 00:50:26

################################################################################
                     [1m Learning iteration 922/2000 [0m                      

                       Computation: 41081 steps/s (collection: 2.278s, learning 0.115s)
             Mean action noise std: 2.72
          Mean value_function loss: 416.2808
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 62.5682
                       Mean reward: 172.74
               Mean episode length: 74.40
    Episode_Reward/reaching_object: 0.4455
     Episode_Reward/lifting_object: 32.6231
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 52.4583
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 2.39s
                      Time elapsed: 00:43:08
                               ETA: 00:50:23

################################################################################
                     [1m Learning iteration 923/2000 [0m                      

                       Computation: 39961 steps/s (collection: 2.326s, learning 0.134s)
             Mean action noise std: 2.72
          Mean value_function loss: 423.6368
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 62.5684
                       Mean reward: 176.88
               Mean episode length: 80.21
    Episode_Reward/reaching_object: 0.4569
     Episode_Reward/lifting_object: 34.1830
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 52.7917
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.46s
                      Time elapsed: 00:43:11
                               ETA: 00:50:20

################################################################################
                     [1m Learning iteration 924/2000 [0m                      

                       Computation: 40758 steps/s (collection: 2.275s, learning 0.137s)
             Mean action noise std: 2.72
          Mean value_function loss: 426.2994
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 62.5676
                       Mean reward: 183.34
               Mean episode length: 83.54
    Episode_Reward/reaching_object: 0.4671
     Episode_Reward/lifting_object: 35.1093
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 53.1250
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.41s
                      Time elapsed: 00:43:13
                               ETA: 00:50:16

################################################################################
                     [1m Learning iteration 925/2000 [0m                      

                       Computation: 39694 steps/s (collection: 2.350s, learning 0.126s)
             Mean action noise std: 2.72
          Mean value_function loss: 429.3437
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 62.5681
                       Mean reward: 186.65
               Mean episode length: 78.74
    Episode_Reward/reaching_object: 0.4484
     Episode_Reward/lifting_object: 34.1324
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 50.0000
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.48s
                      Time elapsed: 00:43:15
                               ETA: 00:50:13

################################################################################
                     [1m Learning iteration 926/2000 [0m                      

                       Computation: 40831 steps/s (collection: 2.280s, learning 0.128s)
             Mean action noise std: 2.72
          Mean value_function loss: 451.0749
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 62.5696
                       Mean reward: 167.41
               Mean episode length: 73.40
    Episode_Reward/reaching_object: 0.4425
     Episode_Reward/lifting_object: 32.6179
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0305
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 49.7500
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 2.41s
                      Time elapsed: 00:43:18
                               ETA: 00:50:10

################################################################################
                     [1m Learning iteration 927/2000 [0m                      

                       Computation: 41557 steps/s (collection: 2.212s, learning 0.154s)
             Mean action noise std: 2.72
          Mean value_function loss: 462.6512
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 62.5700
                       Mean reward: 183.69
               Mean episode length: 78.11
    Episode_Reward/reaching_object: 0.4646
     Episode_Reward/lifting_object: 35.3791
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 50.0417
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 2.37s
                      Time elapsed: 00:43:20
                               ETA: 00:50:07

################################################################################
                     [1m Learning iteration 928/2000 [0m                      

                       Computation: 40474 steps/s (collection: 2.289s, learning 0.140s)
             Mean action noise std: 2.72
          Mean value_function loss: 464.6015
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 62.5701
                       Mean reward: 184.13
               Mean episode length: 81.84
    Episode_Reward/reaching_object: 0.4791
     Episode_Reward/lifting_object: 37.3579
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0317
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 51.0417
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.43s
                      Time elapsed: 00:43:23
                               ETA: 00:50:03

################################################################################
                     [1m Learning iteration 929/2000 [0m                      

                       Computation: 40464 steps/s (collection: 2.279s, learning 0.151s)
             Mean action noise std: 2.72
          Mean value_function loss: 455.9396
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 62.5706
                       Mean reward: 177.50
               Mean episode length: 76.92
    Episode_Reward/reaching_object: 0.4781
     Episode_Reward/lifting_object: 37.7732
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 46.3750
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 2.43s
                      Time elapsed: 00:43:25
                               ETA: 00:50:00

################################################################################
                     [1m Learning iteration 930/2000 [0m                      

                       Computation: 40638 steps/s (collection: 2.286s, learning 0.133s)
             Mean action noise std: 2.72
          Mean value_function loss: 450.3783
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 62.5719
                       Mean reward: 184.47
               Mean episode length: 75.90
    Episode_Reward/reaching_object: 0.4778
     Episode_Reward/lifting_object: 37.7912
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 50.2917
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 2.42s
                      Time elapsed: 00:43:28
                               ETA: 00:49:57

################################################################################
                     [1m Learning iteration 931/2000 [0m                      

                       Computation: 38458 steps/s (collection: 2.427s, learning 0.130s)
             Mean action noise std: 2.72
          Mean value_function loss: 462.4056
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.5736
                       Mean reward: 195.18
               Mean episode length: 82.63
    Episode_Reward/reaching_object: 0.4831
     Episode_Reward/lifting_object: 38.9455
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0313
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 47.3333
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.56s
                      Time elapsed: 00:43:30
                               ETA: 00:49:54

################################################################################
                     [1m Learning iteration 932/2000 [0m                      

                       Computation: 41005 steps/s (collection: 2.254s, learning 0.143s)
             Mean action noise std: 2.72
          Mean value_function loss: 464.7315
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 62.5735
                       Mean reward: 227.28
               Mean episode length: 88.54
    Episode_Reward/reaching_object: 0.4939
     Episode_Reward/lifting_object: 40.1937
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 45.1250
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.40s
                      Time elapsed: 00:43:32
                               ETA: 00:49:51

################################################################################
                     [1m Learning iteration 933/2000 [0m                      

                       Computation: 38381 steps/s (collection: 2.419s, learning 0.143s)
             Mean action noise std: 2.72
          Mean value_function loss: 455.6346
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 62.5721
                       Mean reward: 215.09
               Mean episode length: 86.09
    Episode_Reward/reaching_object: 0.5024
     Episode_Reward/lifting_object: 41.8442
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 0.0417
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 45.2500
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 2.56s
                      Time elapsed: 00:43:35
                               ETA: 00:49:47

################################################################################
                     [1m Learning iteration 934/2000 [0m                      

                       Computation: 40114 steps/s (collection: 2.310s, learning 0.141s)
             Mean action noise std: 2.72
          Mean value_function loss: 439.0531
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 62.5703
                       Mean reward: 245.73
               Mean episode length: 93.69
    Episode_Reward/reaching_object: 0.5266
     Episode_Reward/lifting_object: 43.7250
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0324
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 45.2500
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.45s
                      Time elapsed: 00:43:37
                               ETA: 00:49:44

################################################################################
                     [1m Learning iteration 935/2000 [0m                      

                       Computation: 39055 steps/s (collection: 2.373s, learning 0.144s)
             Mean action noise std: 2.72
          Mean value_function loss: 466.1957
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 62.5690
                       Mean reward: 212.66
               Mean episode length: 85.33
    Episode_Reward/reaching_object: 0.5237
     Episode_Reward/lifting_object: 44.8594
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0312
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 43.7917
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 2.52s
                      Time elapsed: 00:43:40
                               ETA: 00:49:41

################################################################################
                     [1m Learning iteration 936/2000 [0m                      

                       Computation: 40687 steps/s (collection: 2.291s, learning 0.125s)
             Mean action noise std: 2.72
          Mean value_function loss: 467.8273
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 62.5687
                       Mean reward: 247.68
               Mean episode length: 92.51
    Episode_Reward/reaching_object: 0.5148
     Episode_Reward/lifting_object: 44.3087
      Episode_Reward/object_height: 0.0063
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0308
      Episode_Termination/time_out: 0.0000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 40.1667
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 2.42s
                      Time elapsed: 00:43:42
                               ETA: 00:49:38

################################################################################
                     [1m Learning iteration 937/2000 [0m                      

                       Computation: 40988 steps/s (collection: 2.270s, learning 0.129s)
             Mean action noise std: 2.73
          Mean value_function loss: 466.8124
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 62.5697
                       Mean reward: 263.35
               Mean episode length: 94.76
    Episode_Reward/reaching_object: 0.5495
     Episode_Reward/lifting_object: 48.2163
      Episode_Reward/object_height: 0.0074
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0320
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 38.8333
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.40s
                      Time elapsed: 00:43:45
                               ETA: 00:49:35

################################################################################
                     [1m Learning iteration 938/2000 [0m                      

                       Computation: 42019 steps/s (collection: 2.220s, learning 0.120s)
             Mean action noise std: 2.73
          Mean value_function loss: 459.7438
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 62.5717
                       Mean reward: 257.54
               Mean episode length: 97.32
    Episode_Reward/reaching_object: 0.5775
     Episode_Reward/lifting_object: 50.9121
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 37.3750
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.34s
                      Time elapsed: 00:43:47
                               ETA: 00:49:31

################################################################################
                     [1m Learning iteration 939/2000 [0m                      

                       Computation: 41991 steps/s (collection: 2.212s, learning 0.129s)
             Mean action noise std: 2.73
          Mean value_function loss: 464.9318
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 62.5731
                       Mean reward: 248.66
               Mean episode length: 89.94
    Episode_Reward/reaching_object: 0.5805
     Episode_Reward/lifting_object: 52.2587
      Episode_Reward/object_height: 0.0091
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 39.6250
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.34s
                      Time elapsed: 00:43:50
                               ETA: 00:49:28

################################################################################
                     [1m Learning iteration 940/2000 [0m                      

                       Computation: 42287 steps/s (collection: 2.213s, learning 0.112s)
             Mean action noise std: 2.73
          Mean value_function loss: 473.8527
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 62.5753
                       Mean reward: 297.24
               Mean episode length: 105.11
    Episode_Reward/reaching_object: 0.6103
     Episode_Reward/lifting_object: 55.8441
      Episode_Reward/object_height: 0.0104
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 0.0833
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 33.7083
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.32s
                      Time elapsed: 00:43:52
                               ETA: 00:49:25

################################################################################
                     [1m Learning iteration 941/2000 [0m                      

                       Computation: 41224 steps/s (collection: 2.267s, learning 0.118s)
             Mean action noise std: 2.73
          Mean value_function loss: 458.0620
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 62.5764
                       Mean reward: 260.12
               Mean episode length: 97.68
    Episode_Reward/reaching_object: 0.6124
     Episode_Reward/lifting_object: 56.4359
      Episode_Reward/object_height: 0.0111
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 36.0833
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.38s
                      Time elapsed: 00:43:54
                               ETA: 00:49:21

################################################################################
                     [1m Learning iteration 942/2000 [0m                      

                       Computation: 39976 steps/s (collection: 2.325s, learning 0.134s)
             Mean action noise std: 2.73
          Mean value_function loss: 498.7064
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 62.5776
                       Mean reward: 319.59
               Mean episode length: 108.69
    Episode_Reward/reaching_object: 0.6277
     Episode_Reward/lifting_object: 58.4019
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 36.4583
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.46s
                      Time elapsed: 00:43:57
                               ETA: 00:49:18

################################################################################
                     [1m Learning iteration 943/2000 [0m                      

                       Computation: 41160 steps/s (collection: 2.258s, learning 0.130s)
             Mean action noise std: 2.73
          Mean value_function loss: 452.6439
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 62.5777
                       Mean reward: 333.00
               Mean episode length: 111.83
    Episode_Reward/reaching_object: 0.6670
     Episode_Reward/lifting_object: 62.4350
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0359
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 33.2083
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.39s
                      Time elapsed: 00:43:59
                               ETA: 00:49:15

################################################################################
                     [1m Learning iteration 944/2000 [0m                      

                       Computation: 40664 steps/s (collection: 2.282s, learning 0.135s)
             Mean action noise std: 2.73
          Mean value_function loss: 454.3416
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 62.5780
                       Mean reward: 335.88
               Mean episode length: 113.24
    Episode_Reward/reaching_object: 0.6659
     Episode_Reward/lifting_object: 63.6412
      Episode_Reward/object_height: 0.0146
        Episode_Reward/action_rate: -0.0227
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 29.0417
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.42s
                      Time elapsed: 00:44:01
                               ETA: 00:49:12

################################################################################
                     [1m Learning iteration 945/2000 [0m                      

                       Computation: 40711 steps/s (collection: 2.302s, learning 0.113s)
             Mean action noise std: 2.73
          Mean value_function loss: 472.4060
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 62.5810
                       Mean reward: 376.84
               Mean episode length: 121.08
    Episode_Reward/reaching_object: 0.7376
     Episode_Reward/lifting_object: 71.6100
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0246
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 29.3750
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.41s
                      Time elapsed: 00:44:04
                               ETA: 00:49:09

################################################################################
                     [1m Learning iteration 946/2000 [0m                      

                       Computation: 41625 steps/s (collection: 2.230s, learning 0.132s)
             Mean action noise std: 2.73
          Mean value_function loss: 461.8694
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 62.5814
                       Mean reward: 380.48
               Mean episode length: 123.92
    Episode_Reward/reaching_object: 0.7423
     Episode_Reward/lifting_object: 71.5475
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 29.2083
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.36s
                      Time elapsed: 00:44:06
                               ETA: 00:49:05

################################################################################
                     [1m Learning iteration 947/2000 [0m                      

                       Computation: 42615 steps/s (collection: 2.177s, learning 0.129s)
             Mean action noise std: 2.73
          Mean value_function loss: 458.6405
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 62.5830
                       Mean reward: 402.46
               Mean episode length: 128.19
    Episode_Reward/reaching_object: 0.7700
     Episode_Reward/lifting_object: 74.8147
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 25.4583
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 2.31s
                      Time elapsed: 00:44:09
                               ETA: 00:49:02

################################################################################
                     [1m Learning iteration 948/2000 [0m                      

                       Computation: 42280 steps/s (collection: 2.199s, learning 0.126s)
             Mean action noise std: 2.73
          Mean value_function loss: 446.6439
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 62.5842
                       Mean reward: 378.74
               Mean episode length: 125.15
    Episode_Reward/reaching_object: 0.8290
     Episode_Reward/lifting_object: 81.7653
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 25.9167
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.33s
                      Time elapsed: 00:44:11
                               ETA: 00:48:59

################################################################################
                     [1m Learning iteration 949/2000 [0m                      

                       Computation: 42678 steps/s (collection: 2.172s, learning 0.131s)
             Mean action noise std: 2.73
          Mean value_function loss: 452.7348
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 62.5849
                       Mean reward: 438.51
               Mean episode length: 138.00
    Episode_Reward/reaching_object: 0.7983
     Episode_Reward/lifting_object: 78.9074
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 24.2917
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.30s
                      Time elapsed: 00:44:13
                               ETA: 00:48:55

################################################################################
                     [1m Learning iteration 950/2000 [0m                      

                       Computation: 39115 steps/s (collection: 2.376s, learning 0.137s)
             Mean action noise std: 2.73
          Mean value_function loss: 452.8585
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 62.5854
                       Mean reward: 477.66
               Mean episode length: 143.72
    Episode_Reward/reaching_object: 0.9066
     Episode_Reward/lifting_object: 90.7013
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0435
      Episode_Termination/time_out: 2.7917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.51s
                      Time elapsed: 00:44:16
                               ETA: 00:48:52

################################################################################
                     [1m Learning iteration 951/2000 [0m                      

                       Computation: 42987 steps/s (collection: 2.184s, learning 0.103s)
             Mean action noise std: 2.73
          Mean value_function loss: 444.1787
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 62.5853
                       Mean reward: 451.43
               Mean episode length: 141.43
    Episode_Reward/reaching_object: 0.8212
     Episode_Reward/lifting_object: 81.4977
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 24.5417
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.29s
                      Time elapsed: 00:44:18
                               ETA: 00:48:49

################################################################################
                     [1m Learning iteration 952/2000 [0m                      

                       Computation: 42814 steps/s (collection: 2.158s, learning 0.138s)
             Mean action noise std: 2.73
          Mean value_function loss: 443.8806
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 62.5886
                       Mean reward: 465.22
               Mean episode length: 142.76
    Episode_Reward/reaching_object: 0.8833
     Episode_Reward/lifting_object: 89.0025
      Episode_Reward/object_height: 0.0271
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 24.3750
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.30s
                      Time elapsed: 00:44:20
                               ETA: 00:48:46

################################################################################
                     [1m Learning iteration 953/2000 [0m                      

                       Computation: 42075 steps/s (collection: 2.204s, learning 0.133s)
             Mean action noise std: 2.73
          Mean value_function loss: 432.6834
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 62.5937
                       Mean reward: 399.88
               Mean episode length: 130.84
    Episode_Reward/reaching_object: 0.9052
     Episode_Reward/lifting_object: 90.7905
      Episode_Reward/object_height: 0.0280
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0432
      Episode_Termination/time_out: 5.1667
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 23.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.34s
                      Time elapsed: 00:44:23
                               ETA: 00:48:42

################################################################################
                     [1m Learning iteration 954/2000 [0m                      

                       Computation: 41906 steps/s (collection: 2.201s, learning 0.145s)
             Mean action noise std: 2.73
          Mean value_function loss: 439.5756
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 62.5958
                       Mean reward: 448.29
               Mean episode length: 140.78
    Episode_Reward/reaching_object: 0.8957
     Episode_Reward/lifting_object: 90.3516
      Episode_Reward/object_height: 0.0284
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 4.9583
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 21.0833
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.35s
                      Time elapsed: 00:44:25
                               ETA: 00:48:39

################################################################################
                     [1m Learning iteration 955/2000 [0m                      

                       Computation: 42789 steps/s (collection: 2.193s, learning 0.104s)
             Mean action noise std: 2.73
          Mean value_function loss: 420.3834
               Mean surrogate loss: 0.0084
                 Mean entropy loss: 62.5979
                       Mean reward: 465.12
               Mean episode length: 144.67
    Episode_Reward/reaching_object: 0.9004
     Episode_Reward/lifting_object: 90.5901
      Episode_Reward/object_height: 0.0291
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 20.8750
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.30s
                      Time elapsed: 00:44:27
                               ETA: 00:48:36

################################################################################
                     [1m Learning iteration 956/2000 [0m                      

                       Computation: 42156 steps/s (collection: 2.188s, learning 0.144s)
             Mean action noise std: 2.73
          Mean value_function loss: 410.6686
               Mean surrogate loss: 0.0062
                 Mean entropy loss: 62.5982
                       Mean reward: 473.01
               Mean episode length: 146.95
    Episode_Reward/reaching_object: 0.9201
     Episode_Reward/lifting_object: 93.0146
      Episode_Reward/object_height: 0.0296
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 19.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.33s
                      Time elapsed: 00:44:30
                               ETA: 00:48:32

################################################################################
                     [1m Learning iteration 957/2000 [0m                      

                       Computation: 41484 steps/s (collection: 2.230s, learning 0.140s)
             Mean action noise std: 2.73
          Mean value_function loss: 418.6501
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 62.5984
                       Mean reward: 524.30
               Mean episode length: 162.94
    Episode_Reward/reaching_object: 0.9792
     Episode_Reward/lifting_object: 99.5539
      Episode_Reward/object_height: 0.0326
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 21.0000
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.37s
                      Time elapsed: 00:44:32
                               ETA: 00:48:29

################################################################################
                     [1m Learning iteration 958/2000 [0m                      

                       Computation: 42848 steps/s (collection: 2.166s, learning 0.128s)
             Mean action noise std: 2.73
          Mean value_function loss: 414.8899
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 62.5985
                       Mean reward: 394.57
               Mean episode length: 126.41
    Episode_Reward/reaching_object: 0.9266
     Episode_Reward/lifting_object: 93.6828
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 5.7500
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 19.5417
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.29s
                      Time elapsed: 00:44:34
                               ETA: 00:48:26

################################################################################
                     [1m Learning iteration 959/2000 [0m                      

                       Computation: 41677 steps/s (collection: 2.241s, learning 0.118s)
             Mean action noise std: 2.73
          Mean value_function loss: 399.4433
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 62.5988
                       Mean reward: 590.56
               Mean episode length: 174.93
    Episode_Reward/reaching_object: 0.9885
     Episode_Reward/lifting_object: 100.8873
      Episode_Reward/object_height: 0.0339
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 7.7917
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 18.4167
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.36s
                      Time elapsed: 00:44:37
                               ETA: 00:48:23

################################################################################
                     [1m Learning iteration 960/2000 [0m                      

                       Computation: 42388 steps/s (collection: 2.188s, learning 0.131s)
             Mean action noise std: 2.73
          Mean value_function loss: 414.5020
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 62.5990
                       Mean reward: 569.15
               Mean episode length: 171.28
    Episode_Reward/reaching_object: 0.9676
     Episode_Reward/lifting_object: 98.4510
      Episode_Reward/object_height: 0.0324
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 19.5833
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 2.32s
                      Time elapsed: 00:44:39
                               ETA: 00:48:19

################################################################################
                     [1m Learning iteration 961/2000 [0m                      

                       Computation: 42494 steps/s (collection: 2.187s, learning 0.126s)
             Mean action noise std: 2.73
          Mean value_function loss: 426.6361
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 62.5988
                       Mean reward: 562.19
               Mean episode length: 168.45
    Episode_Reward/reaching_object: 0.9943
     Episode_Reward/lifting_object: 101.4800
      Episode_Reward/object_height: 0.0332
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 20.6250
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.31s
                      Time elapsed: 00:44:41
                               ETA: 00:48:16

################################################################################
                     [1m Learning iteration 962/2000 [0m                      

                       Computation: 42623 steps/s (collection: 2.178s, learning 0.128s)
             Mean action noise std: 2.73
          Mean value_function loss: 431.0818
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 62.5992
                       Mean reward: 482.71
               Mean episode length: 148.55
    Episode_Reward/reaching_object: 0.9935
     Episode_Reward/lifting_object: 101.2711
      Episode_Reward/object_height: 0.0336
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 19.4167
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.31s
                      Time elapsed: 00:44:44
                               ETA: 00:48:13

################################################################################
                     [1m Learning iteration 963/2000 [0m                      

                       Computation: 42187 steps/s (collection: 2.202s, learning 0.129s)
             Mean action noise std: 2.73
          Mean value_function loss: 419.3391
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 62.6001
                       Mean reward: 477.83
               Mean episode length: 147.24
    Episode_Reward/reaching_object: 0.9380
     Episode_Reward/lifting_object: 94.4442
      Episode_Reward/object_height: 0.0311
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 19.4583
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.33s
                      Time elapsed: 00:44:46
                               ETA: 00:48:09

################################################################################
                     [1m Learning iteration 964/2000 [0m                      

                       Computation: 41931 steps/s (collection: 2.239s, learning 0.106s)
             Mean action noise std: 2.73
          Mean value_function loss: 411.0081
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 62.6034
                       Mean reward: 506.23
               Mean episode length: 159.45
    Episode_Reward/reaching_object: 0.9395
     Episode_Reward/lifting_object: 95.2302
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 20.1667
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.34s
                      Time elapsed: 00:44:48
                               ETA: 00:48:06

################################################################################
                     [1m Learning iteration 965/2000 [0m                      

                       Computation: 42468 steps/s (collection: 2.179s, learning 0.136s)
             Mean action noise std: 2.73
          Mean value_function loss: 418.8606
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 62.6090
                       Mean reward: 462.29
               Mean episode length: 143.01
    Episode_Reward/reaching_object: 0.9696
     Episode_Reward/lifting_object: 98.8582
      Episode_Reward/object_height: 0.0324
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 7.1250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 19.5417
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.31s
                      Time elapsed: 00:44:51
                               ETA: 00:48:03

################################################################################
                     [1m Learning iteration 966/2000 [0m                      

                       Computation: 42347 steps/s (collection: 2.180s, learning 0.141s)
             Mean action noise std: 2.73
          Mean value_function loss: 424.3500
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 62.6135
                       Mean reward: 481.85
               Mean episode length: 146.77
    Episode_Reward/reaching_object: 0.9477
     Episode_Reward/lifting_object: 95.3019
      Episode_Reward/object_height: 0.0300
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 20.3750
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.32s
                      Time elapsed: 00:44:53
                               ETA: 00:47:59

################################################################################
                     [1m Learning iteration 967/2000 [0m                      

                       Computation: 43111 steps/s (collection: 2.181s, learning 0.100s)
             Mean action noise std: 2.73
          Mean value_function loss: 439.3442
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 62.6178
                       Mean reward: 466.57
               Mean episode length: 145.81
    Episode_Reward/reaching_object: 0.9678
     Episode_Reward/lifting_object: 98.0893
      Episode_Reward/object_height: 0.0321
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 6.7083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 20.9167
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.28s
                      Time elapsed: 00:44:55
                               ETA: 00:47:56

################################################################################
                     [1m Learning iteration 968/2000 [0m                      

                       Computation: 42108 steps/s (collection: 2.188s, learning 0.146s)
             Mean action noise std: 2.73
          Mean value_function loss: 451.7184
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 62.6223
                       Mean reward: 423.11
               Mean episode length: 135.85
    Episode_Reward/reaching_object: 0.9743
     Episode_Reward/lifting_object: 98.2347
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 6.2917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 19.5833
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.33s
                      Time elapsed: 00:44:57
                               ETA: 00:47:53

################################################################################
                     [1m Learning iteration 969/2000 [0m                      

                       Computation: 42146 steps/s (collection: 2.199s, learning 0.133s)
             Mean action noise std: 2.73
          Mean value_function loss: 432.4904
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 62.6262
                       Mean reward: 440.93
               Mean episode length: 138.13
    Episode_Reward/reaching_object: 0.9467
     Episode_Reward/lifting_object: 95.9205
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 6.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 20.8750
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.33s
                      Time elapsed: 00:45:00
                               ETA: 00:47:50

################################################################################
                     [1m Learning iteration 970/2000 [0m                      

                       Computation: 42758 steps/s (collection: 2.165s, learning 0.134s)
             Mean action noise std: 2.74
          Mean value_function loss: 415.6607
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 62.6303
                       Mean reward: 553.37
               Mean episode length: 165.60
    Episode_Reward/reaching_object: 0.9940
     Episode_Reward/lifting_object: 101.5071
      Episode_Reward/object_height: 0.0335
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 6.7500
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 17.6250
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.30s
                      Time elapsed: 00:45:02
                               ETA: 00:47:46

################################################################################
                     [1m Learning iteration 971/2000 [0m                      

                       Computation: 42436 steps/s (collection: 2.199s, learning 0.118s)
             Mean action noise std: 2.74
          Mean value_function loss: 420.2665
               Mean surrogate loss: 0.0209
                 Mean entropy loss: 62.6326
                       Mean reward: 494.24
               Mean episode length: 148.55
    Episode_Reward/reaching_object: 0.9879
     Episode_Reward/lifting_object: 100.8479
      Episode_Reward/object_height: 0.0335
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 7.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.32s
                      Time elapsed: 00:45:04
                               ETA: 00:47:43

################################################################################
                     [1m Learning iteration 972/2000 [0m                      

                       Computation: 41887 steps/s (collection: 2.195s, learning 0.152s)
             Mean action noise std: 2.74
          Mean value_function loss: 414.2681
               Mean surrogate loss: 0.0197
                 Mean entropy loss: 62.6331
                       Mean reward: 557.38
               Mean episode length: 164.81
    Episode_Reward/reaching_object: 1.0361
     Episode_Reward/lifting_object: 106.2135
      Episode_Reward/object_height: 0.0364
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 7.5000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 16.1667
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.35s
                      Time elapsed: 00:45:07
                               ETA: 00:47:40

################################################################################
                     [1m Learning iteration 973/2000 [0m                      

                       Computation: 42409 steps/s (collection: 2.188s, learning 0.130s)
             Mean action noise std: 2.74
          Mean value_function loss: 424.4779
               Mean surrogate loss: 0.0198
                 Mean entropy loss: 62.6334
                       Mean reward: 586.00
               Mean episode length: 169.41
    Episode_Reward/reaching_object: 1.0277
     Episode_Reward/lifting_object: 105.0934
      Episode_Reward/object_height: 0.0347
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 6.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 16.7500
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.32s
                      Time elapsed: 00:45:09
                               ETA: 00:47:37

################################################################################
                     [1m Learning iteration 974/2000 [0m                      

                       Computation: 42150 steps/s (collection: 2.200s, learning 0.133s)
             Mean action noise std: 2.74
          Mean value_function loss: 413.5508
               Mean surrogate loss: 0.0117
                 Mean entropy loss: 62.6336
                       Mean reward: 517.70
               Mean episode length: 155.99
    Episode_Reward/reaching_object: 1.0576
     Episode_Reward/lifting_object: 108.6123
      Episode_Reward/object_height: 0.0373
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.33s
                      Time elapsed: 00:45:11
                               ETA: 00:47:33

################################################################################
                     [1m Learning iteration 975/2000 [0m                      

                       Computation: 42365 steps/s (collection: 2.177s, learning 0.144s)
             Mean action noise std: 2.74
          Mean value_function loss: 423.8133
               Mean surrogate loss: 0.0102
                 Mean entropy loss: 62.6342
                       Mean reward: 533.32
               Mean episode length: 159.40
    Episode_Reward/reaching_object: 1.0515
     Episode_Reward/lifting_object: 107.8768
      Episode_Reward/object_height: 0.0366
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 7.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 17.0417
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 2.32s
                      Time elapsed: 00:45:14
                               ETA: 00:47:30

################################################################################
                     [1m Learning iteration 976/2000 [0m                      

                       Computation: 41867 steps/s (collection: 2.217s, learning 0.131s)
             Mean action noise std: 2.74
          Mean value_function loss: 429.9258
               Mean surrogate loss: 0.0115
                 Mean entropy loss: 62.6346
                       Mean reward: 536.39
               Mean episode length: 156.11
    Episode_Reward/reaching_object: 1.0827
     Episode_Reward/lifting_object: 112.4073
      Episode_Reward/object_height: 0.0376
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 7.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 16.7500
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.35s
                      Time elapsed: 00:45:16
                               ETA: 00:47:27

################################################################################
                     [1m Learning iteration 977/2000 [0m                      

                       Computation: 42231 steps/s (collection: 2.196s, learning 0.132s)
             Mean action noise std: 2.74
          Mean value_function loss: 444.6602
               Mean surrogate loss: 0.0107
                 Mean entropy loss: 62.6349
                       Mean reward: 504.67
               Mean episode length: 151.95
    Episode_Reward/reaching_object: 1.0443
     Episode_Reward/lifting_object: 107.4100
      Episode_Reward/object_height: 0.0363
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 17.1667
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.33s
                      Time elapsed: 00:45:18
                               ETA: 00:47:24

################################################################################
                     [1m Learning iteration 978/2000 [0m                      

                       Computation: 42780 steps/s (collection: 2.197s, learning 0.101s)
             Mean action noise std: 2.74
          Mean value_function loss: 435.5991
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 62.6357
                       Mean reward: 533.13
               Mean episode length: 157.59
    Episode_Reward/reaching_object: 1.0607
     Episode_Reward/lifting_object: 109.9184
      Episode_Reward/object_height: 0.0375
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 7.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 17.3333
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.30s
                      Time elapsed: 00:45:21
                               ETA: 00:47:20

################################################################################
                     [1m Learning iteration 979/2000 [0m                      

                       Computation: 41895 steps/s (collection: 2.211s, learning 0.135s)
             Mean action noise std: 2.74
          Mean value_function loss: 439.8698
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 62.6404
                       Mean reward: 596.02
               Mean episode length: 174.00
    Episode_Reward/reaching_object: 1.1215
     Episode_Reward/lifting_object: 116.1710
      Episode_Reward/object_height: 0.0395
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 8.9167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 18.7917
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.35s
                      Time elapsed: 00:45:23
                               ETA: 00:47:17

################################################################################
                     [1m Learning iteration 980/2000 [0m                      

                       Computation: 42319 steps/s (collection: 2.189s, learning 0.134s)
             Mean action noise std: 2.74
          Mean value_function loss: 476.9199
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 62.6451
                       Mean reward: 488.72
               Mean episode length: 145.20
    Episode_Reward/reaching_object: 1.0401
     Episode_Reward/lifting_object: 107.8290
      Episode_Reward/object_height: 0.0348
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 6.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 2.32s
                      Time elapsed: 00:45:25
                               ETA: 00:47:14

################################################################################
                     [1m Learning iteration 981/2000 [0m                      

                       Computation: 41896 steps/s (collection: 2.207s, learning 0.140s)
             Mean action noise std: 2.74
          Mean value_function loss: 468.8304
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 62.6464
                       Mean reward: 509.49
               Mean episode length: 148.67
    Episode_Reward/reaching_object: 1.0163
     Episode_Reward/lifting_object: 105.3516
      Episode_Reward/object_height: 0.0343
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 7.0000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 21.6250
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.35s
                      Time elapsed: 00:45:28
                               ETA: 00:47:11

################################################################################
                     [1m Learning iteration 982/2000 [0m                      

                       Computation: 41516 steps/s (collection: 2.237s, learning 0.131s)
             Mean action noise std: 2.74
          Mean value_function loss: 464.3643
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 62.6472
                       Mean reward: 505.09
               Mean episode length: 149.51
    Episode_Reward/reaching_object: 0.9901
     Episode_Reward/lifting_object: 102.0632
      Episode_Reward/object_height: 0.0333
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 19.8333
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.37s
                      Time elapsed: 00:45:30
                               ETA: 00:47:07

################################################################################
                     [1m Learning iteration 983/2000 [0m                      

                       Computation: 40533 steps/s (collection: 2.287s, learning 0.138s)
             Mean action noise std: 2.74
          Mean value_function loss: 463.2344
               Mean surrogate loss: 0.0061
                 Mean entropy loss: 62.6491
                       Mean reward: 611.24
               Mean episode length: 172.32
    Episode_Reward/reaching_object: 1.0338
     Episode_Reward/lifting_object: 107.2897
      Episode_Reward/object_height: 0.0348
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 5.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 17.7083
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 2.43s
                      Time elapsed: 00:45:33
                               ETA: 00:47:04

################################################################################
                     [1m Learning iteration 984/2000 [0m                      

                       Computation: 40276 steps/s (collection: 2.306s, learning 0.135s)
             Mean action noise std: 2.74
          Mean value_function loss: 458.5843
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 62.6505
                       Mean reward: 573.93
               Mean episode length: 165.78
    Episode_Reward/reaching_object: 1.0519
     Episode_Reward/lifting_object: 109.7410
      Episode_Reward/object_height: 0.0355
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 20.7083
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 2.44s
                      Time elapsed: 00:45:35
                               ETA: 00:47:01

################################################################################
                     [1m Learning iteration 985/2000 [0m                      

                       Computation: 41130 steps/s (collection: 2.272s, learning 0.118s)
             Mean action noise std: 2.74
          Mean value_function loss: 495.9728
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.6529
                       Mean reward: 517.87
               Mean episode length: 151.90
    Episode_Reward/reaching_object: 0.9873
     Episode_Reward/lifting_object: 102.4238
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 4.2917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 23.4583
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 2.39s
                      Time elapsed: 00:45:37
                               ETA: 00:46:58

################################################################################
                     [1m Learning iteration 986/2000 [0m                      

                       Computation: 41441 steps/s (collection: 2.230s, learning 0.142s)
             Mean action noise std: 2.74
          Mean value_function loss: 529.0616
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 62.6616
                       Mean reward: 552.49
               Mean episode length: 160.04
    Episode_Reward/reaching_object: 0.9824
     Episode_Reward/lifting_object: 102.2714
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 5.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 25.7917
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 2.37s
                      Time elapsed: 00:45:40
                               ETA: 00:46:55

################################################################################
                     [1m Learning iteration 987/2000 [0m                      

                       Computation: 40508 steps/s (collection: 2.289s, learning 0.138s)
             Mean action noise std: 2.74
          Mean value_function loss: 533.1285
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 62.6785
                       Mean reward: 467.06
               Mean episode length: 136.89
    Episode_Reward/reaching_object: 0.9375
     Episode_Reward/lifting_object: 96.8452
      Episode_Reward/object_height: 0.0285
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 26.0417
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.43s
                      Time elapsed: 00:45:42
                               ETA: 00:46:52

################################################################################
                     [1m Learning iteration 988/2000 [0m                      

                       Computation: 40052 steps/s (collection: 2.334s, learning 0.121s)
             Mean action noise std: 2.74
          Mean value_function loss: 539.5819
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 62.6925
                       Mean reward: 491.20
               Mean episode length: 142.32
    Episode_Reward/reaching_object: 0.9359
     Episode_Reward/lifting_object: 96.3615
      Episode_Reward/object_height: 0.0282
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 3.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 26.5417
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 2.45s
                      Time elapsed: 00:45:45
                               ETA: 00:46:48

################################################################################
                     [1m Learning iteration 989/2000 [0m                      

                       Computation: 40875 steps/s (collection: 2.275s, learning 0.130s)
             Mean action noise std: 2.74
          Mean value_function loss: 489.8451
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 62.7020
                       Mean reward: 500.50
               Mean episode length: 147.98
    Episode_Reward/reaching_object: 0.9123
     Episode_Reward/lifting_object: 93.2040
      Episode_Reward/object_height: 0.0276
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 22.5833
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.40s
                      Time elapsed: 00:45:47
                               ETA: 00:46:45

################################################################################
                     [1m Learning iteration 990/2000 [0m                      

                       Computation: 41464 steps/s (collection: 2.235s, learning 0.136s)
             Mean action noise std: 2.74
          Mean value_function loss: 497.6857
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 62.7084
                       Mean reward: 521.78
               Mean episode length: 152.05
    Episode_Reward/reaching_object: 0.9510
     Episode_Reward/lifting_object: 98.0741
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 4.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 23.5833
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.37s
                      Time elapsed: 00:45:49
                               ETA: 00:46:42

################################################################################
                     [1m Learning iteration 991/2000 [0m                      

                       Computation: 41356 steps/s (collection: 2.244s, learning 0.133s)
             Mean action noise std: 2.74
          Mean value_function loss: 495.6516
               Mean surrogate loss: 0.0074
                 Mean entropy loss: 62.7104
                       Mean reward: 504.16
               Mean episode length: 147.31
    Episode_Reward/reaching_object: 0.9989
     Episode_Reward/lifting_object: 103.5265
      Episode_Reward/object_height: 0.0315
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 5.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 22.7500
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.38s
                      Time elapsed: 00:45:52
                               ETA: 00:46:39

################################################################################
                     [1m Learning iteration 992/2000 [0m                      

                       Computation: 41318 steps/s (collection: 2.251s, learning 0.128s)
             Mean action noise std: 2.74
          Mean value_function loss: 495.2064
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 62.7106
                       Mean reward: 500.81
               Mean episode length: 147.41
    Episode_Reward/reaching_object: 0.9440
     Episode_Reward/lifting_object: 96.9079
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 3.3750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 21.7917
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.38s
                      Time elapsed: 00:45:54
                               ETA: 00:46:36

################################################################################
                     [1m Learning iteration 993/2000 [0m                      

                       Computation: 42256 steps/s (collection: 2.226s, learning 0.100s)
             Mean action noise std: 2.74
          Mean value_function loss: 485.8980
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 62.7115
                       Mean reward: 478.78
               Mean episode length: 139.27
    Episode_Reward/reaching_object: 0.9331
     Episode_Reward/lifting_object: 96.0513
      Episode_Reward/object_height: 0.0272
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 22.5000
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.33s
                      Time elapsed: 00:45:56
                               ETA: 00:46:33

################################################################################
                     [1m Learning iteration 994/2000 [0m                      

                       Computation: 38798 steps/s (collection: 2.405s, learning 0.129s)
             Mean action noise std: 2.75
          Mean value_function loss: 535.6028
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 62.7133
                       Mean reward: 454.60
               Mean episode length: 139.91
    Episode_Reward/reaching_object: 0.9650
     Episode_Reward/lifting_object: 98.5311
      Episode_Reward/object_height: 0.0275
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 3.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 24.6667
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.53s
                      Time elapsed: 00:45:59
                               ETA: 00:46:30

################################################################################
                     [1m Learning iteration 995/2000 [0m                      

                       Computation: 40060 steps/s (collection: 2.345s, learning 0.109s)
             Mean action noise std: 2.75
          Mean value_function loss: 496.5973
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 62.7158
                       Mean reward: 490.64
               Mean episode length: 146.19
    Episode_Reward/reaching_object: 0.9425
     Episode_Reward/lifting_object: 96.0701
      Episode_Reward/object_height: 0.0269
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 3.5833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 23.7083
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.45s
                      Time elapsed: 00:46:01
                               ETA: 00:46:26

################################################################################
                     [1m Learning iteration 996/2000 [0m                      

                       Computation: 41246 steps/s (collection: 2.264s, learning 0.120s)
             Mean action noise std: 2.75
          Mean value_function loss: 499.0051
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 62.7169
                       Mean reward: 490.13
               Mean episode length: 145.37
    Episode_Reward/reaching_object: 0.9831
     Episode_Reward/lifting_object: 100.7584
      Episode_Reward/object_height: 0.0285
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0423
      Episode_Termination/time_out: 4.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 24.9167
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.38s
                      Time elapsed: 00:46:04
                               ETA: 00:46:23

################################################################################
                     [1m Learning iteration 997/2000 [0m                      

                       Computation: 38055 steps/s (collection: 2.457s, learning 0.126s)
             Mean action noise std: 2.75
          Mean value_function loss: 533.9293
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 62.7185
                       Mean reward: 535.27
               Mean episode length: 156.25
    Episode_Reward/reaching_object: 0.9974
     Episode_Reward/lifting_object: 102.6497
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 4.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.58s
                      Time elapsed: 00:46:06
                               ETA: 00:46:20

################################################################################
                     [1m Learning iteration 998/2000 [0m                      

                       Computation: 40922 steps/s (collection: 2.286s, learning 0.117s)
             Mean action noise std: 2.75
          Mean value_function loss: 543.9606
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 62.7209
                       Mean reward: 412.18
               Mean episode length: 130.86
    Episode_Reward/reaching_object: 0.9446
     Episode_Reward/lifting_object: 96.2280
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 28.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 2.40s
                      Time elapsed: 00:46:09
                               ETA: 00:46:17

################################################################################
                     [1m Learning iteration 999/2000 [0m                      

                       Computation: 39252 steps/s (collection: 2.389s, learning 0.115s)
             Mean action noise std: 2.75
          Mean value_function loss: 530.5144
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 62.7251
                       Mean reward: 452.53
               Mean episode length: 138.88
    Episode_Reward/reaching_object: 0.9163
     Episode_Reward/lifting_object: 93.1208
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 26.1667
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 2.50s
                      Time elapsed: 00:46:11
                               ETA: 00:46:14

################################################################################
                     [1m Learning iteration 1000/2000 [0m                     

                       Computation: 13296 steps/s (collection: 7.261s, learning 0.133s)
             Mean action noise std: 2.75
          Mean value_function loss: 516.0369
               Mean surrogate loss: 0.0053
                 Mean entropy loss: 62.7283
                       Mean reward: 489.28
               Mean episode length: 146.83
    Episode_Reward/reaching_object: 0.8809
     Episode_Reward/lifting_object: 89.3811
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0394
      Episode_Termination/time_out: 3.2500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 7.39s
                      Time elapsed: 00:46:19
                               ETA: 00:46:16

################################################################################
                     [1m Learning iteration 1001/2000 [0m                     

                       Computation: 13743 steps/s (collection: 7.018s, learning 0.135s)
             Mean action noise std: 2.75
          Mean value_function loss: 504.4166
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 62.7303
                       Mean reward: 541.45
               Mean episode length: 156.58
    Episode_Reward/reaching_object: 0.9778
     Episode_Reward/lifting_object: 100.3915
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 4.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 23.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 7.15s
                      Time elapsed: 00:46:26
                               ETA: 00:46:18

################################################################################
                     [1m Learning iteration 1002/2000 [0m                     

                       Computation: 13461 steps/s (collection: 7.151s, learning 0.151s)
             Mean action noise std: 2.75
          Mean value_function loss: 520.9419
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 62.7317
                       Mean reward: 441.91
               Mean episode length: 133.47
    Episode_Reward/reaching_object: 0.9260
     Episode_Reward/lifting_object: 94.5320
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 3.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 24.3333
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 7.30s
                      Time elapsed: 00:46:33
                               ETA: 00:46:19

################################################################################
                     [1m Learning iteration 1003/2000 [0m                     

                       Computation: 13751 steps/s (collection: 7.015s, learning 0.134s)
             Mean action noise std: 2.75
          Mean value_function loss: 504.4491
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 62.7345
                       Mean reward: 519.39
               Mean episode length: 150.29
    Episode_Reward/reaching_object: 0.8811
     Episode_Reward/lifting_object: 89.7458
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 23.2083
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 7.15s
                      Time elapsed: 00:46:40
                               ETA: 00:46:21

################################################################################
                     [1m Learning iteration 1004/2000 [0m                     

                       Computation: 14144 steps/s (collection: 6.817s, learning 0.134s)
             Mean action noise std: 2.75
          Mean value_function loss: 539.1076
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 62.7395
                       Mean reward: 504.13
               Mean episode length: 147.12
    Episode_Reward/reaching_object: 0.9032
     Episode_Reward/lifting_object: 92.0028
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0388
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 25.4583
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 6.95s
                      Time elapsed: 00:46:47
                               ETA: 00:46:22

################################################################################
                     [1m Learning iteration 1005/2000 [0m                     

                       Computation: 13771 steps/s (collection: 7.003s, learning 0.135s)
             Mean action noise std: 2.75
          Mean value_function loss: 514.8405
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 62.7410
                       Mean reward: 463.50
               Mean episode length: 137.97
    Episode_Reward/reaching_object: 0.9457
     Episode_Reward/lifting_object: 96.8529
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 24.2083
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 7.14s
                      Time elapsed: 00:46:54
                               ETA: 00:46:24

################################################################################
                     [1m Learning iteration 1006/2000 [0m                     

                       Computation: 13432 steps/s (collection: 7.186s, learning 0.133s)
             Mean action noise std: 2.75
          Mean value_function loss: 491.8408
               Mean surrogate loss: 0.0080
                 Mean entropy loss: 62.7414
                       Mean reward: 588.81
               Mean episode length: 168.50
    Episode_Reward/reaching_object: 0.9517
     Episode_Reward/lifting_object: 97.2544
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 3.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 21.7083
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 7.32s
                      Time elapsed: 00:47:02
                               ETA: 00:46:25

################################################################################
                     [1m Learning iteration 1007/2000 [0m                     

                       Computation: 12773 steps/s (collection: 7.571s, learning 0.125s)
             Mean action noise std: 2.75
          Mean value_function loss: 1439482.0266
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 62.7418
                       Mean reward: 487.15
               Mean episode length: 144.94
    Episode_Reward/reaching_object: 0.9671
     Episode_Reward/lifting_object: 98.9705
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 3.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 20.9583
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 7.70s
                      Time elapsed: 00:47:09
                               ETA: 00:46:27

################################################################################
                     [1m Learning iteration 1008/2000 [0m                     

                       Computation: 15482 steps/s (collection: 6.228s, learning 0.122s)
             Mean action noise std: 2.75
          Mean value_function loss: 103848220790.5011
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 62.7422
                       Mean reward: 526.97
               Mean episode length: 153.66
    Episode_Reward/reaching_object: 1.0937
     Episode_Reward/lifting_object: 113.8023
      Episode_Reward/object_height: 0.0324
        Episode_Reward/action_rate: -42.7986
          Episode_Reward/joint_vel: -17054.1602
      Episode_Termination/time_out: 5.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 17.7917
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 6.35s
                      Time elapsed: 00:47:16
                               ETA: 00:46:28

################################################################################
                     [1m Learning iteration 1009/2000 [0m                     

                       Computation: 42502 steps/s (collection: 2.184s, learning 0.129s)
             Mean action noise std: 2.75
          Mean value_function loss: 473.0328
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.7448
                       Mean reward: 570.04
               Mean episode length: 162.78
    Episode_Reward/reaching_object: 1.0061
     Episode_Reward/lifting_object: 103.2604
      Episode_Reward/object_height: 0.0279
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 19.2917
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 2.31s
                      Time elapsed: 00:47:18
                               ETA: 00:46:25

################################################################################
                     [1m Learning iteration 1010/2000 [0m                     

                       Computation: 42894 steps/s (collection: 2.163s, learning 0.129s)
             Mean action noise std: 2.75
          Mean value_function loss: 495.9738
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 62.7548
                       Mean reward: 539.52
               Mean episode length: 155.68
    Episode_Reward/reaching_object: 1.0783
     Episode_Reward/lifting_object: 111.5710
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0430
      Episode_Termination/time_out: 5.8750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 19.1250
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 2.29s
                      Time elapsed: 00:47:20
                               ETA: 00:46:21

################################################################################
                     [1m Learning iteration 1011/2000 [0m                     

                       Computation: 42218 steps/s (collection: 2.206s, learning 0.123s)
             Mean action noise std: 2.75
          Mean value_function loss: 460.3634
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.7641
                       Mean reward: 591.17
               Mean episode length: 167.96
    Episode_Reward/reaching_object: 1.1113
     Episode_Reward/lifting_object: 115.7740
      Episode_Reward/object_height: 0.0324
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0446
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 16.8333
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 2.33s
                      Time elapsed: 00:47:23
                               ETA: 00:46:18

################################################################################
                     [1m Learning iteration 1012/2000 [0m                     

                       Computation: 42670 steps/s (collection: 2.204s, learning 0.100s)
             Mean action noise std: 2.75
          Mean value_function loss: 451.2302
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 62.7753
                       Mean reward: 593.32
               Mean episode length: 171.99
    Episode_Reward/reaching_object: 1.0859
     Episode_Reward/lifting_object: 112.2976
      Episode_Reward/object_height: 0.0310
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0426
      Episode_Termination/time_out: 6.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 18.2500
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 2.30s
                      Time elapsed: 00:47:25
                               ETA: 00:46:15

################################################################################
                     [1m Learning iteration 1013/2000 [0m                     

                       Computation: 42712 steps/s (collection: 2.170s, learning 0.132s)
             Mean action noise std: 2.75
          Mean value_function loss: 430.0974
               Mean surrogate loss: 0.0112
                 Mean entropy loss: 62.7829
                       Mean reward: 587.86
               Mean episode length: 167.04
    Episode_Reward/reaching_object: 1.1067
     Episode_Reward/lifting_object: 114.9294
      Episode_Reward/object_height: 0.0314
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 16.2917
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 2.30s
                      Time elapsed: 00:47:27
                               ETA: 00:46:11

################################################################################
                     [1m Learning iteration 1014/2000 [0m                     

                       Computation: 42871 steps/s (collection: 2.161s, learning 0.132s)
             Mean action noise std: 2.75
          Mean value_function loss: 447.0765
               Mean surrogate loss: 0.0070
                 Mean entropy loss: 62.7839
                       Mean reward: 595.48
               Mean episode length: 169.07
    Episode_Reward/reaching_object: 1.1507
     Episode_Reward/lifting_object: 120.2520
      Episode_Reward/object_height: 0.0337
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 7.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 16.2083
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 2.29s
                      Time elapsed: 00:47:30
                               ETA: 00:46:08

################################################################################
                     [1m Learning iteration 1015/2000 [0m                     

                       Computation: 43196 steps/s (collection: 2.157s, learning 0.119s)
             Mean action noise std: 2.75
          Mean value_function loss: 403.5869
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 62.7859
                       Mean reward: 688.52
               Mean episode length: 193.29
    Episode_Reward/reaching_object: 1.1992
     Episode_Reward/lifting_object: 125.4256
      Episode_Reward/object_height: 0.0364
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 13.3750
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 2.28s
                      Time elapsed: 00:47:32
                               ETA: 00:46:05

################################################################################
                     [1m Learning iteration 1016/2000 [0m                     

                       Computation: 42214 steps/s (collection: 2.188s, learning 0.141s)
             Mean action noise std: 2.76
          Mean value_function loss: 382.7037
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 62.7965
                       Mean reward: 650.11
               Mean episode length: 180.88
    Episode_Reward/reaching_object: 1.2321
     Episode_Reward/lifting_object: 129.1722
      Episode_Reward/object_height: 0.0377
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 8.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 2.33s
                      Time elapsed: 00:47:34
                               ETA: 00:46:02

################################################################################
                     [1m Learning iteration 1017/2000 [0m                     

                       Computation: 41558 steps/s (collection: 2.221s, learning 0.145s)
             Mean action noise std: 2.76
          Mean value_function loss: 394.7244
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 62.8170
                       Mean reward: 614.07
               Mean episode length: 171.96
    Episode_Reward/reaching_object: 1.2093
     Episode_Reward/lifting_object: 126.5486
      Episode_Reward/object_height: 0.0367
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 10.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 13.5833
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 2.37s
                      Time elapsed: 00:47:37
                               ETA: 00:45:58

################################################################################
                     [1m Learning iteration 1018/2000 [0m                     

                       Computation: 41963 steps/s (collection: 2.198s, learning 0.145s)
             Mean action noise std: 2.76
          Mean value_function loss: 398.2186
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 62.8325
                       Mean reward: 665.74
               Mean episode length: 184.33
    Episode_Reward/reaching_object: 1.1848
     Episode_Reward/lifting_object: 123.5034
      Episode_Reward/object_height: 0.0350
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 8.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.5417
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 2.34s
                      Time elapsed: 00:47:39
                               ETA: 00:45:55

################################################################################
                     [1m Learning iteration 1019/2000 [0m                     

                       Computation: 40950 steps/s (collection: 2.284s, learning 0.117s)
             Mean action noise std: 2.76
          Mean value_function loss: 379.6199
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 62.8463
                       Mean reward: 642.80
               Mean episode length: 181.68
    Episode_Reward/reaching_object: 1.2313
     Episode_Reward/lifting_object: 128.3145
      Episode_Reward/object_height: 0.0374
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 2.40s
                      Time elapsed: 00:47:41
                               ETA: 00:45:52

################################################################################
                     [1m Learning iteration 1020/2000 [0m                     

                       Computation: 42455 steps/s (collection: 2.202s, learning 0.113s)
             Mean action noise std: 2.76
          Mean value_function loss: 369.0573
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 62.8580
                       Mean reward: 653.58
               Mean episode length: 184.72
    Episode_Reward/reaching_object: 1.2454
     Episode_Reward/lifting_object: 129.2849
      Episode_Reward/object_height: 0.0378
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 2.32s
                      Time elapsed: 00:47:44
                               ETA: 00:45:49

################################################################################
                     [1m Learning iteration 1021/2000 [0m                     

                       Computation: 42872 steps/s (collection: 2.160s, learning 0.133s)
             Mean action noise std: 2.76
          Mean value_function loss: 377.1665
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 62.8752
                       Mean reward: 737.90
               Mean episode length: 204.54
    Episode_Reward/reaching_object: 1.2857
     Episode_Reward/lifting_object: 134.0259
      Episode_Reward/object_height: 0.0386
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 11.7917
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 2.29s
                      Time elapsed: 00:47:46
                               ETA: 00:45:45

################################################################################
                     [1m Learning iteration 1022/2000 [0m                     

                       Computation: 41992 steps/s (collection: 2.217s, learning 0.124s)
             Mean action noise std: 2.77
          Mean value_function loss: 372.5335
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 62.8880
                       Mean reward: 688.27
               Mean episode length: 193.26
    Episode_Reward/reaching_object: 1.2799
     Episode_Reward/lifting_object: 133.0702
      Episode_Reward/object_height: 0.0384
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 2.34s
                      Time elapsed: 00:47:48
                               ETA: 00:45:42

################################################################################
                     [1m Learning iteration 1023/2000 [0m                     

                       Computation: 41428 steps/s (collection: 2.236s, learning 0.137s)
             Mean action noise std: 2.77
          Mean value_function loss: 355.7912
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 62.9062
                       Mean reward: 657.46
               Mean episode length: 181.35
    Episode_Reward/reaching_object: 1.2871
     Episode_Reward/lifting_object: 133.7977
      Episode_Reward/object_height: 0.0385
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 2.37s
                      Time elapsed: 00:47:51
                               ETA: 00:45:39

################################################################################
                     [1m Learning iteration 1024/2000 [0m                     

                       Computation: 40037 steps/s (collection: 2.322s, learning 0.133s)
             Mean action noise std: 2.77
          Mean value_function loss: 353.8656
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 62.9224
                       Mean reward: 698.28
               Mean episode length: 193.26
    Episode_Reward/reaching_object: 1.3025
     Episode_Reward/lifting_object: 135.9640
      Episode_Reward/object_height: 0.0396
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 9.9167
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 2.46s
                      Time elapsed: 00:47:53
                               ETA: 00:45:36

################################################################################
                     [1m Learning iteration 1025/2000 [0m                     

                       Computation: 40936 steps/s (collection: 2.271s, learning 0.131s)
             Mean action noise std: 2.77
          Mean value_function loss: 335.4076
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 62.9336
                       Mean reward: 710.05
               Mean episode length: 193.26
    Episode_Reward/reaching_object: 1.2979
     Episode_Reward/lifting_object: 135.0025
      Episode_Reward/object_height: 0.0396
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 2.40s
                      Time elapsed: 00:47:56
                               ETA: 00:45:33

################################################################################
                     [1m Learning iteration 1026/2000 [0m                     

                       Computation: 39715 steps/s (collection: 2.341s, learning 0.135s)
             Mean action noise std: 2.77
          Mean value_function loss: 345.4584
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 62.9354
                       Mean reward: 689.89
               Mean episode length: 192.90
    Episode_Reward/reaching_object: 1.2930
     Episode_Reward/lifting_object: 134.1090
      Episode_Reward/object_height: 0.0389
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 8.6667
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 2.48s
                      Time elapsed: 00:47:58
                               ETA: 00:45:29

################################################################################
                     [1m Learning iteration 1027/2000 [0m                     

                       Computation: 40010 steps/s (collection: 2.323s, learning 0.134s)
             Mean action noise std: 2.77
          Mean value_function loss: 329.7405
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 62.9365
                       Mean reward: 763.20
               Mean episode length: 209.61
    Episode_Reward/reaching_object: 1.3094
     Episode_Reward/lifting_object: 135.7425
      Episode_Reward/object_height: 0.0393
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 11.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 2.46s
                      Time elapsed: 00:48:00
                               ETA: 00:45:26

################################################################################
                     [1m Learning iteration 1028/2000 [0m                     

                       Computation: 40314 steps/s (collection: 2.304s, learning 0.135s)
             Mean action noise std: 2.77
          Mean value_function loss: 334.5094
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 62.9384
                       Mean reward: 682.29
               Mean episode length: 191.19
    Episode_Reward/reaching_object: 1.3189
     Episode_Reward/lifting_object: 137.2367
      Episode_Reward/object_height: 0.0400
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 11.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 2.44s
                      Time elapsed: 00:48:03
                               ETA: 00:45:23

################################################################################
                     [1m Learning iteration 1029/2000 [0m                     

                       Computation: 39287 steps/s (collection: 2.367s, learning 0.135s)
             Mean action noise std: 2.77
          Mean value_function loss: 326.0379
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 62.9429
                       Mean reward: 731.63
               Mean episode length: 200.00
    Episode_Reward/reaching_object: 1.3298
     Episode_Reward/lifting_object: 137.9325
      Episode_Reward/object_height: 0.0397
        Episode_Reward/action_rate: -0.0390
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 2.50s
                      Time elapsed: 00:48:05
                               ETA: 00:45:20

################################################################################
                     [1m Learning iteration 1030/2000 [0m                     

                       Computation: 41314 steps/s (collection: 2.268s, learning 0.111s)
             Mean action noise std: 2.77
          Mean value_function loss: 302.0959
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 62.9453
                       Mean reward: 776.28
               Mean episode length: 211.60
    Episode_Reward/reaching_object: 1.3760
     Episode_Reward/lifting_object: 143.3876
      Episode_Reward/object_height: 0.0421
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 2.38s
                      Time elapsed: 00:48:08
                               ETA: 00:45:17

################################################################################
                     [1m Learning iteration 1031/2000 [0m                     

                       Computation: 40634 steps/s (collection: 2.296s, learning 0.124s)
             Mean action noise std: 2.77
          Mean value_function loss: 317.0277
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 62.9463
                       Mean reward: 724.52
               Mean episode length: 199.62
    Episode_Reward/reaching_object: 1.3690
     Episode_Reward/lifting_object: 142.0642
      Episode_Reward/object_height: 0.0414
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0472
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 2.42s
                      Time elapsed: 00:48:10
                               ETA: 00:45:14

################################################################################
                     [1m Learning iteration 1032/2000 [0m                     

                       Computation: 40640 steps/s (collection: 2.265s, learning 0.154s)
             Mean action noise std: 2.77
          Mean value_function loss: 319.2212
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.9486
                       Mean reward: 708.17
               Mean episode length: 195.13
    Episode_Reward/reaching_object: 1.3483
     Episode_Reward/lifting_object: 140.2828
      Episode_Reward/object_height: 0.0409
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 2.42s
                      Time elapsed: 00:48:13
                               ETA: 00:45:11

################################################################################
                     [1m Learning iteration 1033/2000 [0m                     

                       Computation: 40648 steps/s (collection: 2.288s, learning 0.130s)
             Mean action noise std: 2.77
          Mean value_function loss: 341.9477
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 62.9571
                       Mean reward: 651.49
               Mean episode length: 184.14
    Episode_Reward/reaching_object: 1.3453
     Episode_Reward/lifting_object: 139.9023
      Episode_Reward/object_height: 0.0403
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 2.42s
                      Time elapsed: 00:48:15
                               ETA: 00:45:07

################################################################################
                     [1m Learning iteration 1034/2000 [0m                     

                       Computation: 37235 steps/s (collection: 2.463s, learning 0.177s)
             Mean action noise std: 2.77
          Mean value_function loss: 325.9405
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 62.9696
                       Mean reward: 690.98
               Mean episode length: 192.53
    Episode_Reward/reaching_object: 1.3613
     Episode_Reward/lifting_object: 141.5240
      Episode_Reward/object_height: 0.0417
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 2.64s
                      Time elapsed: 00:48:18
                               ETA: 00:45:04

################################################################################
                     [1m Learning iteration 1035/2000 [0m                     

                       Computation: 39765 steps/s (collection: 2.345s, learning 0.128s)
             Mean action noise std: 2.78
          Mean value_function loss: 326.2350
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 62.9860
                       Mean reward: 759.62
               Mean episode length: 206.48
    Episode_Reward/reaching_object: 1.3635
     Episode_Reward/lifting_object: 141.7609
      Episode_Reward/object_height: 0.0409
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 2.47s
                      Time elapsed: 00:48:20
                               ETA: 00:45:01

################################################################################
                     [1m Learning iteration 1036/2000 [0m                     

                       Computation: 39612 steps/s (collection: 2.330s, learning 0.152s)
             Mean action noise std: 2.78
          Mean value_function loss: 339.5348
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 62.9931
                       Mean reward: 696.98
               Mean episode length: 193.21
    Episode_Reward/reaching_object: 1.3035
     Episode_Reward/lifting_object: 134.7049
      Episode_Reward/object_height: 0.0384
        Episode_Reward/action_rate: -0.0388
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.9583
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 2.48s
                      Time elapsed: 00:48:23
                               ETA: 00:44:58

################################################################################
                     [1m Learning iteration 1037/2000 [0m                     

                       Computation: 38648 steps/s (collection: 2.383s, learning 0.161s)
             Mean action noise std: 2.78
          Mean value_function loss: 316.9933
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 63.0008
                       Mean reward: 699.52
               Mean episode length: 195.35
    Episode_Reward/reaching_object: 1.3329
     Episode_Reward/lifting_object: 137.4821
      Episode_Reward/object_height: 0.0394
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 2.54s
                      Time elapsed: 00:48:25
                               ETA: 00:44:55

################################################################################
                     [1m Learning iteration 1038/2000 [0m                     

                       Computation: 38186 steps/s (collection: 2.403s, learning 0.172s)
             Mean action noise std: 2.78
          Mean value_function loss: 334.5892
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 63.0236
                       Mean reward: 729.64
               Mean episode length: 198.56
    Episode_Reward/reaching_object: 1.3746
     Episode_Reward/lifting_object: 143.0050
      Episode_Reward/object_height: 0.0414
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0462
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 2.57s
                      Time elapsed: 00:48:28
                               ETA: 00:44:52

################################################################################
                     [1m Learning iteration 1039/2000 [0m                     

                       Computation: 38419 steps/s (collection: 2.417s, learning 0.142s)
             Mean action noise std: 2.78
          Mean value_function loss: 359.5278
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 63.0465
                       Mean reward: 626.45
               Mean episode length: 176.81
    Episode_Reward/reaching_object: 1.3190
     Episode_Reward/lifting_object: 136.9397
      Episode_Reward/object_height: 0.0390
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 2.56s
                      Time elapsed: 00:48:30
                               ETA: 00:44:49

################################################################################
                     [1m Learning iteration 1040/2000 [0m                     

                       Computation: 40729 steps/s (collection: 2.275s, learning 0.138s)
             Mean action noise std: 2.78
          Mean value_function loss: 325.8748
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 63.0532
                       Mean reward: 700.52
               Mean episode length: 193.84
    Episode_Reward/reaching_object: 1.3716
     Episode_Reward/lifting_object: 142.5997
      Episode_Reward/object_height: 0.0409
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 2.41s
                      Time elapsed: 00:48:33
                               ETA: 00:44:46

################################################################################
                     [1m Learning iteration 1041/2000 [0m                     

                       Computation: 40581 steps/s (collection: 2.290s, learning 0.133s)
             Mean action noise std: 2.78
          Mean value_function loss: 329.2318
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 63.0561
                       Mean reward: 658.11
               Mean episode length: 182.42
    Episode_Reward/reaching_object: 1.3309
     Episode_Reward/lifting_object: 138.3450
      Episode_Reward/object_height: 0.0400
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.4583
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 2.42s
                      Time elapsed: 00:48:35
                               ETA: 00:44:43

################################################################################
                     [1m Learning iteration 1042/2000 [0m                     

                       Computation: 41017 steps/s (collection: 2.276s, learning 0.121s)
             Mean action noise std: 2.79
          Mean value_function loss: 310.9667
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 63.0651
                       Mean reward: 657.82
               Mean episode length: 185.34
    Episode_Reward/reaching_object: 1.3260
     Episode_Reward/lifting_object: 137.3332
      Episode_Reward/object_height: 0.0396
        Episode_Reward/action_rate: -0.0391
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 2.40s
                      Time elapsed: 00:48:38
                               ETA: 00:44:40

################################################################################
                     [1m Learning iteration 1043/2000 [0m                     

                       Computation: 40877 steps/s (collection: 2.272s, learning 0.133s)
             Mean action noise std: 2.79
          Mean value_function loss: 317.6211
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 63.0871
                       Mean reward: 762.69
               Mean episode length: 207.18
    Episode_Reward/reaching_object: 1.3975
     Episode_Reward/lifting_object: 145.6517
      Episode_Reward/object_height: 0.0421
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 2.40s
                      Time elapsed: 00:48:40
                               ETA: 00:44:37

################################################################################
                     [1m Learning iteration 1044/2000 [0m                     

                       Computation: 41031 steps/s (collection: 2.264s, learning 0.132s)
             Mean action noise std: 2.79
          Mean value_function loss: 330.7755
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 63.1057
                       Mean reward: 623.95
               Mean episode length: 175.59
    Episode_Reward/reaching_object: 1.3266
     Episode_Reward/lifting_object: 137.3750
      Episode_Reward/object_height: 0.0394
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.7083
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 2.40s
                      Time elapsed: 00:48:42
                               ETA: 00:44:33

################################################################################
                     [1m Learning iteration 1045/2000 [0m                     

                       Computation: 39834 steps/s (collection: 2.326s, learning 0.142s)
             Mean action noise std: 2.79
          Mean value_function loss: 319.6519
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 63.1161
                       Mean reward: 683.32
               Mean episode length: 189.12
    Episode_Reward/reaching_object: 1.3235
     Episode_Reward/lifting_object: 137.3410
      Episode_Reward/object_height: 0.0392
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 2.47s
                      Time elapsed: 00:48:45
                               ETA: 00:44:30

################################################################################
                     [1m Learning iteration 1046/2000 [0m                     

                       Computation: 39747 steps/s (collection: 2.318s, learning 0.155s)
             Mean action noise std: 2.79
          Mean value_function loss: 315.7780
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 63.1317
                       Mean reward: 663.46
               Mean episode length: 185.75
    Episode_Reward/reaching_object: 1.3278
     Episode_Reward/lifting_object: 137.3438
      Episode_Reward/object_height: 0.0392
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0448
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 2.47s
                      Time elapsed: 00:48:47
                               ETA: 00:44:27

################################################################################
                     [1m Learning iteration 1047/2000 [0m                     

                       Computation: 39506 steps/s (collection: 2.320s, learning 0.168s)
             Mean action noise std: 2.79
          Mean value_function loss: 306.1631
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 63.1366
                       Mean reward: 700.90
               Mean episode length: 194.24
    Episode_Reward/reaching_object: 1.3493
     Episode_Reward/lifting_object: 140.2517
      Episode_Reward/object_height: 0.0398
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 2.49s
                      Time elapsed: 00:48:50
                               ETA: 00:44:24

################################################################################
                     [1m Learning iteration 1048/2000 [0m                     

                       Computation: 37687 steps/s (collection: 2.460s, learning 0.149s)
             Mean action noise std: 2.79
          Mean value_function loss: 314.2973
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 63.1378
                       Mean reward: 686.70
               Mean episode length: 190.77
    Episode_Reward/reaching_object: 1.3381
     Episode_Reward/lifting_object: 139.2301
      Episode_Reward/object_height: 0.0397
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 2.61s
                      Time elapsed: 00:48:52
                               ETA: 00:44:21

################################################################################
                     [1m Learning iteration 1049/2000 [0m                     

                       Computation: 39656 steps/s (collection: 2.347s, learning 0.132s)
             Mean action noise std: 2.79
          Mean value_function loss: 304.4596
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 63.1418
                       Mean reward: 696.28
               Mean episode length: 192.92
    Episode_Reward/reaching_object: 1.3619
     Episode_Reward/lifting_object: 141.2787
      Episode_Reward/object_height: 0.0401
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 2.48s
                      Time elapsed: 00:48:55
                               ETA: 00:44:18

################################################################################
                     [1m Learning iteration 1050/2000 [0m                     

                       Computation: 41462 steps/s (collection: 2.240s, learning 0.131s)
             Mean action noise std: 2.80
          Mean value_function loss: 301.6152
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 63.1563
                       Mean reward: 674.56
               Mean episode length: 189.05
    Episode_Reward/reaching_object: 1.2699
     Episode_Reward/lifting_object: 130.8745
      Episode_Reward/object_height: 0.0365
        Episode_Reward/action_rate: -0.0380
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 2.37s
                      Time elapsed: 00:48:57
                               ETA: 00:44:15

################################################################################
                     [1m Learning iteration 1051/2000 [0m                     

                       Computation: 40221 steps/s (collection: 2.298s, learning 0.146s)
             Mean action noise std: 2.80
          Mean value_function loss: 347.7255
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 63.1752
                       Mean reward: 631.00
               Mean episode length: 177.84
    Episode_Reward/reaching_object: 1.3245
     Episode_Reward/lifting_object: 137.3621
      Episode_Reward/object_height: 0.0381
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 8.8750
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 2.44s
                      Time elapsed: 00:49:00
                               ETA: 00:44:12

################################################################################
                     [1m Learning iteration 1052/2000 [0m                     

                       Computation: 42119 steps/s (collection: 2.201s, learning 0.133s)
             Mean action noise std: 2.80
          Mean value_function loss: 315.0619
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 63.1886
                       Mean reward: 701.98
               Mean episode length: 193.25
    Episode_Reward/reaching_object: 1.3309
     Episode_Reward/lifting_object: 138.1521
      Episode_Reward/object_height: 0.0383
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 2.33s
                      Time elapsed: 00:49:02
                               ETA: 00:44:09

################################################################################
                     [1m Learning iteration 1053/2000 [0m                     

                       Computation: 41714 steps/s (collection: 2.242s, learning 0.114s)
             Mean action noise std: 2.80
          Mean value_function loss: 307.5580
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 63.2003
                       Mean reward: 679.11
               Mean episode length: 188.56
    Episode_Reward/reaching_object: 1.2629
     Episode_Reward/lifting_object: 130.4252
      Episode_Reward/object_height: 0.0362
        Episode_Reward/action_rate: -0.0375
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 2.36s
                      Time elapsed: 00:49:04
                               ETA: 00:44:05

################################################################################
                     [1m Learning iteration 1054/2000 [0m                     

                       Computation: 42216 steps/s (collection: 2.198s, learning 0.131s)
             Mean action noise std: 2.80
          Mean value_function loss: 316.2611
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 63.2256
                       Mean reward: 666.17
               Mean episode length: 187.70
    Episode_Reward/reaching_object: 1.2680
     Episode_Reward/lifting_object: 129.9899
      Episode_Reward/object_height: 0.0361
        Episode_Reward/action_rate: -0.0378
          Episode_Reward/joint_vel: -0.0445
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 2.33s
                      Time elapsed: 00:49:07
                               ETA: 00:44:02

################################################################################
                     [1m Learning iteration 1055/2000 [0m                     

                       Computation: 41283 steps/s (collection: 2.257s, learning 0.125s)
             Mean action noise std: 2.81
          Mean value_function loss: 347.7372
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 63.2505
                       Mean reward: 653.92
               Mean episode length: 184.11
    Episode_Reward/reaching_object: 1.2214
     Episode_Reward/lifting_object: 125.8644
      Episode_Reward/object_height: 0.0343
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 10.2083
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 2.38s
                      Time elapsed: 00:49:09
                               ETA: 00:43:59

################################################################################
                     [1m Learning iteration 1056/2000 [0m                     

                       Computation: 40909 steps/s (collection: 2.253s, learning 0.150s)
             Mean action noise std: 2.81
          Mean value_function loss: 303.5296
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 63.2558
                       Mean reward: 682.55
               Mean episode length: 188.72
    Episode_Reward/reaching_object: 1.2965
     Episode_Reward/lifting_object: 135.0064
      Episode_Reward/object_height: 0.0370
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.6250
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 2.40s
                      Time elapsed: 00:49:11
                               ETA: 00:43:56

################################################################################
                     [1m Learning iteration 1057/2000 [0m                     

                       Computation: 41642 steps/s (collection: 2.206s, learning 0.155s)
             Mean action noise std: 2.81
          Mean value_function loss: 312.1423
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 63.2583
                       Mean reward: 765.84
               Mean episode length: 207.06
    Episode_Reward/reaching_object: 1.3504
     Episode_Reward/lifting_object: 140.9846
      Episode_Reward/object_height: 0.0380
        Episode_Reward/action_rate: -0.0399
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.7500
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 2.36s
                      Time elapsed: 00:49:14
                               ETA: 00:43:53

################################################################################
                     [1m Learning iteration 1058/2000 [0m                     

                       Computation: 40449 steps/s (collection: 2.292s, learning 0.138s)
             Mean action noise std: 2.81
          Mean value_function loss: 312.3084
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 63.2617
                       Mean reward: 595.21
               Mean episode length: 168.93
    Episode_Reward/reaching_object: 1.2838
     Episode_Reward/lifting_object: 132.6665
      Episode_Reward/object_height: 0.0356
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 2.43s
                      Time elapsed: 00:49:16
                               ETA: 00:43:50

################################################################################
                     [1m Learning iteration 1059/2000 [0m                     

                       Computation: 40544 steps/s (collection: 2.290s, learning 0.135s)
             Mean action noise std: 2.81
          Mean value_function loss: 312.1537
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 63.2682
                       Mean reward: 690.01
               Mean episode length: 190.82
    Episode_Reward/reaching_object: 1.2487
     Episode_Reward/lifting_object: 129.7010
      Episode_Reward/object_height: 0.0344
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 2.42s
                      Time elapsed: 00:49:19
                               ETA: 00:43:46

################################################################################
                     [1m Learning iteration 1060/2000 [0m                     

                       Computation: 41900 steps/s (collection: 2.204s, learning 0.143s)
             Mean action noise std: 2.81
          Mean value_function loss: 309.3790
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 63.2758
                       Mean reward: 760.20
               Mean episode length: 206.66
    Episode_Reward/reaching_object: 1.3343
     Episode_Reward/lifting_object: 139.5099
      Episode_Reward/object_height: 0.0376
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0456
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 2.35s
                      Time elapsed: 00:49:21
                               ETA: 00:43:43

################################################################################
                     [1m Learning iteration 1061/2000 [0m                     

                       Computation: 40250 steps/s (collection: 2.305s, learning 0.138s)
             Mean action noise std: 2.81
          Mean value_function loss: 308.1017
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 63.2778
                       Mean reward: 664.38
               Mean episode length: 185.06
    Episode_Reward/reaching_object: 1.2910
     Episode_Reward/lifting_object: 134.3791
      Episode_Reward/object_height: 0.0363
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 2.44s
                      Time elapsed: 00:49:23
                               ETA: 00:43:40

################################################################################
                     [1m Learning iteration 1062/2000 [0m                     

                       Computation: 40127 steps/s (collection: 2.302s, learning 0.148s)
             Mean action noise std: 2.81
          Mean value_function loss: 323.0169
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 63.2797
                       Mean reward: 668.88
               Mean episode length: 184.29
    Episode_Reward/reaching_object: 1.3179
     Episode_Reward/lifting_object: 137.1139
      Episode_Reward/object_height: 0.0365
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.5417
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 2.45s
                      Time elapsed: 00:49:26
                               ETA: 00:43:37

################################################################################
                     [1m Learning iteration 1063/2000 [0m                     

                       Computation: 41707 steps/s (collection: 2.218s, learning 0.139s)
             Mean action noise std: 2.81
          Mean value_function loss: 321.6227
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.2886
                       Mean reward: 608.05
               Mean episode length: 172.27
    Episode_Reward/reaching_object: 1.2398
     Episode_Reward/lifting_object: 128.7679
      Episode_Reward/object_height: 0.0341
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 10.2500
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 2.36s
                      Time elapsed: 00:49:28
                               ETA: 00:43:34

################################################################################
                     [1m Learning iteration 1064/2000 [0m                     

                       Computation: 40684 steps/s (collection: 2.266s, learning 0.150s)
             Mean action noise std: 2.81
          Mean value_function loss: 310.3865
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 63.3086
                       Mean reward: 701.34
               Mean episode length: 193.03
    Episode_Reward/reaching_object: 1.2998
     Episode_Reward/lifting_object: 136.0610
      Episode_Reward/object_height: 0.0364
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 12.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 2.42s
                      Time elapsed: 00:49:31
                               ETA: 00:43:31

################################################################################
                     [1m Learning iteration 1065/2000 [0m                     

                       Computation: 40868 steps/s (collection: 2.268s, learning 0.137s)
             Mean action noise std: 2.82
          Mean value_function loss: 339.1570
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 63.3311
                       Mean reward: 710.72
               Mean episode length: 196.20
    Episode_Reward/reaching_object: 1.3269
     Episode_Reward/lifting_object: 138.1213
      Episode_Reward/object_height: 0.0373
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.6667
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 2.41s
                      Time elapsed: 00:49:33
                               ETA: 00:43:28

################################################################################
                     [1m Learning iteration 1066/2000 [0m                     

                       Computation: 41253 steps/s (collection: 2.253s, learning 0.130s)
             Mean action noise std: 2.82
          Mean value_function loss: 286.0416
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 63.3444
                       Mean reward: 734.32
               Mean episode length: 201.36
    Episode_Reward/reaching_object: 1.3315
     Episode_Reward/lifting_object: 138.9888
      Episode_Reward/object_height: 0.0381
        Episode_Reward/action_rate: -0.0395
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 8.0833
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 2.38s
                      Time elapsed: 00:49:35
                               ETA: 00:43:25

################################################################################
                     [1m Learning iteration 1067/2000 [0m                     

                       Computation: 41673 steps/s (collection: 2.194s, learning 0.165s)
             Mean action noise std: 2.82
          Mean value_function loss: 309.1114
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 63.3532
                       Mean reward: 617.78
               Mean episode length: 175.20
    Episode_Reward/reaching_object: 1.2876
     Episode_Reward/lifting_object: 134.0847
      Episode_Reward/object_height: 0.0369
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0441
      Episode_Termination/time_out: 12.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 2.36s
                      Time elapsed: 00:49:38
                               ETA: 00:43:21

################################################################################
                     [1m Learning iteration 1068/2000 [0m                     

                       Computation: 40532 steps/s (collection: 2.286s, learning 0.139s)
             Mean action noise std: 2.82
          Mean value_function loss: 292.2341
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 63.3672
                       Mean reward: 716.26
               Mean episode length: 196.45
    Episode_Reward/reaching_object: 1.3059
     Episode_Reward/lifting_object: 136.5064
      Episode_Reward/object_height: 0.0370
        Episode_Reward/action_rate: -0.0389
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 2.43s
                      Time elapsed: 00:49:40
                               ETA: 00:43:18

################################################################################
                     [1m Learning iteration 1069/2000 [0m                     

                       Computation: 40580 steps/s (collection: 2.273s, learning 0.149s)
             Mean action noise std: 2.82
          Mean value_function loss: 272.3293
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 63.3719
                       Mean reward: 767.59
               Mean episode length: 208.72
    Episode_Reward/reaching_object: 1.3300
     Episode_Reward/lifting_object: 138.4661
      Episode_Reward/object_height: 0.0385
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 2.42s
                      Time elapsed: 00:49:43
                               ETA: 00:43:15

################################################################################
                     [1m Learning iteration 1070/2000 [0m                     

                       Computation: 41429 steps/s (collection: 2.240s, learning 0.133s)
             Mean action noise std: 2.82
          Mean value_function loss: 272.8946
               Mean surrogate loss: 0.0120
                 Mean entropy loss: 63.3739
                       Mean reward: 752.77
               Mean episode length: 205.85
    Episode_Reward/reaching_object: 1.3383
     Episode_Reward/lifting_object: 140.5029
      Episode_Reward/object_height: 0.0391
        Episode_Reward/action_rate: -0.0396
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.6667
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 2.37s
                      Time elapsed: 00:49:45
                               ETA: 00:43:12

################################################################################
                     [1m Learning iteration 1071/2000 [0m                     

                       Computation: 41628 steps/s (collection: 2.234s, learning 0.127s)
             Mean action noise std: 2.82
          Mean value_function loss: 260.5048
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 63.3748
                       Mean reward: 706.33
               Mean episode length: 194.15
    Episode_Reward/reaching_object: 1.3803
     Episode_Reward/lifting_object: 145.0471
      Episode_Reward/object_height: 0.0401
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0461
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 2.36s
                      Time elapsed: 00:49:47
                               ETA: 00:43:09

################################################################################
                     [1m Learning iteration 1072/2000 [0m                     

                       Computation: 39698 steps/s (collection: 2.331s, learning 0.146s)
             Mean action noise std: 2.82
          Mean value_function loss: 290.0817
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 63.3763
                       Mean reward: 755.77
               Mean episode length: 206.26
    Episode_Reward/reaching_object: 1.3987
     Episode_Reward/lifting_object: 147.1584
      Episode_Reward/object_height: 0.0406
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0459
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 2.48s
                      Time elapsed: 00:49:50
                               ETA: 00:43:06

################################################################################
                     [1m Learning iteration 1073/2000 [0m                     

                       Computation: 41077 steps/s (collection: 2.256s, learning 0.137s)
             Mean action noise std: 2.82
          Mean value_function loss: 277.5023
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 63.3855
                       Mean reward: 759.26
               Mean episode length: 206.27
    Episode_Reward/reaching_object: 1.4228
     Episode_Reward/lifting_object: 149.8782
      Episode_Reward/object_height: 0.0416
        Episode_Reward/action_rate: -0.0418
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 2.39s
                      Time elapsed: 00:49:52
                               ETA: 00:43:03

################################################################################
                     [1m Learning iteration 1074/2000 [0m                     

                       Computation: 41997 steps/s (collection: 2.203s, learning 0.138s)
             Mean action noise std: 2.82
          Mean value_function loss: 294.4833
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 63.4043
                       Mean reward: 802.67
               Mean episode length: 216.94
    Episode_Reward/reaching_object: 1.4616
     Episode_Reward/lifting_object: 154.3644
      Episode_Reward/object_height: 0.0431
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 2.34s
                      Time elapsed: 00:49:55
                               ETA: 00:42:59

################################################################################
                     [1m Learning iteration 1075/2000 [0m                     

                       Computation: 40850 steps/s (collection: 2.264s, learning 0.142s)
             Mean action noise std: 2.82
          Mean value_function loss: 293.4819
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 63.4104
                       Mean reward: 745.87
               Mean episode length: 202.46
    Episode_Reward/reaching_object: 1.4409
     Episode_Reward/lifting_object: 152.0753
      Episode_Reward/object_height: 0.0421
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 2.41s
                      Time elapsed: 00:49:57
                               ETA: 00:42:56

################################################################################
                     [1m Learning iteration 1076/2000 [0m                     

                       Computation: 42473 steps/s (collection: 2.187s, learning 0.128s)
             Mean action noise std: 2.82
          Mean value_function loss: 298.0446
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 63.4185
                       Mean reward: 732.42
               Mean episode length: 199.11
    Episode_Reward/reaching_object: 1.4423
     Episode_Reward/lifting_object: 152.5142
      Episode_Reward/object_height: 0.0419
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0478
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.8750
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 2.31s
                      Time elapsed: 00:49:59
                               ETA: 00:42:53

################################################################################
                     [1m Learning iteration 1077/2000 [0m                     

                       Computation: 42437 steps/s (collection: 2.215s, learning 0.102s)
             Mean action noise std: 2.83
          Mean value_function loss: 297.9237
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 63.4266
                       Mean reward: 734.61
               Mean episode length: 198.99
    Episode_Reward/reaching_object: 1.4188
     Episode_Reward/lifting_object: 149.5777
      Episode_Reward/object_height: 0.0413
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 2.32s
                      Time elapsed: 00:50:02
                               ETA: 00:42:50

################################################################################
                     [1m Learning iteration 1078/2000 [0m                     

                       Computation: 38915 steps/s (collection: 2.374s, learning 0.153s)
             Mean action noise std: 2.83
          Mean value_function loss: 298.4980
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 63.4438
                       Mean reward: 736.27
               Mean episode length: 200.82
    Episode_Reward/reaching_object: 1.4156
     Episode_Reward/lifting_object: 149.6959
      Episode_Reward/object_height: 0.0414
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.2917
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 2.53s
                      Time elapsed: 00:50:04
                               ETA: 00:42:47

################################################################################
                     [1m Learning iteration 1079/2000 [0m                     

                       Computation: 41513 steps/s (collection: 2.236s, learning 0.132s)
             Mean action noise std: 2.83
          Mean value_function loss: 298.7401
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 63.4703
                       Mean reward: 726.39
               Mean episode length: 199.75
    Episode_Reward/reaching_object: 1.3794
     Episode_Reward/lifting_object: 144.9548
      Episode_Reward/object_height: 0.0392
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.3750
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 2.37s
                      Time elapsed: 00:50:07
                               ETA: 00:42:44

################################################################################
                     [1m Learning iteration 1080/2000 [0m                     

                       Computation: 42592 steps/s (collection: 2.182s, learning 0.126s)
             Mean action noise std: 2.83
          Mean value_function loss: 270.7888
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 63.4787
                       Mean reward: 706.01
               Mean episode length: 192.74
    Episode_Reward/reaching_object: 1.4127
     Episode_Reward/lifting_object: 148.8441
      Episode_Reward/object_height: 0.0408
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 10.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 2.31s
                      Time elapsed: 00:50:09
                               ETA: 00:42:41

################################################################################
                     [1m Learning iteration 1081/2000 [0m                     

                       Computation: 40822 steps/s (collection: 2.271s, learning 0.138s)
             Mean action noise std: 2.83
          Mean value_function loss: 289.5353
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 63.4939
                       Mean reward: 742.46
               Mean episode length: 203.61
    Episode_Reward/reaching_object: 1.4632
     Episode_Reward/lifting_object: 154.3124
      Episode_Reward/object_height: 0.0419
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0490
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 2.41s
                      Time elapsed: 00:50:11
                               ETA: 00:42:38

################################################################################
                     [1m Learning iteration 1082/2000 [0m                     

                       Computation: 41944 steps/s (collection: 2.215s, learning 0.129s)
             Mean action noise std: 2.83
          Mean value_function loss: 304.8495
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 63.5040
                       Mean reward: 697.34
               Mean episode length: 193.40
    Episode_Reward/reaching_object: 1.3850
     Episode_Reward/lifting_object: 145.3131
      Episode_Reward/object_height: 0.0390
        Episode_Reward/action_rate: -0.0408
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 2.34s
                      Time elapsed: 00:50:14
                               ETA: 00:42:34

################################################################################
                     [1m Learning iteration 1083/2000 [0m                     

                       Computation: 40445 steps/s (collection: 2.299s, learning 0.131s)
             Mean action noise std: 2.83
          Mean value_function loss: 290.0592
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 63.5064
                       Mean reward: 745.49
               Mean episode length: 204.16
    Episode_Reward/reaching_object: 1.4239
     Episode_Reward/lifting_object: 149.3784
      Episode_Reward/object_height: 0.0400
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.3333
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 2.43s
                      Time elapsed: 00:50:16
                               ETA: 00:42:31

################################################################################
                     [1m Learning iteration 1084/2000 [0m                     

                       Computation: 42232 steps/s (collection: 2.201s, learning 0.127s)
             Mean action noise std: 2.83
          Mean value_function loss: 307.3588
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 63.5084
                       Mean reward: 746.80
               Mean episode length: 203.40
    Episode_Reward/reaching_object: 1.4254
     Episode_Reward/lifting_object: 150.0270
      Episode_Reward/object_height: 0.0403
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 2.33s
                      Time elapsed: 00:50:18
                               ETA: 00:42:28

################################################################################
                     [1m Learning iteration 1085/2000 [0m                     

                       Computation: 42909 steps/s (collection: 2.166s, learning 0.125s)
             Mean action noise std: 2.84
          Mean value_function loss: 319.4413
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 63.5201
                       Mean reward: 713.80
               Mean episode length: 199.28
    Episode_Reward/reaching_object: 1.4256
     Episode_Reward/lifting_object: 149.3418
      Episode_Reward/object_height: 0.0397
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 8.5000
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 2.29s
                      Time elapsed: 00:50:21
                               ETA: 00:42:25

################################################################################
                     [1m Learning iteration 1086/2000 [0m                     

                       Computation: 41929 steps/s (collection: 2.195s, learning 0.150s)
             Mean action noise std: 2.84
          Mean value_function loss: 330.4820
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 63.5470
                       Mean reward: 769.18
               Mean episode length: 208.90
    Episode_Reward/reaching_object: 1.4054
     Episode_Reward/lifting_object: 148.1222
      Episode_Reward/object_height: 0.0391
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 2.34s
                      Time elapsed: 00:50:23
                               ETA: 00:42:22

################################################################################
                     [1m Learning iteration 1087/2000 [0m                     

                       Computation: 42361 steps/s (collection: 2.193s, learning 0.128s)
             Mean action noise std: 2.84
          Mean value_function loss: 322.8788
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 63.5731
                       Mean reward: 775.28
               Mean episode length: 210.77
    Episode_Reward/reaching_object: 1.4334
     Episode_Reward/lifting_object: 151.2049
      Episode_Reward/object_height: 0.0403
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.7083
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 2.32s
                      Time elapsed: 00:50:25
                               ETA: 00:42:19

################################################################################
                     [1m Learning iteration 1088/2000 [0m                     

                       Computation: 42476 steps/s (collection: 2.178s, learning 0.137s)
             Mean action noise std: 2.84
          Mean value_function loss: 356.2406
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 63.5881
                       Mean reward: 683.27
               Mean episode length: 186.33
    Episode_Reward/reaching_object: 1.3670
     Episode_Reward/lifting_object: 143.7218
      Episode_Reward/object_height: 0.0376
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 2.31s
                      Time elapsed: 00:50:28
                               ETA: 00:42:15

################################################################################
                     [1m Learning iteration 1089/2000 [0m                     

                       Computation: 42608 steps/s (collection: 2.171s, learning 0.136s)
             Mean action noise std: 2.85
          Mean value_function loss: 340.5839
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 63.5968
                       Mean reward: 722.84
               Mean episode length: 197.98
    Episode_Reward/reaching_object: 1.3750
     Episode_Reward/lifting_object: 144.9003
      Episode_Reward/object_height: 0.0382
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.1667
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 2.31s
                      Time elapsed: 00:50:30
                               ETA: 00:42:12

################################################################################
                     [1m Learning iteration 1090/2000 [0m                     

                       Computation: 42714 steps/s (collection: 2.162s, learning 0.139s)
             Mean action noise std: 2.85
          Mean value_function loss: 322.7414
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 63.6072
                       Mean reward: 789.67
               Mean episode length: 213.70
    Episode_Reward/reaching_object: 1.4151
     Episode_Reward/lifting_object: 149.1580
      Episode_Reward/object_height: 0.0392
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 7.4583
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 2.30s
                      Time elapsed: 00:50:32
                               ETA: 00:42:09

################################################################################
                     [1m Learning iteration 1091/2000 [0m                     

                       Computation: 43297 steps/s (collection: 2.161s, learning 0.110s)
             Mean action noise std: 2.85
          Mean value_function loss: 343.5416
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 63.6100
                       Mean reward: 711.05
               Mean episode length: 195.65
    Episode_Reward/reaching_object: 1.3375
     Episode_Reward/lifting_object: 140.0250
      Episode_Reward/object_height: 0.0361
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 9.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 9.0833
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 2.27s
                      Time elapsed: 00:50:35
                               ETA: 00:42:06

################################################################################
                     [1m Learning iteration 1092/2000 [0m                     

                       Computation: 41603 steps/s (collection: 2.219s, learning 0.144s)
             Mean action noise std: 2.85
          Mean value_function loss: 288.5510
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 63.6122
                       Mean reward: 725.16
               Mean episode length: 197.33
    Episode_Reward/reaching_object: 1.4242
     Episode_Reward/lifting_object: 150.2904
      Episode_Reward/object_height: 0.0391
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 10.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5000
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 2.36s
                      Time elapsed: 00:50:37
                               ETA: 00:42:03

################################################################################
                     [1m Learning iteration 1093/2000 [0m                     

                       Computation: 41050 steps/s (collection: 2.265s, learning 0.129s)
             Mean action noise std: 2.85
          Mean value_function loss: 278.8735
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 63.6135
                       Mean reward: 771.51
               Mean episode length: 209.51
    Episode_Reward/reaching_object: 1.4373
     Episode_Reward/lifting_object: 151.3420
      Episode_Reward/object_height: 0.0395
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 2.39s
                      Time elapsed: 00:50:39
                               ETA: 00:42:00

################################################################################
                     [1m Learning iteration 1094/2000 [0m                     

                       Computation: 41506 steps/s (collection: 2.250s, learning 0.119s)
             Mean action noise std: 2.85
          Mean value_function loss: 262.8228
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 63.6151
                       Mean reward: 748.27
               Mean episode length: 206.27
    Episode_Reward/reaching_object: 1.4543
     Episode_Reward/lifting_object: 153.2866
      Episode_Reward/object_height: 0.0398
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 12.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 2.37s
                      Time elapsed: 00:50:42
                               ETA: 00:41:57

################################################################################
                     [1m Learning iteration 1095/2000 [0m                     

                       Computation: 42185 steps/s (collection: 2.186s, learning 0.144s)
             Mean action noise std: 2.85
          Mean value_function loss: 264.7816
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 63.6176
                       Mean reward: 810.82
               Mean episode length: 219.32
    Episode_Reward/reaching_object: 1.5179
     Episode_Reward/lifting_object: 160.6931
      Episode_Reward/object_height: 0.0421
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 2.33s
                      Time elapsed: 00:50:44
                               ETA: 00:41:53

################################################################################
                     [1m Learning iteration 1096/2000 [0m                     

                       Computation: 42940 steps/s (collection: 2.151s, learning 0.139s)
             Mean action noise std: 2.85
          Mean value_function loss: 259.9973
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 63.6233
                       Mean reward: 812.11
               Mean episode length: 220.17
    Episode_Reward/reaching_object: 1.4379
     Episode_Reward/lifting_object: 151.1043
      Episode_Reward/object_height: 0.0389
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0494
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 2.29s
                      Time elapsed: 00:50:46
                               ETA: 00:41:50

################################################################################
                     [1m Learning iteration 1097/2000 [0m                     

                       Computation: 42468 steps/s (collection: 2.180s, learning 0.135s)
             Mean action noise std: 2.85
          Mean value_function loss: 286.7493
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 63.6293
                       Mean reward: 747.99
               Mean episode length: 206.74
    Episode_Reward/reaching_object: 1.3978
     Episode_Reward/lifting_object: 146.4168
      Episode_Reward/object_height: 0.0372
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 2.31s
                      Time elapsed: 00:50:49
                               ETA: 00:41:47

################################################################################
                     [1m Learning iteration 1098/2000 [0m                     

                       Computation: 43237 steps/s (collection: 2.170s, learning 0.104s)
             Mean action noise std: 2.85
          Mean value_function loss: 271.2446
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 63.6310
                       Mean reward: 796.94
               Mean episode length: 215.39
    Episode_Reward/reaching_object: 1.5034
     Episode_Reward/lifting_object: 158.3384
      Episode_Reward/object_height: 0.0403
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 2.27s
                      Time elapsed: 00:50:51
                               ETA: 00:41:44

################################################################################
                     [1m Learning iteration 1099/2000 [0m                     

                       Computation: 42264 steps/s (collection: 2.192s, learning 0.134s)
             Mean action noise std: 2.85
          Mean value_function loss: 273.9304
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 63.6327
                       Mean reward: 787.81
               Mean episode length: 213.80
    Episode_Reward/reaching_object: 1.4426
     Episode_Reward/lifting_object: 150.7307
      Episode_Reward/object_height: 0.0383
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 2.33s
                      Time elapsed: 00:50:53
                               ETA: 00:41:41

################################################################################
                     [1m Learning iteration 1100/2000 [0m                     

                       Computation: 42614 steps/s (collection: 2.185s, learning 0.122s)
             Mean action noise std: 2.85
          Mean value_function loss: 265.6339
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 63.6408
                       Mean reward: 747.72
               Mean episode length: 203.52
    Episode_Reward/reaching_object: 1.4101
     Episode_Reward/lifting_object: 147.2206
      Episode_Reward/object_height: 0.0369
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.7917
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 2.31s
                      Time elapsed: 00:50:55
                               ETA: 00:41:38

################################################################################
                     [1m Learning iteration 1101/2000 [0m                     

                       Computation: 41167 steps/s (collection: 2.258s, learning 0.130s)
             Mean action noise std: 2.85
          Mean value_function loss: 242.0485
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 63.6665
                       Mean reward: 781.83
               Mean episode length: 210.95
    Episode_Reward/reaching_object: 1.4496
     Episode_Reward/lifting_object: 151.2335
      Episode_Reward/object_height: 0.0378
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 2.39s
                      Time elapsed: 00:50:58
                               ETA: 00:41:34

################################################################################
                     [1m Learning iteration 1102/2000 [0m                     

                       Computation: 42558 steps/s (collection: 2.184s, learning 0.126s)
             Mean action noise std: 2.86
          Mean value_function loss: 248.7369
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 63.6859
                       Mean reward: 781.64
               Mean episode length: 210.47
    Episode_Reward/reaching_object: 1.4524
     Episode_Reward/lifting_object: 152.2516
      Episode_Reward/object_height: 0.0376
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 2.31s
                      Time elapsed: 00:51:00
                               ETA: 00:41:31

################################################################################
                     [1m Learning iteration 1103/2000 [0m                     

                       Computation: 42623 steps/s (collection: 2.180s, learning 0.127s)
             Mean action noise std: 2.86
          Mean value_function loss: 225.1025
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 63.7076
                       Mean reward: 853.90
               Mean episode length: 230.00
    Episode_Reward/reaching_object: 1.5166
     Episode_Reward/lifting_object: 159.1335
      Episode_Reward/object_height: 0.0394
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 2.31s
                      Time elapsed: 00:51:02
                               ETA: 00:41:28

################################################################################
                     [1m Learning iteration 1104/2000 [0m                     

                       Computation: 40635 steps/s (collection: 2.290s, learning 0.129s)
             Mean action noise std: 2.86
          Mean value_function loss: 250.1025
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 63.7225
                       Mean reward: 767.28
               Mean episode length: 208.84
    Episode_Reward/reaching_object: 1.4764
     Episode_Reward/lifting_object: 154.6261
      Episode_Reward/object_height: 0.0378
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 2.42s
                      Time elapsed: 00:51:05
                               ETA: 00:41:25

################################################################################
                     [1m Learning iteration 1105/2000 [0m                     

                       Computation: 41628 steps/s (collection: 2.228s, learning 0.134s)
             Mean action noise std: 2.86
          Mean value_function loss: 255.1867
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 63.7297
                       Mean reward: 774.62
               Mean episode length: 208.22
    Episode_Reward/reaching_object: 1.4540
     Episode_Reward/lifting_object: 152.1196
      Episode_Reward/object_height: 0.0372
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0507
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 2.36s
                      Time elapsed: 00:51:07
                               ETA: 00:41:22

################################################################################
                     [1m Learning iteration 1106/2000 [0m                     

                       Computation: 41332 steps/s (collection: 2.246s, learning 0.132s)
             Mean action noise std: 2.86
          Mean value_function loss: 285.5352
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 63.7382
                       Mean reward: 796.77
               Mean episode length: 212.60
    Episode_Reward/reaching_object: 1.4568
     Episode_Reward/lifting_object: 152.3576
      Episode_Reward/object_height: 0.0371
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.5833
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 2.38s
                      Time elapsed: 00:51:10
                               ETA: 00:41:19

################################################################################
                     [1m Learning iteration 1107/2000 [0m                     

                       Computation: 40390 steps/s (collection: 2.265s, learning 0.169s)
             Mean action noise std: 2.86
          Mean value_function loss: 269.9249
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 63.7553
                       Mean reward: 788.14
               Mean episode length: 214.13
    Episode_Reward/reaching_object: 1.4685
     Episode_Reward/lifting_object: 153.6866
      Episode_Reward/object_height: 0.0376
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.1667
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 2.43s
                      Time elapsed: 00:51:12
                               ETA: 00:41:16

################################################################################
                     [1m Learning iteration 1108/2000 [0m                     

                       Computation: 40603 steps/s (collection: 2.281s, learning 0.140s)
             Mean action noise std: 2.87
          Mean value_function loss: 275.9625
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 63.7867
                       Mean reward: 796.87
               Mean episode length: 215.03
    Episode_Reward/reaching_object: 1.4968
     Episode_Reward/lifting_object: 157.2274
      Episode_Reward/object_height: 0.0382
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.4167
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 2.42s
                      Time elapsed: 00:51:15
                               ETA: 00:41:13

################################################################################
                     [1m Learning iteration 1109/2000 [0m                     

                       Computation: 39661 steps/s (collection: 2.340s, learning 0.138s)
             Mean action noise std: 2.87
          Mean value_function loss: 259.6047
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 63.8074
                       Mean reward: 841.00
               Mean episode length: 225.28
    Episode_Reward/reaching_object: 1.5077
     Episode_Reward/lifting_object: 158.3255
      Episode_Reward/object_height: 0.0385
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 2.48s
                      Time elapsed: 00:51:17
                               ETA: 00:41:10

################################################################################
                     [1m Learning iteration 1110/2000 [0m                     

                       Computation: 40125 steps/s (collection: 2.288s, learning 0.162s)
             Mean action noise std: 2.87
          Mean value_function loss: 246.6506
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 63.8190
                       Mean reward: 768.31
               Mean episode length: 207.04
    Episode_Reward/reaching_object: 1.4891
     Episode_Reward/lifting_object: 156.4887
      Episode_Reward/object_height: 0.0380
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 2.45s
                      Time elapsed: 00:51:19
                               ETA: 00:41:07

################################################################################
                     [1m Learning iteration 1111/2000 [0m                     

                       Computation: 40969 steps/s (collection: 2.262s, learning 0.138s)
             Mean action noise std: 2.87
          Mean value_function loss: 248.4480
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 63.8298
                       Mean reward: 791.93
               Mean episode length: 213.33
    Episode_Reward/reaching_object: 1.4742
     Episode_Reward/lifting_object: 154.5294
      Episode_Reward/object_height: 0.0373
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 2.40s
                      Time elapsed: 00:51:22
                               ETA: 00:41:04

################################################################################
                     [1m Learning iteration 1112/2000 [0m                     

                       Computation: 40069 steps/s (collection: 2.298s, learning 0.156s)
             Mean action noise std: 2.87
          Mean value_function loss: 273.4083
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 63.8324
                       Mean reward: 804.44
               Mean episode length: 217.74
    Episode_Reward/reaching_object: 1.4606
     Episode_Reward/lifting_object: 152.6420
      Episode_Reward/object_height: 0.0370
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 6.0417
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 2.45s
                      Time elapsed: 00:51:24
                               ETA: 00:41:01

################################################################################
                     [1m Learning iteration 1113/2000 [0m                     

                       Computation: 40715 steps/s (collection: 2.275s, learning 0.139s)
             Mean action noise std: 2.87
          Mean value_function loss: 246.8142
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 63.8339
                       Mean reward: 791.53
               Mean episode length: 213.79
    Episode_Reward/reaching_object: 1.4284
     Episode_Reward/lifting_object: 149.1821
      Episode_Reward/object_height: 0.0360
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.9167
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 2.41s
                      Time elapsed: 00:51:27
                               ETA: 00:40:58

################################################################################
                     [1m Learning iteration 1114/2000 [0m                     

                       Computation: 39002 steps/s (collection: 2.373s, learning 0.148s)
             Mean action noise std: 2.87
          Mean value_function loss: 246.5651
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 63.8357
                       Mean reward: 744.63
               Mean episode length: 205.04
    Episode_Reward/reaching_object: 1.4773
     Episode_Reward/lifting_object: 154.0139
      Episode_Reward/object_height: 0.0369
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 2.52s
                      Time elapsed: 00:51:29
                               ETA: 00:40:55

################################################################################
                     [1m Learning iteration 1115/2000 [0m                     

                       Computation: 40989 steps/s (collection: 2.256s, learning 0.142s)
             Mean action noise std: 2.87
          Mean value_function loss: 233.6364
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 63.8375
                       Mean reward: 769.99
               Mean episode length: 208.27
    Episode_Reward/reaching_object: 1.4881
     Episode_Reward/lifting_object: 155.2657
      Episode_Reward/object_height: 0.0378
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 2.40s
                      Time elapsed: 00:51:32
                               ETA: 00:40:52

################################################################################
                     [1m Learning iteration 1116/2000 [0m                     

                       Computation: 42281 steps/s (collection: 2.198s, learning 0.127s)
             Mean action noise std: 2.87
          Mean value_function loss: 237.7802
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 63.8435
                       Mean reward: 773.59
               Mean episode length: 209.26
    Episode_Reward/reaching_object: 1.4847
     Episode_Reward/lifting_object: 155.1415
      Episode_Reward/object_height: 0.0379
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.9583
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 2.32s
                      Time elapsed: 00:51:34
                               ETA: 00:40:48

################################################################################
                     [1m Learning iteration 1117/2000 [0m                     

                       Computation: 40442 steps/s (collection: 2.303s, learning 0.128s)
             Mean action noise std: 2.88
          Mean value_function loss: 236.7924
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 63.8625
                       Mean reward: 832.56
               Mean episode length: 223.85
    Episode_Reward/reaching_object: 1.4859
     Episode_Reward/lifting_object: 154.7171
      Episode_Reward/object_height: 0.0373
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 2.43s
                      Time elapsed: 00:51:36
                               ETA: 00:40:45

################################################################################
                     [1m Learning iteration 1118/2000 [0m                     

                       Computation: 41497 steps/s (collection: 2.233s, learning 0.136s)
             Mean action noise std: 2.88
          Mean value_function loss: 238.7935
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 63.8760
                       Mean reward: 834.25
               Mean episode length: 223.40
    Episode_Reward/reaching_object: 1.5140
     Episode_Reward/lifting_object: 157.6148
      Episode_Reward/object_height: 0.0380
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 2.37s
                      Time elapsed: 00:51:39
                               ETA: 00:40:42

################################################################################
                     [1m Learning iteration 1119/2000 [0m                     

                       Computation: 40583 steps/s (collection: 2.298s, learning 0.125s)
             Mean action noise std: 2.88
          Mean value_function loss: 246.3061
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 63.8913
                       Mean reward: 826.08
               Mean episode length: 222.12
    Episode_Reward/reaching_object: 1.5103
     Episode_Reward/lifting_object: 157.5893
      Episode_Reward/object_height: 0.0374
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 2.42s
                      Time elapsed: 00:51:41
                               ETA: 00:40:39

################################################################################
                     [1m Learning iteration 1120/2000 [0m                     

                       Computation: 40532 steps/s (collection: 2.296s, learning 0.129s)
             Mean action noise std: 2.88
          Mean value_function loss: 227.8271
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 63.9117
                       Mean reward: 814.66
               Mean episode length: 219.42
    Episode_Reward/reaching_object: 1.5105
     Episode_Reward/lifting_object: 157.3330
      Episode_Reward/object_height: 0.0371
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 2.43s
                      Time elapsed: 00:51:44
                               ETA: 00:40:36

################################################################################
                     [1m Learning iteration 1121/2000 [0m                     

                       Computation: 41808 steps/s (collection: 2.233s, learning 0.118s)
             Mean action noise std: 2.89
          Mean value_function loss: 238.6247
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 63.9489
                       Mean reward: 747.98
               Mean episode length: 203.88
    Episode_Reward/reaching_object: 1.4436
     Episode_Reward/lifting_object: 149.2878
      Episode_Reward/object_height: 0.0347
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 2.35s
                      Time elapsed: 00:51:46
                               ETA: 00:40:33

################################################################################
                     [1m Learning iteration 1122/2000 [0m                     

                       Computation: 40094 steps/s (collection: 2.318s, learning 0.134s)
             Mean action noise std: 2.89
          Mean value_function loss: 221.5562
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 63.9724
                       Mean reward: 850.90
               Mean episode length: 228.24
    Episode_Reward/reaching_object: 1.5144
     Episode_Reward/lifting_object: 157.7260
      Episode_Reward/object_height: 0.0368
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 2.45s
                      Time elapsed: 00:51:48
                               ETA: 00:40:30

################################################################################
                     [1m Learning iteration 1123/2000 [0m                     

                       Computation: 40350 steps/s (collection: 2.283s, learning 0.153s)
             Mean action noise std: 2.89
          Mean value_function loss: 211.4907
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 63.9804
                       Mean reward: 770.67
               Mean episode length: 210.07
    Episode_Reward/reaching_object: 1.5436
     Episode_Reward/lifting_object: 160.8955
      Episode_Reward/object_height: 0.0374
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 2.44s
                      Time elapsed: 00:51:51
                               ETA: 00:40:27

################################################################################
                     [1m Learning iteration 1124/2000 [0m                     

                       Computation: 40138 steps/s (collection: 2.309s, learning 0.141s)
             Mean action noise std: 2.89
          Mean value_function loss: 214.0292
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 63.9965
                       Mean reward: 819.17
               Mean episode length: 220.79
    Episode_Reward/reaching_object: 1.5449
     Episode_Reward/lifting_object: 160.9388
      Episode_Reward/object_height: 0.0377
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 2.45s
                      Time elapsed: 00:51:53
                               ETA: 00:40:24

################################################################################
                     [1m Learning iteration 1125/2000 [0m                     

                       Computation: 40719 steps/s (collection: 2.287s, learning 0.128s)
             Mean action noise std: 2.89
          Mean value_function loss: 246.4786
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 64.0154
                       Mean reward: 861.04
               Mean episode length: 232.00
    Episode_Reward/reaching_object: 1.5303
     Episode_Reward/lifting_object: 159.6696
      Episode_Reward/object_height: 0.0373
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 2.41s
                      Time elapsed: 00:51:56
                               ETA: 00:40:21

################################################################################
                     [1m Learning iteration 1126/2000 [0m                     

                       Computation: 40145 steps/s (collection: 2.326s, learning 0.123s)
             Mean action noise std: 2.90
          Mean value_function loss: 235.3709
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 64.0428
                       Mean reward: 823.70
               Mean episode length: 220.81
    Episode_Reward/reaching_object: 1.5213
     Episode_Reward/lifting_object: 158.8831
      Episode_Reward/object_height: 0.0371
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 2.45s
                      Time elapsed: 00:51:58
                               ETA: 00:40:18

################################################################################
                     [1m Learning iteration 1127/2000 [0m                     

                       Computation: 39786 steps/s (collection: 2.333s, learning 0.137s)
             Mean action noise std: 2.90
          Mean value_function loss: 234.6736
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 64.0585
                       Mean reward: 772.78
               Mean episode length: 209.38
    Episode_Reward/reaching_object: 1.4986
     Episode_Reward/lifting_object: 156.4887
      Episode_Reward/object_height: 0.0365
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 2.47s
                      Time elapsed: 00:52:01
                               ETA: 00:40:15

################################################################################
                     [1m Learning iteration 1128/2000 [0m                     

                       Computation: 39578 steps/s (collection: 2.370s, learning 0.114s)
             Mean action noise std: 2.90
          Mean value_function loss: 245.5412
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 64.0665
                       Mean reward: 818.76
               Mean episode length: 218.84
    Episode_Reward/reaching_object: 1.5238
     Episode_Reward/lifting_object: 159.6991
      Episode_Reward/object_height: 0.0376
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 2.48s
                      Time elapsed: 00:52:03
                               ETA: 00:40:12

################################################################################
                     [1m Learning iteration 1129/2000 [0m                     

                       Computation: 41142 steps/s (collection: 2.281s, learning 0.108s)
             Mean action noise std: 2.90
          Mean value_function loss: 284.0252
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 64.0865
                       Mean reward: 786.84
               Mean episode length: 212.63
    Episode_Reward/reaching_object: 1.4626
     Episode_Reward/lifting_object: 152.3111
      Episode_Reward/object_height: 0.0355
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7500
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 2.39s
                      Time elapsed: 00:52:05
                               ETA: 00:40:09

################################################################################
                     [1m Learning iteration 1130/2000 [0m                     

                       Computation: 40996 steps/s (collection: 2.286s, learning 0.112s)
             Mean action noise std: 2.90
          Mean value_function loss: 262.8904
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 64.1091
                       Mean reward: 726.16
               Mean episode length: 199.01
    Episode_Reward/reaching_object: 1.4675
     Episode_Reward/lifting_object: 153.5601
      Episode_Reward/object_height: 0.0360
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.0000
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 2.40s
                      Time elapsed: 00:52:08
                               ETA: 00:40:06

################################################################################
                     [1m Learning iteration 1131/2000 [0m                     

                       Computation: 42735 steps/s (collection: 2.192s, learning 0.109s)
             Mean action noise std: 2.91
          Mean value_function loss: 253.6626
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 64.1283
                       Mean reward: 833.99
               Mean episode length: 223.69
    Episode_Reward/reaching_object: 1.5118
     Episode_Reward/lifting_object: 158.5032
      Episode_Reward/object_height: 0.0370
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 2.30s
                      Time elapsed: 00:52:10
                               ETA: 00:40:03

################################################################################
                     [1m Learning iteration 1132/2000 [0m                     

                       Computation: 42194 steps/s (collection: 2.226s, learning 0.104s)
             Mean action noise std: 2.91
          Mean value_function loss: 236.1571
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 64.1372
                       Mean reward: 823.94
               Mean episode length: 220.08
    Episode_Reward/reaching_object: 1.4985
     Episode_Reward/lifting_object: 156.7922
      Episode_Reward/object_height: 0.0363
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 2.33s
                      Time elapsed: 00:52:13
                               ETA: 00:40:00

################################################################################
                     [1m Learning iteration 1133/2000 [0m                     

                       Computation: 41665 steps/s (collection: 2.231s, learning 0.129s)
             Mean action noise std: 2.91
          Mean value_function loss: 248.3203
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 64.1411
                       Mean reward: 761.64
               Mean episode length: 206.77
    Episode_Reward/reaching_object: 1.4435
     Episode_Reward/lifting_object: 150.3342
      Episode_Reward/object_height: 0.0346
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5000
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 2.36s
                      Time elapsed: 00:52:15
                               ETA: 00:39:57

################################################################################
                     [1m Learning iteration 1134/2000 [0m                     

                       Computation: 40798 steps/s (collection: 2.300s, learning 0.109s)
             Mean action noise std: 2.91
          Mean value_function loss: 249.9581
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 64.1529
                       Mean reward: 761.55
               Mean episode length: 207.10
    Episode_Reward/reaching_object: 1.4099
     Episode_Reward/lifting_object: 146.7779
      Episode_Reward/object_height: 0.0335
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.2917
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 2.41s
                      Time elapsed: 00:52:17
                               ETA: 00:39:54

################################################################################
                     [1m Learning iteration 1135/2000 [0m                     

                       Computation: 40788 steps/s (collection: 2.305s, learning 0.106s)
             Mean action noise std: 2.91
          Mean value_function loss: 258.3458
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 64.1778
                       Mean reward: 777.46
               Mean episode length: 211.77
    Episode_Reward/reaching_object: 1.4863
     Episode_Reward/lifting_object: 155.2405
      Episode_Reward/object_height: 0.0353
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 2.41s
                      Time elapsed: 00:52:20
                               ETA: 00:39:51

################################################################################
                     [1m Learning iteration 1136/2000 [0m                     

                       Computation: 41152 steps/s (collection: 2.280s, learning 0.109s)
             Mean action noise std: 2.92
          Mean value_function loss: 234.7576
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 64.2015
                       Mean reward: 808.42
               Mean episode length: 218.70
    Episode_Reward/reaching_object: 1.4764
     Episode_Reward/lifting_object: 153.4056
      Episode_Reward/object_height: 0.0349
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 2.39s
                      Time elapsed: 00:52:22
                               ETA: 00:39:48

################################################################################
                     [1m Learning iteration 1137/2000 [0m                     

                       Computation: 40866 steps/s (collection: 2.283s, learning 0.122s)
             Mean action noise std: 2.92
          Mean value_function loss: 236.4473
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 64.2194
                       Mean reward: 809.53
               Mean episode length: 218.27
    Episode_Reward/reaching_object: 1.5031
     Episode_Reward/lifting_object: 156.5233
      Episode_Reward/object_height: 0.0351
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 2.41s
                      Time elapsed: 00:52:24
                               ETA: 00:39:44

################################################################################
                     [1m Learning iteration 1138/2000 [0m                     

                       Computation: 41059 steps/s (collection: 2.256s, learning 0.138s)
             Mean action noise std: 2.92
          Mean value_function loss: 223.8257
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 64.2320
                       Mean reward: 834.56
               Mean episode length: 223.32
    Episode_Reward/reaching_object: 1.5131
     Episode_Reward/lifting_object: 157.5233
      Episode_Reward/object_height: 0.0352
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 2.39s
                      Time elapsed: 00:52:27
                               ETA: 00:39:41

################################################################################
                     [1m Learning iteration 1139/2000 [0m                     

                       Computation: 41913 steps/s (collection: 2.215s, learning 0.131s)
             Mean action noise std: 2.92
          Mean value_function loss: 219.3100
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 64.2394
                       Mean reward: 801.48
               Mean episode length: 217.44
    Episode_Reward/reaching_object: 1.5228
     Episode_Reward/lifting_object: 158.6102
      Episode_Reward/object_height: 0.0352
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 2.35s
                      Time elapsed: 00:52:29
                               ETA: 00:39:38

################################################################################
                     [1m Learning iteration 1140/2000 [0m                     

                       Computation: 40917 steps/s (collection: 2.269s, learning 0.134s)
             Mean action noise std: 2.92
          Mean value_function loss: 266.2494
               Mean surrogate loss: 0.0071
                 Mean entropy loss: 64.2418
                       Mean reward: 729.38
               Mean episode length: 198.77
    Episode_Reward/reaching_object: 1.4574
     Episode_Reward/lifting_object: 151.4067
      Episode_Reward/object_height: 0.0330
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 2.40s
                      Time elapsed: 00:52:32
                               ETA: 00:39:35

################################################################################
                     [1m Learning iteration 1141/2000 [0m                     

                       Computation: 41862 steps/s (collection: 2.226s, learning 0.123s)
             Mean action noise std: 2.92
          Mean value_function loss: 263.8079
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 64.2428
                       Mean reward: 793.43
               Mean episode length: 215.44
    Episode_Reward/reaching_object: 1.5214
     Episode_Reward/lifting_object: 158.6148
      Episode_Reward/object_height: 0.0344
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 2.35s
                      Time elapsed: 00:52:34
                               ETA: 00:39:32

################################################################################
                     [1m Learning iteration 1142/2000 [0m                     

                       Computation: 41443 steps/s (collection: 2.234s, learning 0.138s)
             Mean action noise std: 2.92
          Mean value_function loss: 232.5424
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 64.2459
                       Mean reward: 765.73
               Mean episode length: 207.24
    Episode_Reward/reaching_object: 1.4705
     Episode_Reward/lifting_object: 151.9941
      Episode_Reward/object_height: 0.0326
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 2.37s
                      Time elapsed: 00:52:36
                               ETA: 00:39:29

################################################################################
                     [1m Learning iteration 1143/2000 [0m                     

                       Computation: 40910 steps/s (collection: 2.306s, learning 0.097s)
             Mean action noise std: 2.92
          Mean value_function loss: 221.3254
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 64.2520
                       Mean reward: 821.55
               Mean episode length: 220.90
    Episode_Reward/reaching_object: 1.5202
     Episode_Reward/lifting_object: 158.8673
      Episode_Reward/object_height: 0.0340
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 2.40s
                      Time elapsed: 00:52:39
                               ETA: 00:39:26

################################################################################
                     [1m Learning iteration 1144/2000 [0m                     

                       Computation: 42226 steps/s (collection: 2.225s, learning 0.103s)
             Mean action noise std: 2.92
          Mean value_function loss: 211.6946
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 64.2568
                       Mean reward: 815.97
               Mean episode length: 220.00
    Episode_Reward/reaching_object: 1.5383
     Episode_Reward/lifting_object: 160.6274
      Episode_Reward/object_height: 0.0340
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 2.33s
                      Time elapsed: 00:52:41
                               ETA: 00:39:23

################################################################################
                     [1m Learning iteration 1145/2000 [0m                     

                       Computation: 40452 steps/s (collection: 2.284s, learning 0.146s)
             Mean action noise std: 2.92
          Mean value_function loss: 249.7452
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 64.2633
                       Mean reward: 743.44
               Mean episode length: 203.06
    Episode_Reward/reaching_object: 1.4885
     Episode_Reward/lifting_object: 154.9097
      Episode_Reward/object_height: 0.0324
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 2.43s
                      Time elapsed: 00:52:44
                               ETA: 00:39:20

################################################################################
                     [1m Learning iteration 1146/2000 [0m                     

                       Computation: 37818 steps/s (collection: 2.494s, learning 0.106s)
             Mean action noise std: 2.92
          Mean value_function loss: 266.7717
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 64.2751
                       Mean reward: 797.78
               Mean episode length: 213.64
    Episode_Reward/reaching_object: 1.4834
     Episode_Reward/lifting_object: 154.7794
      Episode_Reward/object_height: 0.0320
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.4583
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 2.60s
                      Time elapsed: 00:52:46
                               ETA: 00:39:17

################################################################################
                     [1m Learning iteration 1147/2000 [0m                     

                       Computation: 41521 steps/s (collection: 2.226s, learning 0.141s)
             Mean action noise std: 2.93
          Mean value_function loss: 226.5872
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.2918
                       Mean reward: 852.24
               Mean episode length: 227.08
    Episode_Reward/reaching_object: 1.5264
     Episode_Reward/lifting_object: 159.4466
      Episode_Reward/object_height: 0.0328
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 2.37s
                      Time elapsed: 00:52:48
                               ETA: 00:39:14

################################################################################
                     [1m Learning iteration 1148/2000 [0m                     

                       Computation: 40675 steps/s (collection: 2.289s, learning 0.128s)
             Mean action noise std: 2.93
          Mean value_function loss: 249.9105
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 64.3092
                       Mean reward: 799.65
               Mean episode length: 215.74
    Episode_Reward/reaching_object: 1.5062
     Episode_Reward/lifting_object: 157.3423
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 2.42s
                      Time elapsed: 00:52:51
                               ETA: 00:39:11

################################################################################
                     [1m Learning iteration 1149/2000 [0m                     

                       Computation: 39294 steps/s (collection: 2.360s, learning 0.142s)
             Mean action noise std: 2.93
          Mean value_function loss: 251.5637
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 64.3217
                       Mean reward: 822.55
               Mean episode length: 221.68
    Episode_Reward/reaching_object: 1.5253
     Episode_Reward/lifting_object: 159.4391
      Episode_Reward/object_height: 0.0322
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.9167
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 2.50s
                      Time elapsed: 00:52:53
                               ETA: 00:39:08

################################################################################
                     [1m Learning iteration 1150/2000 [0m                     

                       Computation: 40883 steps/s (collection: 2.265s, learning 0.140s)
             Mean action noise std: 2.93
          Mean value_function loss: 274.3610
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 64.3336
                       Mean reward: 729.57
               Mean episode length: 200.32
    Episode_Reward/reaching_object: 1.4762
     Episode_Reward/lifting_object: 153.2024
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.2083
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 2.40s
                      Time elapsed: 00:52:56
                               ETA: 00:39:05

################################################################################
                     [1m Learning iteration 1151/2000 [0m                     

                       Computation: 39373 steps/s (collection: 2.359s, learning 0.138s)
             Mean action noise std: 2.93
          Mean value_function loss: 271.0145
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 64.3491
                       Mean reward: 733.80
               Mean episode length: 200.12
    Episode_Reward/reaching_object: 1.4872
     Episode_Reward/lifting_object: 155.0475
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 2.50s
                      Time elapsed: 00:52:58
                               ETA: 00:39:02

################################################################################
                     [1m Learning iteration 1152/2000 [0m                     

                       Computation: 40994 steps/s (collection: 2.259s, learning 0.139s)
             Mean action noise std: 2.94
          Mean value_function loss: 242.3113
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 64.3674
                       Mean reward: 749.19
               Mean episode length: 207.50
    Episode_Reward/reaching_object: 1.4531
     Episode_Reward/lifting_object: 150.6213
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 2.40s
                      Time elapsed: 00:53:01
                               ETA: 00:38:59

################################################################################
                     [1m Learning iteration 1153/2000 [0m                     

                       Computation: 38518 steps/s (collection: 2.411s, learning 0.141s)
             Mean action noise std: 2.94
          Mean value_function loss: 220.6669
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 64.3851
                       Mean reward: 839.62
               Mean episode length: 225.63
    Episode_Reward/reaching_object: 1.5555
     Episode_Reward/lifting_object: 161.8287
      Episode_Reward/object_height: 0.0316
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 2.55s
                      Time elapsed: 00:53:03
                               ETA: 00:38:56

################################################################################
                     [1m Learning iteration 1154/2000 [0m                     

                       Computation: 39353 steps/s (collection: 2.356s, learning 0.142s)
             Mean action noise std: 2.94
          Mean value_function loss: 221.3136
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 64.4064
                       Mean reward: 807.63
               Mean episode length: 218.03
    Episode_Reward/reaching_object: 1.5020
     Episode_Reward/lifting_object: 156.2277
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 2.50s
                      Time elapsed: 00:53:06
                               ETA: 00:38:53

################################################################################
                     [1m Learning iteration 1155/2000 [0m                     

                       Computation: 40998 steps/s (collection: 2.265s, learning 0.133s)
             Mean action noise std: 2.94
          Mean value_function loss: 192.5958
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 64.4194
                       Mean reward: 801.97
               Mean episode length: 215.31
    Episode_Reward/reaching_object: 1.5397
     Episode_Reward/lifting_object: 160.8013
      Episode_Reward/object_height: 0.0313
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 2.40s
                      Time elapsed: 00:53:08
                               ETA: 00:38:50

################################################################################
                     [1m Learning iteration 1156/2000 [0m                     

                       Computation: 39360 steps/s (collection: 2.350s, learning 0.147s)
             Mean action noise std: 2.94
          Mean value_function loss: 232.0224
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 64.4303
                       Mean reward: 767.88
               Mean episode length: 210.29
    Episode_Reward/reaching_object: 1.5333
     Episode_Reward/lifting_object: 159.4974
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.0491
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 2.50s
                      Time elapsed: 00:53:11
                               ETA: 00:38:47

################################################################################
                     [1m Learning iteration 1157/2000 [0m                     

                       Computation: 40014 steps/s (collection: 2.321s, learning 0.136s)
             Mean action noise std: 2.94
          Mean value_function loss: 216.7294
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 64.4367
                       Mean reward: 790.66
               Mean episode length: 214.16
    Episode_Reward/reaching_object: 1.5339
     Episode_Reward/lifting_object: 159.8056
      Episode_Reward/object_height: 0.0312
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 2.46s
                      Time elapsed: 00:53:13
                               ETA: 00:38:44

################################################################################
                     [1m Learning iteration 1158/2000 [0m                     

                       Computation: 37625 steps/s (collection: 2.484s, learning 0.129s)
             Mean action noise std: 2.95
          Mean value_function loss: 216.9362
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 64.4471
                       Mean reward: 732.36
               Mean episode length: 200.12
    Episode_Reward/reaching_object: 1.5017
     Episode_Reward/lifting_object: 155.7729
      Episode_Reward/object_height: 0.0305
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 2.61s
                      Time elapsed: 00:53:16
                               ETA: 00:38:42

################################################################################
                     [1m Learning iteration 1159/2000 [0m                     

                       Computation: 40482 steps/s (collection: 2.282s, learning 0.146s)
             Mean action noise std: 2.95
          Mean value_function loss: 232.3842
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 64.4609
                       Mean reward: 812.75
               Mean episode length: 220.53
    Episode_Reward/reaching_object: 1.5336
     Episode_Reward/lifting_object: 159.8161
      Episode_Reward/object_height: 0.0309
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 2.43s
                      Time elapsed: 00:53:18
                               ETA: 00:38:39

################################################################################
                     [1m Learning iteration 1160/2000 [0m                     

                       Computation: 39928 steps/s (collection: 2.301s, learning 0.161s)
             Mean action noise std: 2.95
          Mean value_function loss: 203.9028
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 64.4784
                       Mean reward: 814.38
               Mean episode length: 219.16
    Episode_Reward/reaching_object: 1.5316
     Episode_Reward/lifting_object: 159.0519
      Episode_Reward/object_height: 0.0307
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 2.46s
                      Time elapsed: 00:53:21
                               ETA: 00:38:36

################################################################################
                     [1m Learning iteration 1161/2000 [0m                     

                       Computation: 40327 steps/s (collection: 2.299s, learning 0.139s)
             Mean action noise std: 2.95
          Mean value_function loss: 206.8905
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 64.4842
                       Mean reward: 828.24
               Mean episode length: 221.40
    Episode_Reward/reaching_object: 1.5749
     Episode_Reward/lifting_object: 164.1769
      Episode_Reward/object_height: 0.0318
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 2.44s
                      Time elapsed: 00:53:23
                               ETA: 00:38:33

################################################################################
                     [1m Learning iteration 1162/2000 [0m                     

                       Computation: 41743 steps/s (collection: 2.216s, learning 0.139s)
             Mean action noise std: 2.95
          Mean value_function loss: 207.9259
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 64.4877
                       Mean reward: 813.14
               Mean episode length: 218.46
    Episode_Reward/reaching_object: 1.5178
     Episode_Reward/lifting_object: 158.0635
      Episode_Reward/object_height: 0.0302
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 2.35s
                      Time elapsed: 00:53:25
                               ETA: 00:38:30

################################################################################
                     [1m Learning iteration 1163/2000 [0m                     

                       Computation: 40112 steps/s (collection: 2.323s, learning 0.128s)
             Mean action noise std: 2.95
          Mean value_function loss: 214.2466
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 64.4905
                       Mean reward: 773.39
               Mean episode length: 210.60
    Episode_Reward/reaching_object: 1.4995
     Episode_Reward/lifting_object: 155.6508
      Episode_Reward/object_height: 0.0294
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 2.45s
                      Time elapsed: 00:53:28
                               ETA: 00:38:27

################################################################################
                     [1m Learning iteration 1164/2000 [0m                     

                       Computation: 41612 steps/s (collection: 2.235s, learning 0.127s)
             Mean action noise std: 2.95
          Mean value_function loss: 207.2684
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 64.4955
                       Mean reward: 772.97
               Mean episode length: 210.02
    Episode_Reward/reaching_object: 1.5076
     Episode_Reward/lifting_object: 156.3934
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 2.36s
                      Time elapsed: 00:53:30
                               ETA: 00:38:23

################################################################################
                     [1m Learning iteration 1165/2000 [0m                     

                       Computation: 40857 steps/s (collection: 2.269s, learning 0.137s)
             Mean action noise std: 2.95
          Mean value_function loss: 202.9236
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 64.5035
                       Mean reward: 775.37
               Mean episode length: 210.71
    Episode_Reward/reaching_object: 1.5301
     Episode_Reward/lifting_object: 158.8602
      Episode_Reward/object_height: 0.0295
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7917
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 2.41s
                      Time elapsed: 00:53:33
                               ETA: 00:38:20

################################################################################
                     [1m Learning iteration 1166/2000 [0m                     

                       Computation: 40850 steps/s (collection: 2.292s, learning 0.115s)
             Mean action noise std: 2.95
          Mean value_function loss: 193.0683
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 64.5107
                       Mean reward: 821.04
               Mean episode length: 221.11
    Episode_Reward/reaching_object: 1.5650
     Episode_Reward/lifting_object: 163.4946
      Episode_Reward/object_height: 0.0303
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 2.41s
                      Time elapsed: 00:53:35
                               ETA: 00:38:17

################################################################################
                     [1m Learning iteration 1167/2000 [0m                     

                       Computation: 41104 steps/s (collection: 2.255s, learning 0.137s)
             Mean action noise std: 2.95
          Mean value_function loss: 10153.3998
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 64.5172
                       Mean reward: 776.40
               Mean episode length: 210.96
    Episode_Reward/reaching_object: 1.5094
     Episode_Reward/lifting_object: 157.3979
      Episode_Reward/object_height: 0.0290
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -5.5186
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 2.39s
                      Time elapsed: 00:53:37
                               ETA: 00:38:14

################################################################################
                     [1m Learning iteration 1168/2000 [0m                     

                       Computation: 40848 steps/s (collection: 2.271s, learning 0.136s)
             Mean action noise std: 2.95
          Mean value_function loss: 219.6796
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.5214
                       Mean reward: 789.67
               Mean episode length: 214.65
    Episode_Reward/reaching_object: 1.5452
     Episode_Reward/lifting_object: 160.9430
      Episode_Reward/object_height: 0.0299
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 2.41s
                      Time elapsed: 00:53:40
                               ETA: 00:38:11

################################################################################
                     [1m Learning iteration 1169/2000 [0m                     

                       Computation: 41356 steps/s (collection: 2.270s, learning 0.107s)
             Mean action noise std: 2.96
          Mean value_function loss: 208.0281
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.5300
                       Mean reward: 831.42
               Mean episode length: 221.75
    Episode_Reward/reaching_object: 1.5109
     Episode_Reward/lifting_object: 157.5889
      Episode_Reward/object_height: 0.0291
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 2.38s
                      Time elapsed: 00:53:42
                               ETA: 00:38:08

################################################################################
                     [1m Learning iteration 1170/2000 [0m                     

                       Computation: 41131 steps/s (collection: 2.257s, learning 0.133s)
             Mean action noise std: 2.96
          Mean value_function loss: 216.8133
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 64.5397
                       Mean reward: 817.70
               Mean episode length: 220.51
    Episode_Reward/reaching_object: 1.5345
     Episode_Reward/lifting_object: 159.7340
      Episode_Reward/object_height: 0.0293
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 2.39s
                      Time elapsed: 00:53:45
                               ETA: 00:38:05

################################################################################
                     [1m Learning iteration 1171/2000 [0m                     

                       Computation: 40822 steps/s (collection: 2.273s, learning 0.135s)
             Mean action noise std: 2.96
          Mean value_function loss: 214.6568
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 64.5487
                       Mean reward: 829.46
               Mean episode length: 222.73
    Episode_Reward/reaching_object: 1.5263
     Episode_Reward/lifting_object: 158.9727
      Episode_Reward/object_height: 0.0291
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 2.41s
                      Time elapsed: 00:53:47
                               ETA: 00:38:02

################################################################################
                     [1m Learning iteration 1172/2000 [0m                     

                       Computation: 41241 steps/s (collection: 2.245s, learning 0.139s)
             Mean action noise std: 2.96
          Mean value_function loss: 237.5096
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 64.5592
                       Mean reward: 801.70
               Mean episode length: 217.56
    Episode_Reward/reaching_object: 1.5240
     Episode_Reward/lifting_object: 158.4763
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5000
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 2.38s
                      Time elapsed: 00:53:49
                               ETA: 00:37:59

################################################################################
                     [1m Learning iteration 1173/2000 [0m                     

                       Computation: 41573 steps/s (collection: 2.240s, learning 0.125s)
             Mean action noise std: 2.96
          Mean value_function loss: 245.7210
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 64.5655
                       Mean reward: 804.86
               Mean episode length: 216.02
    Episode_Reward/reaching_object: 1.5134
     Episode_Reward/lifting_object: 157.1191
      Episode_Reward/object_height: 0.0285
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 2.36s
                      Time elapsed: 00:53:52
                               ETA: 00:37:56

################################################################################
                     [1m Learning iteration 1174/2000 [0m                     

                       Computation: 40077 steps/s (collection: 2.304s, learning 0.149s)
             Mean action noise std: 2.96
          Mean value_function loss: 211.5181
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 64.5768
                       Mean reward: 825.76
               Mean episode length: 222.27
    Episode_Reward/reaching_object: 1.5157
     Episode_Reward/lifting_object: 157.6874
      Episode_Reward/object_height: 0.0286
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 2.45s
                      Time elapsed: 00:53:54
                               ETA: 00:37:53

################################################################################
                     [1m Learning iteration 1175/2000 [0m                     

                       Computation: 38821 steps/s (collection: 2.399s, learning 0.133s)
             Mean action noise std: 2.96
          Mean value_function loss: 206.9501
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 64.5950
                       Mean reward: 795.78
               Mean episode length: 215.10
    Episode_Reward/reaching_object: 1.4638
     Episode_Reward/lifting_object: 152.0418
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 2.53s
                      Time elapsed: 00:53:57
                               ETA: 00:37:51

################################################################################
                     [1m Learning iteration 1176/2000 [0m                     

                       Computation: 41108 steps/s (collection: 2.259s, learning 0.132s)
             Mean action noise std: 2.97
          Mean value_function loss: 251.9469
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 64.6072
                       Mean reward: 794.34
               Mean episode length: 214.52
    Episode_Reward/reaching_object: 1.5074
     Episode_Reward/lifting_object: 156.1323
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 2.39s
                      Time elapsed: 00:53:59
                               ETA: 00:37:48

################################################################################
                     [1m Learning iteration 1177/2000 [0m                     

                       Computation: 41508 steps/s (collection: 2.265s, learning 0.104s)
             Mean action noise std: 2.97
          Mean value_function loss: 223.4867
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 64.6283
                       Mean reward: 766.68
               Mean episode length: 208.73
    Episode_Reward/reaching_object: 1.5030
     Episode_Reward/lifting_object: 156.0490
      Episode_Reward/object_height: 0.0278
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 2.37s
                      Time elapsed: 00:54:01
                               ETA: 00:37:44

################################################################################
                     [1m Learning iteration 1178/2000 [0m                     

                       Computation: 40853 steps/s (collection: 2.278s, learning 0.128s)
             Mean action noise std: 2.97
          Mean value_function loss: 211.4340
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 64.6545
                       Mean reward: 788.39
               Mean episode length: 213.10
    Episode_Reward/reaching_object: 1.5115
     Episode_Reward/lifting_object: 156.9716
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 12.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 2.41s
                      Time elapsed: 00:54:04
                               ETA: 00:37:41

################################################################################
                     [1m Learning iteration 1179/2000 [0m                     

                       Computation: 39771 steps/s (collection: 2.326s, learning 0.146s)
             Mean action noise std: 2.97
          Mean value_function loss: 196.7916
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 64.6717
                       Mean reward: 824.86
               Mean episode length: 224.61
    Episode_Reward/reaching_object: 1.5684
     Episode_Reward/lifting_object: 163.3364
      Episode_Reward/object_height: 0.0289
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 2.47s
                      Time elapsed: 00:54:06
                               ETA: 00:37:39

################################################################################
                     [1m Learning iteration 1180/2000 [0m                     

                       Computation: 39313 steps/s (collection: 2.354s, learning 0.147s)
             Mean action noise std: 2.98
          Mean value_function loss: 212.9193
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 64.6945
                       Mean reward: 819.53
               Mean episode length: 220.26
    Episode_Reward/reaching_object: 1.5104
     Episode_Reward/lifting_object: 157.4093
      Episode_Reward/object_height: 0.0274
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 2.50s
                      Time elapsed: 00:54:09
                               ETA: 00:37:36

################################################################################
                     [1m Learning iteration 1181/2000 [0m                     

                       Computation: 38072 steps/s (collection: 2.431s, learning 0.151s)
             Mean action noise std: 2.98
          Mean value_function loss: 243.6858
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 64.7098
                       Mean reward: 799.98
               Mean episode length: 215.58
    Episode_Reward/reaching_object: 1.5187
     Episode_Reward/lifting_object: 158.5710
      Episode_Reward/object_height: 0.0273
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 2.58s
                      Time elapsed: 00:54:11
                               ETA: 00:37:33

################################################################################
                     [1m Learning iteration 1182/2000 [0m                     

                       Computation: 40007 steps/s (collection: 2.323s, learning 0.134s)
             Mean action noise std: 2.98
          Mean value_function loss: 246.5002
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 64.7248
                       Mean reward: 796.16
               Mean episode length: 215.42
    Episode_Reward/reaching_object: 1.5247
     Episode_Reward/lifting_object: 158.0917
      Episode_Reward/object_height: 0.0272
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 2.46s
                      Time elapsed: 00:54:14
                               ETA: 00:37:30

################################################################################
                     [1m Learning iteration 1183/2000 [0m                     

                       Computation: 39833 steps/s (collection: 2.343s, learning 0.125s)
             Mean action noise std: 2.98
          Mean value_function loss: 221.8693
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 64.7343
                       Mean reward: 772.75
               Mean episode length: 209.25
    Episode_Reward/reaching_object: 1.4873
     Episode_Reward/lifting_object: 154.6576
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 2.47s
                      Time elapsed: 00:54:16
                               ETA: 00:37:27

################################################################################
                     [1m Learning iteration 1184/2000 [0m                     

                       Computation: 41586 steps/s (collection: 2.210s, learning 0.154s)
             Mean action noise std: 2.98
          Mean value_function loss: 208.9966
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 64.7455
                       Mean reward: 769.71
               Mean episode length: 208.27
    Episode_Reward/reaching_object: 1.5074
     Episode_Reward/lifting_object: 156.6115
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 2.36s
                      Time elapsed: 00:54:19
                               ETA: 00:37:24

################################################################################
                     [1m Learning iteration 1185/2000 [0m                     

                       Computation: 40157 steps/s (collection: 2.276s, learning 0.172s)
             Mean action noise std: 2.98
          Mean value_function loss: 213.0733
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 64.7521
                       Mean reward: 739.00
               Mean episode length: 202.45
    Episode_Reward/reaching_object: 1.4683
     Episode_Reward/lifting_object: 152.1078
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.5833
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 2.45s
                      Time elapsed: 00:54:21
                               ETA: 00:37:21

################################################################################
                     [1m Learning iteration 1186/2000 [0m                     

                       Computation: 38701 steps/s (collection: 2.393s, learning 0.148s)
             Mean action noise std: 2.98
          Mean value_function loss: 186.6430
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.7648
                       Mean reward: 818.39
               Mean episode length: 219.94
    Episode_Reward/reaching_object: 1.5540
     Episode_Reward/lifting_object: 161.7633
      Episode_Reward/object_height: 0.0272
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0664
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 2.54s
                      Time elapsed: 00:54:24
                               ETA: 00:37:18

################################################################################
                     [1m Learning iteration 1187/2000 [0m                     

                       Computation: 37461 steps/s (collection: 2.461s, learning 0.163s)
             Mean action noise std: 2.99
          Mean value_function loss: 204.5266
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.7818
                       Mean reward: 798.32
               Mean episode length: 214.69
    Episode_Reward/reaching_object: 1.5488
     Episode_Reward/lifting_object: 161.7596
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 2.62s
                      Time elapsed: 00:54:26
                               ETA: 00:37:15

################################################################################
                     [1m Learning iteration 1188/2000 [0m                     

                       Computation: 38942 steps/s (collection: 2.390s, learning 0.135s)
             Mean action noise std: 2.99
          Mean value_function loss: 193.8817
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 64.8046
                       Mean reward: 827.06
               Mean episode length: 221.85
    Episode_Reward/reaching_object: 1.5165
     Episode_Reward/lifting_object: 158.2590
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 2.52s
                      Time elapsed: 00:54:29
                               ETA: 00:37:12

################################################################################
                     [1m Learning iteration 1189/2000 [0m                     

                       Computation: 40600 steps/s (collection: 2.277s, learning 0.145s)
             Mean action noise std: 2.99
          Mean value_function loss: 167.1637
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 64.8119
                       Mean reward: 870.87
               Mean episode length: 232.66
    Episode_Reward/reaching_object: 1.5477
     Episode_Reward/lifting_object: 161.4865
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 2.42s
                      Time elapsed: 00:54:31
                               ETA: 00:37:09

################################################################################
                     [1m Learning iteration 1190/2000 [0m                     

                       Computation: 40774 steps/s (collection: 2.302s, learning 0.109s)
             Mean action noise std: 2.99
          Mean value_function loss: 207.5299
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 64.8175
                       Mean reward: 755.28
               Mean episode length: 206.31
    Episode_Reward/reaching_object: 1.5387
     Episode_Reward/lifting_object: 160.6037
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 2.41s
                      Time elapsed: 00:54:34
                               ETA: 00:37:06

################################################################################
                     [1m Learning iteration 1191/2000 [0m                     

                       Computation: 39209 steps/s (collection: 2.352s, learning 0.156s)
             Mean action noise std: 2.99
          Mean value_function loss: 196.7525
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 64.8262
                       Mean reward: 781.57
               Mean episode length: 213.57
    Episode_Reward/reaching_object: 1.5315
     Episode_Reward/lifting_object: 159.4834
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 2.51s
                      Time elapsed: 00:54:36
                               ETA: 00:37:03

################################################################################
                     [1m Learning iteration 1192/2000 [0m                     

                       Computation: 39930 steps/s (collection: 2.332s, learning 0.130s)
             Mean action noise std: 2.99
          Mean value_function loss: 206.7326
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 64.8384
                       Mean reward: 850.44
               Mean episode length: 226.91
    Episode_Reward/reaching_object: 1.5480
     Episode_Reward/lifting_object: 161.7882
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 2.46s
                      Time elapsed: 00:54:39
                               ETA: 00:37:00

################################################################################
                     [1m Learning iteration 1193/2000 [0m                     

                       Computation: 40944 steps/s (collection: 2.253s, learning 0.148s)
             Mean action noise std: 3.00
          Mean value_function loss: 210.0885
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 64.8565
                       Mean reward: 737.39
               Mean episode length: 202.41
    Episode_Reward/reaching_object: 1.5247
     Episode_Reward/lifting_object: 158.7375
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 2.40s
                      Time elapsed: 00:54:41
                               ETA: 00:36:57

################################################################################
                     [1m Learning iteration 1194/2000 [0m                     

                       Computation: 40738 steps/s (collection: 2.283s, learning 0.130s)
             Mean action noise std: 3.00
          Mean value_function loss: 193.0957
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 64.8772
                       Mean reward: 792.57
               Mean episode length: 214.38
    Episode_Reward/reaching_object: 1.5196
     Episode_Reward/lifting_object: 158.4152
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 2.41s
                      Time elapsed: 00:54:43
                               ETA: 00:36:54

################################################################################
                     [1m Learning iteration 1195/2000 [0m                     

                       Computation: 40037 steps/s (collection: 2.304s, learning 0.152s)
             Mean action noise std: 3.00
          Mean value_function loss: 196.1415
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 64.8887
                       Mean reward: 822.00
               Mean episode length: 221.58
    Episode_Reward/reaching_object: 1.5420
     Episode_Reward/lifting_object: 160.8892
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 2.46s
                      Time elapsed: 00:54:46
                               ETA: 00:36:52

################################################################################
                     [1m Learning iteration 1196/2000 [0m                     

                       Computation: 36457 steps/s (collection: 2.573s, learning 0.124s)
             Mean action noise std: 3.00
          Mean value_function loss: 199.0848
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 64.9068
                       Mean reward: 852.55
               Mean episode length: 229.29
    Episode_Reward/reaching_object: 1.4809
     Episode_Reward/lifting_object: 153.6784
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 2.70s
                      Time elapsed: 00:54:49
                               ETA: 00:36:49

################################################################################
                     [1m Learning iteration 1197/2000 [0m                     

                       Computation: 40308 steps/s (collection: 2.300s, learning 0.139s)
             Mean action noise std: 3.00
          Mean value_function loss: 194.1116
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 64.9280
                       Mean reward: 764.75
               Mean episode length: 208.40
    Episode_Reward/reaching_object: 1.5194
     Episode_Reward/lifting_object: 157.6739
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 2.44s
                      Time elapsed: 00:54:51
                               ETA: 00:36:46

################################################################################
                     [1m Learning iteration 1198/2000 [0m                     

                       Computation: 40199 steps/s (collection: 2.318s, learning 0.127s)
             Mean action noise std: 3.01
          Mean value_function loss: 207.6663
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 64.9444
                       Mean reward: 763.34
               Mean episode length: 208.03
    Episode_Reward/reaching_object: 1.5207
     Episode_Reward/lifting_object: 158.1323
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 2.45s
                      Time elapsed: 00:54:54
                               ETA: 00:36:43

################################################################################
                     [1m Learning iteration 1199/2000 [0m                     

                       Computation: 41089 steps/s (collection: 2.246s, learning 0.146s)
             Mean action noise std: 3.01
          Mean value_function loss: 219.3967
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 64.9672
                       Mean reward: 762.06
               Mean episode length: 207.47
    Episode_Reward/reaching_object: 1.4453
     Episode_Reward/lifting_object: 149.4716
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 12.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.6250
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 2.39s
                      Time elapsed: 00:54:56
                               ETA: 00:36:40

################################################################################
                     [1m Learning iteration 1200/2000 [0m                     

                       Computation: 40607 steps/s (collection: 2.283s, learning 0.138s)
             Mean action noise std: 3.01
          Mean value_function loss: 228.5418
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 64.9830
                       Mean reward: 797.51
               Mean episode length: 214.62
    Episode_Reward/reaching_object: 1.5521
     Episode_Reward/lifting_object: 162.2093
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 2.42s
                      Time elapsed: 00:54:58
                               ETA: 00:36:37

################################################################################
                     [1m Learning iteration 1201/2000 [0m                     

                       Computation: 40870 steps/s (collection: 2.263s, learning 0.143s)
             Mean action noise std: 3.01
          Mean value_function loss: 202.7050
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 64.9909
                       Mean reward: 823.88
               Mean episode length: 219.94
    Episode_Reward/reaching_object: 1.5459
     Episode_Reward/lifting_object: 161.4485
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 2.41s
                      Time elapsed: 00:55:01
                               ETA: 00:36:34

################################################################################
                     [1m Learning iteration 1202/2000 [0m                     

                       Computation: 40567 steps/s (collection: 2.286s, learning 0.138s)
             Mean action noise std: 3.01
          Mean value_function loss: 200.6331
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 65.0089
                       Mean reward: 805.57
               Mean episode length: 218.88
    Episode_Reward/reaching_object: 1.5558
     Episode_Reward/lifting_object: 162.1737
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 2.42s
                      Time elapsed: 00:55:03
                               ETA: 00:36:31

################################################################################
                     [1m Learning iteration 1203/2000 [0m                     

                       Computation: 40246 steps/s (collection: 2.323s, learning 0.120s)
             Mean action noise std: 3.02
          Mean value_function loss: 184.1347
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 65.0300
                       Mean reward: 777.40
               Mean episode length: 210.82
    Episode_Reward/reaching_object: 1.5524
     Episode_Reward/lifting_object: 161.7037
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 2.44s
                      Time elapsed: 00:55:06
                               ETA: 00:36:28

################################################################################
                     [1m Learning iteration 1204/2000 [0m                     

                       Computation: 40463 steps/s (collection: 2.295s, learning 0.134s)
             Mean action noise std: 3.02
          Mean value_function loss: 208.2564
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 65.0442
                       Mean reward: 817.07
               Mean episode length: 220.48
    Episode_Reward/reaching_object: 1.5361
     Episode_Reward/lifting_object: 160.2605
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 2.43s
                      Time elapsed: 00:55:08
                               ETA: 00:36:25

################################################################################
                     [1m Learning iteration 1205/2000 [0m                     

                       Computation: 41098 steps/s (collection: 2.256s, learning 0.136s)
             Mean action noise std: 3.02
          Mean value_function loss: 203.6935
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 65.0511
                       Mean reward: 803.03
               Mean episode length: 215.02
    Episode_Reward/reaching_object: 1.5338
     Episode_Reward/lifting_object: 160.5603
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 2.39s
                      Time elapsed: 00:55:10
                               ETA: 00:36:22

################################################################################
                     [1m Learning iteration 1206/2000 [0m                     

                       Computation: 40979 steps/s (collection: 2.276s, learning 0.123s)
             Mean action noise std: 3.02
          Mean value_function loss: 202.7820
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 65.0652
                       Mean reward: 751.18
               Mean episode length: 203.39
    Episode_Reward/reaching_object: 1.5193
     Episode_Reward/lifting_object: 158.9206
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 2.40s
                      Time elapsed: 00:55:13
                               ETA: 00:36:19

################################################################################
                     [1m Learning iteration 1207/2000 [0m                     

                       Computation: 39932 steps/s (collection: 2.331s, learning 0.131s)
             Mean action noise std: 3.02
          Mean value_function loss: 183.6345
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 65.0746
                       Mean reward: 858.31
               Mean episode length: 227.66
    Episode_Reward/reaching_object: 1.5629
     Episode_Reward/lifting_object: 163.5485
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 2.46s
                      Time elapsed: 00:55:15
                               ETA: 00:36:16

################################################################################
                     [1m Learning iteration 1208/2000 [0m                     

                       Computation: 41970 steps/s (collection: 2.207s, learning 0.136s)
             Mean action noise std: 3.02
          Mean value_function loss: 195.3714
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 65.0814
                       Mean reward: 796.01
               Mean episode length: 214.52
    Episode_Reward/reaching_object: 1.5411
     Episode_Reward/lifting_object: 160.9387
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 2.34s
                      Time elapsed: 00:55:18
                               ETA: 00:36:13

################################################################################
                     [1m Learning iteration 1209/2000 [0m                     

                       Computation: 42003 steps/s (collection: 2.219s, learning 0.122s)
             Mean action noise std: 3.03
          Mean value_function loss: 174.5367
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 65.1013
                       Mean reward: 779.76
               Mean episode length: 212.22
    Episode_Reward/reaching_object: 1.5318
     Episode_Reward/lifting_object: 160.1148
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 13.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 2.34s
                      Time elapsed: 00:55:20
                               ETA: 00:36:10

################################################################################
                     [1m Learning iteration 1210/2000 [0m                     

                       Computation: 40717 steps/s (collection: 2.273s, learning 0.142s)
             Mean action noise std: 3.03
          Mean value_function loss: 180.2775
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 65.1277
                       Mean reward: 813.16
               Mean episode length: 218.89
    Episode_Reward/reaching_object: 1.5846
     Episode_Reward/lifting_object: 165.7922
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0682
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 2.41s
                      Time elapsed: 00:55:22
                               ETA: 00:36:07

################################################################################
                     [1m Learning iteration 1211/2000 [0m                     

                       Computation: 41873 steps/s (collection: 2.236s, learning 0.112s)
             Mean action noise std: 3.03
          Mean value_function loss: 166.0259
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 65.1383
                       Mean reward: 897.93
               Mean episode length: 237.49
    Episode_Reward/reaching_object: 1.5869
     Episode_Reward/lifting_object: 166.3008
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0679
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 2.35s
                      Time elapsed: 00:55:25
                               ETA: 00:36:04

################################################################################
                     [1m Learning iteration 1212/2000 [0m                     

                       Computation: 41112 steps/s (collection: 2.254s, learning 0.137s)
             Mean action noise std: 3.03
          Mean value_function loss: 190.7679
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 65.1592
                       Mean reward: 852.56
               Mean episode length: 226.32
    Episode_Reward/reaching_object: 1.5744
     Episode_Reward/lifting_object: 165.6642
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 2.39s
                      Time elapsed: 00:55:27
                               ETA: 00:36:01

################################################################################
                     [1m Learning iteration 1213/2000 [0m                     

                       Computation: 41327 steps/s (collection: 2.252s, learning 0.127s)
             Mean action noise std: 3.04
          Mean value_function loss: 204.3711
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 65.1751
                       Mean reward: 782.75
               Mean episode length: 210.75
    Episode_Reward/reaching_object: 1.5154
     Episode_Reward/lifting_object: 158.9373
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 2.38s
                      Time elapsed: 00:55:30
                               ETA: 00:35:58

################################################################################
                     [1m Learning iteration 1214/2000 [0m                     

                       Computation: 41543 steps/s (collection: 2.238s, learning 0.128s)
             Mean action noise std: 3.04
          Mean value_function loss: 206.2585
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 65.1933
                       Mean reward: 802.44
               Mean episode length: 215.80
    Episode_Reward/reaching_object: 1.5391
     Episode_Reward/lifting_object: 161.4748
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2083
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 2.37s
                      Time elapsed: 00:55:32
                               ETA: 00:35:55

################################################################################
                     [1m Learning iteration 1215/2000 [0m                     

                       Computation: 41418 steps/s (collection: 2.257s, learning 0.117s)
             Mean action noise std: 3.04
          Mean value_function loss: 203.1367
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 65.2125
                       Mean reward: 857.56
               Mean episode length: 227.91
    Episode_Reward/reaching_object: 1.5875
     Episode_Reward/lifting_object: 167.6618
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0677
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 2.37s
                      Time elapsed: 00:55:34
                               ETA: 00:35:52

################################################################################
                     [1m Learning iteration 1216/2000 [0m                     

                       Computation: 40004 steps/s (collection: 2.324s, learning 0.133s)
             Mean action noise std: 3.04
          Mean value_function loss: 176.2070
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 65.2208
                       Mean reward: 843.35
               Mean episode length: 225.13
    Episode_Reward/reaching_object: 1.5266
     Episode_Reward/lifting_object: 160.6424
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0650
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 2.46s
                      Time elapsed: 00:55:37
                               ETA: 00:35:49

################################################################################
                     [1m Learning iteration 1217/2000 [0m                     

                       Computation: 41383 steps/s (collection: 2.244s, learning 0.132s)
             Mean action noise std: 3.04
          Mean value_function loss: 216.3135
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 65.2367
                       Mean reward: 804.71
               Mean episode length: 215.48
    Episode_Reward/reaching_object: 1.5513
     Episode_Reward/lifting_object: 163.8101
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0655
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 2.38s
                      Time elapsed: 00:55:39
                               ETA: 00:35:46

################################################################################
                     [1m Learning iteration 1218/2000 [0m                     

                       Computation: 42081 steps/s (collection: 2.236s, learning 0.100s)
             Mean action noise std: 3.05
          Mean value_function loss: 186.4440
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 65.2582
                       Mean reward: 860.13
               Mean episode length: 229.43
    Episode_Reward/reaching_object: 1.5734
     Episode_Reward/lifting_object: 166.2199
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0549
          Episode_Reward/joint_vel: -0.0671
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 2.34s
                      Time elapsed: 00:55:41
                               ETA: 00:35:43

################################################################################
                     [1m Learning iteration 1219/2000 [0m                     

                       Computation: 42020 steps/s (collection: 2.240s, learning 0.099s)
             Mean action noise std: 3.05
          Mean value_function loss: 223.0227
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 65.2809
                       Mean reward: 807.10
               Mean episode length: 217.91
    Episode_Reward/reaching_object: 1.5292
     Episode_Reward/lifting_object: 161.2689
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 2.34s
                      Time elapsed: 00:55:44
                               ETA: 00:35:40

################################################################################
                     [1m Learning iteration 1220/2000 [0m                     

                       Computation: 41412 steps/s (collection: 2.240s, learning 0.134s)
             Mean action noise std: 3.05
          Mean value_function loss: 208.7313
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 65.2985
                       Mean reward: 818.44
               Mean episode length: 218.44
    Episode_Reward/reaching_object: 1.5170
     Episode_Reward/lifting_object: 160.1364
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 2.37s
                      Time elapsed: 00:55:46
                               ETA: 00:35:37

################################################################################
                     [1m Learning iteration 1221/2000 [0m                     

                       Computation: 41058 steps/s (collection: 2.258s, learning 0.136s)
             Mean action noise std: 3.05
          Mean value_function loss: 193.3224
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 65.3160
                       Mean reward: 822.23
               Mean episode length: 220.18
    Episode_Reward/reaching_object: 1.5180
     Episode_Reward/lifting_object: 160.2729
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 2.39s
                      Time elapsed: 00:55:49
                               ETA: 00:35:34

################################################################################
                     [1m Learning iteration 1222/2000 [0m                     

                       Computation: 42201 steps/s (collection: 2.198s, learning 0.132s)
             Mean action noise std: 3.05
          Mean value_function loss: 220.5609
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 65.3275
                       Mean reward: 820.30
               Mean episode length: 220.68
    Episode_Reward/reaching_object: 1.5533
     Episode_Reward/lifting_object: 163.7108
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.6667
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 2.33s
                      Time elapsed: 00:55:51
                               ETA: 00:35:31

################################################################################
                     [1m Learning iteration 1223/2000 [0m                     

                       Computation: 41352 steps/s (collection: 2.229s, learning 0.148s)
             Mean action noise std: 3.06
          Mean value_function loss: 214.7146
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 65.3416
                       Mean reward: 878.59
               Mean episode length: 233.25
    Episode_Reward/reaching_object: 1.5340
     Episode_Reward/lifting_object: 162.1588
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 2.38s
                      Time elapsed: 00:55:53
                               ETA: 00:35:28

################################################################################
                     [1m Learning iteration 1224/2000 [0m                     

                       Computation: 41695 steps/s (collection: 2.215s, learning 0.143s)
             Mean action noise std: 3.06
          Mean value_function loss: 183.8092
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 65.3670
                       Mean reward: 818.13
               Mean episode length: 219.02
    Episode_Reward/reaching_object: 1.5581
     Episode_Reward/lifting_object: 164.3161
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 2.36s
                      Time elapsed: 00:55:56
                               ETA: 00:35:25

################################################################################
                     [1m Learning iteration 1225/2000 [0m                     

                       Computation: 40735 steps/s (collection: 2.271s, learning 0.142s)
             Mean action noise std: 3.06
          Mean value_function loss: 171.9842
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 65.3776
                       Mean reward: 808.61
               Mean episode length: 218.04
    Episode_Reward/reaching_object: 1.5747
     Episode_Reward/lifting_object: 165.7211
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0674
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 2.41s
                      Time elapsed: 00:55:58
                               ETA: 00:35:23

################################################################################
                     [1m Learning iteration 1226/2000 [0m                     

                       Computation: 41704 steps/s (collection: 2.229s, learning 0.129s)
             Mean action noise std: 3.06
          Mean value_function loss: 162.2592
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 65.3869
                       Mean reward: 823.40
               Mean episode length: 220.84
    Episode_Reward/reaching_object: 1.5510
     Episode_Reward/lifting_object: 163.5739
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 2.36s
                      Time elapsed: 00:56:00
                               ETA: 00:35:20

################################################################################
                     [1m Learning iteration 1227/2000 [0m                     

                       Computation: 40662 steps/s (collection: 2.289s, learning 0.128s)
             Mean action noise std: 3.06
          Mean value_function loss: 195.5553
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 65.4022
                       Mean reward: 785.68
               Mean episode length: 211.74
    Episode_Reward/reaching_object: 1.5354
     Episode_Reward/lifting_object: 162.0072
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 2.42s
                      Time elapsed: 00:56:03
                               ETA: 00:35:17

################################################################################
                     [1m Learning iteration 1228/2000 [0m                     

                       Computation: 41978 steps/s (collection: 2.230s, learning 0.112s)
             Mean action noise std: 3.06
          Mean value_function loss: 187.0690
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 65.4168
                       Mean reward: 832.14
               Mean episode length: 223.96
    Episode_Reward/reaching_object: 1.5769
     Episode_Reward/lifting_object: 165.9900
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 2.34s
                      Time elapsed: 00:56:05
                               ETA: 00:35:14

################################################################################
                     [1m Learning iteration 1229/2000 [0m                     

                       Computation: 41774 steps/s (collection: 2.235s, learning 0.118s)
             Mean action noise std: 3.07
          Mean value_function loss: 182.1237
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 65.4368
                       Mean reward: 863.37
               Mean episode length: 232.56
    Episode_Reward/reaching_object: 1.5436
     Episode_Reward/lifting_object: 161.9473
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0665
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 2.35s
                      Time elapsed: 00:56:07
                               ETA: 00:35:11

################################################################################
                     [1m Learning iteration 1230/2000 [0m                     

                       Computation: 42226 steps/s (collection: 2.206s, learning 0.122s)
             Mean action noise std: 3.07
          Mean value_function loss: 184.2743
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 65.4561
                       Mean reward: 816.39
               Mean episode length: 219.57
    Episode_Reward/reaching_object: 1.5364
     Episode_Reward/lifting_object: 161.3759
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 2.33s
                      Time elapsed: 00:56:10
                               ETA: 00:35:08

################################################################################
                     [1m Learning iteration 1231/2000 [0m                     

                       Computation: 40466 steps/s (collection: 2.314s, learning 0.115s)
             Mean action noise std: 3.07
          Mean value_function loss: 178.9079
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 65.4728
                       Mean reward: 837.31
               Mean episode length: 224.87
    Episode_Reward/reaching_object: 1.5813
     Episode_Reward/lifting_object: 166.1848
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 2.43s
                      Time elapsed: 00:56:12
                               ETA: 00:35:05

################################################################################
                     [1m Learning iteration 1232/2000 [0m                     

                       Computation: 41579 steps/s (collection: 2.236s, learning 0.128s)
             Mean action noise std: 3.07
          Mean value_function loss: 197.4730
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 65.4802
                       Mean reward: 845.41
               Mean episode length: 226.26
    Episode_Reward/reaching_object: 1.5551
     Episode_Reward/lifting_object: 163.2457
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 2.36s
                      Time elapsed: 00:56:15
                               ETA: 00:35:02

################################################################################
                     [1m Learning iteration 1233/2000 [0m                     

                       Computation: 41693 steps/s (collection: 2.252s, learning 0.106s)
             Mean action noise std: 3.07
          Mean value_function loss: 175.2128
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 65.4951
                       Mean reward: 876.36
               Mean episode length: 232.26
    Episode_Reward/reaching_object: 1.5911
     Episode_Reward/lifting_object: 168.2747
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0673
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 2.36s
                      Time elapsed: 00:56:17
                               ETA: 00:34:59

################################################################################
                     [1m Learning iteration 1234/2000 [0m                     

                       Computation: 41792 steps/s (collection: 2.230s, learning 0.123s)
             Mean action noise std: 3.08
          Mean value_function loss: 193.0527
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 65.5139
                       Mean reward: 753.47
               Mean episode length: 205.18
    Episode_Reward/reaching_object: 1.5228
     Episode_Reward/lifting_object: 160.1482
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 2.35s
                      Time elapsed: 00:56:19
                               ETA: 00:34:56

################################################################################
                     [1m Learning iteration 1235/2000 [0m                     

                       Computation: 41672 steps/s (collection: 2.234s, learning 0.125s)
             Mean action noise std: 3.08
          Mean value_function loss: 222.6475
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 65.5324
                       Mean reward: 827.00
               Mean episode length: 220.74
    Episode_Reward/reaching_object: 1.5260
     Episode_Reward/lifting_object: 160.5947
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 2.36s
                      Time elapsed: 00:56:22
                               ETA: 00:34:53

################################################################################
                     [1m Learning iteration 1236/2000 [0m                     

                       Computation: 41031 steps/s (collection: 2.289s, learning 0.107s)
             Mean action noise std: 3.08
          Mean value_function loss: 172.7829
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 65.5472
                       Mean reward: 826.78
               Mean episode length: 222.22
    Episode_Reward/reaching_object: 1.5691
     Episode_Reward/lifting_object: 165.4898
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 2.40s
                      Time elapsed: 00:56:24
                               ETA: 00:34:50

################################################################################
                     [1m Learning iteration 1237/2000 [0m                     

                       Computation: 40590 steps/s (collection: 2.269s, learning 0.153s)
             Mean action noise std: 3.08
          Mean value_function loss: 178.7664
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 65.5689
                       Mean reward: 863.16
               Mean episode length: 230.71
    Episode_Reward/reaching_object: 1.5734
     Episode_Reward/lifting_object: 166.2271
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0659
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 2.42s
                      Time elapsed: 00:56:26
                               ETA: 00:34:47

################################################################################
                     [1m Learning iteration 1238/2000 [0m                     

                       Computation: 41401 steps/s (collection: 2.244s, learning 0.131s)
             Mean action noise std: 3.09
          Mean value_function loss: 205.6430
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 65.5908
                       Mean reward: 800.70
               Mean episode length: 215.14
    Episode_Reward/reaching_object: 1.5361
     Episode_Reward/lifting_object: 161.8063
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 2.37s
                      Time elapsed: 00:56:29
                               ETA: 00:34:44

################################################################################
                     [1m Learning iteration 1239/2000 [0m                     

                       Computation: 41970 steps/s (collection: 2.203s, learning 0.139s)
             Mean action noise std: 3.09
          Mean value_function loss: 181.2355
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 65.6091
                       Mean reward: 839.60
               Mean episode length: 224.84
    Episode_Reward/reaching_object: 1.5517
     Episode_Reward/lifting_object: 163.8733
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 2.34s
                      Time elapsed: 00:56:31
                               ETA: 00:34:41

################################################################################
                     [1m Learning iteration 1240/2000 [0m                     

                       Computation: 41542 steps/s (collection: 2.227s, learning 0.140s)
             Mean action noise std: 3.09
          Mean value_function loss: 211.2712
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 65.6327
                       Mean reward: 809.52
               Mean episode length: 216.45
    Episode_Reward/reaching_object: 1.5346
     Episode_Reward/lifting_object: 161.7994
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0547
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 2.37s
                      Time elapsed: 00:56:34
                               ETA: 00:34:38

################################################################################
                     [1m Learning iteration 1241/2000 [0m                     

                       Computation: 41347 steps/s (collection: 2.268s, learning 0.110s)
             Mean action noise std: 3.09
          Mean value_function loss: 234.6328
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 65.6539
                       Mean reward: 847.22
               Mean episode length: 226.02
    Episode_Reward/reaching_object: 1.5231
     Episode_Reward/lifting_object: 160.1865
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 2.38s
                      Time elapsed: 00:56:36
                               ETA: 00:34:35

################################################################################
                     [1m Learning iteration 1242/2000 [0m                     

                       Computation: 41254 steps/s (collection: 2.240s, learning 0.142s)
             Mean action noise std: 3.09
          Mean value_function loss: 149.6127
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 65.6664
                       Mean reward: 885.64
               Mean episode length: 234.61
    Episode_Reward/reaching_object: 1.5970
     Episode_Reward/lifting_object: 168.7389
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0670
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 2.38s
                      Time elapsed: 00:56:38
                               ETA: 00:34:32

################################################################################
                     [1m Learning iteration 1243/2000 [0m                     

                       Computation: 40840 steps/s (collection: 2.251s, learning 0.156s)
             Mean action noise std: 3.10
          Mean value_function loss: 159.1582
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 65.6753
                       Mean reward: 820.75
               Mean episode length: 220.49
    Episode_Reward/reaching_object: 1.5643
     Episode_Reward/lifting_object: 165.3810
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0660
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 2.41s
                      Time elapsed: 00:56:41
                               ETA: 00:34:29

################################################################################
                     [1m Learning iteration 1244/2000 [0m                     

                       Computation: 40885 steps/s (collection: 2.294s, learning 0.110s)
             Mean action noise std: 3.10
          Mean value_function loss: 206.2110
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 65.6877
                       Mean reward: 888.13
               Mean episode length: 235.78
    Episode_Reward/reaching_object: 1.5632
     Episode_Reward/lifting_object: 165.5177
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 2.40s
                      Time elapsed: 00:56:43
                               ETA: 00:34:26

################################################################################
                     [1m Learning iteration 1245/2000 [0m                     

                       Computation: 41065 steps/s (collection: 2.256s, learning 0.138s)
             Mean action noise std: 3.10
          Mean value_function loss: 178.7811
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 65.7052
                       Mean reward: 877.11
               Mean episode length: 233.61
    Episode_Reward/reaching_object: 1.5383
     Episode_Reward/lifting_object: 162.1150
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0553
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 2.39s
                      Time elapsed: 00:56:46
                               ETA: 00:34:23

################################################################################
                     [1m Learning iteration 1246/2000 [0m                     

                       Computation: 40959 steps/s (collection: 2.264s, learning 0.136s)
             Mean action noise std: 3.10
          Mean value_function loss: 199.6034
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 65.7212
                       Mean reward: 849.92
               Mean episode length: 226.92
    Episode_Reward/reaching_object: 1.5508
     Episode_Reward/lifting_object: 163.7656
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0652
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 2.40s
                      Time elapsed: 00:56:48
                               ETA: 00:34:20

################################################################################
                     [1m Learning iteration 1247/2000 [0m                     

                       Computation: 40720 steps/s (collection: 2.281s, learning 0.133s)
             Mean action noise std: 3.10
          Mean value_function loss: 164.5050
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 65.7405
                       Mean reward: 835.84
               Mean episode length: 222.41
    Episode_Reward/reaching_object: 1.5611
     Episode_Reward/lifting_object: 165.1977
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0657
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 2.41s
                      Time elapsed: 00:56:50
                               ETA: 00:34:17

################################################################################
                     [1m Learning iteration 1248/2000 [0m                     

                       Computation: 40938 steps/s (collection: 2.255s, learning 0.146s)
             Mean action noise std: 3.11
          Mean value_function loss: 203.9191
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 65.7615
                       Mean reward: 846.00
               Mean episode length: 226.20
    Episode_Reward/reaching_object: 1.5733
     Episode_Reward/lifting_object: 166.2461
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 2.40s
                      Time elapsed: 00:56:53
                               ETA: 00:34:15

################################################################################
                     [1m Learning iteration 1249/2000 [0m                     

                       Computation: 41965 steps/s (collection: 2.215s, learning 0.127s)
             Mean action noise std: 3.11
          Mean value_function loss: 163.4267
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 65.7807
                       Mean reward: 881.92
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 1.5968
     Episode_Reward/lifting_object: 169.3920
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 2.34s
                      Time elapsed: 00:56:55
                               ETA: 00:34:12

################################################################################
                     [1m Learning iteration 1250/2000 [0m                     

                       Computation: 42450 steps/s (collection: 2.208s, learning 0.108s)
             Mean action noise std: 3.11
          Mean value_function loss: 177.9350
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 65.8002
                       Mean reward: 822.78
               Mean episode length: 220.02
    Episode_Reward/reaching_object: 1.5417
     Episode_Reward/lifting_object: 162.7758
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 2.32s
                      Time elapsed: 00:56:57
                               ETA: 00:34:09

################################################################################
                     [1m Learning iteration 1251/2000 [0m                     

                       Computation: 41163 steps/s (collection: 2.259s, learning 0.129s)
             Mean action noise std: 3.11
          Mean value_function loss: 200.3460
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 65.8192
                       Mean reward: 795.86
               Mean episode length: 214.42
    Episode_Reward/reaching_object: 1.5277
     Episode_Reward/lifting_object: 161.3568
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.39s
                      Time elapsed: 00:57:00
                               ETA: 00:34:06

################################################################################
                     [1m Learning iteration 1252/2000 [0m                     

                       Computation: 41001 steps/s (collection: 2.277s, learning 0.121s)
             Mean action noise std: 3.12
          Mean value_function loss: 198.7338
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 65.8367
                       Mean reward: 855.19
               Mean episode length: 227.73
    Episode_Reward/reaching_object: 1.5278
     Episode_Reward/lifting_object: 161.1164
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.7083
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 2.40s
                      Time elapsed: 00:57:02
                               ETA: 00:34:03

################################################################################
                     [1m Learning iteration 1253/2000 [0m                     

                       Computation: 41955 steps/s (collection: 2.240s, learning 0.103s)
             Mean action noise std: 3.12
          Mean value_function loss: 165.3225
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 65.8594
                       Mean reward: 816.99
               Mean episode length: 219.58
    Episode_Reward/reaching_object: 1.5628
     Episode_Reward/lifting_object: 165.2970
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.34s
                      Time elapsed: 00:57:05
                               ETA: 00:34:00

################################################################################
                     [1m Learning iteration 1254/2000 [0m                     

                       Computation: 40948 steps/s (collection: 2.271s, learning 0.130s)
             Mean action noise std: 3.12
          Mean value_function loss: 172.4083
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 65.8746
                       Mean reward: 840.10
               Mean episode length: 225.05
    Episode_Reward/reaching_object: 1.6018
     Episode_Reward/lifting_object: 169.9605
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 2.40s
                      Time elapsed: 00:57:07
                               ETA: 00:33:57

################################################################################
                     [1m Learning iteration 1255/2000 [0m                     

                       Computation: 41633 steps/s (collection: 2.223s, learning 0.138s)
             Mean action noise std: 3.12
          Mean value_function loss: 181.7129
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 65.8925
                       Mean reward: 855.21
               Mean episode length: 227.66
    Episode_Reward/reaching_object: 1.5772
     Episode_Reward/lifting_object: 166.6354
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 2.36s
                      Time elapsed: 00:57:09
                               ETA: 00:33:54

################################################################################
                     [1m Learning iteration 1256/2000 [0m                     

                       Computation: 41067 steps/s (collection: 2.275s, learning 0.119s)
             Mean action noise std: 3.12
          Mean value_function loss: 176.5282
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 65.9126
                       Mean reward: 823.31
               Mean episode length: 225.93
    Episode_Reward/reaching_object: 1.5549
     Episode_Reward/lifting_object: 163.3756
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 2.39s
                      Time elapsed: 00:57:12
                               ETA: 00:33:51

################################################################################
                     [1m Learning iteration 1257/2000 [0m                     

                       Computation: 41622 steps/s (collection: 2.241s, learning 0.121s)
             Mean action noise std: 3.13
          Mean value_function loss: 188.5813
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 65.9266
                       Mean reward: 828.69
               Mean episode length: 221.35
    Episode_Reward/reaching_object: 1.5277
     Episode_Reward/lifting_object: 161.1712
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 2.36s
                      Time elapsed: 00:57:14
                               ETA: 00:33:48

################################################################################
                     [1m Learning iteration 1258/2000 [0m                     

                       Computation: 41609 steps/s (collection: 2.220s, learning 0.143s)
             Mean action noise std: 3.13
          Mean value_function loss: 185.9392
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 65.9383
                       Mean reward: 836.87
               Mean episode length: 224.47
    Episode_Reward/reaching_object: 1.5604
     Episode_Reward/lifting_object: 164.4490
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0566
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 2.36s
                      Time elapsed: 00:57:16
                               ETA: 00:33:45

################################################################################
                     [1m Learning iteration 1259/2000 [0m                     

                       Computation: 41941 steps/s (collection: 2.214s, learning 0.130s)
             Mean action noise std: 3.13
          Mean value_function loss: 162.8715
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 65.9550
                       Mean reward: 830.37
               Mean episode length: 222.07
    Episode_Reward/reaching_object: 1.5705
     Episode_Reward/lifting_object: 166.0874
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 2.34s
                      Time elapsed: 00:57:19
                               ETA: 00:33:42

################################################################################
                     [1m Learning iteration 1260/2000 [0m                     

                       Computation: 41523 steps/s (collection: 2.260s, learning 0.107s)
             Mean action noise std: 3.13
          Mean value_function loss: 149.2407
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 65.9712
                       Mean reward: 870.94
               Mean episode length: 231.24
    Episode_Reward/reaching_object: 1.5878
     Episode_Reward/lifting_object: 167.9342
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 2.37s
                      Time elapsed: 00:57:21
                               ETA: 00:33:39

################################################################################
                     [1m Learning iteration 1261/2000 [0m                     

                       Computation: 41122 steps/s (collection: 2.246s, learning 0.145s)
             Mean action noise std: 3.13
          Mean value_function loss: 184.5844
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 65.9822
                       Mean reward: 819.50
               Mean episode length: 220.51
    Episode_Reward/reaching_object: 1.5104
     Episode_Reward/lifting_object: 158.9390
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 2.39s
                      Time elapsed: 00:57:24
                               ETA: 00:33:36

################################################################################
                     [1m Learning iteration 1262/2000 [0m                     

                       Computation: 40980 steps/s (collection: 2.242s, learning 0.157s)
             Mean action noise std: 3.13
          Mean value_function loss: 183.1195
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 65.9957
                       Mean reward: 813.11
               Mean episode length: 218.61
    Episode_Reward/reaching_object: 1.5756
     Episode_Reward/lifting_object: 166.2187
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 2.40s
                      Time elapsed: 00:57:26
                               ETA: 00:33:33

################################################################################
                     [1m Learning iteration 1263/2000 [0m                     

                       Computation: 41568 steps/s (collection: 2.236s, learning 0.129s)
             Mean action noise std: 3.14
          Mean value_function loss: 164.8958
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 66.0079
                       Mean reward: 877.65
               Mean episode length: 232.95
    Episode_Reward/reaching_object: 1.5457
     Episode_Reward/lifting_object: 162.8604
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 2.36s
                      Time elapsed: 00:57:28
                               ETA: 00:33:30

################################################################################
                     [1m Learning iteration 1264/2000 [0m                     

                       Computation: 41481 steps/s (collection: 2.232s, learning 0.138s)
             Mean action noise std: 3.14
          Mean value_function loss: 183.8441
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 66.0148
                       Mean reward: 844.88
               Mean episode length: 226.09
    Episode_Reward/reaching_object: 1.5203
     Episode_Reward/lifting_object: 159.9479
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0555
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 2.37s
                      Time elapsed: 00:57:31
                               ETA: 00:33:27

################################################################################
                     [1m Learning iteration 1265/2000 [0m                     

                       Computation: 42545 steps/s (collection: 2.178s, learning 0.133s)
             Mean action noise std: 3.14
          Mean value_function loss: 176.7989
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 66.0252
                       Mean reward: 864.02
               Mean episode length: 231.37
    Episode_Reward/reaching_object: 1.5378
     Episode_Reward/lifting_object: 161.9353
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 2.31s
                      Time elapsed: 00:57:33
                               ETA: 00:33:24

################################################################################
                     [1m Learning iteration 1266/2000 [0m                     

                       Computation: 42667 steps/s (collection: 2.198s, learning 0.106s)
             Mean action noise std: 3.14
          Mean value_function loss: 154.3666
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 66.0429
                       Mean reward: 848.46
               Mean episode length: 225.94
    Episode_Reward/reaching_object: 1.6136
     Episode_Reward/lifting_object: 170.5444
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 2.30s
                      Time elapsed: 00:57:35
                               ETA: 00:33:21

################################################################################
                     [1m Learning iteration 1267/2000 [0m                     

                       Computation: 41702 steps/s (collection: 2.218s, learning 0.139s)
             Mean action noise std: 3.14
          Mean value_function loss: 185.2817
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 66.0570
                       Mean reward: 838.30
               Mean episode length: 224.22
    Episode_Reward/reaching_object: 1.5659
     Episode_Reward/lifting_object: 165.3083
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 2.36s
                      Time elapsed: 00:57:38
                               ETA: 00:33:19

################################################################################
                     [1m Learning iteration 1268/2000 [0m                     

                       Computation: 40935 steps/s (collection: 2.260s, learning 0.142s)
             Mean action noise std: 3.14
          Mean value_function loss: 159.0926
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 66.0709
                       Mean reward: 820.06
               Mean episode length: 219.03
    Episode_Reward/reaching_object: 1.5662
     Episode_Reward/lifting_object: 165.0529
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 2.40s
                      Time elapsed: 00:57:40
                               ETA: 00:33:16

################################################################################
                     [1m Learning iteration 1269/2000 [0m                     

                       Computation: 38738 steps/s (collection: 2.429s, learning 0.109s)
             Mean action noise std: 3.15
          Mean value_function loss: 200.5458
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.0875
                       Mean reward: 826.82
               Mean episode length: 220.17
    Episode_Reward/reaching_object: 1.5584
     Episode_Reward/lifting_object: 164.9036
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 2.54s
                      Time elapsed: 00:57:43
                               ETA: 00:33:13

################################################################################
                     [1m Learning iteration 1270/2000 [0m                     

                       Computation: 40889 steps/s (collection: 2.274s, learning 0.131s)
             Mean action noise std: 3.15
          Mean value_function loss: 145.5918
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.1012
                       Mean reward: 803.09
               Mean episode length: 217.72
    Episode_Reward/reaching_object: 1.5446
     Episode_Reward/lifting_object: 162.9155
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0562
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 2.40s
                      Time elapsed: 00:57:45
                               ETA: 00:33:10

################################################################################
                     [1m Learning iteration 1271/2000 [0m                     

                       Computation: 41638 steps/s (collection: 2.256s, learning 0.105s)
             Mean action noise std: 3.15
          Mean value_function loss: 185.4968
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.1084
                       Mean reward: 819.98
               Mean episode length: 219.50
    Episode_Reward/reaching_object: 1.5896
     Episode_Reward/lifting_object: 168.0722
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 2.36s
                      Time elapsed: 00:57:47
                               ETA: 00:33:07

################################################################################
                     [1m Learning iteration 1272/2000 [0m                     

                       Computation: 41518 steps/s (collection: 2.259s, learning 0.109s)
             Mean action noise std: 3.15
          Mean value_function loss: 184.3531
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 66.1185
                       Mean reward: 789.09
               Mean episode length: 213.78
    Episode_Reward/reaching_object: 1.5227
     Episode_Reward/lifting_object: 160.0972
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0557
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 2.37s
                      Time elapsed: 00:57:50
                               ETA: 00:33:04

################################################################################
                     [1m Learning iteration 1273/2000 [0m                     

                       Computation: 40768 steps/s (collection: 2.268s, learning 0.143s)
             Mean action noise std: 3.15
          Mean value_function loss: 147.4878
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.1317
                       Mean reward: 804.41
               Mean episode length: 216.16
    Episode_Reward/reaching_object: 1.5674
     Episode_Reward/lifting_object: 165.7596
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 2.41s
                      Time elapsed: 00:57:52
                               ETA: 00:33:01

################################################################################
                     [1m Learning iteration 1274/2000 [0m                     

                       Computation: 41955 steps/s (collection: 2.239s, learning 0.104s)
             Mean action noise std: 3.15
          Mean value_function loss: 160.5190
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 66.1423
                       Mean reward: 854.83
               Mean episode length: 227.87
    Episode_Reward/reaching_object: 1.5812
     Episode_Reward/lifting_object: 167.0438
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 2.34s
                      Time elapsed: 00:57:54
                               ETA: 00:32:58

################################################################################
                     [1m Learning iteration 1275/2000 [0m                     

                       Computation: 41196 steps/s (collection: 2.281s, learning 0.105s)
             Mean action noise std: 3.15
          Mean value_function loss: 155.2970
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 66.1547
                       Mean reward: 832.02
               Mean episode length: 222.14
    Episode_Reward/reaching_object: 1.5832
     Episode_Reward/lifting_object: 167.5882
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 2.39s
                      Time elapsed: 00:57:57
                               ETA: 00:32:55

################################################################################
                     [1m Learning iteration 1276/2000 [0m                     

                       Computation: 40581 steps/s (collection: 2.290s, learning 0.133s)
             Mean action noise std: 3.16
          Mean value_function loss: 182.7534
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 66.1702
                       Mean reward: 816.52
               Mean episode length: 219.75
    Episode_Reward/reaching_object: 1.5594
     Episode_Reward/lifting_object: 164.5655
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 2.42s
                      Time elapsed: 00:57:59
                               ETA: 00:32:52

################################################################################
                     [1m Learning iteration 1277/2000 [0m                     

                       Computation: 40395 steps/s (collection: 2.305s, learning 0.129s)
             Mean action noise std: 3.16
          Mean value_function loss: 172.6212
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 66.1835
                       Mean reward: 820.64
               Mean episode length: 222.01
    Episode_Reward/reaching_object: 1.5865
     Episode_Reward/lifting_object: 167.9191
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 2.43s
                      Time elapsed: 00:58:02
                               ETA: 00:32:49

################################################################################
                     [1m Learning iteration 1278/2000 [0m                     

                       Computation: 42281 steps/s (collection: 2.216s, learning 0.109s)
             Mean action noise std: 3.16
          Mean value_function loss: 174.4352
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 66.1994
                       Mean reward: 839.83
               Mean episode length: 224.55
    Episode_Reward/reaching_object: 1.5500
     Episode_Reward/lifting_object: 163.9751
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0567
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 2.32s
                      Time elapsed: 00:58:04
                               ETA: 00:32:47

################################################################################
                     [1m Learning iteration 1279/2000 [0m                     

                       Computation: 40965 steps/s (collection: 2.304s, learning 0.096s)
             Mean action noise std: 3.16
          Mean value_function loss: 165.3877
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 66.2122
                       Mean reward: 802.08
               Mean episode length: 216.97
    Episode_Reward/reaching_object: 1.5423
     Episode_Reward/lifting_object: 162.8079
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 2.40s
                      Time elapsed: 00:58:06
                               ETA: 00:32:44

################################################################################
                     [1m Learning iteration 1280/2000 [0m                     

                       Computation: 42816 steps/s (collection: 2.185s, learning 0.111s)
             Mean action noise std: 3.16
          Mean value_function loss: 215.7160
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 66.2220
                       Mean reward: 869.75
               Mean episode length: 231.28
    Episode_Reward/reaching_object: 1.5585
     Episode_Reward/lifting_object: 164.9899
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 2.30s
                      Time elapsed: 00:58:09
                               ETA: 00:32:41

################################################################################
                     [1m Learning iteration 1281/2000 [0m                     

                       Computation: 43031 steps/s (collection: 2.180s, learning 0.104s)
             Mean action noise std: 3.17
          Mean value_function loss: 206.4506
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 66.2380
                       Mean reward: 786.01
               Mean episode length: 211.82
    Episode_Reward/reaching_object: 1.4534
     Episode_Reward/lifting_object: 152.9341
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 13.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 2.28s
                      Time elapsed: 00:58:11
                               ETA: 00:32:38

################################################################################
                     [1m Learning iteration 1282/2000 [0m                     

                       Computation: 40403 steps/s (collection: 2.313s, learning 0.120s)
             Mean action noise std: 3.17
          Mean value_function loss: 191.5976
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 66.2524
                       Mean reward: 861.74
               Mean episode length: 228.24
    Episode_Reward/reaching_object: 1.5555
     Episode_Reward/lifting_object: 165.2813
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0569
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 2.43s
                      Time elapsed: 00:58:13
                               ETA: 00:32:35

################################################################################
                     [1m Learning iteration 1283/2000 [0m                     

                       Computation: 40704 steps/s (collection: 2.300s, learning 0.115s)
             Mean action noise std: 3.17
          Mean value_function loss: 130.1966
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 66.2673
                       Mean reward: 865.43
               Mean episode length: 230.19
    Episode_Reward/reaching_object: 1.6244
     Episode_Reward/lifting_object: 172.5058
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.42s
                      Time elapsed: 00:58:16
                               ETA: 00:32:32

################################################################################
                     [1m Learning iteration 1284/2000 [0m                     

                       Computation: 41605 steps/s (collection: 2.248s, learning 0.115s)
             Mean action noise std: 3.17
          Mean value_function loss: 203.8019
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 66.2862
                       Mean reward: 815.79
               Mean episode length: 218.64
    Episode_Reward/reaching_object: 1.5466
     Episode_Reward/lifting_object: 163.8181
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0565
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 2.36s
                      Time elapsed: 00:58:18
                               ETA: 00:32:29

################################################################################
                     [1m Learning iteration 1285/2000 [0m                     

                       Computation: 40089 steps/s (collection: 2.342s, learning 0.111s)
             Mean action noise std: 3.17
          Mean value_function loss: 176.5874
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 66.3071
                       Mean reward: 854.64
               Mean episode length: 226.43
    Episode_Reward/reaching_object: 1.5551
     Episode_Reward/lifting_object: 164.7859
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0568
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.45s
                      Time elapsed: 00:58:21
                               ETA: 00:32:26

################################################################################
                     [1m Learning iteration 1286/2000 [0m                     

                       Computation: 41321 steps/s (collection: 2.241s, learning 0.138s)
             Mean action noise std: 3.17
          Mean value_function loss: 180.7749
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 66.3178
                       Mean reward: 803.20
               Mean episode length: 215.49
    Episode_Reward/reaching_object: 1.5147
     Episode_Reward/lifting_object: 160.1268
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 2.38s
                      Time elapsed: 00:58:23
                               ETA: 00:32:23

################################################################################
                     [1m Learning iteration 1287/2000 [0m                     

                       Computation: 42255 steps/s (collection: 2.212s, learning 0.114s)
             Mean action noise std: 3.18
          Mean value_function loss: 173.4764
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 66.3256
                       Mean reward: 785.36
               Mean episode length: 213.14
    Episode_Reward/reaching_object: 1.5299
     Episode_Reward/lifting_object: 161.5135
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 2.33s
                      Time elapsed: 00:58:25
                               ETA: 00:32:20

################################################################################
                     [1m Learning iteration 1288/2000 [0m                     

                       Computation: 40668 steps/s (collection: 2.294s, learning 0.123s)
             Mean action noise std: 3.18
          Mean value_function loss: 148.1500
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 66.3411
                       Mean reward: 759.91
               Mean episode length: 205.95
    Episode_Reward/reaching_object: 1.5655
     Episode_Reward/lifting_object: 164.8518
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 2.42s
                      Time elapsed: 00:58:28
                               ETA: 00:32:17

################################################################################
                     [1m Learning iteration 1289/2000 [0m                     

                       Computation: 42438 steps/s (collection: 2.193s, learning 0.124s)
             Mean action noise std: 3.18
          Mean value_function loss: 147.1411
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 66.3564
                       Mean reward: 820.32
               Mean episode length: 220.17
    Episode_Reward/reaching_object: 1.6065
     Episode_Reward/lifting_object: 169.6454
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.32s
                      Time elapsed: 00:58:30
                               ETA: 00:32:14

################################################################################
                     [1m Learning iteration 1290/2000 [0m                     

                       Computation: 42641 steps/s (collection: 2.179s, learning 0.127s)
             Mean action noise std: 3.18
          Mean value_function loss: 189.1009
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 66.3678
                       Mean reward: 850.55
               Mean episode length: 226.60
    Episode_Reward/reaching_object: 1.5321
     Episode_Reward/lifting_object: 160.8847
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0559
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 2.31s
                      Time elapsed: 00:58:32
                               ETA: 00:32:11

################################################################################
                     [1m Learning iteration 1291/2000 [0m                     

                       Computation: 42035 steps/s (collection: 2.202s, learning 0.137s)
             Mean action noise std: 3.18
          Mean value_function loss: 143.2008
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.3837
                       Mean reward: 800.46
               Mean episode length: 214.39
    Episode_Reward/reaching_object: 1.5755
     Episode_Reward/lifting_object: 165.4608
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 2.34s
                      Time elapsed: 00:58:35
                               ETA: 00:32:09

################################################################################
                     [1m Learning iteration 1292/2000 [0m                     

                       Computation: 41663 steps/s (collection: 2.227s, learning 0.133s)
             Mean action noise std: 3.18
          Mean value_function loss: 179.2352
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 66.3929
                       Mean reward: 792.94
               Mean episode length: 214.50
    Episode_Reward/reaching_object: 1.5634
     Episode_Reward/lifting_object: 164.2514
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 2.36s
                      Time elapsed: 00:58:37
                               ETA: 00:32:06

################################################################################
                     [1m Learning iteration 1293/2000 [0m                     

                       Computation: 41981 steps/s (collection: 2.236s, learning 0.106s)
             Mean action noise std: 3.19
          Mean value_function loss: 163.5196
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 66.4082
                       Mean reward: 842.05
               Mean episode length: 224.55
    Episode_Reward/reaching_object: 1.5476
     Episode_Reward/lifting_object: 162.3846
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 2.34s
                      Time elapsed: 00:58:39
                               ETA: 00:32:03

################################################################################
                     [1m Learning iteration 1294/2000 [0m                     

                       Computation: 40772 steps/s (collection: 2.277s, learning 0.134s)
             Mean action noise std: 3.19
          Mean value_function loss: 142.6548
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 66.4275
                       Mean reward: 821.36
               Mean episode length: 219.69
    Episode_Reward/reaching_object: 1.5463
     Episode_Reward/lifting_object: 162.0680
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0564
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 2.41s
                      Time elapsed: 00:58:42
                               ETA: 00:32:00

################################################################################
                     [1m Learning iteration 1295/2000 [0m                     

                       Computation: 42488 steps/s (collection: 2.211s, learning 0.103s)
             Mean action noise std: 3.19
          Mean value_function loss: 111.9034
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 66.4355
                       Mean reward: 838.35
               Mean episode length: 223.66
    Episode_Reward/reaching_object: 1.6263
     Episode_Reward/lifting_object: 171.4231
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 2.31s
                      Time elapsed: 00:58:44
                               ETA: 00:31:57

################################################################################
                     [1m Learning iteration 1296/2000 [0m                     

                       Computation: 41222 steps/s (collection: 2.268s, learning 0.117s)
             Mean action noise std: 3.19
          Mean value_function loss: 148.8293
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 66.4484
                       Mean reward: 831.61
               Mean episode length: 222.43
    Episode_Reward/reaching_object: 1.5867
     Episode_Reward/lifting_object: 167.1389
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0575
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 2.38s
                      Time elapsed: 00:58:47
                               ETA: 00:31:54

################################################################################
                     [1m Learning iteration 1297/2000 [0m                     

                       Computation: 40975 steps/s (collection: 2.257s, learning 0.142s)
             Mean action noise std: 3.19
          Mean value_function loss: 136.1414
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 66.4608
                       Mean reward: 849.14
               Mean episode length: 225.78
    Episode_Reward/reaching_object: 1.6267
     Episode_Reward/lifting_object: 171.2251
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 2.40s
                      Time elapsed: 00:58:49
                               ETA: 00:31:51

################################################################################
                     [1m Learning iteration 1298/2000 [0m                     

                       Computation: 42133 steps/s (collection: 2.211s, learning 0.122s)
             Mean action noise std: 3.19
          Mean value_function loss: 155.7310
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 66.4681
                       Mean reward: 836.83
               Mean episode length: 222.46
    Episode_Reward/reaching_object: 1.6086
     Episode_Reward/lifting_object: 169.9818
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0582
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 2.33s
                      Time elapsed: 00:58:51
                               ETA: 00:31:48

################################################################################
                     [1m Learning iteration 1299/2000 [0m                     

                       Computation: 42168 steps/s (collection: 2.206s, learning 0.125s)
             Mean action noise std: 3.20
          Mean value_function loss: 167.7532
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.4799
                       Mean reward: 819.97
               Mean episode length: 218.28
    Episode_Reward/reaching_object: 1.5350
     Episode_Reward/lifting_object: 161.6174
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0558
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 2.33s
                      Time elapsed: 00:58:54
                               ETA: 00:31:45

################################################################################
                     [1m Learning iteration 1300/2000 [0m                     

                       Computation: 41899 steps/s (collection: 2.189s, learning 0.158s)
             Mean action noise std: 3.20
          Mean value_function loss: 139.7380
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.4905
                       Mean reward: 834.58
               Mean episode length: 222.68
    Episode_Reward/reaching_object: 1.5399
     Episode_Reward/lifting_object: 162.3184
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0561
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 2.35s
                      Time elapsed: 00:58:56
                               ETA: 00:31:42

################################################################################
                     [1m Learning iteration 1301/2000 [0m                     

                       Computation: 41891 steps/s (collection: 2.209s, learning 0.138s)
             Mean action noise std: 3.20
          Mean value_function loss: 146.5641
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 66.5038
                       Mean reward: 844.81
               Mean episode length: 225.73
    Episode_Reward/reaching_object: 1.5801
     Episode_Reward/lifting_object: 166.7119
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.35s
                      Time elapsed: 00:58:58
                               ETA: 00:31:39

################################################################################
                     [1m Learning iteration 1302/2000 [0m                     

                       Computation: 42310 steps/s (collection: 2.207s, learning 0.116s)
             Mean action noise std: 3.20
          Mean value_function loss: 111.3762
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 66.5234
                       Mean reward: 856.09
               Mean episode length: 227.45
    Episode_Reward/reaching_object: 1.5853
     Episode_Reward/lifting_object: 167.3244
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.32s
                      Time elapsed: 00:59:01
                               ETA: 00:31:36

################################################################################
                     [1m Learning iteration 1303/2000 [0m                     

                       Computation: 41768 steps/s (collection: 2.215s, learning 0.139s)
             Mean action noise std: 3.20
          Mean value_function loss: 145.2585
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 66.5368
                       Mean reward: 853.90
               Mean episode length: 226.39
    Episode_Reward/reaching_object: 1.5917
     Episode_Reward/lifting_object: 168.0398
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0578
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.35s
                      Time elapsed: 00:59:03
                               ETA: 00:31:34

################################################################################
                     [1m Learning iteration 1304/2000 [0m                     

                       Computation: 41980 steps/s (collection: 2.211s, learning 0.131s)
             Mean action noise std: 3.20
          Mean value_function loss: 163.7016
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 66.5485
                       Mean reward: 858.33
               Mean episode length: 227.59
    Episode_Reward/reaching_object: 1.5707
     Episode_Reward/lifting_object: 165.9335
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0572
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 2.34s
                      Time elapsed: 00:59:05
                               ETA: 00:31:31

################################################################################
                     [1m Learning iteration 1305/2000 [0m                     

                       Computation: 42492 steps/s (collection: 2.179s, learning 0.135s)
             Mean action noise std: 3.21
          Mean value_function loss: 115.6500
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.5626
                       Mean reward: 851.77
               Mean episode length: 227.04
    Episode_Reward/reaching_object: 1.6298
     Episode_Reward/lifting_object: 171.9776
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0594
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 2.31s
                      Time elapsed: 00:59:08
                               ETA: 00:31:28

################################################################################
                     [1m Learning iteration 1306/2000 [0m                     

                       Computation: 41751 steps/s (collection: 2.230s, learning 0.124s)
             Mean action noise std: 3.21
          Mean value_function loss: 128.3866
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 66.5738
                       Mean reward: 866.51
               Mean episode length: 231.35
    Episode_Reward/reaching_object: 1.6454
     Episode_Reward/lifting_object: 173.6810
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.35s
                      Time elapsed: 00:59:10
                               ETA: 00:31:25

################################################################################
                     [1m Learning iteration 1307/2000 [0m                     

                       Computation: 41891 steps/s (collection: 2.214s, learning 0.133s)
             Mean action noise std: 3.21
          Mean value_function loss: 145.6640
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 66.5853
                       Mean reward: 843.16
               Mean episode length: 224.10
    Episode_Reward/reaching_object: 1.6143
     Episode_Reward/lifting_object: 170.9285
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0588
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 2.35s
                      Time elapsed: 00:59:12
                               ETA: 00:31:22

################################################################################
                     [1m Learning iteration 1308/2000 [0m                     

                       Computation: 42378 steps/s (collection: 2.212s, learning 0.108s)
             Mean action noise std: 3.21
          Mean value_function loss: 162.0266
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 66.6009
                       Mean reward: 839.87
               Mean episode length: 224.15
    Episode_Reward/reaching_object: 1.6035
     Episode_Reward/lifting_object: 169.3049
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 2.32s
                      Time elapsed: 00:59:15
                               ETA: 00:31:19

################################################################################
                     [1m Learning iteration 1309/2000 [0m                     

                       Computation: 42073 steps/s (collection: 2.213s, learning 0.124s)
             Mean action noise std: 3.21
          Mean value_function loss: 173.3133
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.6136
                       Mean reward: 865.37
               Mean episode length: 228.56
    Episode_Reward/reaching_object: 1.5532
     Episode_Reward/lifting_object: 163.9818
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0571
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.34s
                      Time elapsed: 00:59:17
                               ETA: 00:31:16

################################################################################
                     [1m Learning iteration 1310/2000 [0m                     

                       Computation: 41283 steps/s (collection: 2.255s, learning 0.127s)
             Mean action noise std: 3.21
          Mean value_function loss: 141.6902
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 66.6258
                       Mean reward: 851.15
               Mean episode length: 226.53
    Episode_Reward/reaching_object: 1.5637
     Episode_Reward/lifting_object: 165.1000
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0576
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.38s
                      Time elapsed: 00:59:19
                               ETA: 00:31:13

################################################################################
                     [1m Learning iteration 1311/2000 [0m                     

                       Computation: 40080 steps/s (collection: 2.299s, learning 0.154s)
             Mean action noise std: 3.22
          Mean value_function loss: 146.9224
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 66.6411
                       Mean reward: 818.86
               Mean episode length: 219.98
    Episode_Reward/reaching_object: 1.5946
     Episode_Reward/lifting_object: 168.5682
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 2.45s
                      Time elapsed: 00:59:22
                               ETA: 00:31:10

################################################################################
                     [1m Learning iteration 1312/2000 [0m                     

                       Computation: 41584 steps/s (collection: 2.222s, learning 0.142s)
             Mean action noise std: 3.22
          Mean value_function loss: 126.6482
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 66.6571
                       Mean reward: 843.20
               Mean episode length: 225.73
    Episode_Reward/reaching_object: 1.5848
     Episode_Reward/lifting_object: 167.0253
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 2.36s
                      Time elapsed: 00:59:24
                               ETA: 00:31:07

################################################################################
                     [1m Learning iteration 1313/2000 [0m                     

                       Computation: 42773 steps/s (collection: 2.170s, learning 0.128s)
             Mean action noise std: 3.22
          Mean value_function loss: 147.9792
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 66.6687
                       Mean reward: 924.81
               Mean episode length: 243.45
    Episode_Reward/reaching_object: 1.6391
     Episode_Reward/lifting_object: 173.1786
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0605
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 2.30s
                      Time elapsed: 00:59:26
                               ETA: 00:31:04

################################################################################
                     [1m Learning iteration 1314/2000 [0m                     

                       Computation: 41245 steps/s (collection: 2.239s, learning 0.145s)
             Mean action noise std: 3.22
          Mean value_function loss: 167.3287
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 66.6787
                       Mean reward: 795.14
               Mean episode length: 213.57
    Episode_Reward/reaching_object: 1.5481
     Episode_Reward/lifting_object: 162.5850
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 2.38s
                      Time elapsed: 00:59:29
                               ETA: 00:31:02

################################################################################
                     [1m Learning iteration 1315/2000 [0m                     

                       Computation: 42074 steps/s (collection: 2.204s, learning 0.133s)
             Mean action noise std: 3.22
          Mean value_function loss: 159.7277
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 66.6961
                       Mean reward: 802.28
               Mean episode length: 215.23
    Episode_Reward/reaching_object: 1.5569
     Episode_Reward/lifting_object: 164.1719
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0576
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 2.34s
                      Time elapsed: 00:59:31
                               ETA: 00:30:59

################################################################################
                     [1m Learning iteration 1316/2000 [0m                     

                       Computation: 41589 steps/s (collection: 2.230s, learning 0.134s)
             Mean action noise std: 3.23
          Mean value_function loss: 148.3003
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 66.7150
                       Mean reward: 780.74
               Mean episode length: 210.30
    Episode_Reward/reaching_object: 1.5491
     Episode_Reward/lifting_object: 162.8895
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0574
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 2.36s
                      Time elapsed: 00:59:34
                               ETA: 00:30:56

################################################################################
                     [1m Learning iteration 1317/2000 [0m                     

                       Computation: 41233 steps/s (collection: 2.253s, learning 0.131s)
             Mean action noise std: 3.23
          Mean value_function loss: 175.7189
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 66.7326
                       Mean reward: 797.10
               Mean episode length: 214.28
    Episode_Reward/reaching_object: 1.5747
     Episode_Reward/lifting_object: 166.0806
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 2.38s
                      Time elapsed: 00:59:36
                               ETA: 00:30:53

################################################################################
                     [1m Learning iteration 1318/2000 [0m                     

                       Computation: 42053 steps/s (collection: 2.201s, learning 0.137s)
             Mean action noise std: 3.23
          Mean value_function loss: 148.1015
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 66.7540
                       Mean reward: 832.33
               Mean episode length: 222.31
    Episode_Reward/reaching_object: 1.5817
     Episode_Reward/lifting_object: 167.1502
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 2.34s
                      Time elapsed: 00:59:38
                               ETA: 00:30:50

################################################################################
                     [1m Learning iteration 1319/2000 [0m                     

                       Computation: 42256 steps/s (collection: 2.204s, learning 0.123s)
             Mean action noise std: 3.23
          Mean value_function loss: 135.9376
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 66.7750
                       Mean reward: 826.00
               Mean episode length: 221.18
    Episode_Reward/reaching_object: 1.5727
     Episode_Reward/lifting_object: 165.5682
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0583
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 2.33s
                      Time elapsed: 00:59:41
                               ETA: 00:30:47

################################################################################
                     [1m Learning iteration 1320/2000 [0m                     

                       Computation: 42462 steps/s (collection: 2.187s, learning 0.128s)
             Mean action noise std: 3.23
          Mean value_function loss: 127.9474
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 66.7906
                       Mean reward: 866.98
               Mean episode length: 231.62
    Episode_Reward/reaching_object: 1.6214
     Episode_Reward/lifting_object: 171.2585
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 2.32s
                      Time elapsed: 00:59:43
                               ETA: 00:30:44

################################################################################
                     [1m Learning iteration 1321/2000 [0m                     

                       Computation: 42183 steps/s (collection: 2.207s, learning 0.123s)
             Mean action noise std: 3.24
          Mean value_function loss: 127.8392
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 66.8096
                       Mean reward: 850.67
               Mean episode length: 228.09
    Episode_Reward/reaching_object: 1.6287
     Episode_Reward/lifting_object: 171.9682
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 2.33s
                      Time elapsed: 00:59:45
                               ETA: 00:30:41

################################################################################
                     [1m Learning iteration 1322/2000 [0m                     

                       Computation: 41573 steps/s (collection: 2.231s, learning 0.134s)
             Mean action noise std: 3.24
          Mean value_function loss: 127.5888
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 66.8306
                       Mean reward: 883.59
               Mean episode length: 233.80
    Episode_Reward/reaching_object: 1.6207
     Episode_Reward/lifting_object: 171.2916
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.36s
                      Time elapsed: 00:59:48
                               ETA: 00:30:38

################################################################################
                     [1m Learning iteration 1323/2000 [0m                     

                       Computation: 40816 steps/s (collection: 2.290s, learning 0.119s)
             Mean action noise std: 3.24
          Mean value_function loss: 126.0925
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.8535
                       Mean reward: 881.06
               Mean episode length: 233.49
    Episode_Reward/reaching_object: 1.6394
     Episode_Reward/lifting_object: 173.4792
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.41s
                      Time elapsed: 00:59:50
                               ETA: 00:30:35

################################################################################
                     [1m Learning iteration 1324/2000 [0m                     

                       Computation: 40586 steps/s (collection: 2.279s, learning 0.143s)
             Mean action noise std: 3.24
          Mean value_function loss: 142.1241
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 66.8676
                       Mean reward: 828.46
               Mean episode length: 221.04
    Episode_Reward/reaching_object: 1.5751
     Episode_Reward/lifting_object: 166.2643
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.42s
                      Time elapsed: 00:59:52
                               ETA: 00:30:33

################################################################################
                     [1m Learning iteration 1325/2000 [0m                     

                       Computation: 39995 steps/s (collection: 2.309s, learning 0.149s)
             Mean action noise std: 3.25
          Mean value_function loss: 139.5361
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 66.8817
                       Mean reward: 797.51
               Mean episode length: 213.76
    Episode_Reward/reaching_object: 1.5517
     Episode_Reward/lifting_object: 163.7041
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.46s
                      Time elapsed: 00:59:55
                               ETA: 00:30:30

################################################################################
                     [1m Learning iteration 1326/2000 [0m                     

                       Computation: 40862 steps/s (collection: 2.282s, learning 0.124s)
             Mean action noise std: 3.25
          Mean value_function loss: 106.2033
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 66.9014
                       Mean reward: 866.85
               Mean episode length: 229.68
    Episode_Reward/reaching_object: 1.6153
     Episode_Reward/lifting_object: 170.7535
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.41s
                      Time elapsed: 00:59:57
                               ETA: 00:30:27

################################################################################
                     [1m Learning iteration 1327/2000 [0m                     

                       Computation: 41146 steps/s (collection: 2.269s, learning 0.121s)
             Mean action noise std: 3.25
          Mean value_function loss: 141.1636
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 66.9184
                       Mean reward: 861.05
               Mean episode length: 228.84
    Episode_Reward/reaching_object: 1.6118
     Episode_Reward/lifting_object: 169.9880
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.39s
                      Time elapsed: 01:00:00
                               ETA: 00:30:24

################################################################################
                     [1m Learning iteration 1328/2000 [0m                     

                       Computation: 40830 steps/s (collection: 2.269s, learning 0.139s)
             Mean action noise std: 3.25
          Mean value_function loss: 122.4198
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 66.9361
                       Mean reward: 843.77
               Mean episode length: 224.75
    Episode_Reward/reaching_object: 1.6052
     Episode_Reward/lifting_object: 169.4056
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 2.41s
                      Time elapsed: 01:00:02
                               ETA: 00:30:21

################################################################################
                     [1m Learning iteration 1329/2000 [0m                     

                       Computation: 41290 steps/s (collection: 2.264s, learning 0.116s)
             Mean action noise std: 3.25
          Mean value_function loss: 148.4074
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 66.9457
                       Mean reward: 839.41
               Mean episode length: 222.98
    Episode_Reward/reaching_object: 1.6099
     Episode_Reward/lifting_object: 170.0702
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 2.38s
                      Time elapsed: 01:00:04
                               ETA: 00:30:18

################################################################################
                     [1m Learning iteration 1330/2000 [0m                     

                       Computation: 40065 steps/s (collection: 2.320s, learning 0.134s)
             Mean action noise std: 3.26
          Mean value_function loss: 147.9733
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 66.9578
                       Mean reward: 812.27
               Mean episode length: 218.17
    Episode_Reward/reaching_object: 1.5626
     Episode_Reward/lifting_object: 164.3851
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0585
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.45s
                      Time elapsed: 01:00:07
                               ETA: 00:30:15

################################################################################
                     [1m Learning iteration 1331/2000 [0m                     

                       Computation: 41950 steps/s (collection: 2.214s, learning 0.130s)
             Mean action noise std: 3.26
          Mean value_function loss: 177.2637
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 66.9702
                       Mean reward: 806.20
               Mean episode length: 216.20
    Episode_Reward/reaching_object: 1.5366
     Episode_Reward/lifting_object: 161.7489
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0577
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 2.34s
                      Time elapsed: 01:00:09
                               ETA: 00:30:13

################################################################################
                     [1m Learning iteration 1332/2000 [0m                     

                       Computation: 42031 steps/s (collection: 2.233s, learning 0.106s)
             Mean action noise std: 3.26
          Mean value_function loss: 154.8763
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 66.9850
                       Mean reward: 841.53
               Mean episode length: 225.62
    Episode_Reward/reaching_object: 1.5410
     Episode_Reward/lifting_object: 162.1949
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0579
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 2.34s
                      Time elapsed: 01:00:12
                               ETA: 00:30:10

################################################################################
                     [1m Learning iteration 1333/2000 [0m                     

                       Computation: 17688 steps/s (collection: 5.418s, learning 0.139s)
             Mean action noise std: 3.26
          Mean value_function loss: 124.5546
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 67.0012
                       Mean reward: 830.02
               Mean episode length: 221.63
    Episode_Reward/reaching_object: 1.5547
     Episode_Reward/lifting_object: 163.9199
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0586
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 5.56s
                      Time elapsed: 01:00:17
                               ETA: 00:30:08

################################################################################
                     [1m Learning iteration 1334/2000 [0m                     

                       Computation: 13422 steps/s (collection: 7.196s, learning 0.128s)
             Mean action noise std: 3.26
          Mean value_function loss: 127.1201
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 67.0187
                       Mean reward: 786.63
               Mean episode length: 210.84
    Episode_Reward/reaching_object: 1.5568
     Episode_Reward/lifting_object: 164.1801
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0587
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 7.32s
                      Time elapsed: 01:00:25
                               ETA: 00:30:08

################################################################################
                     [1m Learning iteration 1335/2000 [0m                     

                       Computation: 13563 steps/s (collection: 7.109s, learning 0.139s)
             Mean action noise std: 3.27
          Mean value_function loss: 117.7345
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 67.0375
                       Mean reward: 873.73
               Mean episode length: 232.14
    Episode_Reward/reaching_object: 1.5913
     Episode_Reward/lifting_object: 168.0501
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 7.25s
                      Time elapsed: 01:00:32
                               ETA: 00:30:07

################################################################################
                     [1m Learning iteration 1336/2000 [0m                     

                       Computation: 13616 steps/s (collection: 7.080s, learning 0.140s)
             Mean action noise std: 3.27
          Mean value_function loss: 141.6608
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 67.0528
                       Mean reward: 845.02
               Mean episode length: 223.38
    Episode_Reward/reaching_object: 1.5715
     Episode_Reward/lifting_object: 166.2558
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0593
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 7.22s
                      Time elapsed: 01:00:39
                               ETA: 00:30:07

################################################################################
                     [1m Learning iteration 1337/2000 [0m                     

                       Computation: 13014 steps/s (collection: 7.425s, learning 0.129s)
             Mean action noise std: 3.27
          Mean value_function loss: 114.1048
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.0764
                       Mean reward: 856.79
               Mean episode length: 227.78
    Episode_Reward/reaching_object: 1.6038
     Episode_Reward/lifting_object: 169.6110
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 7.55s
                      Time elapsed: 01:00:47
                               ETA: 00:30:07

################################################################################
                     [1m Learning iteration 1338/2000 [0m                     

                       Computation: 13931 steps/s (collection: 6.924s, learning 0.133s)
             Mean action noise std: 3.27
          Mean value_function loss: 141.0716
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 67.0865
                       Mean reward: 825.96
               Mean episode length: 221.92
    Episode_Reward/reaching_object: 1.5989
     Episode_Reward/lifting_object: 169.1939
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 7.06s
                      Time elapsed: 01:00:54
                               ETA: 00:30:06

################################################################################
                     [1m Learning iteration 1339/2000 [0m                     

                       Computation: 13255 steps/s (collection: 7.294s, learning 0.123s)
             Mean action noise std: 3.27
          Mean value_function loss: 139.5661
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 67.0913
                       Mean reward: 875.58
               Mean episode length: 232.00
    Episode_Reward/reaching_object: 1.6099
     Episode_Reward/lifting_object: 170.4305
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0610
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 7.42s
                      Time elapsed: 01:01:01
                               ETA: 00:30:06

################################################################################
                     [1m Learning iteration 1340/2000 [0m                     

                       Computation: 13694 steps/s (collection: 7.046s, learning 0.133s)
             Mean action noise std: 3.28
          Mean value_function loss: 136.4776
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 67.1038
                       Mean reward: 851.60
               Mean episode length: 227.20
    Episode_Reward/reaching_object: 1.5936
     Episode_Reward/lifting_object: 168.5932
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0605
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 7.18s
                      Time elapsed: 01:01:08
                               ETA: 00:30:05

################################################################################
                     [1m Learning iteration 1341/2000 [0m                     

                       Computation: 12918 steps/s (collection: 7.484s, learning 0.126s)
             Mean action noise std: 3.28
          Mean value_function loss: 143.9781
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 67.1212
                       Mean reward: 833.23
               Mean episode length: 224.43
    Episode_Reward/reaching_object: 1.5809
     Episode_Reward/lifting_object: 166.7901
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0601
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 7.61s
                      Time elapsed: 01:01:16
                               ETA: 00:30:05

################################################################################
                     [1m Learning iteration 1342/2000 [0m                     

                       Computation: 44376 steps/s (collection: 2.100s, learning 0.115s)
             Mean action noise std: 3.28
          Mean value_function loss: 151.6331
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 67.1312
                       Mean reward: 854.90
               Mean episode length: 227.91
    Episode_Reward/reaching_object: 1.5815
     Episode_Reward/lifting_object: 167.4057
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 2.22s
                      Time elapsed: 01:01:18
                               ETA: 00:30:02

################################################################################
                     [1m Learning iteration 1343/2000 [0m                     

                       Computation: 43361 steps/s (collection: 2.136s, learning 0.131s)
             Mean action noise std: 3.28
          Mean value_function loss: 127.1764
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 67.1436
                       Mean reward: 841.55
               Mean episode length: 223.68
    Episode_Reward/reaching_object: 1.5684
     Episode_Reward/lifting_object: 166.2526
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 2.27s
                      Time elapsed: 01:01:20
                               ETA: 00:29:59

################################################################################
                     [1m Learning iteration 1344/2000 [0m                     

                       Computation: 41571 steps/s (collection: 2.228s, learning 0.137s)
             Mean action noise std: 3.28
          Mean value_function loss: 149.4462
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 67.1609
                       Mean reward: 863.28
               Mean episode length: 230.41
    Episode_Reward/reaching_object: 1.6127
     Episode_Reward/lifting_object: 171.1111
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 2.36s
                      Time elapsed: 01:01:23
                               ETA: 00:29:56

################################################################################
                     [1m Learning iteration 1345/2000 [0m                     

                       Computation: 42965 steps/s (collection: 2.151s, learning 0.137s)
             Mean action noise std: 3.28
          Mean value_function loss: 139.6299
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.1776
                       Mean reward: 841.47
               Mean episode length: 223.84
    Episode_Reward/reaching_object: 1.5798
     Episode_Reward/lifting_object: 167.7952
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 2.29s
                      Time elapsed: 01:01:25
                               ETA: 00:29:53

################################################################################
                     [1m Learning iteration 1346/2000 [0m                     

                       Computation: 42760 steps/s (collection: 2.175s, learning 0.124s)
             Mean action noise std: 3.29
          Mean value_function loss: 131.4720
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.1886
                       Mean reward: 869.91
               Mean episode length: 230.28
    Episode_Reward/reaching_object: 1.5850
     Episode_Reward/lifting_object: 167.9365
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 2.30s
                      Time elapsed: 01:01:27
                               ETA: 00:29:50

################################################################################
                     [1m Learning iteration 1347/2000 [0m                     

                       Computation: 41908 steps/s (collection: 2.202s, learning 0.144s)
             Mean action noise std: 3.29
          Mean value_function loss: 134.0156
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 67.1956
                       Mean reward: 872.93
               Mean episode length: 231.31
    Episode_Reward/reaching_object: 1.6228
     Episode_Reward/lifting_object: 172.7344
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 2.35s
                      Time elapsed: 01:01:30
                               ETA: 00:29:47

################################################################################
                     [1m Learning iteration 1348/2000 [0m                     

                       Computation: 42454 steps/s (collection: 2.180s, learning 0.136s)
             Mean action noise std: 3.29
          Mean value_function loss: 125.1535
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 67.2079
                       Mean reward: 923.32
               Mean episode length: 243.48
    Episode_Reward/reaching_object: 1.6257
     Episode_Reward/lifting_object: 172.9996
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 2.32s
                      Time elapsed: 01:01:32
                               ETA: 00:29:44

################################################################################
                     [1m Learning iteration 1349/2000 [0m                     

                       Computation: 42298 steps/s (collection: 2.190s, learning 0.134s)
             Mean action noise std: 3.29
          Mean value_function loss: 149.1925
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 67.2233
                       Mean reward: 897.47
               Mean episode length: 237.29
    Episode_Reward/reaching_object: 1.5694
     Episode_Reward/lifting_object: 166.5730
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0602
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 2.32s
                      Time elapsed: 01:01:34
                               ETA: 00:29:41

################################################################################
                     [1m Learning iteration 1350/2000 [0m                     

                       Computation: 43333 steps/s (collection: 2.126s, learning 0.143s)
             Mean action noise std: 3.29
          Mean value_function loss: 158.2347
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 67.2317
                       Mean reward: 793.12
               Mean episode length: 212.05
    Episode_Reward/reaching_object: 1.5370
     Episode_Reward/lifting_object: 163.1196
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0592
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 2.27s
                      Time elapsed: 01:01:36
                               ETA: 00:29:38

################################################################################
                     [1m Learning iteration 1351/2000 [0m                     

                       Computation: 42533 steps/s (collection: 2.179s, learning 0.132s)
             Mean action noise std: 3.29
          Mean value_function loss: 182.4888
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 67.2397
                       Mean reward: 790.95
               Mean episode length: 211.71
    Episode_Reward/reaching_object: 1.5251
     Episode_Reward/lifting_object: 161.6550
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0589
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 2.31s
                      Time elapsed: 01:01:39
                               ETA: 00:29:35

################################################################################
                     [1m Learning iteration 1352/2000 [0m                     

                       Computation: 43181 steps/s (collection: 2.144s, learning 0.132s)
             Mean action noise std: 3.29
          Mean value_function loss: 134.4650
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 67.2495
                       Mean reward: 823.94
               Mean episode length: 220.95
    Episode_Reward/reaching_object: 1.6005
     Episode_Reward/lifting_object: 169.8839
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 2.28s
                      Time elapsed: 01:01:41
                               ETA: 00:29:32

################################################################################
                     [1m Learning iteration 1353/2000 [0m                     

                       Computation: 42964 steps/s (collection: 2.155s, learning 0.133s)
             Mean action noise std: 3.30
          Mean value_function loss: 104.4374
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 67.2643
                       Mean reward: 875.14
               Mean episode length: 231.30
    Episode_Reward/reaching_object: 1.6210
     Episode_Reward/lifting_object: 172.6794
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 2.29s
                      Time elapsed: 01:01:43
                               ETA: 00:29:29

################################################################################
                     [1m Learning iteration 1354/2000 [0m                     

                       Computation: 43307 steps/s (collection: 2.159s, learning 0.111s)
             Mean action noise std: 3.30
          Mean value_function loss: 124.3943
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 67.2749
                       Mean reward: 867.99
               Mean episode length: 230.05
    Episode_Reward/reaching_object: 1.5549
     Episode_Reward/lifting_object: 165.3714
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0600
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 2.27s
                      Time elapsed: 01:01:46
                               ETA: 00:29:26

################################################################################
                     [1m Learning iteration 1355/2000 [0m                     

                       Computation: 42889 steps/s (collection: 2.174s, learning 0.118s)
             Mean action noise std: 3.30
          Mean value_function loss: 116.3355
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 67.2852
                       Mean reward: 880.64
               Mean episode length: 233.84
    Episode_Reward/reaching_object: 1.6168
     Episode_Reward/lifting_object: 172.2536
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 2.29s
                      Time elapsed: 01:01:48
                               ETA: 00:29:23

################################################################################
                     [1m Learning iteration 1356/2000 [0m                     

                       Computation: 42902 steps/s (collection: 2.161s, learning 0.130s)
             Mean action noise std: 3.30
          Mean value_function loss: 165.8444
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 67.2960
                       Mean reward: 811.95
               Mean episode length: 218.26
    Episode_Reward/reaching_object: 1.5696
     Episode_Reward/lifting_object: 166.9576
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 2.29s
                      Time elapsed: 01:01:50
                               ETA: 00:29:21

################################################################################
                     [1m Learning iteration 1357/2000 [0m                     

                       Computation: 42804 steps/s (collection: 2.163s, learning 0.133s)
             Mean action noise std: 3.30
          Mean value_function loss: 164.9186
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 67.3089
                       Mean reward: 822.49
               Mean episode length: 221.29
    Episode_Reward/reaching_object: 1.5228
     Episode_Reward/lifting_object: 161.8718
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0590
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 2.30s
                      Time elapsed: 01:01:53
                               ETA: 00:29:18

################################################################################
                     [1m Learning iteration 1358/2000 [0m                     

                       Computation: 41301 steps/s (collection: 2.238s, learning 0.142s)
             Mean action noise std: 3.30
          Mean value_function loss: 119.9147
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.3212
                       Mean reward: 856.17
               Mean episode length: 227.82
    Episode_Reward/reaching_object: 1.5826
     Episode_Reward/lifting_object: 168.9525
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 2.38s
                      Time elapsed: 01:01:55
                               ETA: 00:29:15

################################################################################
                     [1m Learning iteration 1359/2000 [0m                     

                       Computation: 43065 steps/s (collection: 2.178s, learning 0.105s)
             Mean action noise std: 3.30
          Mean value_function loss: 132.0590
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 67.3327
                       Mean reward: 844.09
               Mean episode length: 226.09
    Episode_Reward/reaching_object: 1.5846
     Episode_Reward/lifting_object: 168.8427
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 2.28s
                      Time elapsed: 01:01:57
                               ETA: 00:29:12

################################################################################
                     [1m Learning iteration 1360/2000 [0m                     

                       Computation: 43053 steps/s (collection: 2.168s, learning 0.115s)
             Mean action noise std: 3.31
          Mean value_function loss: 140.8265
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.3422
                       Mean reward: 805.47
               Mean episode length: 215.54
    Episode_Reward/reaching_object: 1.5647
     Episode_Reward/lifting_object: 166.9152
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 2.28s
                      Time elapsed: 01:01:59
                               ETA: 00:29:09

################################################################################
                     [1m Learning iteration 1361/2000 [0m                     

                       Computation: 42758 steps/s (collection: 2.176s, learning 0.123s)
             Mean action noise std: 3.31
          Mean value_function loss: 143.0574
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 67.3543
                       Mean reward: 816.58
               Mean episode length: 217.73
    Episode_Reward/reaching_object: 1.5834
     Episode_Reward/lifting_object: 169.3685
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0613
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 2.30s
                      Time elapsed: 01:02:02
                               ETA: 00:29:06

################################################################################
                     [1m Learning iteration 1362/2000 [0m                     

                       Computation: 42200 steps/s (collection: 2.200s, learning 0.129s)
             Mean action noise std: 3.31
          Mean value_function loss: 135.6614
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 67.3719
                       Mean reward: 871.54
               Mean episode length: 231.49
    Episode_Reward/reaching_object: 1.5661
     Episode_Reward/lifting_object: 166.9885
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 2.33s
                      Time elapsed: 01:02:04
                               ETA: 00:29:03

################################################################################
                     [1m Learning iteration 1363/2000 [0m                     

                       Computation: 41918 steps/s (collection: 2.209s, learning 0.137s)
             Mean action noise std: 3.31
          Mean value_function loss: 151.5230
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 67.3846
                       Mean reward: 812.12
               Mean episode length: 217.33
    Episode_Reward/reaching_object: 1.5387
     Episode_Reward/lifting_object: 163.9941
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0598
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 2.35s
                      Time elapsed: 01:02:06
                               ETA: 00:29:00

################################################################################
                     [1m Learning iteration 1364/2000 [0m                     

                       Computation: 42744 steps/s (collection: 2.168s, learning 0.132s)
             Mean action noise std: 3.31
          Mean value_function loss: 135.6067
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 67.3932
                       Mean reward: 886.35
               Mean episode length: 235.64
    Episode_Reward/reaching_object: 1.5706
     Episode_Reward/lifting_object: 167.7101
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 2.30s
                      Time elapsed: 01:02:09
                               ETA: 00:28:57

################################################################################
                     [1m Learning iteration 1365/2000 [0m                     

                       Computation: 42454 steps/s (collection: 2.184s, learning 0.131s)
             Mean action noise std: 3.31
          Mean value_function loss: 145.4681
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 67.4033
                       Mean reward: 860.29
               Mean episode length: 228.77
    Episode_Reward/reaching_object: 1.5622
     Episode_Reward/lifting_object: 166.8485
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0608
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 2.32s
                      Time elapsed: 01:02:11
                               ETA: 00:28:54

################################################################################
                     [1m Learning iteration 1366/2000 [0m                     

                       Computation: 42040 steps/s (collection: 2.193s, learning 0.145s)
             Mean action noise std: 3.32
          Mean value_function loss: 157.0561
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 67.4194
                       Mean reward: 821.28
               Mean episode length: 219.55
    Episode_Reward/reaching_object: 1.5575
     Episode_Reward/lifting_object: 165.9720
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 2.34s
                      Time elapsed: 01:02:13
                               ETA: 00:28:51

################################################################################
                     [1m Learning iteration 1367/2000 [0m                     

                       Computation: 42145 steps/s (collection: 2.195s, learning 0.138s)
             Mean action noise std: 3.32
          Mean value_function loss: 152.0756
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 67.4337
                       Mean reward: 770.24
               Mean episode length: 208.81
    Episode_Reward/reaching_object: 1.5400
     Episode_Reward/lifting_object: 164.3247
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0603
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 2.33s
                      Time elapsed: 01:02:16
                               ETA: 00:28:48

################################################################################
                     [1m Learning iteration 1368/2000 [0m                     

                       Computation: 41895 steps/s (collection: 2.210s, learning 0.136s)
             Mean action noise std: 3.32
          Mean value_function loss: 129.0999
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 67.4424
                       Mean reward: 876.61
               Mean episode length: 232.51
    Episode_Reward/reaching_object: 1.5899
     Episode_Reward/lifting_object: 169.9066
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0620
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 2.35s
                      Time elapsed: 01:02:18
                               ETA: 00:28:45

################################################################################
                     [1m Learning iteration 1369/2000 [0m                     

                       Computation: 42020 steps/s (collection: 2.205s, learning 0.135s)
             Mean action noise std: 3.32
          Mean value_function loss: 144.4318
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 67.4509
                       Mean reward: 825.75
               Mean episode length: 221.10
    Episode_Reward/reaching_object: 1.5729
     Episode_Reward/lifting_object: 167.7980
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0614
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 2.34s
                      Time elapsed: 01:02:20
                               ETA: 00:28:42

################################################################################
                     [1m Learning iteration 1370/2000 [0m                     

                       Computation: 42455 steps/s (collection: 2.189s, learning 0.127s)
             Mean action noise std: 3.32
          Mean value_function loss: 175.8613
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 67.4625
                       Mean reward: 837.30
               Mean episode length: 223.44
    Episode_Reward/reaching_object: 1.5511
     Episode_Reward/lifting_object: 165.3638
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0606
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.1250
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 2.32s
                      Time elapsed: 01:02:23
                               ETA: 00:28:40

################################################################################
                     [1m Learning iteration 1371/2000 [0m                     

                       Computation: 42658 steps/s (collection: 2.155s, learning 0.150s)
             Mean action noise std: 3.32
          Mean value_function loss: 166.4648
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 67.4778
                       Mean reward: 834.06
               Mean episode length: 222.42
    Episode_Reward/reaching_object: 1.5702
     Episode_Reward/lifting_object: 167.8606
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 2.30s
                      Time elapsed: 01:02:25
                               ETA: 00:28:37

################################################################################
                     [1m Learning iteration 1372/2000 [0m                     

                       Computation: 42676 steps/s (collection: 2.166s, learning 0.137s)
             Mean action noise std: 3.33
          Mean value_function loss: 147.8212
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 67.4920
                       Mean reward: 847.64
               Mean episode length: 226.41
    Episode_Reward/reaching_object: 1.5877
     Episode_Reward/lifting_object: 169.2251
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 2.30s
                      Time elapsed: 01:02:27
                               ETA: 00:28:34

################################################################################
                     [1m Learning iteration 1373/2000 [0m                     

                       Computation: 42598 steps/s (collection: 2.184s, learning 0.124s)
             Mean action noise std: 3.33
          Mean value_function loss: 129.7829
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 67.5021
                       Mean reward: 911.65
               Mean episode length: 240.07
    Episode_Reward/reaching_object: 1.5862
     Episode_Reward/lifting_object: 169.6284
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 2.31s
                      Time elapsed: 01:02:30
                               ETA: 00:28:31

################################################################################
                     [1m Learning iteration 1374/2000 [0m                     

                       Computation: 42468 steps/s (collection: 2.191s, learning 0.124s)
             Mean action noise std: 3.33
          Mean value_function loss: 109.5889
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 67.5186
                       Mean reward: 884.34
               Mean episode length: 235.22
    Episode_Reward/reaching_object: 1.6329
     Episode_Reward/lifting_object: 174.5970
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0637
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 2.31s
                      Time elapsed: 01:02:32
                               ETA: 00:28:28

################################################################################
                     [1m Learning iteration 1375/2000 [0m                     

                       Computation: 43409 steps/s (collection: 2.148s, learning 0.117s)
             Mean action noise std: 3.33
          Mean value_function loss: 132.9125
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 67.5341
                       Mean reward: 880.06
               Mean episode length: 233.80
    Episode_Reward/reaching_object: 1.5585
     Episode_Reward/lifting_object: 166.1136
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0612
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 2.26s
                      Time elapsed: 01:02:34
                               ETA: 00:28:25

################################################################################
                     [1m Learning iteration 1376/2000 [0m                     

                       Computation: 42998 steps/s (collection: 2.151s, learning 0.136s)
             Mean action noise std: 3.33
          Mean value_function loss: 149.6991
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 67.5479
                       Mean reward: 839.66
               Mean episode length: 223.44
    Episode_Reward/reaching_object: 1.5529
     Episode_Reward/lifting_object: 165.5839
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0609
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 2.29s
                      Time elapsed: 01:02:36
                               ETA: 00:28:22

################################################################################
                     [1m Learning iteration 1377/2000 [0m                     

                       Computation: 41961 steps/s (collection: 2.203s, learning 0.140s)
             Mean action noise std: 3.33
          Mean value_function loss: 125.7701
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.5541
                       Mean reward: 854.65
               Mean episode length: 227.40
    Episode_Reward/reaching_object: 1.5812
     Episode_Reward/lifting_object: 168.9510
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0619
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 2.34s
                      Time elapsed: 01:02:39
                               ETA: 00:28:19

################################################################################
                     [1m Learning iteration 1378/2000 [0m                     

                       Computation: 42876 steps/s (collection: 2.167s, learning 0.126s)
             Mean action noise std: 3.33
          Mean value_function loss: 128.5888
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 67.5635
                       Mean reward: 891.86
               Mean episode length: 235.49
    Episode_Reward/reaching_object: 1.6432
     Episode_Reward/lifting_object: 175.9188
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 2.29s
                      Time elapsed: 01:02:41
                               ETA: 00:28:16

################################################################################
                     [1m Learning iteration 1379/2000 [0m                     

                       Computation: 41424 steps/s (collection: 2.233s, learning 0.140s)
             Mean action noise std: 3.34
          Mean value_function loss: 79.5599
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 67.5758
                       Mean reward: 885.09
               Mean episode length: 235.15
    Episode_Reward/reaching_object: 1.6230
     Episode_Reward/lifting_object: 173.1194
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 2.37s
                      Time elapsed: 01:02:44
                               ETA: 00:28:13

################################################################################
                     [1m Learning iteration 1380/2000 [0m                     

                       Computation: 41955 steps/s (collection: 2.207s, learning 0.136s)
             Mean action noise std: 3.34
          Mean value_function loss: 120.9710
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 67.5850
                       Mean reward: 836.14
               Mean episode length: 222.85
    Episode_Reward/reaching_object: 1.6087
     Episode_Reward/lifting_object: 171.7320
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 2.34s
                      Time elapsed: 01:02:46
                               ETA: 00:28:10

################################################################################
                     [1m Learning iteration 1381/2000 [0m                     

                       Computation: 42760 steps/s (collection: 2.161s, learning 0.138s)
             Mean action noise std: 3.34
          Mean value_function loss: 131.5033
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 67.5945
                       Mean reward: 811.82
               Mean episode length: 217.48
    Episode_Reward/reaching_object: 1.5873
     Episode_Reward/lifting_object: 169.1350
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 2.30s
                      Time elapsed: 01:02:48
                               ETA: 00:28:07

################################################################################
                     [1m Learning iteration 1382/2000 [0m                     

                       Computation: 43579 steps/s (collection: 2.131s, learning 0.125s)
             Mean action noise std: 3.34
          Mean value_function loss: 128.2215
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 67.6145
                       Mean reward: 887.91
               Mean episode length: 234.57
    Episode_Reward/reaching_object: 1.5805
     Episode_Reward/lifting_object: 168.6881
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0621
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 2.26s
                      Time elapsed: 01:02:50
                               ETA: 00:28:05

################################################################################
                     [1m Learning iteration 1383/2000 [0m                     

                       Computation: 43331 steps/s (collection: 2.146s, learning 0.123s)
             Mean action noise std: 3.34
          Mean value_function loss: 114.3949
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.6324
                       Mean reward: 837.26
               Mean episode length: 223.68
    Episode_Reward/reaching_object: 1.6361
     Episode_Reward/lifting_object: 174.9426
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 2.27s
                      Time elapsed: 01:02:53
                               ETA: 00:28:02

################################################################################
                     [1m Learning iteration 1384/2000 [0m                     

                       Computation: 42555 steps/s (collection: 2.160s, learning 0.150s)
             Mean action noise std: 3.34
          Mean value_function loss: 126.2668
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 67.6419
                       Mean reward: 884.75
               Mean episode length: 235.42
    Episode_Reward/reaching_object: 1.6083
     Episode_Reward/lifting_object: 171.5164
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 2.31s
                      Time elapsed: 01:02:55
                               ETA: 00:27:59

################################################################################
                     [1m Learning iteration 1385/2000 [0m                     

                       Computation: 41456 steps/s (collection: 2.228s, learning 0.144s)
             Mean action noise std: 3.35
          Mean value_function loss: 111.2096
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 67.6578
                       Mean reward: 845.41
               Mean episode length: 226.24
    Episode_Reward/reaching_object: 1.5652
     Episode_Reward/lifting_object: 166.8234
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0617
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 2.37s
                      Time elapsed: 01:02:57
                               ETA: 00:27:56

################################################################################
                     [1m Learning iteration 1386/2000 [0m                     

                       Computation: 40833 steps/s (collection: 2.289s, learning 0.118s)
             Mean action noise std: 3.35
          Mean value_function loss: 129.6835
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 67.6701
                       Mean reward: 827.00
               Mean episode length: 221.43
    Episode_Reward/reaching_object: 1.5575
     Episode_Reward/lifting_object: 166.1891
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 2.41s
                      Time elapsed: 01:03:00
                               ETA: 00:27:53

################################################################################
                     [1m Learning iteration 1387/2000 [0m                     

                       Computation: 39935 steps/s (collection: 2.323s, learning 0.139s)
             Mean action noise std: 3.35
          Mean value_function loss: 128.2750
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 67.6828
                       Mean reward: 839.81
               Mean episode length: 223.52
    Episode_Reward/reaching_object: 1.5987
     Episode_Reward/lifting_object: 171.1432
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 2.46s
                      Time elapsed: 01:03:02
                               ETA: 00:27:50

################################################################################
                     [1m Learning iteration 1388/2000 [0m                     

                       Computation: 40357 steps/s (collection: 2.302s, learning 0.134s)
             Mean action noise std: 3.35
          Mean value_function loss: 132.0463
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 67.6987
                       Mean reward: 885.39
               Mean episode length: 234.53
    Episode_Reward/reaching_object: 1.5811
     Episode_Reward/lifting_object: 169.1858
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0624
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 2.44s
                      Time elapsed: 01:03:05
                               ETA: 00:27:47

################################################################################
                     [1m Learning iteration 1389/2000 [0m                     

                       Computation: 42048 steps/s (collection: 2.225s, learning 0.113s)
             Mean action noise std: 3.35
          Mean value_function loss: 131.9992
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 67.7122
                       Mean reward: 820.78
               Mean episode length: 219.43
    Episode_Reward/reaching_object: 1.5682
     Episode_Reward/lifting_object: 167.9047
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 2.34s
                      Time elapsed: 01:03:07
                               ETA: 00:27:44

################################################################################
                     [1m Learning iteration 1390/2000 [0m                     

                       Computation: 42359 steps/s (collection: 2.194s, learning 0.127s)
             Mean action noise std: 3.36
          Mean value_function loss: 111.4242
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.7250
                       Mean reward: 808.08
               Mean episode length: 216.26
    Episode_Reward/reaching_object: 1.5804
     Episode_Reward/lifting_object: 169.8986
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 2.32s
                      Time elapsed: 01:03:09
                               ETA: 00:27:41

################################################################################
                     [1m Learning iteration 1391/2000 [0m                     

                       Computation: 42144 steps/s (collection: 2.190s, learning 0.143s)
             Mean action noise std: 3.36
          Mean value_function loss: 127.1313
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 67.7336
                       Mean reward: 825.44
               Mean episode length: 219.97
    Episode_Reward/reaching_object: 1.5804
     Episode_Reward/lifting_object: 169.6928
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 2.33s
                      Time elapsed: 01:03:12
                               ETA: 00:27:39

################################################################################
                     [1m Learning iteration 1392/2000 [0m                     

                       Computation: 42839 steps/s (collection: 2.181s, learning 0.114s)
             Mean action noise std: 3.36
          Mean value_function loss: 131.3185
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 67.7460
                       Mean reward: 860.24
               Mean episode length: 229.03
    Episode_Reward/reaching_object: 1.6102
     Episode_Reward/lifting_object: 173.1075
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0641
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 2.29s
                      Time elapsed: 01:03:14
                               ETA: 00:27:36

################################################################################
                     [1m Learning iteration 1393/2000 [0m                     

                       Computation: 43934 steps/s (collection: 2.128s, learning 0.109s)
             Mean action noise std: 3.36
          Mean value_function loss: 113.2832
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 67.7611
                       Mean reward: 853.11
               Mean episode length: 226.27
    Episode_Reward/reaching_object: 1.6022
     Episode_Reward/lifting_object: 172.7913
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 2.24s
                      Time elapsed: 01:03:16
                               ETA: 00:27:33

################################################################################
                     [1m Learning iteration 1394/2000 [0m                     

                       Computation: 43423 steps/s (collection: 2.139s, learning 0.125s)
             Mean action noise std: 3.36
          Mean value_function loss: 151.5420
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 67.7716
                       Mean reward: 860.42
               Mean episode length: 228.35
    Episode_Reward/reaching_object: 1.5889
     Episode_Reward/lifting_object: 171.5724
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0634
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 2.26s
                      Time elapsed: 01:03:18
                               ETA: 00:27:30

################################################################################
                     [1m Learning iteration 1395/2000 [0m                     

                       Computation: 41993 steps/s (collection: 2.223s, learning 0.118s)
             Mean action noise std: 3.36
          Mean value_function loss: 154.0540
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 67.7854
                       Mean reward: 875.50
               Mean episode length: 232.18
    Episode_Reward/reaching_object: 1.5950
     Episode_Reward/lifting_object: 172.0631
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 2.34s
                      Time elapsed: 01:03:21
                               ETA: 00:27:27

################################################################################
                     [1m Learning iteration 1396/2000 [0m                     

                       Computation: 42847 steps/s (collection: 2.164s, learning 0.131s)
             Mean action noise std: 3.37
          Mean value_function loss: 141.9327
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 67.8030
                       Mean reward: 881.05
               Mean episode length: 233.85
    Episode_Reward/reaching_object: 1.5849
     Episode_Reward/lifting_object: 170.8805
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 2.29s
                      Time elapsed: 01:03:23
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 1397/2000 [0m                     

                       Computation: 41274 steps/s (collection: 2.261s, learning 0.121s)
             Mean action noise std: 3.37
          Mean value_function loss: 129.5164
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 67.8193
                       Mean reward: 833.13
               Mean episode length: 221.83
    Episode_Reward/reaching_object: 1.5693
     Episode_Reward/lifting_object: 169.3357
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0629
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 2.38s
                      Time elapsed: 01:03:25
                               ETA: 00:27:21

################################################################################
                     [1m Learning iteration 1398/2000 [0m                     

                       Computation: 42554 steps/s (collection: 2.182s, learning 0.128s)
             Mean action noise std: 3.37
          Mean value_function loss: 130.5047
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 67.8419
                       Mean reward: 878.14
               Mean episode length: 232.00
    Episode_Reward/reaching_object: 1.6144
     Episode_Reward/lifting_object: 174.7206
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 2.31s
                      Time elapsed: 01:03:28
                               ETA: 00:27:18

################################################################################
                     [1m Learning iteration 1399/2000 [0m                     

                       Computation: 42068 steps/s (collection: 2.197s, learning 0.140s)
             Mean action noise std: 3.37
          Mean value_function loss: 150.5672
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 67.8574
                       Mean reward: 842.09
               Mean episode length: 224.45
    Episode_Reward/reaching_object: 1.5500
     Episode_Reward/lifting_object: 167.3595
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0625
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 2.34s
                      Time elapsed: 01:03:30
                               ETA: 00:27:15

################################################################################
                     [1m Learning iteration 1400/2000 [0m                     

                       Computation: 42301 steps/s (collection: 2.199s, learning 0.125s)
             Mean action noise std: 3.37
          Mean value_function loss: 132.4789
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 67.8689
                       Mean reward: 793.56
               Mean episode length: 214.03
    Episode_Reward/reaching_object: 1.5728
     Episode_Reward/lifting_object: 169.7985
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 2.32s
                      Time elapsed: 01:03:32
                               ETA: 00:27:12

################################################################################
                     [1m Learning iteration 1401/2000 [0m                     

                       Computation: 42123 steps/s (collection: 2.201s, learning 0.133s)
             Mean action noise std: 3.38
          Mean value_function loss: 151.3774
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 67.8836
                       Mean reward: 865.66
               Mean episode length: 229.62
    Episode_Reward/reaching_object: 1.5563
     Episode_Reward/lifting_object: 167.8095
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 2.33s
                      Time elapsed: 01:03:35
                               ETA: 00:27:10

################################################################################
                     [1m Learning iteration 1402/2000 [0m                     

                       Computation: 42713 steps/s (collection: 2.170s, learning 0.132s)
             Mean action noise std: 3.38
          Mean value_function loss: 133.4305
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 67.9001
                       Mean reward: 842.01
               Mean episode length: 224.64
    Episode_Reward/reaching_object: 1.5708
     Episode_Reward/lifting_object: 169.2952
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 2.30s
                      Time elapsed: 01:03:37
                               ETA: 00:27:07

################################################################################
                     [1m Learning iteration 1403/2000 [0m                     

                       Computation: 43230 steps/s (collection: 2.148s, learning 0.126s)
             Mean action noise std: 3.38
          Mean value_function loss: 169.3806
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 67.9158
                       Mean reward: 842.34
               Mean episode length: 224.69
    Episode_Reward/reaching_object: 1.5498
     Episode_Reward/lifting_object: 166.8394
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0627
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 2.27s
                      Time elapsed: 01:03:39
                               ETA: 00:27:04

################################################################################
                     [1m Learning iteration 1404/2000 [0m                     

                       Computation: 41968 steps/s (collection: 2.201s, learning 0.141s)
             Mean action noise std: 3.38
          Mean value_function loss: 164.6525
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.9220
                       Mean reward: 841.80
               Mean episode length: 223.77
    Episode_Reward/reaching_object: 1.5219
     Episode_Reward/lifting_object: 163.6754
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 2.34s
                      Time elapsed: 01:03:42
                               ETA: 00:27:01

################################################################################
                     [1m Learning iteration 1405/2000 [0m                     

                       Computation: 42467 steps/s (collection: 2.193s, learning 0.122s)
             Mean action noise std: 3.38
          Mean value_function loss: 132.7919
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 67.9333
                       Mean reward: 813.04
               Mean episode length: 217.60
    Episode_Reward/reaching_object: 1.5515
     Episode_Reward/lifting_object: 166.7759
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 2.31s
                      Time elapsed: 01:03:44
                               ETA: 00:26:58

################################################################################
                     [1m Learning iteration 1406/2000 [0m                     

                       Computation: 41472 steps/s (collection: 2.241s, learning 0.130s)
             Mean action noise std: 3.39
          Mean value_function loss: 130.0977
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 67.9518
                       Mean reward: 834.78
               Mean episode length: 222.05
    Episode_Reward/reaching_object: 1.5639
     Episode_Reward/lifting_object: 168.3460
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 2.37s
                      Time elapsed: 01:03:46
                               ETA: 00:26:55

################################################################################
                     [1m Learning iteration 1407/2000 [0m                     

                       Computation: 41617 steps/s (collection: 2.213s, learning 0.149s)
             Mean action noise std: 3.39
          Mean value_function loss: 101.3005
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 67.9699
                       Mean reward: 886.45
               Mean episode length: 234.81
    Episode_Reward/reaching_object: 1.5838
     Episode_Reward/lifting_object: 170.5777
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 2.36s
                      Time elapsed: 01:03:49
                               ETA: 00:26:52

################################################################################
                     [1m Learning iteration 1408/2000 [0m                     

                       Computation: 39998 steps/s (collection: 2.331s, learning 0.127s)
             Mean action noise std: 3.39
          Mean value_function loss: 128.2528
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.9890
                       Mean reward: 820.10
               Mean episode length: 218.14
    Episode_Reward/reaching_object: 1.5631
     Episode_Reward/lifting_object: 168.0127
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 2.46s
                      Time elapsed: 01:03:51
                               ETA: 00:26:49

################################################################################
                     [1m Learning iteration 1409/2000 [0m                     

                       Computation: 41809 steps/s (collection: 2.212s, learning 0.140s)
             Mean action noise std: 3.39
          Mean value_function loss: 115.9319
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 68.0040
                       Mean reward: 870.33
               Mean episode length: 230.64
    Episode_Reward/reaching_object: 1.5727
     Episode_Reward/lifting_object: 169.1329
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0639
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 2.35s
                      Time elapsed: 01:03:54
                               ETA: 00:26:47

################################################################################
                     [1m Learning iteration 1410/2000 [0m                     

                       Computation: 38754 steps/s (collection: 2.405s, learning 0.132s)
             Mean action noise std: 3.39
          Mean value_function loss: 124.2666
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 68.0184
                       Mean reward: 798.18
               Mean episode length: 213.80
    Episode_Reward/reaching_object: 1.5602
     Episode_Reward/lifting_object: 167.6373
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 2.54s
                      Time elapsed: 01:03:56
                               ETA: 00:26:44

################################################################################
                     [1m Learning iteration 1411/2000 [0m                     

                       Computation: 41954 steps/s (collection: 2.234s, learning 0.110s)
             Mean action noise std: 3.40
          Mean value_function loss: 128.2207
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.0265
                       Mean reward: 824.44
               Mean episode length: 219.95
    Episode_Reward/reaching_object: 1.5526
     Episode_Reward/lifting_object: 166.6529
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 2.34s
                      Time elapsed: 01:03:58
                               ETA: 00:26:41

################################################################################
                     [1m Learning iteration 1412/2000 [0m                     

                       Computation: 42329 steps/s (collection: 2.218s, learning 0.104s)
             Mean action noise std: 3.40
          Mean value_function loss: 121.3656
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 68.0394
                       Mean reward: 854.53
               Mean episode length: 227.65
    Episode_Reward/reaching_object: 1.6054
     Episode_Reward/lifting_object: 172.5843
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 2.32s
                      Time elapsed: 01:04:01
                               ETA: 00:26:38

################################################################################
                     [1m Learning iteration 1413/2000 [0m                     

                       Computation: 42218 steps/s (collection: 2.200s, learning 0.128s)
             Mean action noise std: 3.40
          Mean value_function loss: 141.3794
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 68.0521
                       Mean reward: 864.43
               Mean episode length: 228.71
    Episode_Reward/reaching_object: 1.5586
     Episode_Reward/lifting_object: 167.4080
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 2.33s
                      Time elapsed: 01:04:03
                               ETA: 00:26:35

################################################################################
                     [1m Learning iteration 1414/2000 [0m                     

                       Computation: 42254 steps/s (collection: 2.216s, learning 0.111s)
             Mean action noise std: 3.40
          Mean value_function loss: 126.5667
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 68.0640
                       Mean reward: 881.08
               Mean episode length: 233.14
    Episode_Reward/reaching_object: 1.5733
     Episode_Reward/lifting_object: 169.5054
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 2.33s
                      Time elapsed: 01:04:05
                               ETA: 00:26:32

################################################################################
                     [1m Learning iteration 1415/2000 [0m                     

                       Computation: 43208 steps/s (collection: 2.170s, learning 0.106s)
             Mean action noise std: 3.40
          Mean value_function loss: 130.1481
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 68.0690
                       Mean reward: 840.96
               Mean episode length: 224.15
    Episode_Reward/reaching_object: 1.5823
     Episode_Reward/lifting_object: 170.4811
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0647
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 2.28s
                      Time elapsed: 01:04:08
                               ETA: 00:26:29

################################################################################
                     [1m Learning iteration 1416/2000 [0m                     

                       Computation: 42297 steps/s (collection: 2.215s, learning 0.110s)
             Mean action noise std: 3.40
          Mean value_function loss: 148.3566
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.0794
                       Mean reward: 833.46
               Mean episode length: 222.64
    Episode_Reward/reaching_object: 1.5668
     Episode_Reward/lifting_object: 168.8180
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 2.32s
                      Time elapsed: 01:04:10
                               ETA: 00:26:26

################################################################################
                     [1m Learning iteration 1417/2000 [0m                     

                       Computation: 42400 steps/s (collection: 2.215s, learning 0.104s)
             Mean action noise std: 3.40
          Mean value_function loss: 171.0264
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.0895
                       Mean reward: 837.79
               Mean episode length: 223.04
    Episode_Reward/reaching_object: 1.5326
     Episode_Reward/lifting_object: 164.9138
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 13.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 2.32s
                      Time elapsed: 01:04:12
                               ETA: 00:26:24

################################################################################
                     [1m Learning iteration 1418/2000 [0m                     

                       Computation: 41813 steps/s (collection: 2.245s, learning 0.106s)
             Mean action noise std: 3.40
          Mean value_function loss: 127.8494
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 68.1019
                       Mean reward: 854.01
               Mean episode length: 227.28
    Episode_Reward/reaching_object: 1.5679
     Episode_Reward/lifting_object: 169.1045
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 2.35s
                      Time elapsed: 01:04:15
                               ETA: 00:26:21

################################################################################
                     [1m Learning iteration 1419/2000 [0m                     

                       Computation: 41606 steps/s (collection: 2.248s, learning 0.115s)
             Mean action noise std: 3.41
          Mean value_function loss: 124.3978
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 68.1087
                       Mean reward: 870.14
               Mean episode length: 231.69
    Episode_Reward/reaching_object: 1.5956
     Episode_Reward/lifting_object: 172.2426
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 2.36s
                      Time elapsed: 01:04:17
                               ETA: 00:26:18

################################################################################
                     [1m Learning iteration 1420/2000 [0m                     

                       Computation: 42834 steps/s (collection: 2.191s, learning 0.104s)
             Mean action noise std: 3.41
          Mean value_function loss: 113.4854
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 68.1183
                       Mean reward: 849.22
               Mean episode length: 225.44
    Episode_Reward/reaching_object: 1.5814
     Episode_Reward/lifting_object: 171.0811
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 2.29s
                      Time elapsed: 01:04:19
                               ETA: 00:26:15

################################################################################
                     [1m Learning iteration 1421/2000 [0m                     

                       Computation: 42342 steps/s (collection: 2.186s, learning 0.136s)
             Mean action noise std: 3.41
          Mean value_function loss: 157.2484
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 68.1263
                       Mean reward: 812.24
               Mean episode length: 216.75
    Episode_Reward/reaching_object: 1.5459
     Episode_Reward/lifting_object: 166.8487
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0637
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 2.32s
                      Time elapsed: 01:04:22
                               ETA: 00:26:12

################################################################################
                     [1m Learning iteration 1422/2000 [0m                     

                       Computation: 42065 steps/s (collection: 2.221s, learning 0.116s)
             Mean action noise std: 3.41
          Mean value_function loss: 121.1910
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 68.1355
                       Mean reward: 868.42
               Mean episode length: 229.88
    Episode_Reward/reaching_object: 1.5775
     Episode_Reward/lifting_object: 170.5873
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 2.34s
                      Time elapsed: 01:04:24
                               ETA: 00:26:09

################################################################################
                     [1m Learning iteration 1423/2000 [0m                     

                       Computation: 41818 steps/s (collection: 2.218s, learning 0.133s)
             Mean action noise std: 3.41
          Mean value_function loss: 158.7110
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 68.1469
                       Mean reward: 820.96
               Mean episode length: 219.61
    Episode_Reward/reaching_object: 1.5642
     Episode_Reward/lifting_object: 168.5807
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 2.35s
                      Time elapsed: 01:04:26
                               ETA: 00:26:06

################################################################################
                     [1m Learning iteration 1424/2000 [0m                     

                       Computation: 42610 steps/s (collection: 2.179s, learning 0.128s)
             Mean action noise std: 3.41
          Mean value_function loss: 128.1026
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.1541
                       Mean reward: 834.08
               Mean episode length: 224.11
    Episode_Reward/reaching_object: 1.5431
     Episode_Reward/lifting_object: 165.9181
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 2.31s
                      Time elapsed: 01:04:29
                               ETA: 00:26:03

################################################################################
                     [1m Learning iteration 1425/2000 [0m                     

                       Computation: 42443 steps/s (collection: 2.187s, learning 0.129s)
             Mean action noise std: 3.41
          Mean value_function loss: 132.0494
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.1624
                       Mean reward: 820.88
               Mean episode length: 218.93
    Episode_Reward/reaching_object: 1.5823
     Episode_Reward/lifting_object: 170.6775
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0651
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 2.32s
                      Time elapsed: 01:04:31
                               ETA: 00:26:01

################################################################################
                     [1m Learning iteration 1426/2000 [0m                     

                       Computation: 41694 steps/s (collection: 2.217s, learning 0.141s)
             Mean action noise std: 3.41
          Mean value_function loss: 147.0062
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 68.1719
                       Mean reward: 817.58
               Mean episode length: 218.40
    Episode_Reward/reaching_object: 1.5573
     Episode_Reward/lifting_object: 167.8218
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 2.36s
                      Time elapsed: 01:04:33
                               ETA: 00:25:58

################################################################################
                     [1m Learning iteration 1427/2000 [0m                     

                       Computation: 42327 steps/s (collection: 2.192s, learning 0.131s)
             Mean action noise std: 3.42
          Mean value_function loss: 142.3101
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 68.1857
                       Mean reward: 876.91
               Mean episode length: 232.46
    Episode_Reward/reaching_object: 1.5595
     Episode_Reward/lifting_object: 167.9194
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0643
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 2.32s
                      Time elapsed: 01:04:36
                               ETA: 00:25:55

################################################################################
                     [1m Learning iteration 1428/2000 [0m                     

                       Computation: 42194 steps/s (collection: 2.192s, learning 0.138s)
             Mean action noise std: 3.42
          Mean value_function loss: 134.7832
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.1948
                       Mean reward: 849.40
               Mean episode length: 226.04
    Episode_Reward/reaching_object: 1.5768
     Episode_Reward/lifting_object: 169.8956
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0649
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 2.33s
                      Time elapsed: 01:04:38
                               ETA: 00:25:52

################################################################################
                     [1m Learning iteration 1429/2000 [0m                     

                       Computation: 39430 steps/s (collection: 2.367s, learning 0.127s)
             Mean action noise std: 3.42
          Mean value_function loss: 132.5540
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 68.2083
                       Mean reward: 824.53
               Mean episode length: 220.72
    Episode_Reward/reaching_object: 1.5733
     Episode_Reward/lifting_object: 169.3943
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 2.49s
                      Time elapsed: 01:04:40
                               ETA: 00:25:49

################################################################################
                     [1m Learning iteration 1430/2000 [0m                     

                       Computation: 41428 steps/s (collection: 2.235s, learning 0.138s)
             Mean action noise std: 3.42
          Mean value_function loss: 171.2153
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 68.2202
                       Mean reward: 844.65
               Mean episode length: 224.70
    Episode_Reward/reaching_object: 1.5372
     Episode_Reward/lifting_object: 165.5152
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 2.37s
                      Time elapsed: 01:04:43
                               ETA: 00:25:46

################################################################################
                     [1m Learning iteration 1431/2000 [0m                     

                       Computation: 38646 steps/s (collection: 2.398s, learning 0.146s)
             Mean action noise std: 3.42
          Mean value_function loss: 149.8167
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 68.2338
                       Mean reward: 844.76
               Mean episode length: 226.54
    Episode_Reward/reaching_object: 1.5507
     Episode_Reward/lifting_object: 166.3887
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 2.54s
                      Time elapsed: 01:04:45
                               ETA: 00:25:44

################################################################################
                     [1m Learning iteration 1432/2000 [0m                     

                       Computation: 41991 steps/s (collection: 2.220s, learning 0.121s)
             Mean action noise std: 3.42
          Mean value_function loss: 134.3388
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 68.2455
                       Mean reward: 857.69
               Mean episode length: 228.42
    Episode_Reward/reaching_object: 1.5592
     Episode_Reward/lifting_object: 168.0447
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 2.34s
                      Time elapsed: 01:04:48
                               ETA: 00:25:41

################################################################################
                     [1m Learning iteration 1433/2000 [0m                     

                       Computation: 41335 steps/s (collection: 2.270s, learning 0.109s)
             Mean action noise std: 3.43
          Mean value_function loss: 133.2394
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 68.2609
                       Mean reward: 841.90
               Mean episode length: 223.84
    Episode_Reward/reaching_object: 1.5951
     Episode_Reward/lifting_object: 171.7445
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 2.38s
                      Time elapsed: 01:04:50
                               ETA: 00:25:38

################################################################################
                     [1m Learning iteration 1434/2000 [0m                     

                       Computation: 40961 steps/s (collection: 2.259s, learning 0.141s)
             Mean action noise std: 3.43
          Mean value_function loss: 109.4799
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 68.2747
                       Mean reward: 888.64
               Mean episode length: 234.80
    Episode_Reward/reaching_object: 1.5955
     Episode_Reward/lifting_object: 172.2161
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0657
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 2.40s
                      Time elapsed: 01:04:52
                               ETA: 00:25:35

################################################################################
                     [1m Learning iteration 1435/2000 [0m                     

                       Computation: 41781 steps/s (collection: 2.231s, learning 0.122s)
             Mean action noise std: 3.43
          Mean value_function loss: 132.7295
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 68.2904
                       Mean reward: 834.40
               Mean episode length: 222.23
    Episode_Reward/reaching_object: 1.5442
     Episode_Reward/lifting_object: 166.5052
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0640
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 2.35s
                      Time elapsed: 01:04:55
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 1436/2000 [0m                     

                       Computation: 41975 steps/s (collection: 2.224s, learning 0.118s)
             Mean action noise std: 3.43
          Mean value_function loss: 128.0483
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 68.3018
                       Mean reward: 852.78
               Mean episode length: 226.28
    Episode_Reward/reaching_object: 1.5844
     Episode_Reward/lifting_object: 170.5639
      Episode_Reward/object_height: 0.0201
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 2.34s
                      Time elapsed: 01:04:57
                               ETA: 00:25:29

################################################################################
                     [1m Learning iteration 1437/2000 [0m                     

                       Computation: 41754 steps/s (collection: 2.213s, learning 0.141s)
             Mean action noise std: 3.43
          Mean value_function loss: 111.0630
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 68.3135
                       Mean reward: 828.32
               Mean episode length: 220.62
    Episode_Reward/reaching_object: 1.5598
     Episode_Reward/lifting_object: 168.0084
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0645
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 2.35s
                      Time elapsed: 01:05:00
                               ETA: 00:25:26

################################################################################
                     [1m Learning iteration 1438/2000 [0m                     

                       Computation: 42103 steps/s (collection: 2.208s, learning 0.127s)
             Mean action noise std: 3.43
          Mean value_function loss: 91.6329
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 68.3211
                       Mean reward: 888.22
               Mean episode length: 234.53
    Episode_Reward/reaching_object: 1.5865
     Episode_Reward/lifting_object: 170.9880
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0522
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 2.33s
                      Time elapsed: 01:05:02
                               ETA: 00:25:24

################################################################################
                     [1m Learning iteration 1439/2000 [0m                     

                       Computation: 41803 steps/s (collection: 2.222s, learning 0.130s)
             Mean action noise std: 3.44
          Mean value_function loss: 101.7545
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 68.3342
                       Mean reward: 856.81
               Mean episode length: 228.11
    Episode_Reward/reaching_object: 1.6023
     Episode_Reward/lifting_object: 172.2969
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 2.35s
                      Time elapsed: 01:05:04
                               ETA: 00:25:21

################################################################################
                     [1m Learning iteration 1440/2000 [0m                     

                       Computation: 41778 steps/s (collection: 2.220s, learning 0.133s)
             Mean action noise std: 3.44
          Mean value_function loss: 116.2000
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 68.3432
                       Mean reward: 851.96
               Mean episode length: 226.71
    Episode_Reward/reaching_object: 1.5840
     Episode_Reward/lifting_object: 170.4284
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 2.35s
                      Time elapsed: 01:05:07
                               ETA: 00:25:18

################################################################################
                     [1m Learning iteration 1441/2000 [0m                     

                       Computation: 42670 steps/s (collection: 2.176s, learning 0.128s)
             Mean action noise std: 3.44
          Mean value_function loss: 130.7987
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 68.3575
                       Mean reward: 817.02
               Mean episode length: 219.13
    Episode_Reward/reaching_object: 1.5993
     Episode_Reward/lifting_object: 172.0713
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 2.30s
                      Time elapsed: 01:05:09
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 1442/2000 [0m                     

                       Computation: 42769 steps/s (collection: 2.195s, learning 0.103s)
             Mean action noise std: 3.44
          Mean value_function loss: 122.7935
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 68.3753
                       Mean reward: 879.51
               Mean episode length: 233.04
    Episode_Reward/reaching_object: 1.5751
     Episode_Reward/lifting_object: 169.4208
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 2.30s
                      Time elapsed: 01:05:11
                               ETA: 00:25:12

################################################################################
                     [1m Learning iteration 1443/2000 [0m                     

                       Computation: 42221 steps/s (collection: 2.187s, learning 0.142s)
             Mean action noise std: 3.44
          Mean value_function loss: 128.8721
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 68.3915
                       Mean reward: 844.63
               Mean episode length: 224.99
    Episode_Reward/reaching_object: 1.5862
     Episode_Reward/lifting_object: 170.8523
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 2.33s
                      Time elapsed: 01:05:14
                               ETA: 00:25:09

################################################################################
                     [1m Learning iteration 1444/2000 [0m                     

                       Computation: 42246 steps/s (collection: 2.189s, learning 0.138s)
             Mean action noise std: 3.44
          Mean value_function loss: 113.8208
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 68.4026
                       Mean reward: 881.10
               Mean episode length: 234.05
    Episode_Reward/reaching_object: 1.5915
     Episode_Reward/lifting_object: 171.4009
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 2.33s
                      Time elapsed: 01:05:16
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 1445/2000 [0m                     

                       Computation: 42362 steps/s (collection: 2.201s, learning 0.120s)
             Mean action noise std: 3.45
          Mean value_function loss: 120.6152
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 68.4103
                       Mean reward: 846.89
               Mean episode length: 225.52
    Episode_Reward/reaching_object: 1.5570
     Episode_Reward/lifting_object: 167.3028
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0648
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 2.32s
                      Time elapsed: 01:05:18
                               ETA: 00:25:04

################################################################################
                     [1m Learning iteration 1446/2000 [0m                     

                       Computation: 41729 steps/s (collection: 2.220s, learning 0.136s)
             Mean action noise std: 3.45
          Mean value_function loss: 120.4227
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 68.4225
                       Mean reward: 868.31
               Mean episode length: 230.61
    Episode_Reward/reaching_object: 1.5803
     Episode_Reward/lifting_object: 169.9015
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 2.36s
                      Time elapsed: 01:05:21
                               ETA: 00:25:01

################################################################################
                     [1m Learning iteration 1447/2000 [0m                     

                       Computation: 42492 steps/s (collection: 2.188s, learning 0.125s)
             Mean action noise std: 3.45
          Mean value_function loss: 115.3383
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 68.4393
                       Mean reward: 836.00
               Mean episode length: 223.75
    Episode_Reward/reaching_object: 1.5784
     Episode_Reward/lifting_object: 169.6134
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0659
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 2.31s
                      Time elapsed: 01:05:23
                               ETA: 00:24:58

################################################################################
                     [1m Learning iteration 1448/2000 [0m                     

                       Computation: 41971 steps/s (collection: 2.215s, learning 0.127s)
             Mean action noise std: 3.45
          Mean value_function loss: 115.1788
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 68.4565
                       Mean reward: 883.57
               Mean episode length: 234.74
    Episode_Reward/reaching_object: 1.5859
     Episode_Reward/lifting_object: 170.3434
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 2.34s
                      Time elapsed: 01:05:25
                               ETA: 00:24:55

################################################################################
                     [1m Learning iteration 1449/2000 [0m                     

                       Computation: 42257 steps/s (collection: 2.194s, learning 0.132s)
             Mean action noise std: 3.45
          Mean value_function loss: 125.5424
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 68.4676
                       Mean reward: 865.10
               Mean episode length: 229.54
    Episode_Reward/reaching_object: 1.5465
     Episode_Reward/lifting_object: 165.8135
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0646
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 2.33s
                      Time elapsed: 01:05:28
                               ETA: 00:24:52

################################################################################
                     [1m Learning iteration 1450/2000 [0m                     

                       Computation: 42503 steps/s (collection: 2.178s, learning 0.135s)
             Mean action noise std: 3.46
          Mean value_function loss: 134.6052
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 68.4803
                       Mean reward: 855.05
               Mean episode length: 227.13
    Episode_Reward/reaching_object: 1.5769
     Episode_Reward/lifting_object: 169.2789
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 2.31s
                      Time elapsed: 01:05:30
                               ETA: 00:24:49

################################################################################
                     [1m Learning iteration 1451/2000 [0m                     

                       Computation: 42049 steps/s (collection: 2.204s, learning 0.134s)
             Mean action noise std: 3.46
          Mean value_function loss: 116.8939
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 68.4958
                       Mean reward: 872.84
               Mean episode length: 231.13
    Episode_Reward/reaching_object: 1.5953
     Episode_Reward/lifting_object: 171.3083
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0667
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 2.34s
                      Time elapsed: 01:05:32
                               ETA: 00:24:46

################################################################################
                     [1m Learning iteration 1452/2000 [0m                     

                       Computation: 41844 steps/s (collection: 2.221s, learning 0.128s)
             Mean action noise std: 3.46
          Mean value_function loss: 110.1481
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 68.5100
                       Mean reward: 908.16
               Mean episode length: 238.77
    Episode_Reward/reaching_object: 1.6082
     Episode_Reward/lifting_object: 172.8880
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0672
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 2.35s
                      Time elapsed: 01:05:35
                               ETA: 00:24:44

################################################################################
                     [1m Learning iteration 1453/2000 [0m                     

                       Computation: 41947 steps/s (collection: 2.216s, learning 0.128s)
             Mean action noise std: 3.46
          Mean value_function loss: 97.1955
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 68.5260
                       Mean reward: 874.24
               Mean episode length: 231.62
    Episode_Reward/reaching_object: 1.6067
     Episode_Reward/lifting_object: 172.6850
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 2.34s
                      Time elapsed: 01:05:37
                               ETA: 00:24:41

################################################################################
                     [1m Learning iteration 1454/2000 [0m                     

                       Computation: 37697 steps/s (collection: 2.475s, learning 0.133s)
             Mean action noise std: 3.46
          Mean value_function loss: 118.1532
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.5411
                       Mean reward: 873.03
               Mean episode length: 231.41
    Episode_Reward/reaching_object: 1.5604
     Episode_Reward/lifting_object: 167.3592
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 2.61s
                      Time elapsed: 01:05:39
                               ETA: 00:24:38

################################################################################
                     [1m Learning iteration 1455/2000 [0m                     

                       Computation: 41774 steps/s (collection: 2.215s, learning 0.138s)
             Mean action noise std: 3.46
          Mean value_function loss: 129.5966
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.5497
                       Mean reward: 854.49
               Mean episode length: 227.48
    Episode_Reward/reaching_object: 1.5660
     Episode_Reward/lifting_object: 168.1492
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 2.35s
                      Time elapsed: 01:05:42
                               ETA: 00:24:35

################################################################################
                     [1m Learning iteration 1456/2000 [0m                     

                       Computation: 42562 steps/s (collection: 2.190s, learning 0.120s)
             Mean action noise std: 3.47
          Mean value_function loss: 88.2639
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 68.5615
                       Mean reward: 852.13
               Mean episode length: 226.84
    Episode_Reward/reaching_object: 1.6403
     Episode_Reward/lifting_object: 176.1701
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0687
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 2.31s
                      Time elapsed: 01:05:44
                               ETA: 00:24:32

################################################################################
                     [1m Learning iteration 1457/2000 [0m                     

                       Computation: 41895 steps/s (collection: 2.213s, learning 0.133s)
             Mean action noise std: 3.47
          Mean value_function loss: 127.6588
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.5808
                       Mean reward: 874.81
               Mean episode length: 230.61
    Episode_Reward/reaching_object: 1.5915
     Episode_Reward/lifting_object: 171.0293
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0670
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 2.35s
                      Time elapsed: 01:05:46
                               ETA: 00:24:29

################################################################################
                     [1m Learning iteration 1458/2000 [0m                     

                       Computation: 42080 steps/s (collection: 2.192s, learning 0.144s)
             Mean action noise std: 3.47
          Mean value_function loss: 126.9173
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 68.5963
                       Mean reward: 848.68
               Mean episode length: 225.05
    Episode_Reward/reaching_object: 1.6041
     Episode_Reward/lifting_object: 172.1773
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0675
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 2.34s
                      Time elapsed: 01:05:49
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 1459/2000 [0m                     

                       Computation: 42130 steps/s (collection: 2.202s, learning 0.131s)
             Mean action noise std: 3.47
          Mean value_function loss: 119.2966
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 68.6122
                       Mean reward: 800.67
               Mean episode length: 215.63
    Episode_Reward/reaching_object: 1.5935
     Episode_Reward/lifting_object: 170.6604
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 2.33s
                      Time elapsed: 01:05:51
                               ETA: 00:24:24

################################################################################
                     [1m Learning iteration 1460/2000 [0m                     

                       Computation: 41237 steps/s (collection: 2.228s, learning 0.156s)
             Mean action noise std: 3.47
          Mean value_function loss: 85.6115
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 68.6208
                       Mean reward: 882.69
               Mean episode length: 233.75
    Episode_Reward/reaching_object: 1.6210
     Episode_Reward/lifting_object: 173.8003
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 2.38s
                      Time elapsed: 01:05:54
                               ETA: 00:24:21

################################################################################
                     [1m Learning iteration 1461/2000 [0m                     

                       Computation: 42029 steps/s (collection: 2.211s, learning 0.128s)
             Mean action noise std: 3.48
          Mean value_function loss: 115.9611
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 68.6306
                       Mean reward: 868.62
               Mean episode length: 230.38
    Episode_Reward/reaching_object: 1.6032
     Episode_Reward/lifting_object: 171.6166
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 2.34s
                      Time elapsed: 01:05:56
                               ETA: 00:24:18

################################################################################
                     [1m Learning iteration 1462/2000 [0m                     

                       Computation: 41988 steps/s (collection: 2.210s, learning 0.131s)
             Mean action noise std: 3.48
          Mean value_function loss: 123.7953
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 68.6444
                       Mean reward: 809.43
               Mean episode length: 216.11
    Episode_Reward/reaching_object: 1.5958
     Episode_Reward/lifting_object: 171.1705
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0676
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 2.34s
                      Time elapsed: 01:05:58
                               ETA: 00:24:15

################################################################################
                     [1m Learning iteration 1463/2000 [0m                     

                       Computation: 41374 steps/s (collection: 2.251s, learning 0.125s)
             Mean action noise std: 3.48
          Mean value_function loss: 94.4117
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.6634
                       Mean reward: 864.94
               Mean episode length: 229.84
    Episode_Reward/reaching_object: 1.6132
     Episode_Reward/lifting_object: 173.1444
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 2.38s
                      Time elapsed: 01:06:01
                               ETA: 00:24:12

################################################################################
                     [1m Learning iteration 1464/2000 [0m                     

                       Computation: 41124 steps/s (collection: 2.260s, learning 0.130s)
             Mean action noise std: 3.48
          Mean value_function loss: 90.5273
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 68.6792
                       Mean reward: 884.34
               Mean episode length: 233.90
    Episode_Reward/reaching_object: 1.6205
     Episode_Reward/lifting_object: 174.0141
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 2.39s
                      Time elapsed: 01:06:03
                               ETA: 00:24:10

################################################################################
                     [1m Learning iteration 1465/2000 [0m                     

                       Computation: 40833 steps/s (collection: 2.285s, learning 0.123s)
             Mean action noise std: 3.48
          Mean value_function loss: 90.0293
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 68.6895
                       Mean reward: 900.96
               Mean episode length: 237.72
    Episode_Reward/reaching_object: 1.6398
     Episode_Reward/lifting_object: 176.0248
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0696
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 2.41s
                      Time elapsed: 01:06:05
                               ETA: 00:24:07

################################################################################
                     [1m Learning iteration 1466/2000 [0m                     

                       Computation: 41873 steps/s (collection: 2.216s, learning 0.132s)
             Mean action noise std: 3.48
          Mean value_function loss: 108.6397
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.7013
                       Mean reward: 825.59
               Mean episode length: 220.31
    Episode_Reward/reaching_object: 1.6072
     Episode_Reward/lifting_object: 172.4273
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0681
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 2.35s
                      Time elapsed: 01:06:08
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 1467/2000 [0m                     

                       Computation: 41902 steps/s (collection: 2.216s, learning 0.130s)
             Mean action noise std: 3.49
          Mean value_function loss: 94.6733
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 68.7139
                       Mean reward: 885.57
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 1.6171
     Episode_Reward/lifting_object: 173.7006
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 2.35s
                      Time elapsed: 01:06:10
                               ETA: 00:24:01

################################################################################
                     [1m Learning iteration 1468/2000 [0m                     

                       Computation: 42687 steps/s (collection: 2.188s, learning 0.115s)
             Mean action noise std: 3.49
          Mean value_function loss: 98.9555
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 68.7270
                       Mean reward: 882.85
               Mean episode length: 232.59
    Episode_Reward/reaching_object: 1.5960
     Episode_Reward/lifting_object: 171.4440
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0680
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 2.30s
                      Time elapsed: 01:06:12
                               ETA: 00:23:58

################################################################################
                     [1m Learning iteration 1469/2000 [0m                     

                       Computation: 41076 steps/s (collection: 2.240s, learning 0.153s)
             Mean action noise std: 3.49
          Mean value_function loss: 100.7952
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 68.7389
                       Mean reward: 864.86
               Mean episode length: 229.27
    Episode_Reward/reaching_object: 1.6039
     Episode_Reward/lifting_object: 172.3370
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 2.39s
                      Time elapsed: 01:06:15
                               ETA: 00:23:55

################################################################################
                     [1m Learning iteration 1470/2000 [0m                     

                       Computation: 41464 steps/s (collection: 2.239s, learning 0.132s)
             Mean action noise std: 3.49
          Mean value_function loss: 130.8296
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.7557
                       Mean reward: 801.32
               Mean episode length: 215.34
    Episode_Reward/reaching_object: 1.5980
     Episode_Reward/lifting_object: 171.4318
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 2.37s
                      Time elapsed: 01:06:17
                               ETA: 00:23:53

################################################################################
                     [1m Learning iteration 1471/2000 [0m                     

                       Computation: 40874 steps/s (collection: 2.265s, learning 0.140s)
             Mean action noise std: 3.49
          Mean value_function loss: 117.8561
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 68.7724
                       Mean reward: 881.51
               Mean episode length: 233.64
    Episode_Reward/reaching_object: 1.6188
     Episode_Reward/lifting_object: 174.2229
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0545
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 2.41s
                      Time elapsed: 01:06:20
                               ETA: 00:23:50

################################################################################
                     [1m Learning iteration 1472/2000 [0m                     

                       Computation: 40841 steps/s (collection: 2.274s, learning 0.133s)
             Mean action noise std: 3.50
          Mean value_function loss: 133.9731
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 68.7861
                       Mean reward: 881.09
               Mean episode length: 232.54
    Episode_Reward/reaching_object: 1.6012
     Episode_Reward/lifting_object: 172.3786
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 2.41s
                      Time elapsed: 01:06:22
                               ETA: 00:23:47

################################################################################
                     [1m Learning iteration 1473/2000 [0m                     

                       Computation: 40384 steps/s (collection: 2.292s, learning 0.143s)
             Mean action noise std: 3.50
          Mean value_function loss: 116.0838
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 68.7966
                       Mean reward: 823.86
               Mean episode length: 220.26
    Episode_Reward/reaching_object: 1.5668
     Episode_Reward/lifting_object: 168.1323
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 2.43s
                      Time elapsed: 01:06:24
                               ETA: 00:23:44

################################################################################
                     [1m Learning iteration 1474/2000 [0m                     

                       Computation: 40880 steps/s (collection: 2.264s, learning 0.141s)
             Mean action noise std: 3.50
          Mean value_function loss: 149.1578
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 68.8107
                       Mean reward: 821.35
               Mean episode length: 218.99
    Episode_Reward/reaching_object: 1.5676
     Episode_Reward/lifting_object: 168.7177
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0673
          Episode_Reward/joint_vel: -0.0526
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 2.40s
                      Time elapsed: 01:06:27
                               ETA: 00:23:41

################################################################################
                     [1m Learning iteration 1475/2000 [0m                     

                       Computation: 40513 steps/s (collection: 2.311s, learning 0.116s)
             Mean action noise std: 3.50
          Mean value_function loss: 133.3786
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.8205
                       Mean reward: 779.16
               Mean episode length: 209.27
    Episode_Reward/reaching_object: 1.5397
     Episode_Reward/lifting_object: 165.2715
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0666
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 2.43s
                      Time elapsed: 01:06:29
                               ETA: 00:23:39

################################################################################
                     [1m Learning iteration 1476/2000 [0m                     

                       Computation: 40985 steps/s (collection: 2.264s, learning 0.134s)
             Mean action noise std: 3.50
          Mean value_function loss: 113.2199
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 68.8277
                       Mean reward: 867.93
               Mean episode length: 229.90
    Episode_Reward/reaching_object: 1.5878
     Episode_Reward/lifting_object: 170.5601
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 2.40s
                      Time elapsed: 01:06:32
                               ETA: 00:23:36

################################################################################
                     [1m Learning iteration 1477/2000 [0m                     

                       Computation: 41151 steps/s (collection: 2.264s, learning 0.125s)
             Mean action noise std: 3.50
          Mean value_function loss: 135.8762
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 68.8422
                       Mean reward: 816.38
               Mean episode length: 218.11
    Episode_Reward/reaching_object: 1.5532
     Episode_Reward/lifting_object: 166.8697
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0528
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 2.39s
                      Time elapsed: 01:06:34
                               ETA: 00:23:33

################################################################################
                     [1m Learning iteration 1478/2000 [0m                     

                       Computation: 40970 steps/s (collection: 2.256s, learning 0.143s)
             Mean action noise std: 3.51
          Mean value_function loss: 112.4407
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 68.8570
                       Mean reward: 866.25
               Mean episode length: 229.30
    Episode_Reward/reaching_object: 1.6049
     Episode_Reward/lifting_object: 172.7250
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 2.40s
                      Time elapsed: 01:06:36
                               ETA: 00:23:30

################################################################################
                     [1m Learning iteration 1479/2000 [0m                     

                       Computation: 40069 steps/s (collection: 2.312s, learning 0.142s)
             Mean action noise std: 3.51
          Mean value_function loss: 128.7872
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 68.8695
                       Mean reward: 851.89
               Mean episode length: 227.70
    Episode_Reward/reaching_object: 1.5765
     Episode_Reward/lifting_object: 168.7247
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0681
          Episode_Reward/joint_vel: -0.0533
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 2.45s
                      Time elapsed: 01:06:39
                               ETA: 00:23:27

################################################################################
                     [1m Learning iteration 1480/2000 [0m                     

                       Computation: 41815 steps/s (collection: 2.216s, learning 0.135s)
             Mean action noise std: 3.51
          Mean value_function loss: 123.4695
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 68.8821
                       Mean reward: 806.98
               Mean episode length: 215.94
    Episode_Reward/reaching_object: 1.5769
     Episode_Reward/lifting_object: 169.0889
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 2.35s
                      Time elapsed: 01:06:41
                               ETA: 00:23:25

################################################################################
                     [1m Learning iteration 1481/2000 [0m                     

                       Computation: 42239 steps/s (collection: 2.209s, learning 0.118s)
             Mean action noise std: 3.51
          Mean value_function loss: 131.9045
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 68.8979
                       Mean reward: 875.94
               Mean episode length: 232.00
    Episode_Reward/reaching_object: 1.6092
     Episode_Reward/lifting_object: 172.8988
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 2.33s
                      Time elapsed: 01:06:44
                               ETA: 00:23:22

################################################################################
                     [1m Learning iteration 1482/2000 [0m                     

                       Computation: 42048 steps/s (collection: 2.208s, learning 0.130s)
             Mean action noise std: 3.51
          Mean value_function loss: 135.8966
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 68.9073
                       Mean reward: 865.87
               Mean episode length: 230.47
    Episode_Reward/reaching_object: 1.5895
     Episode_Reward/lifting_object: 170.4979
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0687
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 2.34s
                      Time elapsed: 01:06:46
                               ETA: 00:23:19

################################################################################
                     [1m Learning iteration 1483/2000 [0m                     

                       Computation: 42539 steps/s (collection: 2.190s, learning 0.121s)
             Mean action noise std: 3.51
          Mean value_function loss: 122.5420
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 68.9162
                       Mean reward: 870.90
               Mean episode length: 231.89
    Episode_Reward/reaching_object: 1.5831
     Episode_Reward/lifting_object: 169.8685
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 2.31s
                      Time elapsed: 01:06:48
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 1484/2000 [0m                     

                       Computation: 42126 steps/s (collection: 2.195s, learning 0.139s)
             Mean action noise std: 3.51
          Mean value_function loss: 124.4381
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 68.9242
                       Mean reward: 910.13
               Mean episode length: 239.76
    Episode_Reward/reaching_object: 1.6175
     Episode_Reward/lifting_object: 173.6633
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0701
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 2.33s
                      Time elapsed: 01:06:51
                               ETA: 00:23:13

################################################################################
                     [1m Learning iteration 1485/2000 [0m                     

                       Computation: 42054 steps/s (collection: 2.224s, learning 0.114s)
             Mean action noise std: 3.52
          Mean value_function loss: 119.7540
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 68.9320
                       Mean reward: 886.04
               Mean episode length: 234.11
    Episode_Reward/reaching_object: 1.6101
     Episode_Reward/lifting_object: 173.0253
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 2.34s
                      Time elapsed: 01:06:53
                               ETA: 00:23:10

################################################################################
                     [1m Learning iteration 1486/2000 [0m                     

                       Computation: 40394 steps/s (collection: 2.295s, learning 0.139s)
             Mean action noise std: 3.52
          Mean value_function loss: 141.8085
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 68.9394
                       Mean reward: 823.39
               Mean episode length: 219.75
    Episode_Reward/reaching_object: 1.5703
     Episode_Reward/lifting_object: 168.2519
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 2.43s
                      Time elapsed: 01:06:55
                               ETA: 00:23:08

################################################################################
                     [1m Learning iteration 1487/2000 [0m                     

                       Computation: 40589 steps/s (collection: 2.279s, learning 0.143s)
             Mean action noise std: 3.52
          Mean value_function loss: 118.6061
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 68.9525
                       Mean reward: 861.24
               Mean episode length: 228.90
    Episode_Reward/reaching_object: 1.5764
     Episode_Reward/lifting_object: 169.5418
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 2.42s
                      Time elapsed: 01:06:58
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 1488/2000 [0m                     

                       Computation: 41916 steps/s (collection: 2.211s, learning 0.135s)
             Mean action noise std: 3.52
          Mean value_function loss: 133.3195
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 68.9711
                       Mean reward: 814.60
               Mean episode length: 217.55
    Episode_Reward/reaching_object: 1.5700
     Episode_Reward/lifting_object: 168.7495
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 2.35s
                      Time elapsed: 01:07:00
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 1489/2000 [0m                     

                       Computation: 41404 steps/s (collection: 2.232s, learning 0.143s)
             Mean action noise std: 3.52
          Mean value_function loss: 121.1077
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 68.9839
                       Mean reward: 851.45
               Mean episode length: 227.27
    Episode_Reward/reaching_object: 1.6112
     Episode_Reward/lifting_object: 173.5386
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0702
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 2.37s
                      Time elapsed: 01:07:02
                               ETA: 00:22:59

################################################################################
                     [1m Learning iteration 1490/2000 [0m                     

                       Computation: 42062 steps/s (collection: 2.199s, learning 0.138s)
             Mean action noise std: 3.53
          Mean value_function loss: 122.1862
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 68.9978
                       Mean reward: 835.27
               Mean episode length: 223.10
    Episode_Reward/reaching_object: 1.5597
     Episode_Reward/lifting_object: 167.8718
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0682
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 2.34s
                      Time elapsed: 01:07:05
                               ETA: 00:22:56

################################################################################
                     [1m Learning iteration 1491/2000 [0m                     

                       Computation: 41829 steps/s (collection: 2.217s, learning 0.133s)
             Mean action noise std: 3.53
          Mean value_function loss: 122.0505
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 69.0177
                       Mean reward: 829.45
               Mean episode length: 220.81
    Episode_Reward/reaching_object: 1.6013
     Episode_Reward/lifting_object: 172.2777
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0700
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 2.35s
                      Time elapsed: 01:07:07
                               ETA: 00:22:54

################################################################################
                     [1m Learning iteration 1492/2000 [0m                     

                       Computation: 42604 steps/s (collection: 2.193s, learning 0.114s)
             Mean action noise std: 3.53
          Mean value_function loss: 133.7842
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 69.0346
                       Mean reward: 875.05
               Mean episode length: 231.35
    Episode_Reward/reaching_object: 1.6089
     Episode_Reward/lifting_object: 173.3190
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 2.31s
                      Time elapsed: 01:07:09
                               ETA: 00:22:51

################################################################################
                     [1m Learning iteration 1493/2000 [0m                     

                       Computation: 42599 steps/s (collection: 2.194s, learning 0.114s)
             Mean action noise std: 3.53
          Mean value_function loss: 130.2597
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 69.0454
                       Mean reward: 882.04
               Mean episode length: 234.00
    Episode_Reward/reaching_object: 1.5681
     Episode_Reward/lifting_object: 168.4278
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0690
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 2.31s
                      Time elapsed: 01:07:12
                               ETA: 00:22:48

################################################################################
                     [1m Learning iteration 1494/2000 [0m                     

                       Computation: 42277 steps/s (collection: 2.187s, learning 0.138s)
             Mean action noise std: 3.53
          Mean value_function loss: 113.6078
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 69.0602
                       Mean reward: 853.66
               Mean episode length: 228.29
    Episode_Reward/reaching_object: 1.6163
     Episode_Reward/lifting_object: 173.9934
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0711
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 2.33s
                      Time elapsed: 01:07:14
                               ETA: 00:22:45

################################################################################
                     [1m Learning iteration 1495/2000 [0m                     

                       Computation: 42368 steps/s (collection: 2.178s, learning 0.143s)
             Mean action noise std: 3.54
          Mean value_function loss: 119.2055
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 69.0759
                       Mean reward: 819.79
               Mean episode length: 217.97
    Episode_Reward/reaching_object: 1.5490
     Episode_Reward/lifting_object: 166.6097
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 2.32s
                      Time elapsed: 01:07:16
                               ETA: 00:22:42

################################################################################
                     [1m Learning iteration 1496/2000 [0m                     

                       Computation: 42847 steps/s (collection: 2.191s, learning 0.103s)
             Mean action noise std: 3.54
          Mean value_function loss: 148.1862
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 69.0881
                       Mean reward: 830.21
               Mean episode length: 221.47
    Episode_Reward/reaching_object: 1.5838
     Episode_Reward/lifting_object: 170.5988
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0700
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 2.29s
                      Time elapsed: 01:07:19
                               ETA: 00:22:39

################################################################################
                     [1m Learning iteration 1497/2000 [0m                     

                       Computation: 41986 steps/s (collection: 2.195s, learning 0.147s)
             Mean action noise std: 3.54
          Mean value_function loss: 124.5916
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 69.1013
                       Mean reward: 845.39
               Mean episode length: 225.90
    Episode_Reward/reaching_object: 1.5793
     Episode_Reward/lifting_object: 169.5467
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 2.34s
                      Time elapsed: 01:07:21
                               ETA: 00:22:37

################################################################################
                     [1m Learning iteration 1498/2000 [0m                     

                       Computation: 42458 steps/s (collection: 2.184s, learning 0.132s)
             Mean action noise std: 3.54
          Mean value_function loss: 123.3051
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 69.1195
                       Mean reward: 858.48
               Mean episode length: 228.20
    Episode_Reward/reaching_object: 1.5888
     Episode_Reward/lifting_object: 171.0536
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0702
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 2.32s
                      Time elapsed: 01:07:23
                               ETA: 00:22:34

################################################################################
                     [1m Learning iteration 1499/2000 [0m                     

                       Computation: 41945 steps/s (collection: 2.224s, learning 0.120s)
             Mean action noise std: 3.54
          Mean value_function loss: 131.2709
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 69.1308
                       Mean reward: 850.41
               Mean episode length: 226.05
    Episode_Reward/reaching_object: 1.5704
     Episode_Reward/lifting_object: 169.0219
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 2.34s
                      Time elapsed: 01:07:26
                               ETA: 00:22:31

################################################################################
                     [1m Learning iteration 1500/2000 [0m                     

                       Computation: 41942 steps/s (collection: 2.204s, learning 0.140s)
             Mean action noise std: 3.54
          Mean value_function loss: 137.2093
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 69.1429
                       Mean reward: 817.53
               Mean episode length: 218.85
    Episode_Reward/reaching_object: 1.5716
     Episode_Reward/lifting_object: 169.2108
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 2.34s
                      Time elapsed: 01:07:28
                               ETA: 00:22:28

################################################################################
                     [1m Learning iteration 1501/2000 [0m                     

                       Computation: 39859 steps/s (collection: 2.336s, learning 0.131s)
             Mean action noise std: 3.55
          Mean value_function loss: 124.8348
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.1595
                       Mean reward: 863.27
               Mean episode length: 229.97
    Episode_Reward/reaching_object: 1.5741
     Episode_Reward/lifting_object: 169.8820
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 2.47s
                      Time elapsed: 01:07:30
                               ETA: 00:22:25

################################################################################
                     [1m Learning iteration 1502/2000 [0m                     

                       Computation: 41233 steps/s (collection: 2.246s, learning 0.139s)
             Mean action noise std: 3.55
          Mean value_function loss: 142.4791
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 69.1752
                       Mean reward: 846.65
               Mean episode length: 224.36
    Episode_Reward/reaching_object: 1.5905
     Episode_Reward/lifting_object: 172.3154
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0707
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 2.38s
                      Time elapsed: 01:07:33
                               ETA: 00:22:23

################################################################################
                     [1m Learning iteration 1503/2000 [0m                     

                       Computation: 41967 steps/s (collection: 2.203s, learning 0.139s)
             Mean action noise std: 3.55
          Mean value_function loss: 119.6323
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 69.1876
                       Mean reward: 898.39
               Mean episode length: 237.48
    Episode_Reward/reaching_object: 1.5947
     Episode_Reward/lifting_object: 172.1429
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0710
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 2.34s
                      Time elapsed: 01:07:35
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 1504/2000 [0m                     

                       Computation: 41844 steps/s (collection: 2.214s, learning 0.135s)
             Mean action noise std: 3.55
          Mean value_function loss: 157.8114
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 69.1988
                       Mean reward: 882.15
               Mean episode length: 233.03
    Episode_Reward/reaching_object: 1.5752
     Episode_Reward/lifting_object: 170.4492
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 2.35s
                      Time elapsed: 01:07:38
                               ETA: 00:22:17

################################################################################
                     [1m Learning iteration 1505/2000 [0m                     

                       Computation: 40893 steps/s (collection: 2.258s, learning 0.146s)
             Mean action noise std: 3.55
          Mean value_function loss: 141.7385
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 69.2084
                       Mean reward: 871.86
               Mean episode length: 231.81
    Episode_Reward/reaching_object: 1.5720
     Episode_Reward/lifting_object: 169.7625
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0705
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 2.40s
                      Time elapsed: 01:07:40
                               ETA: 00:22:14

################################################################################
                     [1m Learning iteration 1506/2000 [0m                     

                       Computation: 40549 steps/s (collection: 2.283s, learning 0.141s)
             Mean action noise std: 3.55
          Mean value_function loss: 142.2726
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 69.2195
                       Mean reward: 864.93
               Mean episode length: 230.11
    Episode_Reward/reaching_object: 1.5640
     Episode_Reward/lifting_object: 168.9695
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0704
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 2.42s
                      Time elapsed: 01:07:42
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 1507/2000 [0m                     

                       Computation: 41874 steps/s (collection: 2.244s, learning 0.104s)
             Mean action noise std: 3.56
          Mean value_function loss: 120.5795
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 69.2276
                       Mean reward: 869.46
               Mean episode length: 230.40
    Episode_Reward/reaching_object: 1.5860
     Episode_Reward/lifting_object: 171.7239
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0713
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 2.35s
                      Time elapsed: 01:07:45
                               ETA: 00:22:09

################################################################################
                     [1m Learning iteration 1508/2000 [0m                     

                       Computation: 40694 steps/s (collection: 2.275s, learning 0.141s)
             Mean action noise std: 3.56
          Mean value_function loss: 163.6918
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 69.2328
                       Mean reward: 815.54
               Mean episode length: 218.18
    Episode_Reward/reaching_object: 1.5393
     Episode_Reward/lifting_object: 166.6030
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0696
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 2.42s
                      Time elapsed: 01:07:47
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 1509/2000 [0m                     

                       Computation: 41255 steps/s (collection: 2.257s, learning 0.126s)
             Mean action noise std: 3.56
          Mean value_function loss: 161.6113
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 69.2373
                       Mean reward: 902.01
               Mean episode length: 238.16
    Episode_Reward/reaching_object: 1.5675
     Episode_Reward/lifting_object: 169.8859
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0708
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 2.38s
                      Time elapsed: 01:07:50
                               ETA: 00:22:03

################################################################################
                     [1m Learning iteration 1510/2000 [0m                     

                       Computation: 40517 steps/s (collection: 2.272s, learning 0.154s)
             Mean action noise std: 3.56
          Mean value_function loss: 139.7169
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.2467
                       Mean reward: 836.71
               Mean episode length: 222.36
    Episode_Reward/reaching_object: 1.5723
     Episode_Reward/lifting_object: 170.5587
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0712
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 2.43s
                      Time elapsed: 01:07:52
                               ETA: 00:22:00

################################################################################
                     [1m Learning iteration 1511/2000 [0m                     

                       Computation: 39681 steps/s (collection: 2.347s, learning 0.131s)
             Mean action noise std: 3.56
          Mean value_function loss: 146.9144
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 69.2594
                       Mean reward: 791.93
               Mean episode length: 213.93
    Episode_Reward/reaching_object: 1.5598
     Episode_Reward/lifting_object: 168.7870
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0707
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 2.48s
                      Time elapsed: 01:07:54
                               ETA: 00:21:57

################################################################################
                     [1m Learning iteration 1512/2000 [0m                     

                       Computation: 39652 steps/s (collection: 2.339s, learning 0.140s)
             Mean action noise std: 3.56
          Mean value_function loss: 132.3178
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.2719
                       Mean reward: 898.90
               Mean episode length: 238.53
    Episode_Reward/reaching_object: 1.5691
     Episode_Reward/lifting_object: 170.3059
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0711
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 2.48s
                      Time elapsed: 01:07:57
                               ETA: 00:21:55

################################################################################
                     [1m Learning iteration 1513/2000 [0m                     

                       Computation: 40420 steps/s (collection: 2.300s, learning 0.132s)
             Mean action noise std: 3.57
          Mean value_function loss: 141.0377
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 69.2890
                       Mean reward: 881.28
               Mean episode length: 235.06
    Episode_Reward/reaching_object: 1.5963
     Episode_Reward/lifting_object: 173.0586
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0723
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 2.43s
                      Time elapsed: 01:07:59
                               ETA: 00:21:52

################################################################################
                     [1m Learning iteration 1514/2000 [0m                     

                       Computation: 41288 steps/s (collection: 2.269s, learning 0.112s)
             Mean action noise std: 3.57
          Mean value_function loss: 146.6666
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 69.3048
                       Mean reward: 880.42
               Mean episode length: 233.61
    Episode_Reward/reaching_object: 1.5450
     Episode_Reward/lifting_object: 167.3551
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0703
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 2.38s
                      Time elapsed: 01:08:02
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 1515/2000 [0m                     

                       Computation: 40257 steps/s (collection: 2.299s, learning 0.143s)
             Mean action noise std: 3.57
          Mean value_function loss: 157.6301
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 69.3170
                       Mean reward: 845.86
               Mean episode length: 225.71
    Episode_Reward/reaching_object: 1.5506
     Episode_Reward/lifting_object: 168.0842
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0706
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 2.44s
                      Time elapsed: 01:08:04
                               ETA: 00:21:46

################################################################################
                     [1m Learning iteration 1516/2000 [0m                     

                       Computation: 41293 steps/s (collection: 2.256s, learning 0.124s)
             Mean action noise std: 3.57
          Mean value_function loss: 106.0046
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 69.3267
                       Mean reward: 891.78
               Mean episode length: 237.43
    Episode_Reward/reaching_object: 1.5803
     Episode_Reward/lifting_object: 171.2601
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0719
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 2.38s
                      Time elapsed: 01:08:07
                               ETA: 00:21:43

################################################################################
                     [1m Learning iteration 1517/2000 [0m                     

                       Computation: 42071 steps/s (collection: 2.223s, learning 0.113s)
             Mean action noise std: 3.57
          Mean value_function loss: 98.4979
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 69.3347
                       Mean reward: 868.27
               Mean episode length: 230.78
    Episode_Reward/reaching_object: 1.5822
     Episode_Reward/lifting_object: 171.7205
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0718
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 2.34s
                      Time elapsed: 01:08:09
                               ETA: 00:21:41

################################################################################
                     [1m Learning iteration 1518/2000 [0m                     

                       Computation: 40557 steps/s (collection: 2.290s, learning 0.134s)
             Mean action noise std: 3.57
          Mean value_function loss: 108.2217
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 69.3485
                       Mean reward: 858.64
               Mean episode length: 228.46
    Episode_Reward/reaching_object: 1.5720
     Episode_Reward/lifting_object: 170.4366
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0717
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 2.42s
                      Time elapsed: 01:08:11
                               ETA: 00:21:38

################################################################################
                     [1m Learning iteration 1519/2000 [0m                     

                       Computation: 40912 steps/s (collection: 2.269s, learning 0.133s)
             Mean action noise std: 3.58
          Mean value_function loss: 111.9397
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 69.3654
                       Mean reward: 899.13
               Mean episode length: 237.99
    Episode_Reward/reaching_object: 1.6176
     Episode_Reward/lifting_object: 175.4012
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0736
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 2.40s
                      Time elapsed: 01:08:14
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 1520/2000 [0m                     

                       Computation: 41653 steps/s (collection: 2.228s, learning 0.132s)
             Mean action noise std: 3.58
          Mean value_function loss: 147.9422
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 69.3757
                       Mean reward: 890.83
               Mean episode length: 235.50
    Episode_Reward/reaching_object: 1.5518
     Episode_Reward/lifting_object: 168.3753
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0708
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 2.36s
                      Time elapsed: 01:08:16
                               ETA: 00:21:32

################################################################################
                     [1m Learning iteration 1521/2000 [0m                     

                       Computation: 40388 steps/s (collection: 2.280s, learning 0.154s)
             Mean action noise std: 3.58
          Mean value_function loss: 136.6184
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.3849
                       Mean reward: 828.41
               Mean episode length: 219.98
    Episode_Reward/reaching_object: 1.5483
     Episode_Reward/lifting_object: 167.5444
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0704
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 2.43s
                      Time elapsed: 01:08:18
                               ETA: 00:21:30

################################################################################
                     [1m Learning iteration 1522/2000 [0m                     

                       Computation: 39394 steps/s (collection: 2.360s, learning 0.136s)
             Mean action noise std: 3.58
          Mean value_function loss: 141.8453
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 69.3988
                       Mean reward: 869.21
               Mean episode length: 231.73
    Episode_Reward/reaching_object: 1.5589
     Episode_Reward/lifting_object: 168.5664
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0711
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 2.50s
                      Time elapsed: 01:08:21
                               ETA: 00:21:27

################################################################################
                     [1m Learning iteration 1523/2000 [0m                     

                       Computation: 39495 steps/s (collection: 2.353s, learning 0.136s)
             Mean action noise std: 3.58
          Mean value_function loss: 158.7018
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 69.4171
                       Mean reward: 842.17
               Mean episode length: 225.10
    Episode_Reward/reaching_object: 1.5349
     Episode_Reward/lifting_object: 165.8459
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0700
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 2.49s
                      Time elapsed: 01:08:23
                               ETA: 00:21:24

################################################################################
                     [1m Learning iteration 1524/2000 [0m                     

                       Computation: 40206 steps/s (collection: 2.319s, learning 0.126s)
             Mean action noise std: 3.58
          Mean value_function loss: 172.8627
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 69.4305
                       Mean reward: 807.81
               Mean episode length: 215.52
    Episode_Reward/reaching_object: 1.5368
     Episode_Reward/lifting_object: 166.2468
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0700
          Episode_Reward/joint_vel: -0.0542
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 2.44s
                      Time elapsed: 01:08:26
                               ETA: 00:21:21

################################################################################
                     [1m Learning iteration 1525/2000 [0m                     

                       Computation: 38656 steps/s (collection: 2.410s, learning 0.133s)
             Mean action noise std: 3.59
          Mean value_function loss: 177.5379
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 69.4429
                       Mean reward: 839.46
               Mean episode length: 223.66
    Episode_Reward/reaching_object: 1.5403
     Episode_Reward/lifting_object: 166.3068
      Episode_Reward/object_height: 0.0195
        Episode_Reward/action_rate: -0.0704
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 2.54s
                      Time elapsed: 01:08:28
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 1526/2000 [0m                     

                       Computation: 38004 steps/s (collection: 2.423s, learning 0.164s)
             Mean action noise std: 3.59
          Mean value_function loss: 154.6975
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 69.4529
                       Mean reward: 888.77
               Mean episode length: 234.35
    Episode_Reward/reaching_object: 1.5616
     Episode_Reward/lifting_object: 169.0721
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0712
          Episode_Reward/joint_vel: -0.0531
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 2.59s
                      Time elapsed: 01:08:31
                               ETA: 00:21:16

################################################################################
                     [1m Learning iteration 1527/2000 [0m                     

                       Computation: 38365 steps/s (collection: 2.425s, learning 0.138s)
             Mean action noise std: 3.59
          Mean value_function loss: 128.6424
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 69.4625
                       Mean reward: 863.61
               Mean episode length: 228.57
    Episode_Reward/reaching_object: 1.5706
     Episode_Reward/lifting_object: 170.0948
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0714
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 2.56s
                      Time elapsed: 01:08:34
                               ETA: 00:21:13

################################################################################
                     [1m Learning iteration 1528/2000 [0m                     

                       Computation: 39556 steps/s (collection: 2.351s, learning 0.134s)
             Mean action noise std: 3.59
          Mean value_function loss: 129.6212
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 69.4726
                       Mean reward: 867.82
               Mean episode length: 230.65
    Episode_Reward/reaching_object: 1.5854
     Episode_Reward/lifting_object: 171.5747
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0723
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 2.49s
                      Time elapsed: 01:08:36
                               ETA: 00:21:10

################################################################################
                     [1m Learning iteration 1529/2000 [0m                     

                       Computation: 40688 steps/s (collection: 2.278s, learning 0.138s)
             Mean action noise std: 3.59
          Mean value_function loss: 124.9893
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 69.4917
                       Mean reward: 867.67
               Mean episode length: 230.13
    Episode_Reward/reaching_object: 1.6060
     Episode_Reward/lifting_object: 173.3113
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 2.42s
                      Time elapsed: 01:08:39
                               ETA: 00:21:08

################################################################################
                     [1m Learning iteration 1530/2000 [0m                     

                       Computation: 41981 steps/s (collection: 2.214s, learning 0.128s)
             Mean action noise std: 3.60
          Mean value_function loss: 130.8964
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.5072
                       Mean reward: 836.13
               Mean episode length: 221.97
    Episode_Reward/reaching_object: 1.5699
     Episode_Reward/lifting_object: 169.4276
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0718
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 2.34s
                      Time elapsed: 01:08:41
                               ETA: 00:21:05

################################################################################
                     [1m Learning iteration 1531/2000 [0m                     

                       Computation: 41515 steps/s (collection: 2.226s, learning 0.142s)
             Mean action noise std: 3.60
          Mean value_function loss: 125.0315
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 69.5223
                       Mean reward: 818.54
               Mean episode length: 219.38
    Episode_Reward/reaching_object: 1.6075
     Episode_Reward/lifting_object: 173.5869
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0734
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 2.37s
                      Time elapsed: 01:08:43
                               ETA: 00:21:02

################################################################################
                     [1m Learning iteration 1532/2000 [0m                     

                       Computation: 41748 steps/s (collection: 2.224s, learning 0.131s)
             Mean action noise std: 3.60
          Mean value_function loss: 119.3987
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 69.5439
                       Mean reward: 832.90
               Mean episode length: 221.71
    Episode_Reward/reaching_object: 1.5887
     Episode_Reward/lifting_object: 171.4440
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0727
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 2.35s
                      Time elapsed: 01:08:46
                               ETA: 00:20:59

################################################################################
                     [1m Learning iteration 1533/2000 [0m                     

                       Computation: 42178 steps/s (collection: 2.222s, learning 0.109s)
             Mean action noise std: 3.60
          Mean value_function loss: 109.9008
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 69.5563
                       Mean reward: 855.31
               Mean episode length: 227.79
    Episode_Reward/reaching_object: 1.6008
     Episode_Reward/lifting_object: 172.9498
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 2.33s
                      Time elapsed: 01:08:48
                               ETA: 00:20:56

################################################################################
                     [1m Learning iteration 1534/2000 [0m                     

                       Computation: 40585 steps/s (collection: 2.273s, learning 0.149s)
             Mean action noise std: 3.60
          Mean value_function loss: 106.0367
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 69.5640
                       Mean reward: 878.06
               Mean episode length: 233.02
    Episode_Reward/reaching_object: 1.6132
     Episode_Reward/lifting_object: 174.2642
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0740
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 2.42s
                      Time elapsed: 01:08:50
                               ETA: 00:20:54

################################################################################
                     [1m Learning iteration 1535/2000 [0m                     

                       Computation: 40767 steps/s (collection: 2.268s, learning 0.143s)
             Mean action noise std: 3.60
          Mean value_function loss: 127.6891
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 69.5760
                       Mean reward: 882.05
               Mean episode length: 233.82
    Episode_Reward/reaching_object: 1.5910
     Episode_Reward/lifting_object: 171.5854
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 2.41s
                      Time elapsed: 01:08:53
                               ETA: 00:20:51

################################################################################
                     [1m Learning iteration 1536/2000 [0m                     

                       Computation: 42243 steps/s (collection: 2.213s, learning 0.114s)
             Mean action noise std: 3.61
          Mean value_function loss: 138.7565
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 69.5889
                       Mean reward: 857.70
               Mean episode length: 228.86
    Episode_Reward/reaching_object: 1.5762
     Episode_Reward/lifting_object: 169.8059
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0726
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 2.33s
                      Time elapsed: 01:08:55
                               ETA: 00:20:48

################################################################################
                     [1m Learning iteration 1537/2000 [0m                     

                       Computation: 40733 steps/s (collection: 2.276s, learning 0.137s)
             Mean action noise std: 3.61
          Mean value_function loss: 114.7554
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 69.6025
                       Mean reward: 869.70
               Mean episode length: 232.65
    Episode_Reward/reaching_object: 1.5609
     Episode_Reward/lifting_object: 168.0978
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.0720
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 2.41s
                      Time elapsed: 01:08:57
                               ETA: 00:20:45

################################################################################
                     [1m Learning iteration 1538/2000 [0m                     

                       Computation: 41195 steps/s (collection: 2.252s, learning 0.134s)
             Mean action noise std: 3.61
          Mean value_function loss: 136.1171
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 69.6196
                       Mean reward: 865.23
               Mean episode length: 229.93
    Episode_Reward/reaching_object: 1.5376
     Episode_Reward/lifting_object: 165.6615
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0709
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 2.39s
                      Time elapsed: 01:09:00
                               ETA: 00:20:42

################################################################################
                     [1m Learning iteration 1539/2000 [0m                     

                       Computation: 35348 steps/s (collection: 2.631s, learning 0.150s)
             Mean action noise std: 3.61
          Mean value_function loss: 118.8836
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 69.6268
                       Mean reward: 870.43
               Mean episode length: 231.68
    Episode_Reward/reaching_object: 1.5895
     Episode_Reward/lifting_object: 171.6422
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0734
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 2.78s
                      Time elapsed: 01:09:03
                               ETA: 00:20:40

################################################################################
                     [1m Learning iteration 1540/2000 [0m                     

                       Computation: 39136 steps/s (collection: 2.354s, learning 0.158s)
             Mean action noise std: 3.61
          Mean value_function loss: 97.1360
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 69.6379
                       Mean reward: 860.71
               Mean episode length: 229.15
    Episode_Reward/reaching_object: 1.5681
     Episode_Reward/lifting_object: 169.3987
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0727
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 2.51s
                      Time elapsed: 01:09:05
                               ETA: 00:20:37

################################################################################
                     [1m Learning iteration 1541/2000 [0m                     

                       Computation: 40641 steps/s (collection: 2.279s, learning 0.140s)
             Mean action noise std: 3.62
          Mean value_function loss: 122.3384
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 69.6578
                       Mean reward: 860.22
               Mean episode length: 228.33
    Episode_Reward/reaching_object: 1.5496
     Episode_Reward/lifting_object: 166.9244
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0718
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 2.42s
                      Time elapsed: 01:09:08
                               ETA: 00:20:34

################################################################################
                     [1m Learning iteration 1542/2000 [0m                     

                       Computation: 40417 steps/s (collection: 2.317s, learning 0.115s)
             Mean action noise std: 3.62
          Mean value_function loss: 128.3277
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 69.6754
                       Mean reward: 875.18
               Mean episode length: 231.88
    Episode_Reward/reaching_object: 1.5964
     Episode_Reward/lifting_object: 172.3737
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0739
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 2.43s
                      Time elapsed: 01:09:10
                               ETA: 00:20:31

################################################################################
                     [1m Learning iteration 1543/2000 [0m                     

                       Computation: 40451 steps/s (collection: 2.289s, learning 0.142s)
             Mean action noise std: 3.62
          Mean value_function loss: 110.9700
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 69.6894
                       Mean reward: 890.82
               Mean episode length: 235.53
    Episode_Reward/reaching_object: 1.6075
     Episode_Reward/lifting_object: 173.7827
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0744
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 2.43s
                      Time elapsed: 01:09:12
                               ETA: 00:20:29

################################################################################
                     [1m Learning iteration 1544/2000 [0m                     

                       Computation: 39815 steps/s (collection: 2.316s, learning 0.153s)
             Mean action noise std: 3.62
          Mean value_function loss: 183.3003
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 69.6963
                       Mean reward: 847.07
               Mean episode length: 225.87
    Episode_Reward/reaching_object: 1.5692
     Episode_Reward/lifting_object: 169.4061
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 2.47s
                      Time elapsed: 01:09:15
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 1545/2000 [0m                     

                       Computation: 40709 steps/s (collection: 2.280s, learning 0.135s)
             Mean action noise std: 3.62
          Mean value_function loss: 161.2835
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.7044
                       Mean reward: 818.78
               Mean episode length: 218.24
    Episode_Reward/reaching_object: 1.5322
     Episode_Reward/lifting_object: 164.9697
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0713
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 2.41s
                      Time elapsed: 01:09:17
                               ETA: 00:20:23

################################################################################
                     [1m Learning iteration 1546/2000 [0m                     

                       Computation: 42343 steps/s (collection: 2.184s, learning 0.138s)
             Mean action noise std: 3.62
          Mean value_function loss: 141.9807
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 69.7206
                       Mean reward: 836.27
               Mean episode length: 223.35
    Episode_Reward/reaching_object: 1.5670
     Episode_Reward/lifting_object: 168.8367
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0727
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 2.32s
                      Time elapsed: 01:09:20
                               ETA: 00:20:20

################################################################################
                     [1m Learning iteration 1547/2000 [0m                     

                       Computation: 41895 steps/s (collection: 2.207s, learning 0.140s)
             Mean action noise std: 3.63
          Mean value_function loss: 165.4821
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 69.7396
                       Mean reward: 796.60
               Mean episode length: 213.41
    Episode_Reward/reaching_object: 1.5018
     Episode_Reward/lifting_object: 161.5989
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 2.35s
                      Time elapsed: 01:09:22
                               ETA: 00:20:18

################################################################################
                     [1m Learning iteration 1548/2000 [0m                     

                       Computation: 41146 steps/s (collection: 2.248s, learning 0.141s)
             Mean action noise std: 3.63
          Mean value_function loss: 116.2834
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 69.7543
                       Mean reward: 871.22
               Mean episode length: 231.35
    Episode_Reward/reaching_object: 1.5735
     Episode_Reward/lifting_object: 169.9001
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 2.39s
                      Time elapsed: 01:09:24
                               ETA: 00:20:15

################################################################################
                     [1m Learning iteration 1549/2000 [0m                     

                       Computation: 41973 steps/s (collection: 2.215s, learning 0.127s)
             Mean action noise std: 3.63
          Mean value_function loss: 134.0985
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 69.7636
                       Mean reward: 823.62
               Mean episode length: 220.36
    Episode_Reward/reaching_object: 1.5899
     Episode_Reward/lifting_object: 171.4783
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0740
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 2.34s
                      Time elapsed: 01:09:27
                               ETA: 00:20:12

################################################################################
                     [1m Learning iteration 1550/2000 [0m                     

                       Computation: 41897 steps/s (collection: 2.233s, learning 0.114s)
             Mean action noise std: 3.63
          Mean value_function loss: 121.1811
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 69.7723
                       Mean reward: 815.18
               Mean episode length: 216.72
    Episode_Reward/reaching_object: 1.5534
     Episode_Reward/lifting_object: 167.6395
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0725
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 2.35s
                      Time elapsed: 01:09:29
                               ETA: 00:20:09

################################################################################
                     [1m Learning iteration 1551/2000 [0m                     

                       Computation: 40423 steps/s (collection: 2.290s, learning 0.142s)
             Mean action noise std: 3.63
          Mean value_function loss: 104.7035
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 69.7818
                       Mean reward: 904.18
               Mean episode length: 238.56
    Episode_Reward/reaching_object: 1.6052
     Episode_Reward/lifting_object: 173.0992
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 2.43s
                      Time elapsed: 01:09:32
                               ETA: 00:20:06

################################################################################
                     [1m Learning iteration 1552/2000 [0m                     

                       Computation: 42173 steps/s (collection: 2.199s, learning 0.132s)
             Mean action noise std: 3.63
          Mean value_function loss: 119.8131
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 69.7913
                       Mean reward: 878.65
               Mean episode length: 233.08
    Episode_Reward/reaching_object: 1.5661
     Episode_Reward/lifting_object: 168.5226
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 2.33s
                      Time elapsed: 01:09:34
                               ETA: 00:20:04

################################################################################
                     [1m Learning iteration 1553/2000 [0m                     

                       Computation: 41549 steps/s (collection: 2.243s, learning 0.123s)
             Mean action noise std: 3.64
          Mean value_function loss: 132.3452
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 69.8037
                       Mean reward: 857.65
               Mean episode length: 228.52
    Episode_Reward/reaching_object: 1.5798
     Episode_Reward/lifting_object: 170.1043
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0738
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 2.37s
                      Time elapsed: 01:09:36
                               ETA: 00:20:01

################################################################################
                     [1m Learning iteration 1554/2000 [0m                     

                       Computation: 41270 steps/s (collection: 2.242s, learning 0.140s)
             Mean action noise std: 3.64
          Mean value_function loss: 140.7478
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 69.8142
                       Mean reward: 822.92
               Mean episode length: 219.85
    Episode_Reward/reaching_object: 1.5366
     Episode_Reward/lifting_object: 165.0383
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0721
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 2.38s
                      Time elapsed: 01:09:39
                               ETA: 00:19:58

################################################################################
                     [1m Learning iteration 1555/2000 [0m                     

                       Computation: 40625 steps/s (collection: 2.269s, learning 0.151s)
             Mean action noise std: 3.64
          Mean value_function loss: 149.1014
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 69.8203
                       Mean reward: 853.48
               Mean episode length: 226.12
    Episode_Reward/reaching_object: 1.5706
     Episode_Reward/lifting_object: 169.1875
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0735
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 2.42s
                      Time elapsed: 01:09:41
                               ETA: 00:19:55

################################################################################
                     [1m Learning iteration 1556/2000 [0m                     

                       Computation: 41500 steps/s (collection: 2.253s, learning 0.116s)
             Mean action noise std: 3.64
          Mean value_function loss: 118.4770
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 69.8340
                       Mean reward: 875.77
               Mean episode length: 232.74
    Episode_Reward/reaching_object: 1.5819
     Episode_Reward/lifting_object: 170.0502
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0741
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 2.37s
                      Time elapsed: 01:09:43
                               ETA: 00:19:53

################################################################################
                     [1m Learning iteration 1557/2000 [0m                     

                       Computation: 42237 steps/s (collection: 2.194s, learning 0.133s)
             Mean action noise std: 3.64
          Mean value_function loss: 119.6321
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 69.8499
                       Mean reward: 840.99
               Mean episode length: 223.88
    Episode_Reward/reaching_object: 1.5684
     Episode_Reward/lifting_object: 168.4958
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0736
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 2.33s
                      Time elapsed: 01:09:46
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 1558/2000 [0m                     

                       Computation: 42129 steps/s (collection: 2.197s, learning 0.136s)
             Mean action noise std: 3.65
          Mean value_function loss: 122.5287
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 69.8627
                       Mean reward: 883.48
               Mean episode length: 234.38
    Episode_Reward/reaching_object: 1.6002
     Episode_Reward/lifting_object: 172.1453
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0748
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 2.33s
                      Time elapsed: 01:09:48
                               ETA: 00:19:47

################################################################################
                     [1m Learning iteration 1559/2000 [0m                     

                       Computation: 42392 steps/s (collection: 2.213s, learning 0.106s)
             Mean action noise std: 3.65
          Mean value_function loss: 169.5960
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 69.8808
                       Mean reward: 808.35
               Mean episode length: 216.19
    Episode_Reward/reaching_object: 1.5028
     Episode_Reward/lifting_object: 161.5987
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0706
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 2.32s
                      Time elapsed: 01:09:50
                               ETA: 00:19:44

################################################################################
                     [1m Learning iteration 1560/2000 [0m                     

                       Computation: 41861 steps/s (collection: 2.240s, learning 0.108s)
             Mean action noise std: 3.65
          Mean value_function loss: 118.1680
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 69.8952
                       Mean reward: 850.39
               Mean episode length: 226.38
    Episode_Reward/reaching_object: 1.5729
     Episode_Reward/lifting_object: 169.4002
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0738
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 2.35s
                      Time elapsed: 01:09:53
                               ETA: 00:19:41

################################################################################
                     [1m Learning iteration 1561/2000 [0m                     

                       Computation: 40294 steps/s (collection: 2.330s, learning 0.110s)
             Mean action noise std: 3.65
          Mean value_function loss: 127.6230
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 69.9058
                       Mean reward: 863.17
               Mean episode length: 229.12
    Episode_Reward/reaching_object: 1.5664
     Episode_Reward/lifting_object: 168.7578
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 2.44s
                      Time elapsed: 01:09:55
                               ETA: 00:19:39

################################################################################
                     [1m Learning iteration 1562/2000 [0m                     

                       Computation: 41336 steps/s (collection: 2.232s, learning 0.146s)
             Mean action noise std: 3.65
          Mean value_function loss: 118.4269
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 69.9152
                       Mean reward: 891.13
               Mean episode length: 236.87
    Episode_Reward/reaching_object: 1.5570
     Episode_Reward/lifting_object: 167.4900
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 2.38s
                      Time elapsed: 01:09:58
                               ETA: 00:19:36

################################################################################
                     [1m Learning iteration 1563/2000 [0m                     

                       Computation: 41706 steps/s (collection: 2.233s, learning 0.124s)
             Mean action noise std: 3.65
          Mean value_function loss: 125.3637
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 69.9247
                       Mean reward: 814.53
               Mean episode length: 217.99
    Episode_Reward/reaching_object: 1.5546
     Episode_Reward/lifting_object: 167.2103
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0735
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 2.36s
                      Time elapsed: 01:10:00
                               ETA: 00:19:33

################################################################################
                     [1m Learning iteration 1564/2000 [0m                     

                       Computation: 41237 steps/s (collection: 2.276s, learning 0.108s)
             Mean action noise std: 3.66
          Mean value_function loss: 110.5969
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 69.9443
                       Mean reward: 836.25
               Mean episode length: 221.97
    Episode_Reward/reaching_object: 1.5826
     Episode_Reward/lifting_object: 170.5736
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0744
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 2.38s
                      Time elapsed: 01:10:02
                               ETA: 00:19:30

################################################################################
                     [1m Learning iteration 1565/2000 [0m                     

                       Computation: 41061 steps/s (collection: 2.284s, learning 0.110s)
             Mean action noise std: 3.66
          Mean value_function loss: 108.6900
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 69.9583
                       Mean reward: 815.98
               Mean episode length: 217.57
    Episode_Reward/reaching_object: 1.5826
     Episode_Reward/lifting_object: 170.7243
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0745
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 2.39s
                      Time elapsed: 01:10:05
                               ETA: 00:19:28

################################################################################
                     [1m Learning iteration 1566/2000 [0m                     

                       Computation: 41365 steps/s (collection: 2.263s, learning 0.113s)
             Mean action noise std: 3.66
          Mean value_function loss: 116.9320
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 69.9738
                       Mean reward: 897.08
               Mean episode length: 237.70
    Episode_Reward/reaching_object: 1.6211
     Episode_Reward/lifting_object: 175.2336
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0766
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 2.38s
                      Time elapsed: 01:10:07
                               ETA: 00:19:25

################################################################################
                     [1m Learning iteration 1567/2000 [0m                     

                       Computation: 41564 steps/s (collection: 2.247s, learning 0.119s)
             Mean action noise std: 3.66
          Mean value_function loss: 153.8183
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.9870
                       Mean reward: 884.09
               Mean episode length: 234.15
    Episode_Reward/reaching_object: 1.5763
     Episode_Reward/lifting_object: 170.3945
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0748
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 2.37s
                      Time elapsed: 01:10:09
                               ETA: 00:19:22

################################################################################
                     [1m Learning iteration 1568/2000 [0m                     

                       Computation: 41862 steps/s (collection: 2.243s, learning 0.106s)
             Mean action noise std: 3.66
          Mean value_function loss: 142.8428
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 70.0027
                       Mean reward: 874.81
               Mean episode length: 231.73
    Episode_Reward/reaching_object: 1.5419
     Episode_Reward/lifting_object: 166.5046
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0737
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 2.35s
                      Time elapsed: 01:10:12
                               ETA: 00:19:19

################################################################################
                     [1m Learning iteration 1569/2000 [0m                     

                       Computation: 40737 steps/s (collection: 2.269s, learning 0.144s)
             Mean action noise std: 3.67
          Mean value_function loss: 141.3272
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 70.0134
                       Mean reward: 863.96
               Mean episode length: 229.72
    Episode_Reward/reaching_object: 1.5848
     Episode_Reward/lifting_object: 171.3397
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0757
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 2.41s
                      Time elapsed: 01:10:14
                               ETA: 00:19:17

################################################################################
                     [1m Learning iteration 1570/2000 [0m                     

                       Computation: 42720 steps/s (collection: 2.200s, learning 0.102s)
             Mean action noise std: 3.67
          Mean value_function loss: 139.6869
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 70.0206
                       Mean reward: 870.90
               Mean episode length: 230.46
    Episode_Reward/reaching_object: 1.5295
     Episode_Reward/lifting_object: 165.3567
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0734
          Episode_Reward/joint_vel: -0.0550
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 2.30s
                      Time elapsed: 01:10:16
                               ETA: 00:19:14

################################################################################
                     [1m Learning iteration 1571/2000 [0m                     

                       Computation: 42609 steps/s (collection: 2.210s, learning 0.097s)
             Mean action noise std: 3.67
          Mean value_function loss: 153.4608
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 70.0312
                       Mean reward: 878.58
               Mean episode length: 232.14
    Episode_Reward/reaching_object: 1.5405
     Episode_Reward/lifting_object: 166.6683
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0739
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 2.31s
                      Time elapsed: 01:10:19
                               ETA: 00:19:11

################################################################################
                     [1m Learning iteration 1572/2000 [0m                     

                       Computation: 42213 steps/s (collection: 2.196s, learning 0.133s)
             Mean action noise std: 3.67
          Mean value_function loss: 150.3657
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 70.0428
                       Mean reward: 830.14
               Mean episode length: 221.11
    Episode_Reward/reaching_object: 1.5595
     Episode_Reward/lifting_object: 169.0169
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 2.33s
                      Time elapsed: 01:10:21
                               ETA: 00:19:08

################################################################################
                     [1m Learning iteration 1573/2000 [0m                     

                       Computation: 42588 steps/s (collection: 2.199s, learning 0.109s)
             Mean action noise std: 3.67
          Mean value_function loss: 125.7356
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 70.0520
                       Mean reward: 890.75
               Mean episode length: 235.88
    Episode_Reward/reaching_object: 1.5735
     Episode_Reward/lifting_object: 170.2289
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0756
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 2.31s
                      Time elapsed: 01:10:23
                               ETA: 00:19:05

################################################################################
                     [1m Learning iteration 1574/2000 [0m                     

                       Computation: 41663 steps/s (collection: 2.219s, learning 0.140s)
             Mean action noise std: 3.67
          Mean value_function loss: 121.2607
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 70.0653
                       Mean reward: 884.35
               Mean episode length: 234.65
    Episode_Reward/reaching_object: 1.5700
     Episode_Reward/lifting_object: 170.0492
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 2.36s
                      Time elapsed: 01:10:26
                               ETA: 00:19:03

################################################################################
                     [1m Learning iteration 1575/2000 [0m                     

                       Computation: 41701 steps/s (collection: 2.217s, learning 0.141s)
             Mean action noise std: 3.68
          Mean value_function loss: 125.8146
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 70.0784
                       Mean reward: 827.27
               Mean episode length: 220.51
    Episode_Reward/reaching_object: 1.5578
     Episode_Reward/lifting_object: 168.6675
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0749
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 2.36s
                      Time elapsed: 01:10:28
                               ETA: 00:19:00

################################################################################
                     [1m Learning iteration 1576/2000 [0m                     

                       Computation: 41759 steps/s (collection: 2.211s, learning 0.143s)
             Mean action noise std: 3.68
          Mean value_function loss: 137.1062
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 70.0932
                       Mean reward: 847.81
               Mean episode length: 225.81
    Episode_Reward/reaching_object: 1.5679
     Episode_Reward/lifting_object: 170.2081
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0756
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 2.35s
                      Time elapsed: 01:10:30
                               ETA: 00:18:57

################################################################################
                     [1m Learning iteration 1577/2000 [0m                     

                       Computation: 40516 steps/s (collection: 2.298s, learning 0.128s)
             Mean action noise std: 3.68
          Mean value_function loss: 138.3240
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 70.1037
                       Mean reward: 828.66
               Mean episode length: 221.92
    Episode_Reward/reaching_object: 1.5272
     Episode_Reward/lifting_object: 165.0848
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0739
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 2.43s
                      Time elapsed: 01:10:33
                               ETA: 00:18:54

################################################################################
                     [1m Learning iteration 1578/2000 [0m                     

                       Computation: 42094 steps/s (collection: 2.228s, learning 0.108s)
             Mean action noise std: 3.68
          Mean value_function loss: 156.5025
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 70.1128
                       Mean reward: 861.14
               Mean episode length: 230.30
    Episode_Reward/reaching_object: 1.5658
     Episode_Reward/lifting_object: 169.6546
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0757
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 2.34s
                      Time elapsed: 01:10:35
                               ETA: 00:18:52

################################################################################
                     [1m Learning iteration 1579/2000 [0m                     

                       Computation: 42097 steps/s (collection: 2.203s, learning 0.132s)
             Mean action noise std: 3.68
          Mean value_function loss: 170.4642
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.1278
                       Mean reward: 860.96
               Mean episode length: 229.05
    Episode_Reward/reaching_object: 1.5487
     Episode_Reward/lifting_object: 167.7762
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0750
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.34s
                      Time elapsed: 01:10:38
                               ETA: 00:18:49

################################################################################
                     [1m Learning iteration 1580/2000 [0m                     

                       Computation: 39459 steps/s (collection: 2.341s, learning 0.150s)
             Mean action noise std: 3.68
          Mean value_function loss: 155.6519
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 70.1383
                       Mean reward: 806.76
               Mean episode length: 216.04
    Episode_Reward/reaching_object: 1.5176
     Episode_Reward/lifting_object: 164.2178
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0734
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 2.49s
                      Time elapsed: 01:10:40
                               ETA: 00:18:46

################################################################################
                     [1m Learning iteration 1581/2000 [0m                     

                       Computation: 42166 steps/s (collection: 2.230s, learning 0.102s)
             Mean action noise std: 3.69
          Mean value_function loss: 126.8494
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.1526
                       Mean reward: 849.72
               Mean episode length: 227.08
    Episode_Reward/reaching_object: 1.5680
     Episode_Reward/lifting_object: 170.1742
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0759
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 2.33s
                      Time elapsed: 01:10:42
                               ETA: 00:18:43

################################################################################
                     [1m Learning iteration 1582/2000 [0m                     

                       Computation: 41909 steps/s (collection: 2.222s, learning 0.124s)
             Mean action noise std: 3.69
          Mean value_function loss: 153.6259
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 70.1740
                       Mean reward: 843.04
               Mean episode length: 224.79
    Episode_Reward/reaching_object: 1.5386
     Episode_Reward/lifting_object: 166.8907
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 2.35s
                      Time elapsed: 01:10:45
                               ETA: 00:18:40

################################################################################
                     [1m Learning iteration 1583/2000 [0m                     

                       Computation: 42158 steps/s (collection: 2.213s, learning 0.119s)
             Mean action noise std: 3.69
          Mean value_function loss: 162.7213
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 70.1813
                       Mean reward: 876.56
               Mean episode length: 232.14
    Episode_Reward/reaching_object: 1.5525
     Episode_Reward/lifting_object: 168.6362
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0751
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 2.33s
                      Time elapsed: 01:10:47
                               ETA: 00:18:38

################################################################################
                     [1m Learning iteration 1584/2000 [0m                     

                       Computation: 41558 steps/s (collection: 2.256s, learning 0.110s)
             Mean action noise std: 3.69
          Mean value_function loss: 144.9633
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 70.1845
                       Mean reward: 898.46
               Mean episode length: 238.77
    Episode_Reward/reaching_object: 1.5755
     Episode_Reward/lifting_object: 171.0292
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 2.37s
                      Time elapsed: 01:10:49
                               ETA: 00:18:35

################################################################################
                     [1m Learning iteration 1585/2000 [0m                     

                       Computation: 41340 steps/s (collection: 2.237s, learning 0.141s)
             Mean action noise std: 3.69
          Mean value_function loss: 144.8080
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.1932
                       Mean reward: 853.60
               Mean episode length: 226.78
    Episode_Reward/reaching_object: 1.5596
     Episode_Reward/lifting_object: 168.3172
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0759
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 2.38s
                      Time elapsed: 01:10:52
                               ETA: 00:18:32

################################################################################
                     [1m Learning iteration 1586/2000 [0m                     

                       Computation: 42419 steps/s (collection: 2.192s, learning 0.125s)
             Mean action noise std: 3.69
          Mean value_function loss: 174.3781
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 70.2069
                       Mean reward: 855.89
               Mean episode length: 227.07
    Episode_Reward/reaching_object: 1.5651
     Episode_Reward/lifting_object: 169.8755
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0759
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 2.32s
                      Time elapsed: 01:10:54
                               ETA: 00:18:29

################################################################################
                     [1m Learning iteration 1587/2000 [0m                     

                       Computation: 42469 steps/s (collection: 2.198s, learning 0.117s)
             Mean action noise std: 3.70
          Mean value_function loss: 130.5179
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 70.2237
                       Mean reward: 855.07
               Mean episode length: 226.15
    Episode_Reward/reaching_object: 1.5259
     Episode_Reward/lifting_object: 165.3444
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0743
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 2.31s
                      Time elapsed: 01:10:56
                               ETA: 00:18:27

################################################################################
                     [1m Learning iteration 1588/2000 [0m                     

                       Computation: 41072 steps/s (collection: 2.262s, learning 0.131s)
             Mean action noise std: 3.70
          Mean value_function loss: 136.7743
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 70.2400
                       Mean reward: 838.28
               Mean episode length: 223.51
    Episode_Reward/reaching_object: 1.5756
     Episode_Reward/lifting_object: 170.9786
      Episode_Reward/object_height: 0.0207
        Episode_Reward/action_rate: -0.0765
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 2.39s
                      Time elapsed: 01:10:59
                               ETA: 00:18:24

################################################################################
                     [1m Learning iteration 1589/2000 [0m                     

                       Computation: 42322 steps/s (collection: 2.197s, learning 0.126s)
             Mean action noise std: 3.70
          Mean value_function loss: 140.8644
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 70.2536
                       Mean reward: 874.89
               Mean episode length: 231.65
    Episode_Reward/reaching_object: 1.5634
     Episode_Reward/lifting_object: 169.7577
      Episode_Reward/object_height: 0.0205
        Episode_Reward/action_rate: -0.0758
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 2.32s
                      Time elapsed: 01:11:01
                               ETA: 00:18:21

################################################################################
                     [1m Learning iteration 1590/2000 [0m                     

                       Computation: 41508 steps/s (collection: 2.263s, learning 0.106s)
             Mean action noise std: 3.70
          Mean value_function loss: 143.0009
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 70.2652
                       Mean reward: 896.05
               Mean episode length: 236.47
    Episode_Reward/reaching_object: 1.5890
     Episode_Reward/lifting_object: 172.5799
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0771
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.37s
                      Time elapsed: 01:11:04
                               ETA: 00:18:18

################################################################################
                     [1m Learning iteration 1591/2000 [0m                     

                       Computation: 41314 steps/s (collection: 2.249s, learning 0.130s)
             Mean action noise std: 3.70
          Mean value_function loss: 146.8189
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 70.2767
                       Mean reward: 806.05
               Mean episode length: 215.45
    Episode_Reward/reaching_object: 1.5186
     Episode_Reward/lifting_object: 164.3306
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0739
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.38s
                      Time elapsed: 01:11:06
                               ETA: 00:18:16

################################################################################
                     [1m Learning iteration 1592/2000 [0m                     

                       Computation: 41295 steps/s (collection: 2.244s, learning 0.136s)
             Mean action noise std: 3.71
          Mean value_function loss: 137.4891
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 70.2884
                       Mean reward: 882.85
               Mean episode length: 233.53
    Episode_Reward/reaching_object: 1.5509
     Episode_Reward/lifting_object: 168.1739
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0756
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 2.38s
                      Time elapsed: 01:11:08
                               ETA: 00:18:13

################################################################################
                     [1m Learning iteration 1593/2000 [0m                     

                       Computation: 42555 steps/s (collection: 2.201s, learning 0.109s)
             Mean action noise std: 3.71
          Mean value_function loss: 123.8516
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 70.2967
                       Mean reward: 883.71
               Mean episode length: 234.90
    Episode_Reward/reaching_object: 1.6016
     Episode_Reward/lifting_object: 173.6891
      Episode_Reward/object_height: 0.0203
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.31s
                      Time elapsed: 01:11:11
                               ETA: 00:18:10

################################################################################
                     [1m Learning iteration 1594/2000 [0m                     

                       Computation: 41354 steps/s (collection: 2.249s, learning 0.128s)
             Mean action noise std: 3.71
          Mean value_function loss: 146.0007
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 70.3051
                       Mean reward: 797.50
               Mean episode length: 214.85
    Episode_Reward/reaching_object: 1.5210
     Episode_Reward/lifting_object: 164.1236
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0745
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 2.38s
                      Time elapsed: 01:11:13
                               ETA: 00:18:07

################################################################################
                     [1m Learning iteration 1595/2000 [0m                     

                       Computation: 41470 steps/s (collection: 2.230s, learning 0.141s)
             Mean action noise std: 3.71
          Mean value_function loss: 140.8898
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 70.3097
                       Mean reward: 892.80
               Mean episode length: 236.24
    Episode_Reward/reaching_object: 1.5486
     Episode_Reward/lifting_object: 167.5286
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0755
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.37s
                      Time elapsed: 01:11:15
                               ETA: 00:18:05

################################################################################
                     [1m Learning iteration 1596/2000 [0m                     

                       Computation: 42754 steps/s (collection: 2.200s, learning 0.099s)
             Mean action noise std: 3.71
          Mean value_function loss: 143.7046
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 70.3218
                       Mean reward: 850.06
               Mean episode length: 226.80
    Episode_Reward/reaching_object: 1.5374
     Episode_Reward/lifting_object: 166.3633
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0756
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.30s
                      Time elapsed: 01:11:18
                               ETA: 00:18:02

################################################################################
                     [1m Learning iteration 1597/2000 [0m                     

                       Computation: 40356 steps/s (collection: 2.300s, learning 0.136s)
             Mean action noise std: 3.71
          Mean value_function loss: 142.8051
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 70.3380
                       Mean reward: 816.75
               Mean episode length: 218.07
    Episode_Reward/reaching_object: 1.5564
     Episode_Reward/lifting_object: 168.5669
      Episode_Reward/object_height: 0.0197
        Episode_Reward/action_rate: -0.0761
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 2.44s
                      Time elapsed: 01:11:20
                               ETA: 00:17:59

################################################################################
                     [1m Learning iteration 1598/2000 [0m                     

                       Computation: 41305 steps/s (collection: 2.236s, learning 0.144s)
             Mean action noise std: 3.71
          Mean value_function loss: 130.1758
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 70.3500
                       Mean reward: 833.70
               Mean episode length: 221.40
    Episode_Reward/reaching_object: 1.5522
     Episode_Reward/lifting_object: 168.1787
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0762
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.38s
                      Time elapsed: 01:11:22
                               ETA: 00:17:56

################################################################################
                     [1m Learning iteration 1599/2000 [0m                     

                       Computation: 41714 steps/s (collection: 2.247s, learning 0.109s)
             Mean action noise std: 3.72
          Mean value_function loss: 136.4008
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 70.3617
                       Mean reward: 837.94
               Mean episode length: 223.60
    Episode_Reward/reaching_object: 1.5688
     Episode_Reward/lifting_object: 169.4626
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0772
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.36s
                      Time elapsed: 01:11:25
                               ETA: 00:17:54

################################################################################
                     [1m Learning iteration 1600/2000 [0m                     

                       Computation: 40932 steps/s (collection: 2.258s, learning 0.144s)
             Mean action noise std: 3.72
          Mean value_function loss: 103.1204
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 70.3762
                       Mean reward: 897.47
               Mean episode length: 238.36
    Episode_Reward/reaching_object: 1.5586
     Episode_Reward/lifting_object: 168.4092
      Episode_Reward/object_height: 0.0202
        Episode_Reward/action_rate: -0.0768
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.40s
                      Time elapsed: 01:11:27
                               ETA: 00:17:51

################################################################################
                     [1m Learning iteration 1601/2000 [0m                     

                       Computation: 41194 steps/s (collection: 2.260s, learning 0.126s)
             Mean action noise std: 3.72
          Mean value_function loss: 104.9972
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 70.3923
                       Mean reward: 856.40
               Mean episode length: 227.25
    Episode_Reward/reaching_object: 1.5935
     Episode_Reward/lifting_object: 172.8059
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.39s
                      Time elapsed: 01:11:30
                               ETA: 00:17:48

################################################################################
                     [1m Learning iteration 1602/2000 [0m                     

                       Computation: 40998 steps/s (collection: 2.255s, learning 0.143s)
             Mean action noise std: 3.72
          Mean value_function loss: 141.8472
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 70.4027
                       Mean reward: 872.40
               Mean episode length: 231.13
    Episode_Reward/reaching_object: 1.5738
     Episode_Reward/lifting_object: 170.6267
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.40s
                      Time elapsed: 01:11:32
                               ETA: 00:17:45

################################################################################
                     [1m Learning iteration 1603/2000 [0m                     

                       Computation: 41762 steps/s (collection: 2.216s, learning 0.138s)
             Mean action noise std: 3.73
          Mean value_function loss: 163.1716
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 70.4182
                       Mean reward: 810.13
               Mean episode length: 215.88
    Episode_Reward/reaching_object: 1.5416
     Episode_Reward/lifting_object: 166.7728
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0762
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.35s
                      Time elapsed: 01:11:34
                               ETA: 00:17:43

################################################################################
                     [1m Learning iteration 1604/2000 [0m                     

                       Computation: 40101 steps/s (collection: 2.320s, learning 0.132s)
             Mean action noise std: 3.73
          Mean value_function loss: 111.9770
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 70.4382
                       Mean reward: 878.11
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 1.5760
     Episode_Reward/lifting_object: 170.6864
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.45s
                      Time elapsed: 01:11:37
                               ETA: 00:17:40

################################################################################
                     [1m Learning iteration 1605/2000 [0m                     

                       Computation: 40240 steps/s (collection: 2.321s, learning 0.122s)
             Mean action noise std: 3.73
          Mean value_function loss: 99.6857
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 70.4574
                       Mean reward: 879.10
               Mean episode length: 232.82
    Episode_Reward/reaching_object: 1.6230
     Episode_Reward/lifting_object: 176.2273
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0799
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.44s
                      Time elapsed: 01:11:39
                               ETA: 00:17:37

################################################################################
                     [1m Learning iteration 1606/2000 [0m                     

                       Computation: 42049 steps/s (collection: 2.234s, learning 0.104s)
             Mean action noise std: 3.73
          Mean value_function loss: 109.0770
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 70.4807
                       Mean reward: 885.83
               Mean episode length: 234.50
    Episode_Reward/reaching_object: 1.5873
     Episode_Reward/lifting_object: 171.7192
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0783
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.34s
                      Time elapsed: 01:11:42
                               ETA: 00:17:34

################################################################################
                     [1m Learning iteration 1607/2000 [0m                     

                       Computation: 41282 steps/s (collection: 2.254s, learning 0.127s)
             Mean action noise std: 3.74
          Mean value_function loss: 114.3713
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 70.4983
                       Mean reward: 873.52
               Mean episode length: 232.95
    Episode_Reward/reaching_object: 1.6035
     Episode_Reward/lifting_object: 173.5734
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0792
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 2.38s
                      Time elapsed: 01:11:44
                               ETA: 00:17:32

################################################################################
                     [1m Learning iteration 1608/2000 [0m                     

                       Computation: 41285 steps/s (collection: 2.260s, learning 0.122s)
             Mean action noise std: 3.74
          Mean value_function loss: 114.1563
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 70.5123
                       Mean reward: 906.19
               Mean episode length: 238.26
    Episode_Reward/reaching_object: 1.5746
     Episode_Reward/lifting_object: 170.7899
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 2.38s
                      Time elapsed: 01:11:46
                               ETA: 00:17:29

################################################################################
                     [1m Learning iteration 1609/2000 [0m                     

                       Computation: 42147 steps/s (collection: 2.206s, learning 0.126s)
             Mean action noise std: 3.74
          Mean value_function loss: 112.2795
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.5269
                       Mean reward: 851.26
               Mean episode length: 226.55
    Episode_Reward/reaching_object: 1.5620
     Episode_Reward/lifting_object: 169.1010
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0775
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 2.33s
                      Time elapsed: 01:11:49
                               ETA: 00:17:26

################################################################################
                     [1m Learning iteration 1610/2000 [0m                     

                       Computation: 40414 steps/s (collection: 2.294s, learning 0.138s)
             Mean action noise std: 3.74
          Mean value_function loss: 128.7708
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 70.5422
                       Mean reward: 844.21
               Mean episode length: 223.65
    Episode_Reward/reaching_object: 1.5948
     Episode_Reward/lifting_object: 172.9424
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0792
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 2.43s
                      Time elapsed: 01:11:51
                               ETA: 00:17:23

################################################################################
                     [1m Learning iteration 1611/2000 [0m                     

                       Computation: 40542 steps/s (collection: 2.269s, learning 0.156s)
             Mean action noise std: 3.75
          Mean value_function loss: 129.8296
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 70.5582
                       Mean reward: 842.94
               Mean episode length: 224.23
    Episode_Reward/reaching_object: 1.5599
     Episode_Reward/lifting_object: 168.7250
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0775
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.42s
                      Time elapsed: 01:11:54
                               ETA: 00:17:21

################################################################################
                     [1m Learning iteration 1612/2000 [0m                     

                       Computation: 41058 steps/s (collection: 2.244s, learning 0.151s)
             Mean action noise std: 3.75
          Mean value_function loss: 138.7577
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 70.5699
                       Mean reward: 870.38
               Mean episode length: 231.26
    Episode_Reward/reaching_object: 1.5563
     Episode_Reward/lifting_object: 168.2977
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0775
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 2.39s
                      Time elapsed: 01:11:56
                               ETA: 00:17:18

################################################################################
                     [1m Learning iteration 1613/2000 [0m                     

                       Computation: 40742 steps/s (collection: 2.268s, learning 0.145s)
             Mean action noise std: 3.75
          Mean value_function loss: 146.2537
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 70.5790
                       Mean reward: 797.42
               Mean episode length: 214.46
    Episode_Reward/reaching_object: 1.5637
     Episode_Reward/lifting_object: 168.6801
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 2.41s
                      Time elapsed: 01:11:58
                               ETA: 00:17:15

################################################################################
                     [1m Learning iteration 1614/2000 [0m                     

                       Computation: 41346 steps/s (collection: 2.258s, learning 0.119s)
             Mean action noise std: 3.75
          Mean value_function loss: 129.5864
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 70.5915
                       Mean reward: 880.59
               Mean episode length: 234.26
    Episode_Reward/reaching_object: 1.5941
     Episode_Reward/lifting_object: 172.4339
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0791
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 2.38s
                      Time elapsed: 01:12:01
                               ETA: 00:17:12

################################################################################
                     [1m Learning iteration 1615/2000 [0m                     

                       Computation: 40929 steps/s (collection: 2.264s, learning 0.138s)
             Mean action noise std: 3.75
          Mean value_function loss: 143.1292
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 70.6002
                       Mean reward: 839.61
               Mean episode length: 224.65
    Episode_Reward/reaching_object: 1.5734
     Episode_Reward/lifting_object: 169.7935
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0783
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 2.40s
                      Time elapsed: 01:12:03
                               ETA: 00:17:10

################################################################################
                     [1m Learning iteration 1616/2000 [0m                     

                       Computation: 41829 steps/s (collection: 2.227s, learning 0.123s)
             Mean action noise std: 3.75
          Mean value_function loss: 142.0694
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 70.6068
                       Mean reward: 856.11
               Mean episode length: 228.15
    Episode_Reward/reaching_object: 1.5561
     Episode_Reward/lifting_object: 167.6488
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 2.35s
                      Time elapsed: 01:12:05
                               ETA: 00:17:07

################################################################################
                     [1m Learning iteration 1617/2000 [0m                     

                       Computation: 40753 steps/s (collection: 2.280s, learning 0.132s)
             Mean action noise std: 3.75
          Mean value_function loss: 127.3168
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 70.6139
                       Mean reward: 863.66
               Mean episode length: 228.73
    Episode_Reward/reaching_object: 1.5518
     Episode_Reward/lifting_object: 167.3746
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 2.41s
                      Time elapsed: 01:12:08
                               ETA: 00:17:04

################################################################################
                     [1m Learning iteration 1618/2000 [0m                     

                       Computation: 40725 steps/s (collection: 2.284s, learning 0.130s)
             Mean action noise std: 3.75
          Mean value_function loss: 148.4046
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 70.6233
                       Mean reward: 875.03
               Mean episode length: 232.51
    Episode_Reward/reaching_object: 1.5948
     Episode_Reward/lifting_object: 172.2748
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0795
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 2.41s
                      Time elapsed: 01:12:10
                               ETA: 00:17:01

################################################################################
                     [1m Learning iteration 1619/2000 [0m                     

                       Computation: 41830 steps/s (collection: 2.222s, learning 0.129s)
             Mean action noise std: 3.76
          Mean value_function loss: 122.9520
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 70.6374
                       Mean reward: 854.12
               Mean episode length: 226.95
    Episode_Reward/reaching_object: 1.5742
     Episode_Reward/lifting_object: 169.9247
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0787
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 2.35s
                      Time elapsed: 01:12:13
                               ETA: 00:16:59

################################################################################
                     [1m Learning iteration 1620/2000 [0m                     

                       Computation: 41416 steps/s (collection: 2.225s, learning 0.149s)
             Mean action noise std: 3.76
          Mean value_function loss: 154.0761
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.6520
                       Mean reward: 862.63
               Mean episode length: 228.92
    Episode_Reward/reaching_object: 1.5693
     Episode_Reward/lifting_object: 169.5572
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0785
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 2.37s
                      Time elapsed: 01:12:15
                               ETA: 00:16:56

################################################################################
                     [1m Learning iteration 1621/2000 [0m                     

                       Computation: 41702 steps/s (collection: 2.240s, learning 0.117s)
             Mean action noise std: 3.76
          Mean value_function loss: 142.1426
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 70.6630
                       Mean reward: 849.50
               Mean episode length: 225.94
    Episode_Reward/reaching_object: 1.5675
     Episode_Reward/lifting_object: 169.3795
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0787
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 2.36s
                      Time elapsed: 01:12:17
                               ETA: 00:16:53

################################################################################
                     [1m Learning iteration 1622/2000 [0m                     

                       Computation: 41969 steps/s (collection: 2.231s, learning 0.112s)
             Mean action noise std: 3.76
          Mean value_function loss: 131.0023
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.6784
                       Mean reward: 829.81
               Mean episode length: 223.60
    Episode_Reward/reaching_object: 1.5760
     Episode_Reward/lifting_object: 170.1212
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0791
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 2.34s
                      Time elapsed: 01:12:20
                               ETA: 00:16:50

################################################################################
                     [1m Learning iteration 1623/2000 [0m                     

                       Computation: 41215 steps/s (collection: 2.247s, learning 0.139s)
             Mean action noise std: 3.76
          Mean value_function loss: 115.2046
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 70.6950
                       Mean reward: 880.74
               Mean episode length: 233.44
    Episode_Reward/reaching_object: 1.6003
     Episode_Reward/lifting_object: 173.3340
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0801
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.39s
                      Time elapsed: 01:12:22
                               ETA: 00:16:48

################################################################################
                     [1m Learning iteration 1624/2000 [0m                     

                       Computation: 42214 steps/s (collection: 2.208s, learning 0.121s)
             Mean action noise std: 3.77
          Mean value_function loss: 137.8936
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 70.7059
                       Mean reward: 864.32
               Mean episode length: 229.25
    Episode_Reward/reaching_object: 1.5758
     Episode_Reward/lifting_object: 170.6893
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0791
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.33s
                      Time elapsed: 01:12:24
                               ETA: 00:16:45

################################################################################
                     [1m Learning iteration 1625/2000 [0m                     

                       Computation: 41940 steps/s (collection: 2.216s, learning 0.128s)
             Mean action noise std: 3.77
          Mean value_function loss: 147.3638
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 70.7118
                       Mean reward: 869.56
               Mean episode length: 229.69
    Episode_Reward/reaching_object: 1.5817
     Episode_Reward/lifting_object: 171.5295
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 2.34s
                      Time elapsed: 01:12:27
                               ETA: 00:16:42

################################################################################
                     [1m Learning iteration 1626/2000 [0m                     

                       Computation: 41574 steps/s (collection: 2.214s, learning 0.150s)
             Mean action noise std: 3.77
          Mean value_function loss: 108.2848
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 70.7217
                       Mean reward: 889.43
               Mean episode length: 235.93
    Episode_Reward/reaching_object: 1.6090
     Episode_Reward/lifting_object: 174.5555
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0809
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 2.36s
                      Time elapsed: 01:12:29
                               ETA: 00:16:39

################################################################################
                     [1m Learning iteration 1627/2000 [0m                     

                       Computation: 42230 steps/s (collection: 2.199s, learning 0.129s)
             Mean action noise std: 3.77
          Mean value_function loss: 135.9891
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 70.7344
                       Mean reward: 839.04
               Mean episode length: 222.49
    Episode_Reward/reaching_object: 1.5753
     Episode_Reward/lifting_object: 170.8374
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0794
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 2.33s
                      Time elapsed: 01:12:31
                               ETA: 00:16:37

################################################################################
                     [1m Learning iteration 1628/2000 [0m                     

                       Computation: 41128 steps/s (collection: 2.259s, learning 0.131s)
             Mean action noise std: 3.77
          Mean value_function loss: 144.1160
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 70.7476
                       Mean reward: 843.47
               Mean episode length: 224.58
    Episode_Reward/reaching_object: 1.5528
     Episode_Reward/lifting_object: 167.6589
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0782
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.39s
                      Time elapsed: 01:12:34
                               ETA: 00:16:34

################################################################################
                     [1m Learning iteration 1629/2000 [0m                     

                       Computation: 42009 steps/s (collection: 2.203s, learning 0.137s)
             Mean action noise std: 3.77
          Mean value_function loss: 140.2766
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 70.7591
                       Mean reward: 862.89
               Mean episode length: 229.61
    Episode_Reward/reaching_object: 1.5259
     Episode_Reward/lifting_object: 164.9657
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.34s
                      Time elapsed: 01:12:36
                               ETA: 00:16:31

################################################################################
                     [1m Learning iteration 1630/2000 [0m                     

                       Computation: 41737 steps/s (collection: 2.221s, learning 0.134s)
             Mean action noise std: 3.78
          Mean value_function loss: 125.6638
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 70.7718
                       Mean reward: 884.02
               Mean episode length: 234.13
    Episode_Reward/reaching_object: 1.5910
     Episode_Reward/lifting_object: 172.2933
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0803
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 2.36s
                      Time elapsed: 01:12:39
                               ETA: 00:16:28

################################################################################
                     [1m Learning iteration 1631/2000 [0m                     

                       Computation: 41187 steps/s (collection: 2.248s, learning 0.139s)
             Mean action noise std: 3.78
          Mean value_function loss: 142.6705
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 70.7901
                       Mean reward: 851.53
               Mean episode length: 227.01
    Episode_Reward/reaching_object: 1.5575
     Episode_Reward/lifting_object: 168.5596
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0787
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.39s
                      Time elapsed: 01:12:41
                               ETA: 00:16:26

################################################################################
                     [1m Learning iteration 1632/2000 [0m                     

                       Computation: 41106 steps/s (collection: 2.244s, learning 0.147s)
             Mean action noise std: 3.78
          Mean value_function loss: 151.5649
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 70.8011
                       Mean reward: 861.88
               Mean episode length: 228.16
    Episode_Reward/reaching_object: 1.5444
     Episode_Reward/lifting_object: 166.9424
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0783
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.39s
                      Time elapsed: 01:12:43
                               ETA: 00:16:23

################################################################################
                     [1m Learning iteration 1633/2000 [0m                     

                       Computation: 41209 steps/s (collection: 2.250s, learning 0.136s)
             Mean action noise std: 3.78
          Mean value_function loss: 99.8519
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 70.8085
                       Mean reward: 894.34
               Mean episode length: 236.16
    Episode_Reward/reaching_object: 1.5917
     Episode_Reward/lifting_object: 172.5196
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0803
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 2.39s
                      Time elapsed: 01:12:46
                               ETA: 00:16:20

################################################################################
                     [1m Learning iteration 1634/2000 [0m                     

                       Computation: 41148 steps/s (collection: 2.251s, learning 0.138s)
             Mean action noise std: 3.78
          Mean value_function loss: 139.0060
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.8145
                       Mean reward: 861.08
               Mean episode length: 229.13
    Episode_Reward/reaching_object: 1.5960
     Episode_Reward/lifting_object: 172.6770
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0810
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.39s
                      Time elapsed: 01:12:48
                               ETA: 00:16:17

################################################################################
                     [1m Learning iteration 1635/2000 [0m                     

                       Computation: 41666 steps/s (collection: 2.230s, learning 0.130s)
             Mean action noise std: 3.78
          Mean value_function loss: 140.6368
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 70.8266
                       Mean reward: 839.25
               Mean episode length: 223.93
    Episode_Reward/reaching_object: 1.5611
     Episode_Reward/lifting_object: 168.7963
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.36s
                      Time elapsed: 01:12:50
                               ETA: 00:16:15

################################################################################
                     [1m Learning iteration 1636/2000 [0m                     

                       Computation: 41882 steps/s (collection: 2.236s, learning 0.111s)
             Mean action noise std: 3.79
          Mean value_function loss: 128.4826
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 70.8392
                       Mean reward: 874.58
               Mean episode length: 231.44
    Episode_Reward/reaching_object: 1.5955
     Episode_Reward/lifting_object: 172.7203
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0807
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 2.35s
                      Time elapsed: 01:12:53
                               ETA: 00:16:12

################################################################################
                     [1m Learning iteration 1637/2000 [0m                     

                       Computation: 41103 steps/s (collection: 2.261s, learning 0.131s)
             Mean action noise std: 3.79
          Mean value_function loss: 125.9263
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 70.8505
                       Mean reward: 899.29
               Mean episode length: 236.60
    Episode_Reward/reaching_object: 1.5749
     Episode_Reward/lifting_object: 170.3710
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0801
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 2.39s
                      Time elapsed: 01:12:55
                               ETA: 00:16:09

################################################################################
                     [1m Learning iteration 1638/2000 [0m                     

                       Computation: 41791 steps/s (collection: 2.222s, learning 0.130s)
             Mean action noise std: 3.79
          Mean value_function loss: 118.6282
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 70.8596
                       Mean reward: 927.55
               Mean episode length: 244.69
    Episode_Reward/reaching_object: 1.6123
     Episode_Reward/lifting_object: 174.2262
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 2.35s
                      Time elapsed: 01:12:58
                               ETA: 00:16:06

################################################################################
                     [1m Learning iteration 1639/2000 [0m                     

                       Computation: 41573 steps/s (collection: 2.240s, learning 0.125s)
             Mean action noise std: 3.79
          Mean value_function loss: 154.1377
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 70.8717
                       Mean reward: 831.46
               Mean episode length: 222.61
    Episode_Reward/reaching_object: 1.5629
     Episode_Reward/lifting_object: 169.0374
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0799
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 2.36s
                      Time elapsed: 01:13:00
                               ETA: 00:16:04

################################################################################
                     [1m Learning iteration 1640/2000 [0m                     

                       Computation: 41866 steps/s (collection: 2.210s, learning 0.138s)
             Mean action noise std: 3.79
          Mean value_function loss: 143.2066
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 70.8799
                       Mean reward: 861.06
               Mean episode length: 228.84
    Episode_Reward/reaching_object: 1.5500
     Episode_Reward/lifting_object: 167.5297
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 2.35s
                      Time elapsed: 01:13:02
                               ETA: 00:16:01

################################################################################
                     [1m Learning iteration 1641/2000 [0m                     

                       Computation: 42159 steps/s (collection: 2.206s, learning 0.126s)
             Mean action noise std: 3.79
          Mean value_function loss: 125.7116
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 70.8884
                       Mean reward: 868.95
               Mean episode length: 232.71
    Episode_Reward/reaching_object: 1.5790
     Episode_Reward/lifting_object: 170.2894
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0806
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 2.33s
                      Time elapsed: 01:13:05
                               ETA: 00:15:58

################################################################################
                     [1m Learning iteration 1642/2000 [0m                     

                       Computation: 41670 steps/s (collection: 2.234s, learning 0.125s)
             Mean action noise std: 3.79
          Mean value_function loss: 120.3709
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 70.8966
                       Mean reward: 849.37
               Mean episode length: 226.77
    Episode_Reward/reaching_object: 1.5485
     Episode_Reward/lifting_object: 166.9869
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0794
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.36s
                      Time elapsed: 01:13:07
                               ETA: 00:15:56

################################################################################
                     [1m Learning iteration 1643/2000 [0m                     

                       Computation: 40997 steps/s (collection: 2.275s, learning 0.123s)
             Mean action noise std: 3.80
          Mean value_function loss: 126.7039
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 70.9054
                       Mean reward: 877.93
               Mean episode length: 232.31
    Episode_Reward/reaching_object: 1.5862
     Episode_Reward/lifting_object: 171.3315
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0810
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.40s
                      Time elapsed: 01:13:09
                               ETA: 00:15:53

################################################################################
                     [1m Learning iteration 1644/2000 [0m                     

                       Computation: 41853 steps/s (collection: 2.231s, learning 0.118s)
             Mean action noise std: 3.80
          Mean value_function loss: 119.3364
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 70.9184
                       Mean reward: 839.49
               Mean episode length: 225.34
    Episode_Reward/reaching_object: 1.5901
     Episode_Reward/lifting_object: 171.6167
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 2.35s
                      Time elapsed: 01:13:12
                               ETA: 00:15:50

################################################################################
                     [1m Learning iteration 1645/2000 [0m                     

                       Computation: 42215 steps/s (collection: 2.196s, learning 0.133s)
             Mean action noise std: 3.80
          Mean value_function loss: 116.4467
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.9271
                       Mean reward: 873.13
               Mean episode length: 231.37
    Episode_Reward/reaching_object: 1.5991
     Episode_Reward/lifting_object: 172.5464
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0820
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 2.33s
                      Time elapsed: 01:13:14
                               ETA: 00:15:47

################################################################################
                     [1m Learning iteration 1646/2000 [0m                     

                       Computation: 41526 steps/s (collection: 2.224s, learning 0.143s)
             Mean action noise std: 3.80
          Mean value_function loss: 99.2403
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 70.9384
                       Mean reward: 888.40
               Mean episode length: 234.50
    Episode_Reward/reaching_object: 1.6280
     Episode_Reward/lifting_object: 175.9221
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0829
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 2.37s
                      Time elapsed: 01:13:16
                               ETA: 00:15:45

################################################################################
                     [1m Learning iteration 1647/2000 [0m                     

                       Computation: 42370 steps/s (collection: 2.220s, learning 0.101s)
             Mean action noise std: 3.80
          Mean value_function loss: 126.5695
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 70.9480
                       Mean reward: 891.08
               Mean episode length: 235.87
    Episode_Reward/reaching_object: 1.5902
     Episode_Reward/lifting_object: 171.4128
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0813
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 2.32s
                      Time elapsed: 01:13:19
                               ETA: 00:15:42

################################################################################
                     [1m Learning iteration 1648/2000 [0m                     

                       Computation: 41580 steps/s (collection: 2.228s, learning 0.136s)
             Mean action noise std: 3.80
          Mean value_function loss: 121.2956
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 70.9610
                       Mean reward: 810.40
               Mean episode length: 217.04
    Episode_Reward/reaching_object: 1.5678
     Episode_Reward/lifting_object: 168.8010
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0804
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 2.36s
                      Time elapsed: 01:13:21
                               ETA: 00:15:39

################################################################################
                     [1m Learning iteration 1649/2000 [0m                     

                       Computation: 41655 steps/s (collection: 2.216s, learning 0.144s)
             Mean action noise std: 3.81
          Mean value_function loss: 139.5727
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 70.9731
                       Mean reward: 877.70
               Mean episode length: 233.20
    Episode_Reward/reaching_object: 1.5783
     Episode_Reward/lifting_object: 169.5827
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 2.36s
                      Time elapsed: 01:13:23
                               ETA: 00:15:36

################################################################################
                     [1m Learning iteration 1650/2000 [0m                     

                       Computation: 41746 steps/s (collection: 2.210s, learning 0.145s)
             Mean action noise std: 3.81
          Mean value_function loss: 152.3925
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 70.9809
                       Mean reward: 841.63
               Mean episode length: 223.06
    Episode_Reward/reaching_object: 1.5416
     Episode_Reward/lifting_object: 165.3962
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0790
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 2.35s
                      Time elapsed: 01:13:26
                               ETA: 00:15:34

################################################################################
                     [1m Learning iteration 1651/2000 [0m                     

                       Computation: 40631 steps/s (collection: 2.274s, learning 0.146s)
             Mean action noise std: 3.81
          Mean value_function loss: 122.5560
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 70.9912
                       Mean reward: 864.98
               Mean episode length: 229.39
    Episode_Reward/reaching_object: 1.5900
     Episode_Reward/lifting_object: 170.9914
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0813
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.42s
                      Time elapsed: 01:13:28
                               ETA: 00:15:31

################################################################################
                     [1m Learning iteration 1652/2000 [0m                     

                       Computation: 42089 steps/s (collection: 2.198s, learning 0.138s)
             Mean action noise std: 3.81
          Mean value_function loss: 105.8618
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 70.9992
                       Mean reward: 879.01
               Mean episode length: 232.17
    Episode_Reward/reaching_object: 1.5831
     Episode_Reward/lifting_object: 170.6726
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0809
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 2.34s
                      Time elapsed: 01:13:31
                               ETA: 00:15:28

################################################################################
                     [1m Learning iteration 1653/2000 [0m                     

                       Computation: 40947 steps/s (collection: 2.272s, learning 0.129s)
             Mean action noise std: 3.81
          Mean value_function loss: 138.1946
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.0135
                       Mean reward: 852.63
               Mean episode length: 227.96
    Episode_Reward/reaching_object: 1.6038
     Episode_Reward/lifting_object: 172.3868
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0823
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 18.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 2.40s
                      Time elapsed: 01:13:33
                               ETA: 00:15:25

################################################################################
                     [1m Learning iteration 1654/2000 [0m                     

                       Computation: 42015 steps/s (collection: 2.209s, learning 0.131s)
             Mean action noise std: 3.81
          Mean value_function loss: 141.2835
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 71.0274
                       Mean reward: 817.64
               Mean episode length: 217.59
    Episode_Reward/reaching_object: 1.5491
     Episode_Reward/lifting_object: 166.7184
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0796
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 2.34s
                      Time elapsed: 01:13:35
                               ETA: 00:15:23

################################################################################
                     [1m Learning iteration 1655/2000 [0m                     

                       Computation: 41372 steps/s (collection: 2.237s, learning 0.139s)
             Mean action noise std: 3.82
          Mean value_function loss: 123.9457
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 71.0417
                       Mean reward: 841.25
               Mean episode length: 224.11
    Episode_Reward/reaching_object: 1.5904
     Episode_Reward/lifting_object: 171.5452
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0816
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 2.38s
                      Time elapsed: 01:13:38
                               ETA: 00:15:20

################################################################################
                     [1m Learning iteration 1656/2000 [0m                     

                       Computation: 42378 steps/s (collection: 2.188s, learning 0.132s)
             Mean action noise std: 3.82
          Mean value_function loss: 141.3976
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 71.0512
                       Mean reward: 847.41
               Mean episode length: 224.69
    Episode_Reward/reaching_object: 1.5750
     Episode_Reward/lifting_object: 169.6649
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0810
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.32s
                      Time elapsed: 01:13:40
                               ETA: 00:15:17

################################################################################
                     [1m Learning iteration 1657/2000 [0m                     

                       Computation: 41169 steps/s (collection: 2.255s, learning 0.133s)
             Mean action noise std: 3.82
          Mean value_function loss: 116.4226
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 71.0598
                       Mean reward: 893.64
               Mean episode length: 235.90
    Episode_Reward/reaching_object: 1.5739
     Episode_Reward/lifting_object: 169.5517
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0811
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 2.39s
                      Time elapsed: 01:13:42
                               ETA: 00:15:14

################################################################################
                     [1m Learning iteration 1658/2000 [0m                     

                       Computation: 40905 steps/s (collection: 2.279s, learning 0.124s)
             Mean action noise std: 3.82
          Mean value_function loss: 113.6198
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 71.0706
                       Mean reward: 886.57
               Mean episode length: 234.72
    Episode_Reward/reaching_object: 1.5769
     Episode_Reward/lifting_object: 170.2738
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 2.40s
                      Time elapsed: 01:13:45
                               ETA: 00:15:12

################################################################################
                     [1m Learning iteration 1659/2000 [0m                     

                       Computation: 41909 steps/s (collection: 2.228s, learning 0.118s)
             Mean action noise std: 3.82
          Mean value_function loss: 128.7301
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 71.0787
                       Mean reward: 833.82
               Mean episode length: 222.27
    Episode_Reward/reaching_object: 1.5767
     Episode_Reward/lifting_object: 170.3159
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0815
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 2.35s
                      Time elapsed: 01:13:47
                               ETA: 00:15:09

################################################################################
                     [1m Learning iteration 1660/2000 [0m                     

                       Computation: 41830 steps/s (collection: 2.221s, learning 0.129s)
             Mean action noise std: 3.82
          Mean value_function loss: 138.4231
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 71.0891
                       Mean reward: 866.15
               Mean episode length: 229.73
    Episode_Reward/reaching_object: 1.5667
     Episode_Reward/lifting_object: 169.4582
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 2.35s
                      Time elapsed: 01:13:49
                               ETA: 00:15:06

################################################################################
                     [1m Learning iteration 1661/2000 [0m                     

                       Computation: 41399 steps/s (collection: 2.241s, learning 0.134s)
             Mean action noise std: 3.82
          Mean value_function loss: 118.1390
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 71.0947
                       Mean reward: 879.05
               Mean episode length: 232.91
    Episode_Reward/reaching_object: 1.5721
     Episode_Reward/lifting_object: 170.0501
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0811
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 2.37s
                      Time elapsed: 01:13:52
                               ETA: 00:15:04

################################################################################
                     [1m Learning iteration 1662/2000 [0m                     

                       Computation: 42254 steps/s (collection: 2.210s, learning 0.117s)
             Mean action noise std: 3.83
          Mean value_function loss: 146.9416
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 71.1025
                       Mean reward: 864.73
               Mean episode length: 228.83
    Episode_Reward/reaching_object: 1.5470
     Episode_Reward/lifting_object: 167.3046
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0801
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 2.33s
                      Time elapsed: 01:13:54
                               ETA: 00:15:01

################################################################################
                     [1m Learning iteration 1663/2000 [0m                     

                       Computation: 41698 steps/s (collection: 2.217s, learning 0.140s)
             Mean action noise std: 3.83
          Mean value_function loss: 141.7613
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 71.1186
                       Mean reward: 822.51
               Mean episode length: 219.58
    Episode_Reward/reaching_object: 1.5271
     Episode_Reward/lifting_object: 164.6499
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 2.36s
                      Time elapsed: 01:13:57
                               ETA: 00:14:58

################################################################################
                     [1m Learning iteration 1664/2000 [0m                     

                       Computation: 41288 steps/s (collection: 2.239s, learning 0.142s)
             Mean action noise std: 3.83
          Mean value_function loss: 119.2613
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 71.1304
                       Mean reward: 859.66
               Mean episode length: 229.62
    Episode_Reward/reaching_object: 1.5672
     Episode_Reward/lifting_object: 169.4676
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0813
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 2.38s
                      Time elapsed: 01:13:59
                               ETA: 00:14:55

################################################################################
                     [1m Learning iteration 1665/2000 [0m                     

                       Computation: 41517 steps/s (collection: 2.229s, learning 0.139s)
             Mean action noise std: 3.83
          Mean value_function loss: 101.3363
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 71.1414
                       Mean reward: 907.14
               Mean episode length: 238.96
    Episode_Reward/reaching_object: 1.6148
     Episode_Reward/lifting_object: 175.2548
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0834
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 2.37s
                      Time elapsed: 01:14:01
                               ETA: 00:14:53

################################################################################
                     [1m Learning iteration 1666/2000 [0m                     

                       Computation: 24556 steps/s (collection: 3.870s, learning 0.133s)
             Mean action noise std: 3.83
          Mean value_function loss: 99.9484
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 71.1522
                       Mean reward: 837.61
               Mean episode length: 222.89
    Episode_Reward/reaching_object: 1.5806
     Episode_Reward/lifting_object: 171.2682
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0821
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 4.00s
                      Time elapsed: 01:14:05
                               ETA: 00:14:50

################################################################################
                     [1m Learning iteration 1667/2000 [0m                     

                       Computation: 13688 steps/s (collection: 7.050s, learning 0.132s)
             Mean action noise std: 3.84
          Mean value_function loss: 148.6902
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 71.1686
                       Mean reward: 854.17
               Mean episode length: 225.69
    Episode_Reward/reaching_object: 1.5736
     Episode_Reward/lifting_object: 170.7419
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0816
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 7.18s
                      Time elapsed: 01:14:12
                               ETA: 00:14:48

################################################################################
                     [1m Learning iteration 1668/2000 [0m                     

                       Computation: 13778 steps/s (collection: 7.007s, learning 0.128s)
             Mean action noise std: 3.84
          Mean value_function loss: 117.8658
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 71.1867
                       Mean reward: 811.22
               Mean episode length: 217.28
    Episode_Reward/reaching_object: 1.5510
     Episode_Reward/lifting_object: 167.4512
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 7.13s
                      Time elapsed: 01:14:20
                               ETA: 00:14:47

################################################################################
                     [1m Learning iteration 1669/2000 [0m                     

                       Computation: 13821 steps/s (collection: 6.989s, learning 0.123s)
             Mean action noise std: 3.84
          Mean value_function loss: 145.9951
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 71.1989
                       Mean reward: 829.82
               Mean episode length: 221.74
    Episode_Reward/reaching_object: 1.5285
     Episode_Reward/lifting_object: 165.3565
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0798
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 7.11s
                      Time elapsed: 01:14:27
                               ETA: 00:14:45

################################################################################
                     [1m Learning iteration 1670/2000 [0m                     

                       Computation: 14127 steps/s (collection: 6.826s, learning 0.133s)
             Mean action noise std: 3.84
          Mean value_function loss: 153.0069
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 71.2099
                       Mean reward: 836.93
               Mean episode length: 223.36
    Episode_Reward/reaching_object: 1.5694
     Episode_Reward/lifting_object: 169.4728
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0819
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 6.96s
                      Time elapsed: 01:14:34
                               ETA: 00:14:43

################################################################################
                     [1m Learning iteration 1671/2000 [0m                     

                       Computation: 14027 steps/s (collection: 6.876s, learning 0.132s)
             Mean action noise std: 3.84
          Mean value_function loss: 154.6940
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 71.2208
                       Mean reward: 917.32
               Mean episode length: 242.37
    Episode_Reward/reaching_object: 1.5596
     Episode_Reward/lifting_object: 168.1866
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 7.01s
                      Time elapsed: 01:14:41
                               ETA: 00:14:41

################################################################################
                     [1m Learning iteration 1672/2000 [0m                     

                       Computation: 13937 steps/s (collection: 6.913s, learning 0.141s)
             Mean action noise std: 3.85
          Mean value_function loss: 122.1387
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 71.2367
                       Mean reward: 888.70
               Mean episode length: 235.29
    Episode_Reward/reaching_object: 1.5713
     Episode_Reward/lifting_object: 170.1760
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 7.05s
                      Time elapsed: 01:14:48
                               ETA: 00:14:39

################################################################################
                     [1m Learning iteration 1673/2000 [0m                     

                       Computation: 14194 steps/s (collection: 6.795s, learning 0.131s)
             Mean action noise std: 3.85
          Mean value_function loss: 151.5092
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 71.2465
                       Mean reward: 853.02
               Mean episode length: 226.30
    Episode_Reward/reaching_object: 1.5711
     Episode_Reward/lifting_object: 169.9585
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0819
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 6.93s
                      Time elapsed: 01:14:55
                               ETA: 00:14:38

################################################################################
                     [1m Learning iteration 1674/2000 [0m                     

                       Computation: 14106 steps/s (collection: 6.841s, learning 0.128s)
             Mean action noise std: 3.85
          Mean value_function loss: 155.3151
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 71.2514
                       Mean reward: 855.03
               Mean episode length: 227.22
    Episode_Reward/reaching_object: 1.5624
     Episode_Reward/lifting_object: 168.8422
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 17.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 6.97s
                      Time elapsed: 01:15:02
                               ETA: 00:14:36

################################################################################
                     [1m Learning iteration 1675/2000 [0m                     

                       Computation: 21384 steps/s (collection: 4.474s, learning 0.123s)
             Mean action noise std: 3.85
          Mean value_function loss: 132.5566
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 71.2573
                       Mean reward: 827.56
               Mean episode length: 220.50
    Episode_Reward/reaching_object: 1.5167
     Episode_Reward/lifting_object: 163.1895
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0794
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 4.60s
                      Time elapsed: 01:15:06
                               ETA: 00:14:33

################################################################################
                     [1m Learning iteration 1676/2000 [0m                     

                       Computation: 42781 steps/s (collection: 2.184s, learning 0.114s)
             Mean action noise std: 3.85
          Mean value_function loss: 132.7102
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 71.2676
                       Mean reward: 840.86
               Mean episode length: 224.33
    Episode_Reward/reaching_object: 1.5382
     Episode_Reward/lifting_object: 165.7777
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0803
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 2.30s
                      Time elapsed: 01:15:09
                               ETA: 00:14:31

################################################################################
                     [1m Learning iteration 1677/2000 [0m                     

                       Computation: 41825 steps/s (collection: 2.217s, learning 0.134s)
             Mean action noise std: 3.85
          Mean value_function loss: 144.9451
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 71.2768
                       Mean reward: 843.61
               Mean episode length: 225.33
    Episode_Reward/reaching_object: 1.5625
     Episode_Reward/lifting_object: 168.6040
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0816
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 2.35s
                      Time elapsed: 01:15:11
                               ETA: 00:14:28

################################################################################
                     [1m Learning iteration 1678/2000 [0m                     

                       Computation: 43788 steps/s (collection: 2.113s, learning 0.132s)
             Mean action noise std: 3.85
          Mean value_function loss: 153.3925
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 71.2841
                       Mean reward: 845.96
               Mean episode length: 224.30
    Episode_Reward/reaching_object: 1.5453
     Episode_Reward/lifting_object: 166.6205
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 2.24s
                      Time elapsed: 01:15:13
                               ETA: 00:14:25

################################################################################
                     [1m Learning iteration 1679/2000 [0m                     

                       Computation: 42964 steps/s (collection: 2.161s, learning 0.127s)
             Mean action noise std: 3.85
          Mean value_function loss: 161.0714
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 71.2933
                       Mean reward: 824.54
               Mean episode length: 219.16
    Episode_Reward/reaching_object: 1.5559
     Episode_Reward/lifting_object: 167.5780
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 2.29s
                      Time elapsed: 01:15:15
                               ETA: 00:14:22

################################################################################
                     [1m Learning iteration 1680/2000 [0m                     

                       Computation: 42710 steps/s (collection: 2.175s, learning 0.127s)
             Mean action noise std: 3.86
          Mean value_function loss: 127.3544
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 71.3053
                       Mean reward: 869.24
               Mean episode length: 230.28
    Episode_Reward/reaching_object: 1.5795
     Episode_Reward/lifting_object: 170.4609
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 2.30s
                      Time elapsed: 01:15:18
                               ETA: 00:14:20

################################################################################
                     [1m Learning iteration 1681/2000 [0m                     

                       Computation: 42426 steps/s (collection: 2.186s, learning 0.131s)
             Mean action noise std: 3.86
          Mean value_function loss: 130.6669
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 71.3147
                       Mean reward: 863.24
               Mean episode length: 229.41
    Episode_Reward/reaching_object: 1.5352
     Episode_Reward/lifting_object: 165.3672
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0805
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 2.32s
                      Time elapsed: 01:15:20
                               ETA: 00:14:17

################################################################################
                     [1m Learning iteration 1682/2000 [0m                     

                       Computation: 43143 steps/s (collection: 2.146s, learning 0.132s)
             Mean action noise std: 3.86
          Mean value_function loss: 187.0441
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 71.3214
                       Mean reward: 806.23
               Mean episode length: 216.52
    Episode_Reward/reaching_object: 1.5754
     Episode_Reward/lifting_object: 169.8130
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 2.28s
                      Time elapsed: 01:15:22
                               ETA: 00:14:14

################################################################################
                     [1m Learning iteration 1683/2000 [0m                     

                       Computation: 42764 steps/s (collection: 2.181s, learning 0.118s)
             Mean action noise std: 3.86
          Mean value_function loss: 152.2259
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.3328
                       Mean reward: 877.86
               Mean episode length: 231.44
    Episode_Reward/reaching_object: 1.5951
     Episode_Reward/lifting_object: 172.2261
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0837
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 2.30s
                      Time elapsed: 01:15:25
                               ETA: 00:14:11

################################################################################
                     [1m Learning iteration 1684/2000 [0m                     

                       Computation: 42767 steps/s (collection: 2.168s, learning 0.131s)
             Mean action noise std: 3.86
          Mean value_function loss: 127.7055
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.3441
                       Mean reward: 897.56
               Mean episode length: 237.55
    Episode_Reward/reaching_object: 1.5750
     Episode_Reward/lifting_object: 169.5594
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0829
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 2.30s
                      Time elapsed: 01:15:27
                               ETA: 00:14:09

################################################################################
                     [1m Learning iteration 1685/2000 [0m                     

                       Computation: 42544 steps/s (collection: 2.185s, learning 0.126s)
             Mean action noise std: 3.86
          Mean value_function loss: 184.0393
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 71.3563
                       Mean reward: 887.55
               Mean episode length: 234.88
    Episode_Reward/reaching_object: 1.5532
     Episode_Reward/lifting_object: 167.7737
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 2.31s
                      Time elapsed: 01:15:29
                               ETA: 00:14:06

################################################################################
                     [1m Learning iteration 1686/2000 [0m                     

                       Computation: 42848 steps/s (collection: 2.158s, learning 0.137s)
             Mean action noise std: 3.87
          Mean value_function loss: 130.6266
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 71.3708
                       Mean reward: 857.84
               Mean episode length: 229.16
    Episode_Reward/reaching_object: 1.5298
     Episode_Reward/lifting_object: 165.0926
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0811
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 2.29s
                      Time elapsed: 01:15:32
                               ETA: 00:14:03

################################################################################
                     [1m Learning iteration 1687/2000 [0m                     

                       Computation: 42426 steps/s (collection: 2.175s, learning 0.142s)
             Mean action noise std: 3.87
          Mean value_function loss: 122.7574
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 71.3863
                       Mean reward: 832.65
               Mean episode length: 222.18
    Episode_Reward/reaching_object: 1.6105
     Episode_Reward/lifting_object: 174.2715
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0845
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 2.32s
                      Time elapsed: 01:15:34
                               ETA: 00:14:00

################################################################################
                     [1m Learning iteration 1688/2000 [0m                     

                       Computation: 43297 steps/s (collection: 2.165s, learning 0.106s)
             Mean action noise std: 3.87
          Mean value_function loss: 159.7463
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 71.4012
                       Mean reward: 809.15
               Mean episode length: 217.13
    Episode_Reward/reaching_object: 1.5547
     Episode_Reward/lifting_object: 167.6198
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0822
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 2.27s
                      Time elapsed: 01:15:36
                               ETA: 00:13:58

################################################################################
                     [1m Learning iteration 1689/2000 [0m                     

                       Computation: 40806 steps/s (collection: 2.284s, learning 0.126s)
             Mean action noise std: 3.87
          Mean value_function loss: 156.1224
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 71.4118
                       Mean reward: 880.75
               Mean episode length: 233.58
    Episode_Reward/reaching_object: 1.5439
     Episode_Reward/lifting_object: 166.8015
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0561
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 2.41s
                      Time elapsed: 01:15:39
                               ETA: 00:13:55

################################################################################
                     [1m Learning iteration 1690/2000 [0m                     

                       Computation: 42325 steps/s (collection: 2.180s, learning 0.142s)
             Mean action noise std: 3.87
          Mean value_function loss: 139.2662
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 71.4225
                       Mean reward: 856.40
               Mean episode length: 229.31
    Episode_Reward/reaching_object: 1.5668
     Episode_Reward/lifting_object: 169.5101
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0830
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 2.32s
                      Time elapsed: 01:15:41
                               ETA: 00:13:52

################################################################################
                     [1m Learning iteration 1691/2000 [0m                     

                       Computation: 43452 steps/s (collection: 2.126s, learning 0.137s)
             Mean action noise std: 3.88
          Mean value_function loss: 144.7770
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 71.4367
                       Mean reward: 872.81
               Mean episode length: 231.76
    Episode_Reward/reaching_object: 1.5588
     Episode_Reward/lifting_object: 168.6420
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 2.26s
                      Time elapsed: 01:15:43
                               ETA: 00:13:49

################################################################################
                     [1m Learning iteration 1692/2000 [0m                     

                       Computation: 43370 steps/s (collection: 2.160s, learning 0.107s)
             Mean action noise std: 3.88
          Mean value_function loss: 142.8764
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 71.4536
                       Mean reward: 896.04
               Mean episode length: 235.56
    Episode_Reward/reaching_object: 1.5651
     Episode_Reward/lifting_object: 169.6819
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0830
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 2.27s
                      Time elapsed: 01:15:45
                               ETA: 00:13:47

################################################################################
                     [1m Learning iteration 1693/2000 [0m                     

                       Computation: 43238 steps/s (collection: 2.137s, learning 0.137s)
             Mean action noise std: 3.88
          Mean value_function loss: 133.2918
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 71.4714
                       Mean reward: 899.92
               Mean episode length: 236.98
    Episode_Reward/reaching_object: 1.5897
     Episode_Reward/lifting_object: 172.6122
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0842
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 2.27s
                      Time elapsed: 01:15:48
                               ETA: 00:13:44

################################################################################
                     [1m Learning iteration 1694/2000 [0m                     

                       Computation: 41924 steps/s (collection: 2.205s, learning 0.140s)
             Mean action noise std: 3.88
          Mean value_function loss: 117.0445
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 71.4870
                       Mean reward: 844.78
               Mean episode length: 225.32
    Episode_Reward/reaching_object: 1.5659
     Episode_Reward/lifting_object: 169.5258
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0829
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 2.34s
                      Time elapsed: 01:15:50
                               ETA: 00:13:41

################################################################################
                     [1m Learning iteration 1695/2000 [0m                     

                       Computation: 42676 steps/s (collection: 2.195s, learning 0.109s)
             Mean action noise std: 3.89
          Mean value_function loss: 136.8792
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 71.5032
                       Mean reward: 837.17
               Mean episode length: 222.60
    Episode_Reward/reaching_object: 1.5700
     Episode_Reward/lifting_object: 170.0629
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0833
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 2.30s
                      Time elapsed: 01:15:52
                               ETA: 00:13:38

################################################################################
                     [1m Learning iteration 1696/2000 [0m                     

                       Computation: 42073 steps/s (collection: 2.232s, learning 0.105s)
             Mean action noise std: 3.89
          Mean value_function loss: 131.6714
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.5169
                       Mean reward: 813.71
               Mean episode length: 218.28
    Episode_Reward/reaching_object: 1.5846
     Episode_Reward/lifting_object: 171.8043
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0843
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 2.34s
                      Time elapsed: 01:15:55
                               ETA: 00:13:36

################################################################################
                     [1m Learning iteration 1697/2000 [0m                     

                       Computation: 42944 steps/s (collection: 2.170s, learning 0.119s)
             Mean action noise std: 3.89
          Mean value_function loss: 109.9846
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 71.5331
                       Mean reward: 882.91
               Mean episode length: 233.48
    Episode_Reward/reaching_object: 1.6113
     Episode_Reward/lifting_object: 174.3422
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 2.29s
                      Time elapsed: 01:15:57
                               ETA: 00:13:33

################################################################################
                     [1m Learning iteration 1698/2000 [0m                     

                       Computation: 43107 steps/s (collection: 2.169s, learning 0.111s)
             Mean action noise std: 3.89
          Mean value_function loss: 107.6469
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 71.5449
                       Mean reward: 863.80
               Mean episode length: 228.91
    Episode_Reward/reaching_object: 1.5876
     Episode_Reward/lifting_object: 171.6866
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0844
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 2.28s
                      Time elapsed: 01:15:59
                               ETA: 00:13:30

################################################################################
                     [1m Learning iteration 1699/2000 [0m                     

                       Computation: 42924 steps/s (collection: 2.193s, learning 0.098s)
             Mean action noise std: 3.89
          Mean value_function loss: 102.1464
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 71.5531
                       Mean reward: 883.91
               Mean episode length: 232.87
    Episode_Reward/reaching_object: 1.5748
     Episode_Reward/lifting_object: 170.1272
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0839
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 2.29s
                      Time elapsed: 01:16:01
                               ETA: 00:13:27

################################################################################
                     [1m Learning iteration 1700/2000 [0m                     

                       Computation: 42684 steps/s (collection: 2.180s, learning 0.123s)
             Mean action noise std: 3.89
          Mean value_function loss: 147.0879
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 71.5618
                       Mean reward: 870.11
               Mean episode length: 230.81
    Episode_Reward/reaching_object: 1.5791
     Episode_Reward/lifting_object: 170.4729
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0842
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 2.30s
                      Time elapsed: 01:16:04
                               ETA: 00:13:24

################################################################################
                     [1m Learning iteration 1701/2000 [0m                     

                       Computation: 42189 steps/s (collection: 2.224s, learning 0.106s)
             Mean action noise std: 3.90
          Mean value_function loss: 94.5365
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 71.5777
                       Mean reward: 864.37
               Mean episode length: 229.72
    Episode_Reward/reaching_object: 1.5999
     Episode_Reward/lifting_object: 172.8116
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0855
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 2.33s
                      Time elapsed: 01:16:06
                               ETA: 00:13:22

################################################################################
                     [1m Learning iteration 1702/2000 [0m                     

                       Computation: 43044 steps/s (collection: 2.178s, learning 0.106s)
             Mean action noise std: 3.90
          Mean value_function loss: 136.8502
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.5855
                       Mean reward: 851.98
               Mean episode length: 228.07
    Episode_Reward/reaching_object: 1.5930
     Episode_Reward/lifting_object: 172.0327
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 2.28s
                      Time elapsed: 01:16:08
                               ETA: 00:13:19

################################################################################
                     [1m Learning iteration 1703/2000 [0m                     

                       Computation: 42514 steps/s (collection: 2.200s, learning 0.112s)
             Mean action noise std: 3.90
          Mean value_function loss: 122.1253
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.5969
                       Mean reward: 895.97
               Mean episode length: 237.70
    Episode_Reward/reaching_object: 1.5890
     Episode_Reward/lifting_object: 171.4679
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 2.31s
                      Time elapsed: 01:16:11
                               ETA: 00:13:16

################################################################################
                     [1m Learning iteration 1704/2000 [0m                     

                       Computation: 41736 steps/s (collection: 2.246s, learning 0.110s)
             Mean action noise std: 3.90
          Mean value_function loss: 101.8299
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 71.6090
                       Mean reward: 860.45
               Mean episode length: 228.84
    Episode_Reward/reaching_object: 1.5915
     Episode_Reward/lifting_object: 171.7171
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0851
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 2.36s
                      Time elapsed: 01:16:13
                               ETA: 00:13:14

################################################################################
                     [1m Learning iteration 1705/2000 [0m                     

                       Computation: 42973 steps/s (collection: 2.178s, learning 0.110s)
             Mean action noise std: 3.90
          Mean value_function loss: 157.3226
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.6216
                       Mean reward: 862.36
               Mean episode length: 229.15
    Episode_Reward/reaching_object: 1.5370
     Episode_Reward/lifting_object: 165.3210
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0828
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 2.29s
                      Time elapsed: 01:16:15
                               ETA: 00:13:11

################################################################################
                     [1m Learning iteration 1706/2000 [0m                     

                       Computation: 41650 steps/s (collection: 2.231s, learning 0.129s)
             Mean action noise std: 3.90
          Mean value_function loss: 136.6254
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 71.6321
                       Mean reward: 848.74
               Mean episode length: 226.58
    Episode_Reward/reaching_object: 1.5877
     Episode_Reward/lifting_object: 171.1650
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0851
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 2.36s
                      Time elapsed: 01:16:18
                               ETA: 00:13:08

################################################################################
                     [1m Learning iteration 1707/2000 [0m                     

                       Computation: 42195 steps/s (collection: 2.196s, learning 0.134s)
             Mean action noise std: 3.91
          Mean value_function loss: 127.4295
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.6405
                       Mean reward: 881.64
               Mean episode length: 233.41
    Episode_Reward/reaching_object: 1.5860
     Episode_Reward/lifting_object: 170.6227
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0853
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 2.33s
                      Time elapsed: 01:16:20
                               ETA: 00:13:05

################################################################################
                     [1m Learning iteration 1708/2000 [0m                     

                       Computation: 42433 steps/s (collection: 2.199s, learning 0.118s)
             Mean action noise std: 3.91
          Mean value_function loss: 134.0583
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 71.6512
                       Mean reward: 835.39
               Mean episode length: 222.55
    Episode_Reward/reaching_object: 1.5518
     Episode_Reward/lifting_object: 166.7029
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0838
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 2.32s
                      Time elapsed: 01:16:22
                               ETA: 00:13:03

################################################################################
                     [1m Learning iteration 1709/2000 [0m                     

                       Computation: 41914 steps/s (collection: 2.206s, learning 0.140s)
             Mean action noise std: 3.91
          Mean value_function loss: 153.5528
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 71.6609
                       Mean reward: 890.49
               Mean episode length: 234.87
    Episode_Reward/reaching_object: 1.5873
     Episode_Reward/lifting_object: 171.4752
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 2.35s
                      Time elapsed: 01:16:25
                               ETA: 00:13:00

################################################################################
                     [1m Learning iteration 1710/2000 [0m                     

                       Computation: 42788 steps/s (collection: 2.171s, learning 0.127s)
             Mean action noise std: 3.91
          Mean value_function loss: 113.1374
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 71.6696
                       Mean reward: 871.44
               Mean episode length: 231.75
    Episode_Reward/reaching_object: 1.6102
     Episode_Reward/lifting_object: 173.7830
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0870
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 2.30s
                      Time elapsed: 01:16:27
                               ETA: 00:12:57

################################################################################
                     [1m Learning iteration 1711/2000 [0m                     

                       Computation: 42142 steps/s (collection: 2.191s, learning 0.142s)
             Mean action noise std: 3.91
          Mean value_function loss: 135.8506
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 71.6751
                       Mean reward: 864.14
               Mean episode length: 229.31
    Episode_Reward/reaching_object: 1.5674
     Episode_Reward/lifting_object: 169.4766
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0847
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 2.33s
                      Time elapsed: 01:16:29
                               ETA: 00:12:54

################################################################################
                     [1m Learning iteration 1712/2000 [0m                     

                       Computation: 42551 steps/s (collection: 2.169s, learning 0.141s)
             Mean action noise std: 3.91
          Mean value_function loss: 136.2385
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 71.6824
                       Mean reward: 873.82
               Mean episode length: 231.23
    Episode_Reward/reaching_object: 1.5621
     Episode_Reward/lifting_object: 168.5161
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0848
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 2.31s
                      Time elapsed: 01:16:32
                               ETA: 00:12:52

################################################################################
                     [1m Learning iteration 1713/2000 [0m                     

                       Computation: 43116 steps/s (collection: 2.170s, learning 0.110s)
             Mean action noise std: 3.91
          Mean value_function loss: 121.3318
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 71.6903
                       Mean reward: 859.77
               Mean episode length: 228.87
    Episode_Reward/reaching_object: 1.5873
     Episode_Reward/lifting_object: 171.5971
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0861
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 2.28s
                      Time elapsed: 01:16:34
                               ETA: 00:12:49

################################################################################
                     [1m Learning iteration 1714/2000 [0m                     

                       Computation: 42780 steps/s (collection: 2.159s, learning 0.139s)
             Mean action noise std: 3.92
          Mean value_function loss: 151.9032
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 71.7015
                       Mean reward: 879.27
               Mean episode length: 233.44
    Episode_Reward/reaching_object: 1.5707
     Episode_Reward/lifting_object: 169.7522
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0852
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 2.30s
                      Time elapsed: 01:16:36
                               ETA: 00:12:46

################################################################################
                     [1m Learning iteration 1715/2000 [0m                     

                       Computation: 42518 steps/s (collection: 2.169s, learning 0.143s)
             Mean action noise std: 3.92
          Mean value_function loss: 168.0169
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 71.7074
                       Mean reward: 807.82
               Mean episode length: 216.92
    Episode_Reward/reaching_object: 1.5547
     Episode_Reward/lifting_object: 167.9417
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0846
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 2.31s
                      Time elapsed: 01:16:39
                               ETA: 00:12:43

################################################################################
                     [1m Learning iteration 1716/2000 [0m                     

                       Computation: 40507 steps/s (collection: 2.318s, learning 0.108s)
             Mean action noise std: 3.92
          Mean value_function loss: 140.7003
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 71.7150
                       Mean reward: 841.03
               Mean episode length: 223.92
    Episode_Reward/reaching_object: 1.5571
     Episode_Reward/lifting_object: 168.3029
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 2.43s
                      Time elapsed: 01:16:41
                               ETA: 00:12:41

################################################################################
                     [1m Learning iteration 1717/2000 [0m                     

                       Computation: 41925 steps/s (collection: 2.210s, learning 0.135s)
             Mean action noise std: 3.92
          Mean value_function loss: 152.6319
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 71.7232
                       Mean reward: 875.29
               Mean episode length: 232.55
    Episode_Reward/reaching_object: 1.5479
     Episode_Reward/lifting_object: 166.9525
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0845
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 2.34s
                      Time elapsed: 01:16:43
                               ETA: 00:12:38

################################################################################
                     [1m Learning iteration 1718/2000 [0m                     

                       Computation: 43125 steps/s (collection: 2.145s, learning 0.134s)
             Mean action noise std: 3.92
          Mean value_function loss: 145.6535
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 71.7318
                       Mean reward: 815.80
               Mean episode length: 217.45
    Episode_Reward/reaching_object: 1.5439
     Episode_Reward/lifting_object: 166.6416
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0841
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 2.28s
                      Time elapsed: 01:16:46
                               ETA: 00:12:35

################################################################################
                     [1m Learning iteration 1719/2000 [0m                     

                       Computation: 42784 steps/s (collection: 2.181s, learning 0.117s)
             Mean action noise std: 3.92
          Mean value_function loss: 115.0432
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 71.7442
                       Mean reward: 855.06
               Mean episode length: 227.39
    Episode_Reward/reaching_object: 1.5316
     Episode_Reward/lifting_object: 164.9034
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0835
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 2.30s
                      Time elapsed: 01:16:48
                               ETA: 00:12:32

################################################################################
                     [1m Learning iteration 1720/2000 [0m                     

                       Computation: 42552 steps/s (collection: 2.177s, learning 0.134s)
             Mean action noise std: 3.92
          Mean value_function loss: 96.0768
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.7546
                       Mean reward: 875.60
               Mean episode length: 231.64
    Episode_Reward/reaching_object: 1.6215
     Episode_Reward/lifting_object: 175.6773
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0880
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 2.31s
                      Time elapsed: 01:16:50
                               ETA: 00:12:30

################################################################################
                     [1m Learning iteration 1721/2000 [0m                     

                       Computation: 42170 steps/s (collection: 2.203s, learning 0.129s)
             Mean action noise std: 3.93
          Mean value_function loss: 116.9720
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 71.7673
                       Mean reward: 829.04
               Mean episode length: 220.09
    Episode_Reward/reaching_object: 1.5924
     Episode_Reward/lifting_object: 171.9071
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 2.33s
                      Time elapsed: 01:16:53
                               ETA: 00:12:27

################################################################################
                     [1m Learning iteration 1722/2000 [0m                     

                       Computation: 42620 steps/s (collection: 2.172s, learning 0.134s)
             Mean action noise std: 3.93
          Mean value_function loss: 118.3312
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 71.7801
                       Mean reward: 885.34
               Mean episode length: 234.05
    Episode_Reward/reaching_object: 1.5891
     Episode_Reward/lifting_object: 171.3822
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0865
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 2.31s
                      Time elapsed: 01:16:55
                               ETA: 00:12:24

################################################################################
                     [1m Learning iteration 1723/2000 [0m                     

                       Computation: 42200 steps/s (collection: 2.224s, learning 0.105s)
             Mean action noise std: 3.93
          Mean value_function loss: 143.3022
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 71.7918
                       Mean reward: 851.82
               Mean episode length: 225.57
    Episode_Reward/reaching_object: 1.5596
     Episode_Reward/lifting_object: 167.8732
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0848
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 2.33s
                      Time elapsed: 01:16:57
                               ETA: 00:12:21

################################################################################
                     [1m Learning iteration 1724/2000 [0m                     

                       Computation: 42729 steps/s (collection: 2.169s, learning 0.131s)
             Mean action noise std: 3.93
          Mean value_function loss: 125.3364
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 71.7995
                       Mean reward: 851.88
               Mean episode length: 226.14
    Episode_Reward/reaching_object: 1.5837
     Episode_Reward/lifting_object: 170.6095
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0863
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 2.30s
                      Time elapsed: 01:16:59
                               ETA: 00:12:19

################################################################################
                     [1m Learning iteration 1725/2000 [0m                     

                       Computation: 42212 steps/s (collection: 2.190s, learning 0.139s)
             Mean action noise std: 3.93
          Mean value_function loss: 128.6783
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 71.8085
                       Mean reward: 878.93
               Mean episode length: 231.98
    Episode_Reward/reaching_object: 1.5887
     Episode_Reward/lifting_object: 171.3859
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0864
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 2.33s
                      Time elapsed: 01:17:02
                               ETA: 00:12:16

################################################################################
                     [1m Learning iteration 1726/2000 [0m                     

                       Computation: 42176 steps/s (collection: 2.199s, learning 0.132s)
             Mean action noise std: 3.93
          Mean value_function loss: 126.0801
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 71.8226
                       Mean reward: 873.51
               Mean episode length: 231.87
    Episode_Reward/reaching_object: 1.6026
     Episode_Reward/lifting_object: 172.3315
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0871
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 2.33s
                      Time elapsed: 01:17:04
                               ETA: 00:12:13

################################################################################
                     [1m Learning iteration 1727/2000 [0m                     

                       Computation: 42791 steps/s (collection: 2.177s, learning 0.120s)
             Mean action noise std: 3.94
          Mean value_function loss: 139.3274
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 71.8345
                       Mean reward: 844.69
               Mean episode length: 224.43
    Episode_Reward/reaching_object: 1.5641
     Episode_Reward/lifting_object: 168.2607
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0853
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 2.30s
                      Time elapsed: 01:17:06
                               ETA: 00:12:10

################################################################################
                     [1m Learning iteration 1728/2000 [0m                     

                       Computation: 42361 steps/s (collection: 2.187s, learning 0.134s)
             Mean action noise std: 3.94
          Mean value_function loss: 122.2392
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 71.8453
                       Mean reward: 911.66
               Mean episode length: 239.73
    Episode_Reward/reaching_object: 1.5907
     Episode_Reward/lifting_object: 171.3190
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0866
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 2.32s
                      Time elapsed: 01:17:09
                               ETA: 00:12:08

################################################################################
                     [1m Learning iteration 1729/2000 [0m                     

                       Computation: 41712 steps/s (collection: 2.227s, learning 0.130s)
             Mean action noise std: 3.94
          Mean value_function loss: 124.5548
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 71.8532
                       Mean reward: 861.49
               Mean episode length: 228.93
    Episode_Reward/reaching_object: 1.5971
     Episode_Reward/lifting_object: 171.7624
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0870
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 2.36s
                      Time elapsed: 01:17:11
                               ETA: 00:12:05

################################################################################
                     [1m Learning iteration 1730/2000 [0m                     

                       Computation: 41987 steps/s (collection: 2.214s, learning 0.127s)
             Mean action noise std: 3.94
          Mean value_function loss: 132.9379
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 71.8686
                       Mean reward: 791.07
               Mean episode length: 213.60
    Episode_Reward/reaching_object: 1.5539
     Episode_Reward/lifting_object: 166.9712
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0850
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 2.34s
                      Time elapsed: 01:17:13
                               ETA: 00:12:02

################################################################################
                     [1m Learning iteration 1731/2000 [0m                     

                       Computation: 42470 steps/s (collection: 2.198s, learning 0.116s)
             Mean action noise std: 3.94
          Mean value_function loss: 133.0491
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 71.8862
                       Mean reward: 862.47
               Mean episode length: 229.65
    Episode_Reward/reaching_object: 1.5766
     Episode_Reward/lifting_object: 169.6603
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0861
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 2.31s
                      Time elapsed: 01:17:16
                               ETA: 00:12:00

################################################################################
                     [1m Learning iteration 1732/2000 [0m                     

                       Computation: 41503 steps/s (collection: 2.241s, learning 0.128s)
             Mean action noise std: 3.95
          Mean value_function loss: 135.8762
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 71.8982
                       Mean reward: 893.40
               Mean episode length: 236.13
    Episode_Reward/reaching_object: 1.5901
     Episode_Reward/lifting_object: 171.1636
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0870
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 2.37s
                      Time elapsed: 01:17:18
                               ETA: 00:11:57

################################################################################
                     [1m Learning iteration 1733/2000 [0m                     

                       Computation: 40755 steps/s (collection: 2.267s, learning 0.145s)
             Mean action noise std: 3.95
          Mean value_function loss: 136.4929
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 71.9123
                       Mean reward: 885.70
               Mean episode length: 235.11
    Episode_Reward/reaching_object: 1.5799
     Episode_Reward/lifting_object: 170.0961
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0869
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 2.41s
                      Time elapsed: 01:17:21
                               ETA: 00:11:54

################################################################################
                     [1m Learning iteration 1734/2000 [0m                     

                       Computation: 41846 steps/s (collection: 2.216s, learning 0.133s)
             Mean action noise std: 3.95
          Mean value_function loss: 137.8709
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 71.9201
                       Mean reward: 862.11
               Mean episode length: 230.92
    Episode_Reward/reaching_object: 1.5637
     Episode_Reward/lifting_object: 168.2313
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0860
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 2.35s
                      Time elapsed: 01:17:23
                               ETA: 00:11:51

################################################################################
                     [1m Learning iteration 1735/2000 [0m                     

                       Computation: 43248 steps/s (collection: 2.151s, learning 0.123s)
             Mean action noise std: 3.95
          Mean value_function loss: 127.5481
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 71.9272
                       Mean reward: 918.26
               Mean episode length: 241.35
    Episode_Reward/reaching_object: 1.6241
     Episode_Reward/lifting_object: 175.0107
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 2.27s
                      Time elapsed: 01:17:25
                               ETA: 00:11:49

################################################################################
                     [1m Learning iteration 1736/2000 [0m                     

                       Computation: 42237 steps/s (collection: 2.194s, learning 0.134s)
             Mean action noise std: 3.95
          Mean value_function loss: 146.6516
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 71.9369
                       Mean reward: 840.54
               Mean episode length: 226.17
    Episode_Reward/reaching_object: 1.6024
     Episode_Reward/lifting_object: 172.3620
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0881
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 2.33s
                      Time elapsed: 01:17:27
                               ETA: 00:11:46

################################################################################
                     [1m Learning iteration 1737/2000 [0m                     

                       Computation: 42282 steps/s (collection: 2.192s, learning 0.133s)
             Mean action noise std: 3.95
          Mean value_function loss: 183.2498
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 71.9510
                       Mean reward: 817.57
               Mean episode length: 217.89
    Episode_Reward/reaching_object: 1.5602
     Episode_Reward/lifting_object: 167.8873
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0860
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 2.32s
                      Time elapsed: 01:17:30
                               ETA: 00:11:43

################################################################################
                     [1m Learning iteration 1738/2000 [0m                     

                       Computation: 43080 steps/s (collection: 2.169s, learning 0.113s)
             Mean action noise std: 3.96
          Mean value_function loss: 111.3722
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 71.9644
                       Mean reward: 913.92
               Mean episode length: 240.90
    Episode_Reward/reaching_object: 1.6618
     Episode_Reward/lifting_object: 179.6074
      Episode_Reward/object_height: 0.0277
        Episode_Reward/action_rate: -0.0911
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 2.28s
                      Time elapsed: 01:17:32
                               ETA: 00:11:40

################################################################################
                     [1m Learning iteration 1739/2000 [0m                     

                       Computation: 42381 steps/s (collection: 2.191s, learning 0.128s)
             Mean action noise std: 3.96
          Mean value_function loss: 104.3848
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 71.9749
                       Mean reward: 860.17
               Mean episode length: 227.79
    Episode_Reward/reaching_object: 1.5877
     Episode_Reward/lifting_object: 171.3085
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0878
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 2.32s
                      Time elapsed: 01:17:34
                               ETA: 00:11:38

################################################################################
                     [1m Learning iteration 1740/2000 [0m                     

                       Computation: 41592 steps/s (collection: 2.227s, learning 0.136s)
             Mean action noise std: 3.96
          Mean value_function loss: 119.4541
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 71.9877
                       Mean reward: 861.44
               Mean episode length: 228.52
    Episode_Reward/reaching_object: 1.5888
     Episode_Reward/lifting_object: 171.3724
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 2.36s
                      Time elapsed: 01:17:37
                               ETA: 00:11:35

################################################################################
                     [1m Learning iteration 1741/2000 [0m                     

                       Computation: 42277 steps/s (collection: 2.211s, learning 0.115s)
             Mean action noise std: 3.96
          Mean value_function loss: 120.9455
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 71.9969
                       Mean reward: 892.64
               Mean episode length: 236.56
    Episode_Reward/reaching_object: 1.6066
     Episode_Reward/lifting_object: 173.4186
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 2.33s
                      Time elapsed: 01:17:39
                               ETA: 00:11:32

################################################################################
                     [1m Learning iteration 1742/2000 [0m                     

                       Computation: 43326 steps/s (collection: 2.159s, learning 0.110s)
             Mean action noise std: 3.96
          Mean value_function loss: 177.7291
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 72.0034
                       Mean reward: 862.89
               Mean episode length: 228.56
    Episode_Reward/reaching_object: 1.5446
     Episode_Reward/lifting_object: 166.3915
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0857
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 2.27s
                      Time elapsed: 01:17:41
                               ETA: 00:11:30

################################################################################
                     [1m Learning iteration 1743/2000 [0m                     

                       Computation: 42636 steps/s (collection: 2.175s, learning 0.131s)
             Mean action noise std: 3.96
          Mean value_function loss: 119.2462
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 72.0080
                       Mean reward: 894.27
               Mean episode length: 236.95
    Episode_Reward/reaching_object: 1.6060
     Episode_Reward/lifting_object: 172.9158
      Episode_Reward/object_height: 0.0265
        Episode_Reward/action_rate: -0.0889
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 2.31s
                      Time elapsed: 01:17:44
                               ETA: 00:11:27

################################################################################
                     [1m Learning iteration 1744/2000 [0m                     

                       Computation: 42602 steps/s (collection: 2.194s, learning 0.113s)
             Mean action noise std: 3.96
          Mean value_function loss: 118.9966
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 72.0188
                       Mean reward: 887.37
               Mean episode length: 234.17
    Episode_Reward/reaching_object: 1.5852
     Episode_Reward/lifting_object: 171.2430
      Episode_Reward/object_height: 0.0259
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 2.31s
                      Time elapsed: 01:17:46
                               ETA: 00:11:24

################################################################################
                     [1m Learning iteration 1745/2000 [0m                     

                       Computation: 42136 steps/s (collection: 2.191s, learning 0.142s)
             Mean action noise std: 3.97
          Mean value_function loss: 100.9798
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.0286
                       Mean reward: 891.44
               Mean episode length: 235.43
    Episode_Reward/reaching_object: 1.6176
     Episode_Reward/lifting_object: 174.7573
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0895
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 2.33s
                      Time elapsed: 01:17:48
                               ETA: 00:11:21

################################################################################
                     [1m Learning iteration 1746/2000 [0m                     

                       Computation: 39446 steps/s (collection: 2.361s, learning 0.131s)
             Mean action noise std: 3.97
          Mean value_function loss: 108.9167
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.0390
                       Mean reward: 877.49
               Mean episode length: 232.46
    Episode_Reward/reaching_object: 1.6064
     Episode_Reward/lifting_object: 173.4874
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0890
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 2.49s
                      Time elapsed: 01:17:51
                               ETA: 00:11:19

################################################################################
                     [1m Learning iteration 1747/2000 [0m                     

                       Computation: 41610 steps/s (collection: 2.248s, learning 0.114s)
             Mean action noise std: 3.97
          Mean value_function loss: 126.7444
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 72.0512
                       Mean reward: 877.29
               Mean episode length: 232.60
    Episode_Reward/reaching_object: 1.6074
     Episode_Reward/lifting_object: 173.2872
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 2.36s
                      Time elapsed: 01:17:53
                               ETA: 00:11:16

################################################################################
                     [1m Learning iteration 1748/2000 [0m                     

                       Computation: 39945 steps/s (collection: 2.334s, learning 0.127s)
             Mean action noise std: 3.97
          Mean value_function loss: 146.6320
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 72.0654
                       Mean reward: 823.93
               Mean episode length: 220.15
    Episode_Reward/reaching_object: 1.5613
     Episode_Reward/lifting_object: 168.3599
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0866
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 2.46s
                      Time elapsed: 01:17:56
                               ETA: 00:11:13

################################################################################
                     [1m Learning iteration 1749/2000 [0m                     

                       Computation: 41498 steps/s (collection: 2.230s, learning 0.139s)
             Mean action noise std: 3.97
          Mean value_function loss: 144.6384
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 72.0776
                       Mean reward: 871.65
               Mean episode length: 231.35
    Episode_Reward/reaching_object: 1.5636
     Episode_Reward/lifting_object: 168.3464
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0872
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 2.37s
                      Time elapsed: 01:17:58
                               ETA: 00:11:11

################################################################################
                     [1m Learning iteration 1750/2000 [0m                     

                       Computation: 41466 steps/s (collection: 2.239s, learning 0.132s)
             Mean action noise std: 3.97
          Mean value_function loss: 111.3153
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 72.0848
                       Mean reward: 870.52
               Mean episode length: 231.04
    Episode_Reward/reaching_object: 1.5860
     Episode_Reward/lifting_object: 171.2796
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0879
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 2.37s
                      Time elapsed: 01:18:00
                               ETA: 00:11:08

################################################################################
                     [1m Learning iteration 1751/2000 [0m                     

                       Computation: 41259 steps/s (collection: 2.255s, learning 0.128s)
             Mean action noise std: 3.98
          Mean value_function loss: 114.7926
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.0964
                       Mean reward: 874.22
               Mean episode length: 231.73
    Episode_Reward/reaching_object: 1.5956
     Episode_Reward/lifting_object: 172.4569
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0884
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 2.38s
                      Time elapsed: 01:18:03
                               ETA: 00:11:05

################################################################################
                     [1m Learning iteration 1752/2000 [0m                     

                       Computation: 40948 steps/s (collection: 2.265s, learning 0.136s)
             Mean action noise std: 3.98
          Mean value_function loss: 125.5720
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 72.1118
                       Mean reward: 891.76
               Mean episode length: 235.56
    Episode_Reward/reaching_object: 1.6147
     Episode_Reward/lifting_object: 174.6569
      Episode_Reward/object_height: 0.0266
        Episode_Reward/action_rate: -0.0894
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 2.40s
                      Time elapsed: 01:18:05
                               ETA: 00:11:02

################################################################################
                     [1m Learning iteration 1753/2000 [0m                     

                       Computation: 41669 steps/s (collection: 2.226s, learning 0.133s)
             Mean action noise std: 3.98
          Mean value_function loss: 117.7910
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 72.1263
                       Mean reward: 878.65
               Mean episode length: 232.45
    Episode_Reward/reaching_object: 1.5995
     Episode_Reward/lifting_object: 172.7232
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0889
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 2.36s
                      Time elapsed: 01:18:08
                               ETA: 00:11:00

################################################################################
                     [1m Learning iteration 1754/2000 [0m                     

                       Computation: 42116 steps/s (collection: 2.207s, learning 0.128s)
             Mean action noise std: 3.98
          Mean value_function loss: 143.1610
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.1378
                       Mean reward: 852.62
               Mean episode length: 226.43
    Episode_Reward/reaching_object: 1.5788
     Episode_Reward/lifting_object: 170.5031
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0880
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 2.33s
                      Time elapsed: 01:18:10
                               ETA: 00:10:57

################################################################################
                     [1m Learning iteration 1755/2000 [0m                     

                       Computation: 41871 steps/s (collection: 2.242s, learning 0.106s)
             Mean action noise std: 3.98
          Mean value_function loss: 131.7357
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 72.1435
                       Mean reward: 863.61
               Mean episode length: 228.24
    Episode_Reward/reaching_object: 1.5565
     Episode_Reward/lifting_object: 167.7663
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0871
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 2.35s
                      Time elapsed: 01:18:12
                               ETA: 00:10:54

################################################################################
                     [1m Learning iteration 1756/2000 [0m                     

                       Computation: 41825 steps/s (collection: 2.203s, learning 0.147s)
             Mean action noise std: 3.98
          Mean value_function loss: 131.0272
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 72.1497
                       Mean reward: 872.45
               Mean episode length: 230.52
    Episode_Reward/reaching_object: 1.5425
     Episode_Reward/lifting_object: 166.5785
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0864
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 2.35s
                      Time elapsed: 01:18:15
                               ETA: 00:10:52

################################################################################
                     [1m Learning iteration 1757/2000 [0m                     

                       Computation: 41834 steps/s (collection: 2.203s, learning 0.147s)
             Mean action noise std: 3.99
          Mean value_function loss: 134.6405
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.1606
                       Mean reward: 864.96
               Mean episode length: 229.76
    Episode_Reward/reaching_object: 1.5917
     Episode_Reward/lifting_object: 171.9608
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 2.35s
                      Time elapsed: 01:18:17
                               ETA: 00:10:49

################################################################################
                     [1m Learning iteration 1758/2000 [0m                     

                       Computation: 41600 steps/s (collection: 2.243s, learning 0.120s)
             Mean action noise std: 3.99
          Mean value_function loss: 122.3352
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 72.1699
                       Mean reward: 858.19
               Mean episode length: 227.62
    Episode_Reward/reaching_object: 1.5877
     Episode_Reward/lifting_object: 171.9206
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 2.36s
                      Time elapsed: 01:18:19
                               ETA: 00:10:46

################################################################################
                     [1m Learning iteration 1759/2000 [0m                     

                       Computation: 42392 steps/s (collection: 2.194s, learning 0.124s)
             Mean action noise std: 3.99
          Mean value_function loss: 132.6666
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 72.1787
                       Mean reward: 866.78
               Mean episode length: 230.14
    Episode_Reward/reaching_object: 1.5865
     Episode_Reward/lifting_object: 171.5515
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 2.32s
                      Time elapsed: 01:18:22
                               ETA: 00:10:43

################################################################################
                     [1m Learning iteration 1760/2000 [0m                     

                       Computation: 42304 steps/s (collection: 2.192s, learning 0.132s)
             Mean action noise std: 3.99
          Mean value_function loss: 99.8262
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 72.1946
                       Mean reward: 869.37
               Mean episode length: 230.77
    Episode_Reward/reaching_object: 1.5978
     Episode_Reward/lifting_object: 172.8655
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 2.32s
                      Time elapsed: 01:18:24
                               ETA: 00:10:41

################################################################################
                     [1m Learning iteration 1761/2000 [0m                     

                       Computation: 41861 steps/s (collection: 2.215s, learning 0.133s)
             Mean action noise std: 3.99
          Mean value_function loss: 155.9601
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 72.2049
                       Mean reward: 811.63
               Mean episode length: 217.96
    Episode_Reward/reaching_object: 1.5719
     Episode_Reward/lifting_object: 169.4561
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 2.35s
                      Time elapsed: 01:18:26
                               ETA: 00:10:38

################################################################################
                     [1m Learning iteration 1762/2000 [0m                     

                       Computation: 41827 steps/s (collection: 2.223s, learning 0.127s)
             Mean action noise std: 3.99
          Mean value_function loss: 118.5817
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 72.2113
                       Mean reward: 898.32
               Mean episode length: 237.43
    Episode_Reward/reaching_object: 1.5680
     Episode_Reward/lifting_object: 169.4635
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0884
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 2.35s
                      Time elapsed: 01:18:29
                               ETA: 00:10:35

################################################################################
                     [1m Learning iteration 1763/2000 [0m                     

                       Computation: 42332 steps/s (collection: 2.191s, learning 0.132s)
             Mean action noise std: 4.00
          Mean value_function loss: 122.0247
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 72.2235
                       Mean reward: 845.19
               Mean episode length: 224.89
    Episode_Reward/reaching_object: 1.5276
     Episode_Reward/lifting_object: 164.7054
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0865
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 2.32s
                      Time elapsed: 01:18:31
                               ETA: 00:10:32

################################################################################
                     [1m Learning iteration 1764/2000 [0m                     

                       Computation: 42259 steps/s (collection: 2.204s, learning 0.123s)
             Mean action noise std: 4.00
          Mean value_function loss: 119.7193
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 72.2354
                       Mean reward: 864.44
               Mean episode length: 230.56
    Episode_Reward/reaching_object: 1.5961
     Episode_Reward/lifting_object: 172.5667
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 2.33s
                      Time elapsed: 01:18:33
                               ETA: 00:10:30

################################################################################
                     [1m Learning iteration 1765/2000 [0m                     

                       Computation: 42241 steps/s (collection: 2.187s, learning 0.140s)
             Mean action noise std: 4.00
          Mean value_function loss: 149.1767
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 72.2423
                       Mean reward: 833.25
               Mean episode length: 221.05
    Episode_Reward/reaching_object: 1.5773
     Episode_Reward/lifting_object: 170.4760
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 2.33s
                      Time elapsed: 01:18:36
                               ETA: 00:10:27

################################################################################
                     [1m Learning iteration 1766/2000 [0m                     

                       Computation: 42713 steps/s (collection: 2.171s, learning 0.131s)
             Mean action noise std: 4.00
          Mean value_function loss: 143.3371
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 72.2454
                       Mean reward: 875.27
               Mean episode length: 231.22
    Episode_Reward/reaching_object: 1.5700
     Episode_Reward/lifting_object: 169.6518
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 2.30s
                      Time elapsed: 01:18:38
                               ETA: 00:10:24

################################################################################
                     [1m Learning iteration 1767/2000 [0m                     

                       Computation: 42348 steps/s (collection: 2.188s, learning 0.134s)
             Mean action noise std: 4.00
          Mean value_function loss: 134.5196
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.2544
                       Mean reward: 874.37
               Mean episode length: 232.36
    Episode_Reward/reaching_object: 1.5636
     Episode_Reward/lifting_object: 168.6698
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.0883
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 2.32s
                      Time elapsed: 01:18:40
                               ETA: 00:10:22

################################################################################
                     [1m Learning iteration 1768/2000 [0m                     

                       Computation: 42326 steps/s (collection: 2.185s, learning 0.137s)
             Mean action noise std: 4.00
          Mean value_function loss: 123.2431
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 72.2699
                       Mean reward: 879.83
               Mean episode length: 232.29
    Episode_Reward/reaching_object: 1.6296
     Episode_Reward/lifting_object: 176.6096
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0915
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 2.32s
                      Time elapsed: 01:18:43
                               ETA: 00:10:19

################################################################################
                     [1m Learning iteration 1769/2000 [0m                     

                       Computation: 42675 steps/s (collection: 2.174s, learning 0.130s)
             Mean action noise std: 4.01
          Mean value_function loss: 140.6048
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 72.2897
                       Mean reward: 863.68
               Mean episode length: 228.91
    Episode_Reward/reaching_object: 1.5884
     Episode_Reward/lifting_object: 171.4616
      Episode_Reward/object_height: 0.0262
        Episode_Reward/action_rate: -0.0895
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 2.30s
                      Time elapsed: 01:18:45
                               ETA: 00:10:16

################################################################################
                     [1m Learning iteration 1770/2000 [0m                     

                       Computation: 42515 steps/s (collection: 2.176s, learning 0.137s)
             Mean action noise std: 4.01
          Mean value_function loss: 123.8659
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 72.3060
                       Mean reward: 839.26
               Mean episode length: 224.27
    Episode_Reward/reaching_object: 1.5758
     Episode_Reward/lifting_object: 170.2767
      Episode_Reward/object_height: 0.0261
        Episode_Reward/action_rate: -0.0889
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 2.31s
                      Time elapsed: 01:18:47
                               ETA: 00:10:13

################################################################################
                     [1m Learning iteration 1771/2000 [0m                     

                       Computation: 40634 steps/s (collection: 2.284s, learning 0.136s)
             Mean action noise std: 4.01
          Mean value_function loss: 102.7128
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.3168
                       Mean reward: 880.98
               Mean episode length: 233.00
    Episode_Reward/reaching_object: 1.5970
     Episode_Reward/lifting_object: 172.5804
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0902
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 2.42s
                      Time elapsed: 01:18:50
                               ETA: 00:10:11

################################################################################
                     [1m Learning iteration 1772/2000 [0m                     

                       Computation: 42149 steps/s (collection: 2.195s, learning 0.137s)
             Mean action noise std: 4.01
          Mean value_function loss: 112.2181
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 72.3284
                       Mean reward: 870.94
               Mean episode length: 230.76
    Episode_Reward/reaching_object: 1.6104
     Episode_Reward/lifting_object: 174.1093
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 2.33s
                      Time elapsed: 01:18:52
                               ETA: 00:10:08

################################################################################
                     [1m Learning iteration 1773/2000 [0m                     

                       Computation: 42540 steps/s (collection: 2.199s, learning 0.112s)
             Mean action noise std: 4.01
          Mean value_function loss: 136.2906
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 72.3380
                       Mean reward: 849.60
               Mean episode length: 225.60
    Episode_Reward/reaching_object: 1.5614
     Episode_Reward/lifting_object: 168.6089
      Episode_Reward/object_height: 0.0263
        Episode_Reward/action_rate: -0.0885
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 2.31s
                      Time elapsed: 01:18:54
                               ETA: 00:10:05

################################################################################
                     [1m Learning iteration 1774/2000 [0m                     

                       Computation: 42549 steps/s (collection: 2.187s, learning 0.123s)
             Mean action noise std: 4.01
          Mean value_function loss: 138.6219
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.3463
                       Mean reward: 835.92
               Mean episode length: 222.66
    Episode_Reward/reaching_object: 1.5776
     Episode_Reward/lifting_object: 170.2398
      Episode_Reward/object_height: 0.0264
        Episode_Reward/action_rate: -0.0894
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 2.31s
                      Time elapsed: 01:18:57
                               ETA: 00:10:03

################################################################################
                     [1m Learning iteration 1775/2000 [0m                     

                       Computation: 42479 steps/s (collection: 2.187s, learning 0.128s)
             Mean action noise std: 4.02
          Mean value_function loss: 130.7867
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 72.3586
                       Mean reward: 827.37
               Mean episode length: 219.78
    Episode_Reward/reaching_object: 1.5645
     Episode_Reward/lifting_object: 169.1305
      Episode_Reward/object_height: 0.0267
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 2.31s
                      Time elapsed: 01:18:59
                               ETA: 00:10:00

################################################################################
                     [1m Learning iteration 1776/2000 [0m                     

                       Computation: 42444 steps/s (collection: 2.190s, learning 0.126s)
             Mean action noise std: 4.02
          Mean value_function loss: 132.4205
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.3653
                       Mean reward: 850.38
               Mean episode length: 225.66
    Episode_Reward/reaching_object: 1.5873
     Episode_Reward/lifting_object: 171.3752
      Episode_Reward/object_height: 0.0272
        Episode_Reward/action_rate: -0.0899
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 2.32s
                      Time elapsed: 01:19:01
                               ETA: 00:09:57

################################################################################
                     [1m Learning iteration 1777/2000 [0m                     

                       Computation: 41267 steps/s (collection: 2.246s, learning 0.136s)
             Mean action noise std: 4.02
          Mean value_function loss: 123.8480
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.3761
                       Mean reward: 850.60
               Mean episode length: 226.41
    Episode_Reward/reaching_object: 1.5841
     Episode_Reward/lifting_object: 170.5844
      Episode_Reward/object_height: 0.0268
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 2.38s
                      Time elapsed: 01:19:04
                               ETA: 00:09:55

################################################################################
                     [1m Learning iteration 1778/2000 [0m                     

                       Computation: 41133 steps/s (collection: 2.258s, learning 0.132s)
             Mean action noise std: 4.02
          Mean value_function loss: 117.1916
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 72.3982
                       Mean reward: 880.15
               Mean episode length: 232.91
    Episode_Reward/reaching_object: 1.5916
     Episode_Reward/lifting_object: 171.6664
      Episode_Reward/object_height: 0.0270
        Episode_Reward/action_rate: -0.0905
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 2.39s
                      Time elapsed: 01:19:06
                               ETA: 00:09:52

################################################################################
                     [1m Learning iteration 1779/2000 [0m                     

                       Computation: 41868 steps/s (collection: 2.221s, learning 0.127s)
             Mean action noise std: 4.02
          Mean value_function loss: 114.9665
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.4097
                       Mean reward: 891.16
               Mean episode length: 235.86
    Episode_Reward/reaching_object: 1.5752
     Episode_Reward/lifting_object: 169.4867
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 2.35s
                      Time elapsed: 01:19:08
                               ETA: 00:09:49

################################################################################
                     [1m Learning iteration 1780/2000 [0m                     

                       Computation: 42073 steps/s (collection: 2.220s, learning 0.117s)
             Mean action noise std: 4.03
          Mean value_function loss: 175.1891
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.4258
                       Mean reward: 868.21
               Mean episode length: 230.47
    Episode_Reward/reaching_object: 1.5579
     Episode_Reward/lifting_object: 167.7582
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0890
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 2.34s
                      Time elapsed: 01:19:11
                               ETA: 00:09:46

################################################################################
                     [1m Learning iteration 1781/2000 [0m                     

                       Computation: 41639 steps/s (collection: 2.228s, learning 0.132s)
             Mean action noise std: 4.03
          Mean value_function loss: 123.1743
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.4397
                       Mean reward: 868.61
               Mean episode length: 231.08
    Episode_Reward/reaching_object: 1.5748
     Episode_Reward/lifting_object: 169.5759
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 2.36s
                      Time elapsed: 01:19:13
                               ETA: 00:09:44

################################################################################
                     [1m Learning iteration 1782/2000 [0m                     

                       Computation: 40833 steps/s (collection: 2.274s, learning 0.133s)
             Mean action noise std: 4.03
          Mean value_function loss: 125.8602
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 72.4504
                       Mean reward: 848.51
               Mean episode length: 225.33
    Episode_Reward/reaching_object: 1.5891
     Episode_Reward/lifting_object: 171.7003
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0905
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 2.41s
                      Time elapsed: 01:19:15
                               ETA: 00:09:41

################################################################################
                     [1m Learning iteration 1783/2000 [0m                     

                       Computation: 42620 steps/s (collection: 2.182s, learning 0.125s)
             Mean action noise std: 4.03
          Mean value_function loss: 166.3653
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 72.4561
                       Mean reward: 881.41
               Mean episode length: 233.52
    Episode_Reward/reaching_object: 1.5893
     Episode_Reward/lifting_object: 171.5703
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 2.31s
                      Time elapsed: 01:19:18
                               ETA: 00:09:38

################################################################################
                     [1m Learning iteration 1784/2000 [0m                     

                       Computation: 41226 steps/s (collection: 2.245s, learning 0.140s)
             Mean action noise std: 4.03
          Mean value_function loss: 203.9614
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 72.4616
                       Mean reward: 861.31
               Mean episode length: 227.56
    Episode_Reward/reaching_object: 1.5175
     Episode_Reward/lifting_object: 163.7873
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0873
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.7500
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 2.38s
                      Time elapsed: 01:19:20
                               ETA: 00:09:36

################################################################################
                     [1m Learning iteration 1785/2000 [0m                     

                       Computation: 41350 steps/s (collection: 2.240s, learning 0.138s)
             Mean action noise std: 4.03
          Mean value_function loss: 161.6541
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 72.4681
                       Mean reward: 861.21
               Mean episode length: 228.81
    Episode_Reward/reaching_object: 1.5531
     Episode_Reward/lifting_object: 167.6760
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0891
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 2.38s
                      Time elapsed: 01:19:22
                               ETA: 00:09:33

################################################################################
                     [1m Learning iteration 1786/2000 [0m                     

                       Computation: 41716 steps/s (collection: 2.213s, learning 0.143s)
             Mean action noise std: 4.03
          Mean value_function loss: 194.4194
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 72.4751
                       Mean reward: 844.65
               Mean episode length: 225.65
    Episode_Reward/reaching_object: 1.5639
     Episode_Reward/lifting_object: 169.0764
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 2.36s
                      Time elapsed: 01:19:25
                               ETA: 00:09:30

################################################################################
                     [1m Learning iteration 1787/2000 [0m                     

                       Computation: 41664 steps/s (collection: 2.223s, learning 0.137s)
             Mean action noise std: 4.03
          Mean value_function loss: 159.2073
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 72.4789
                       Mean reward: 855.88
               Mean episode length: 227.02
    Episode_Reward/reaching_object: 1.5688
     Episode_Reward/lifting_object: 169.5924
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0902
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 2.36s
                      Time elapsed: 01:19:27
                               ETA: 00:09:27

################################################################################
                     [1m Learning iteration 1788/2000 [0m                     

                       Computation: 42615 steps/s (collection: 2.178s, learning 0.129s)
             Mean action noise std: 4.04
          Mean value_function loss: 170.6243
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 72.4861
                       Mean reward: 852.09
               Mean episode length: 226.39
    Episode_Reward/reaching_object: 1.5065
     Episode_Reward/lifting_object: 162.2456
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0869
          Episode_Reward/joint_vel: -0.0553
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.3750
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 2.31s
                      Time elapsed: 01:19:29
                               ETA: 00:09:25

################################################################################
                     [1m Learning iteration 1789/2000 [0m                     

                       Computation: 42688 steps/s (collection: 2.166s, learning 0.136s)
             Mean action noise std: 4.04
          Mean value_function loss: 169.5811
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 72.4951
                       Mean reward: 859.73
               Mean episode length: 227.64
    Episode_Reward/reaching_object: 1.5366
     Episode_Reward/lifting_object: 165.8686
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 2.30s
                      Time elapsed: 01:19:32
                               ETA: 00:09:22

################################################################################
                     [1m Learning iteration 1790/2000 [0m                     

                       Computation: 42042 steps/s (collection: 2.195s, learning 0.144s)
             Mean action noise std: 4.04
          Mean value_function loss: 114.7699
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 72.5026
                       Mean reward: 867.21
               Mean episode length: 230.36
    Episode_Reward/reaching_object: 1.5967
     Episode_Reward/lifting_object: 172.6041
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0918
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 2.34s
                      Time elapsed: 01:19:34
                               ETA: 00:09:19

################################################################################
                     [1m Learning iteration 1791/2000 [0m                     

                       Computation: 42151 steps/s (collection: 2.213s, learning 0.120s)
             Mean action noise std: 4.04
          Mean value_function loss: 129.0045
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 72.5182
                       Mean reward: 878.11
               Mean episode length: 233.12
    Episode_Reward/reaching_object: 1.5761
     Episode_Reward/lifting_object: 169.4050
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 2.33s
                      Time elapsed: 01:19:36
                               ETA: 00:09:17

################################################################################
                     [1m Learning iteration 1792/2000 [0m                     

                       Computation: 41412 steps/s (collection: 2.246s, learning 0.128s)
             Mean action noise std: 4.04
          Mean value_function loss: 139.0373
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 72.5293
                       Mean reward: 876.97
               Mean episode length: 233.64
    Episode_Reward/reaching_object: 1.5525
     Episode_Reward/lifting_object: 167.3404
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 2.37s
                      Time elapsed: 01:19:39
                               ETA: 00:09:14

################################################################################
                     [1m Learning iteration 1793/2000 [0m                     

                       Computation: 42535 steps/s (collection: 2.175s, learning 0.136s)
             Mean action noise std: 4.04
          Mean value_function loss: 170.0104
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 72.5390
                       Mean reward: 801.80
               Mean episode length: 215.79
    Episode_Reward/reaching_object: 1.5214
     Episode_Reward/lifting_object: 163.9966
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0880
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 2.31s
                      Time elapsed: 01:19:41
                               ETA: 00:09:11

################################################################################
                     [1m Learning iteration 1794/2000 [0m                     

                       Computation: 42174 steps/s (collection: 2.218s, learning 0.113s)
             Mean action noise std: 4.05
          Mean value_function loss: 121.1114
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.5517
                       Mean reward: 900.63
               Mean episode length: 237.24
    Episode_Reward/reaching_object: 1.5887
     Episode_Reward/lifting_object: 171.6634
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0917
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 2.33s
                      Time elapsed: 01:19:43
                               ETA: 00:09:09

################################################################################
                     [1m Learning iteration 1795/2000 [0m                     

                       Computation: 41282 steps/s (collection: 2.235s, learning 0.147s)
             Mean action noise std: 4.05
          Mean value_function loss: 123.4003
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 72.5639
                       Mean reward: 827.42
               Mean episode length: 223.02
    Episode_Reward/reaching_object: 1.5901
     Episode_Reward/lifting_object: 171.4983
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0920
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 2.38s
                      Time elapsed: 01:19:46
                               ETA: 00:09:06

################################################################################
                     [1m Learning iteration 1796/2000 [0m                     

                       Computation: 41912 steps/s (collection: 2.207s, learning 0.138s)
             Mean action noise std: 4.05
          Mean value_function loss: 162.3916
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 72.5701
                       Mean reward: 844.42
               Mean episode length: 224.76
    Episode_Reward/reaching_object: 1.5449
     Episode_Reward/lifting_object: 166.6425
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0894
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 2.35s
                      Time elapsed: 01:19:48
                               ETA: 00:09:03

################################################################################
                     [1m Learning iteration 1797/2000 [0m                     

                       Computation: 40034 steps/s (collection: 2.322s, learning 0.134s)
             Mean action noise std: 4.05
          Mean value_function loss: 138.9152
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.5775
                       Mean reward: 869.68
               Mean episode length: 231.08
    Episode_Reward/reaching_object: 1.5702
     Episode_Reward/lifting_object: 169.2833
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 2.46s
                      Time elapsed: 01:19:51
                               ETA: 00:09:00

################################################################################
                     [1m Learning iteration 1798/2000 [0m                     

                       Computation: 40548 steps/s (collection: 2.277s, learning 0.148s)
             Mean action noise std: 4.05
          Mean value_function loss: 100.6376
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.5879
                       Mean reward: 895.39
               Mean episode length: 235.96
    Episode_Reward/reaching_object: 1.6163
     Episode_Reward/lifting_object: 174.6648
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0929
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 2.42s
                      Time elapsed: 01:19:53
                               ETA: 00:08:58

################################################################################
                     [1m Learning iteration 1799/2000 [0m                     

                       Computation: 40120 steps/s (collection: 2.309s, learning 0.141s)
             Mean action noise std: 4.06
          Mean value_function loss: 147.7506
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 72.6028
                       Mean reward: 901.36
               Mean episode length: 238.28
    Episode_Reward/reaching_object: 1.6050
     Episode_Reward/lifting_object: 173.8361
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0927
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 2.45s
                      Time elapsed: 01:19:56
                               ETA: 00:08:55

################################################################################
                     [1m Learning iteration 1800/2000 [0m                     

                       Computation: 39286 steps/s (collection: 2.353s, learning 0.149s)
             Mean action noise std: 4.06
          Mean value_function loss: 141.7100
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 72.6167
                       Mean reward: 861.27
               Mean episode length: 229.07
    Episode_Reward/reaching_object: 1.5611
     Episode_Reward/lifting_object: 168.2256
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 2.50s
                      Time elapsed: 01:19:58
                               ETA: 00:08:52

################################################################################
                     [1m Learning iteration 1801/2000 [0m                     

                       Computation: 39108 steps/s (collection: 2.382s, learning 0.132s)
             Mean action noise std: 4.06
          Mean value_function loss: 153.9241
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 72.6221
                       Mean reward: 881.48
               Mean episode length: 234.06
    Episode_Reward/reaching_object: 1.5545
     Episode_Reward/lifting_object: 167.6560
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0906
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 2.51s
                      Time elapsed: 01:20:01
                               ETA: 00:08:50

################################################################################
                     [1m Learning iteration 1802/2000 [0m                     

                       Computation: 39756 steps/s (collection: 2.329s, learning 0.144s)
             Mean action noise std: 4.06
          Mean value_function loss: 119.3704
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 72.6300
                       Mean reward: 860.10
               Mean episode length: 229.64
    Episode_Reward/reaching_object: 1.6001
     Episode_Reward/lifting_object: 172.4868
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0931
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 2.47s
                      Time elapsed: 01:20:03
                               ETA: 00:08:47

################################################################################
                     [1m Learning iteration 1803/2000 [0m                     

                       Computation: 40442 steps/s (collection: 2.296s, learning 0.135s)
             Mean action noise std: 4.06
          Mean value_function loss: 122.6828
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 72.6406
                       Mean reward: 843.20
               Mean episode length: 224.71
    Episode_Reward/reaching_object: 1.5604
     Episode_Reward/lifting_object: 168.1162
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0912
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 2.43s
                      Time elapsed: 01:20:05
                               ETA: 00:08:44

################################################################################
                     [1m Learning iteration 1804/2000 [0m                     

                       Computation: 39706 steps/s (collection: 2.324s, learning 0.152s)
             Mean action noise std: 4.06
          Mean value_function loss: 142.4969
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 72.6530
                       Mean reward: 872.49
               Mean episode length: 231.61
    Episode_Reward/reaching_object: 1.5922
     Episode_Reward/lifting_object: 171.9518
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 2.48s
                      Time elapsed: 01:20:08
                               ETA: 00:08:42

################################################################################
                     [1m Learning iteration 1805/2000 [0m                     

                       Computation: 40203 steps/s (collection: 2.305s, learning 0.141s)
             Mean action noise std: 4.06
          Mean value_function loss: 136.6518
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 72.6648
                       Mean reward: 865.82
               Mean episode length: 229.49
    Episode_Reward/reaching_object: 1.5622
     Episode_Reward/lifting_object: 168.3415
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0916
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 2.45s
                      Time elapsed: 01:20:10
                               ETA: 00:08:39

################################################################################
                     [1m Learning iteration 1806/2000 [0m                     

                       Computation: 41150 steps/s (collection: 2.283s, learning 0.106s)
             Mean action noise std: 4.07
          Mean value_function loss: 117.7940
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 72.6783
                       Mean reward: 891.54
               Mean episode length: 235.48
    Episode_Reward/reaching_object: 1.5737
     Episode_Reward/lifting_object: 169.9529
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0923
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 2.39s
                      Time elapsed: 01:20:13
                               ETA: 00:08:36

################################################################################
                     [1m Learning iteration 1807/2000 [0m                     

                       Computation: 41398 steps/s (collection: 2.235s, learning 0.139s)
             Mean action noise std: 4.07
          Mean value_function loss: 147.0839
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.6884
                       Mean reward: 833.30
               Mean episode length: 221.39
    Episode_Reward/reaching_object: 1.5818
     Episode_Reward/lifting_object: 170.7574
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0928
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 2.37s
                      Time elapsed: 01:20:15
                               ETA: 00:08:34

################################################################################
                     [1m Learning iteration 1808/2000 [0m                     

                       Computation: 41848 steps/s (collection: 2.215s, learning 0.134s)
             Mean action noise std: 4.07
          Mean value_function loss: 161.4052
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 72.6994
                       Mean reward: 836.53
               Mean episode length: 224.05
    Episode_Reward/reaching_object: 1.5605
     Episode_Reward/lifting_object: 167.9772
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0922
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 2.35s
                      Time elapsed: 01:20:17
                               ETA: 00:08:31

################################################################################
                     [1m Learning iteration 1809/2000 [0m                     

                       Computation: 41696 steps/s (collection: 2.217s, learning 0.141s)
             Mean action noise std: 4.07
          Mean value_function loss: 158.3866
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.7135
                       Mean reward: 796.69
               Mean episode length: 214.65
    Episode_Reward/reaching_object: 1.5369
     Episode_Reward/lifting_object: 165.8168
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0912
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 2.36s
                      Time elapsed: 01:20:20
                               ETA: 00:08:28

################################################################################
                     [1m Learning iteration 1810/2000 [0m                     

                       Computation: 40561 steps/s (collection: 2.286s, learning 0.138s)
             Mean action noise std: 4.07
          Mean value_function loss: 133.4022
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 72.7302
                       Mean reward: 905.15
               Mean episode length: 238.49
    Episode_Reward/reaching_object: 1.5993
     Episode_Reward/lifting_object: 173.3956
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0938
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 2.42s
                      Time elapsed: 01:20:22
                               ETA: 00:08:25

################################################################################
                     [1m Learning iteration 1811/2000 [0m                     

                       Computation: 38256 steps/s (collection: 2.410s, learning 0.160s)
             Mean action noise std: 4.08
          Mean value_function loss: 117.3208
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 72.7400
                       Mean reward: 912.97
               Mean episode length: 240.83
    Episode_Reward/reaching_object: 1.5667
     Episode_Reward/lifting_object: 169.6693
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0928
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 2.57s
                      Time elapsed: 01:20:25
                               ETA: 00:08:23

################################################################################
                     [1m Learning iteration 1812/2000 [0m                     

                       Computation: 40810 steps/s (collection: 2.280s, learning 0.129s)
             Mean action noise std: 4.08
          Mean value_function loss: 147.6801
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 72.7487
                       Mean reward: 907.28
               Mean episode length: 239.12
    Episode_Reward/reaching_object: 1.5925
     Episode_Reward/lifting_object: 172.6267
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0940
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 2.41s
                      Time elapsed: 01:20:27
                               ETA: 00:08:20

################################################################################
                     [1m Learning iteration 1813/2000 [0m                     

                       Computation: 42098 steps/s (collection: 2.208s, learning 0.127s)
             Mean action noise std: 4.08
          Mean value_function loss: 156.0691
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.7616
                       Mean reward: 813.06
               Mean episode length: 217.03
    Episode_Reward/reaching_object: 1.5652
     Episode_Reward/lifting_object: 169.2742
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 2.34s
                      Time elapsed: 01:20:30
                               ETA: 00:08:17

################################################################################
                     [1m Learning iteration 1814/2000 [0m                     

                       Computation: 41203 steps/s (collection: 2.252s, learning 0.134s)
             Mean action noise std: 4.08
          Mean value_function loss: 184.5841
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 72.7723
                       Mean reward: 815.98
               Mean episode length: 217.79
    Episode_Reward/reaching_object: 1.5257
     Episode_Reward/lifting_object: 164.5095
      Episode_Reward/object_height: 0.0213
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 2.39s
                      Time elapsed: 01:20:32
                               ETA: 00:08:15

################################################################################
                     [1m Learning iteration 1815/2000 [0m                     

                       Computation: 40524 steps/s (collection: 2.301s, learning 0.125s)
             Mean action noise std: 4.08
          Mean value_function loss: 126.3003
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.7777
                       Mean reward: 902.77
               Mean episode length: 237.82
    Episode_Reward/reaching_object: 1.6028
     Episode_Reward/lifting_object: 173.3528
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0949
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 2.43s
                      Time elapsed: 01:20:34
                               ETA: 00:08:12

################################################################################
                     [1m Learning iteration 1816/2000 [0m                     

                       Computation: 40536 steps/s (collection: 2.314s, learning 0.111s)
             Mean action noise std: 4.08
          Mean value_function loss: 126.9511
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.7859
                       Mean reward: 878.89
               Mean episode length: 232.45
    Episode_Reward/reaching_object: 1.5618
     Episode_Reward/lifting_object: 168.5234
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 2.43s
                      Time elapsed: 01:20:37
                               ETA: 00:08:09

################################################################################
                     [1m Learning iteration 1817/2000 [0m                     

                       Computation: 39840 steps/s (collection: 2.344s, learning 0.123s)
             Mean action noise std: 4.09
          Mean value_function loss: 166.7324
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.7954
                       Mean reward: 856.09
               Mean episode length: 226.71
    Episode_Reward/reaching_object: 1.5518
     Episode_Reward/lifting_object: 167.5980
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0924
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 2.47s
                      Time elapsed: 01:20:39
                               ETA: 00:08:07

################################################################################
                     [1m Learning iteration 1818/2000 [0m                     

                       Computation: 35208 steps/s (collection: 2.595s, learning 0.197s)
             Mean action noise std: 4.09
          Mean value_function loss: 136.7228
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 72.8073
                       Mean reward: 853.58
               Mean episode length: 227.10
    Episode_Reward/reaching_object: 1.5782
     Episode_Reward/lifting_object: 170.5499
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0940
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 2.79s
                      Time elapsed: 01:20:42
                               ETA: 00:08:04

################################################################################
                     [1m Learning iteration 1819/2000 [0m                     

                       Computation: 36644 steps/s (collection: 2.541s, learning 0.142s)
             Mean action noise std: 4.09
          Mean value_function loss: 138.7097
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 72.8152
                       Mean reward: 856.30
               Mean episode length: 228.23
    Episode_Reward/reaching_object: 1.5698
     Episode_Reward/lifting_object: 169.0802
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.0934
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 2.68s
                      Time elapsed: 01:20:45
                               ETA: 00:08:01

################################################################################
                     [1m Learning iteration 1820/2000 [0m                     

                       Computation: 39723 steps/s (collection: 2.339s, learning 0.136s)
             Mean action noise std: 4.09
          Mean value_function loss: 150.7074
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 72.8219
                       Mean reward: 866.01
               Mean episode length: 230.57
    Episode_Reward/reaching_object: 1.5480
     Episode_Reward/lifting_object: 166.8036
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0924
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 2.47s
                      Time elapsed: 01:20:47
                               ETA: 00:07:59

################################################################################
                     [1m Learning iteration 1821/2000 [0m                     

                       Computation: 39511 steps/s (collection: 2.355s, learning 0.133s)
             Mean action noise std: 4.09
          Mean value_function loss: 108.7260
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 72.8289
                       Mean reward: 873.06
               Mean episode length: 231.44
    Episode_Reward/reaching_object: 1.6197
     Episode_Reward/lifting_object: 175.0758
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0960
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 2.49s
                      Time elapsed: 01:20:50
                               ETA: 00:07:56

################################################################################
                     [1m Learning iteration 1822/2000 [0m                     

                       Computation: 40198 steps/s (collection: 2.314s, learning 0.132s)
             Mean action noise std: 4.09
          Mean value_function loss: 131.0725
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 72.8388
                       Mean reward: 901.98
               Mean episode length: 237.62
    Episode_Reward/reaching_object: 1.5785
     Episode_Reward/lifting_object: 170.5801
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0937
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 2.45s
                      Time elapsed: 01:20:52
                               ETA: 00:07:53

################################################################################
                     [1m Learning iteration 1823/2000 [0m                     

                       Computation: 41237 steps/s (collection: 2.254s, learning 0.130s)
             Mean action noise std: 4.09
          Mean value_function loss: 124.2359
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 72.8472
                       Mean reward: 828.97
               Mean episode length: 220.68
    Episode_Reward/reaching_object: 1.5627
     Episode_Reward/lifting_object: 168.4500
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 2.38s
                      Time elapsed: 01:20:55
                               ETA: 00:07:51

################################################################################
                     [1m Learning iteration 1824/2000 [0m                     

                       Computation: 31144 steps/s (collection: 3.034s, learning 0.123s)
             Mean action noise std: 4.10
          Mean value_function loss: 122.0351
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 72.8568
                       Mean reward: 868.90
               Mean episode length: 229.46
    Episode_Reward/reaching_object: 1.5693
     Episode_Reward/lifting_object: 169.1516
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0937
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 3.16s
                      Time elapsed: 01:20:58
                               ETA: 00:07:48

################################################################################
                     [1m Learning iteration 1825/2000 [0m                     

                       Computation: 38203 steps/s (collection: 2.380s, learning 0.194s)
             Mean action noise std: 4.10
          Mean value_function loss: 129.6155
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 72.8664
                       Mean reward: 843.54
               Mean episode length: 224.76
    Episode_Reward/reaching_object: 1.5900
     Episode_Reward/lifting_object: 171.5413
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0950
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 2.57s
                      Time elapsed: 01:21:00
                               ETA: 00:07:45

################################################################################
                     [1m Learning iteration 1826/2000 [0m                     

                       Computation: 39844 steps/s (collection: 2.320s, learning 0.147s)
             Mean action noise std: 4.10
          Mean value_function loss: 121.3540
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.8781
                       Mean reward: 867.26
               Mean episode length: 228.59
    Episode_Reward/reaching_object: 1.5732
     Episode_Reward/lifting_object: 169.8867
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0941
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 2.47s
                      Time elapsed: 01:21:03
                               ETA: 00:07:43

################################################################################
                     [1m Learning iteration 1827/2000 [0m                     

                       Computation: 38937 steps/s (collection: 2.359s, learning 0.166s)
             Mean action noise std: 4.10
          Mean value_function loss: 126.4215
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.8883
                       Mean reward: 883.69
               Mean episode length: 234.01
    Episode_Reward/reaching_object: 1.6029
     Episode_Reward/lifting_object: 173.1287
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0958
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 2.52s
                      Time elapsed: 01:21:05
                               ETA: 00:07:40

################################################################################
                     [1m Learning iteration 1828/2000 [0m                     

                       Computation: 38185 steps/s (collection: 2.394s, learning 0.181s)
             Mean action noise std: 4.10
          Mean value_function loss: 156.6901
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 72.9008
                       Mean reward: 858.83
               Mean episode length: 227.01
    Episode_Reward/reaching_object: 1.5539
     Episode_Reward/lifting_object: 167.9185
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 2.57s
                      Time elapsed: 01:21:08
                               ETA: 00:07:37

################################################################################
                     [1m Learning iteration 1829/2000 [0m                     

                       Computation: 36165 steps/s (collection: 2.605s, learning 0.114s)
             Mean action noise std: 4.10
          Mean value_function loss: 140.8064
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 72.9098
                       Mean reward: 847.48
               Mean episode length: 224.65
    Episode_Reward/reaching_object: 1.5776
     Episode_Reward/lifting_object: 170.5386
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 2.72s
                      Time elapsed: 01:21:11
                               ETA: 00:07:35

################################################################################
                     [1m Learning iteration 1830/2000 [0m                     

                       Computation: 36539 steps/s (collection: 2.548s, learning 0.142s)
             Mean action noise std: 4.10
          Mean value_function loss: 128.1864
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.9165
                       Mean reward: 846.21
               Mean episode length: 224.77
    Episode_Reward/reaching_object: 1.5880
     Episode_Reward/lifting_object: 171.5304
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0953
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 2.69s
                      Time elapsed: 01:21:13
                               ETA: 00:07:32

################################################################################
                     [1m Learning iteration 1831/2000 [0m                     

                       Computation: 41992 steps/s (collection: 2.215s, learning 0.126s)
             Mean action noise std: 4.11
          Mean value_function loss: 102.0953
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 72.9312
                       Mean reward: 855.87
               Mean episode length: 226.79
    Episode_Reward/reaching_object: 1.6018
     Episode_Reward/lifting_object: 173.6346
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0960
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 2.34s
                      Time elapsed: 01:21:16
                               ETA: 00:07:29

################################################################################
                     [1m Learning iteration 1832/2000 [0m                     

                       Computation: 41478 steps/s (collection: 2.248s, learning 0.122s)
             Mean action noise std: 4.11
          Mean value_function loss: 162.1029
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 72.9475
                       Mean reward: 824.05
               Mean episode length: 219.39
    Episode_Reward/reaching_object: 1.5582
     Episode_Reward/lifting_object: 168.6769
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.0937
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 2.37s
                      Time elapsed: 01:21:18
                               ETA: 00:07:27

################################################################################
                     [1m Learning iteration 1833/2000 [0m                     

                       Computation: 40906 steps/s (collection: 2.254s, learning 0.150s)
             Mean action noise std: 4.11
          Mean value_function loss: 188.1051
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 72.9595
                       Mean reward: 871.54
               Mean episode length: 229.77
    Episode_Reward/reaching_object: 1.5746
     Episode_Reward/lifting_object: 170.9525
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0948
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 2.40s
                      Time elapsed: 01:21:20
                               ETA: 00:07:24

################################################################################
                     [1m Learning iteration 1834/2000 [0m                     

                       Computation: 41636 steps/s (collection: 2.227s, learning 0.134s)
             Mean action noise std: 4.11
          Mean value_function loss: 199.8262
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 72.9713
                       Mean reward: 826.04
               Mean episode length: 219.33
    Episode_Reward/reaching_object: 1.5046
     Episode_Reward/lifting_object: 163.1595
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 3.6250
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 2.36s
                      Time elapsed: 01:21:23
                               ETA: 00:07:21

################################################################################
                     [1m Learning iteration 1835/2000 [0m                     

                       Computation: 41374 steps/s (collection: 2.239s, learning 0.137s)
             Mean action noise std: 4.12
          Mean value_function loss: 146.8559
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 72.9811
                       Mean reward: 894.32
               Mean episode length: 235.64
    Episode_Reward/reaching_object: 1.5558
     Episode_Reward/lifting_object: 168.8461
      Episode_Reward/object_height: 0.0208
        Episode_Reward/action_rate: -0.0938
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 2.38s
                      Time elapsed: 01:21:25
                               ETA: 00:07:19

################################################################################
                     [1m Learning iteration 1836/2000 [0m                     

                       Computation: 41954 steps/s (collection: 2.213s, learning 0.130s)
             Mean action noise std: 4.12
          Mean value_function loss: 177.9855
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 72.9897
                       Mean reward: 825.30
               Mean episode length: 220.49
    Episode_Reward/reaching_object: 1.5205
     Episode_Reward/lifting_object: 164.9758
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0924
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 2.34s
                      Time elapsed: 01:21:27
                               ETA: 00:07:16

################################################################################
                     [1m Learning iteration 1837/2000 [0m                     

                       Computation: 41189 steps/s (collection: 2.236s, learning 0.151s)
             Mean action noise std: 4.12
          Mean value_function loss: 171.4096
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 72.9945
                       Mean reward: 796.96
               Mean episode length: 213.54
    Episode_Reward/reaching_object: 1.5387
     Episode_Reward/lifting_object: 167.0012
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 2.39s
                      Time elapsed: 01:21:30
                               ETA: 00:07:13

################################################################################
                     [1m Learning iteration 1838/2000 [0m                     

                       Computation: 38369 steps/s (collection: 2.398s, learning 0.164s)
             Mean action noise std: 4.12
          Mean value_function loss: 133.1797
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.9999
                       Mean reward: 874.81
               Mean episode length: 231.90
    Episode_Reward/reaching_object: 1.5848
     Episode_Reward/lifting_object: 171.9831
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0960
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 2.56s
                      Time elapsed: 01:21:32
                               ETA: 00:07:11

################################################################################
                     [1m Learning iteration 1839/2000 [0m                     

                       Computation: 39540 steps/s (collection: 2.354s, learning 0.132s)
             Mean action noise std: 4.12
          Mean value_function loss: 146.6986
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.0141
                       Mean reward: 861.74
               Mean episode length: 227.98
    Episode_Reward/reaching_object: 1.5298
     Episode_Reward/lifting_object: 166.0743
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0931
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 2.49s
                      Time elapsed: 01:21:35
                               ETA: 00:07:08

################################################################################
                     [1m Learning iteration 1840/2000 [0m                     

                       Computation: 41224 steps/s (collection: 2.270s, learning 0.115s)
             Mean action noise std: 4.12
          Mean value_function loss: 134.0075
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 73.0269
                       Mean reward: 872.11
               Mean episode length: 231.33
    Episode_Reward/reaching_object: 1.5535
     Episode_Reward/lifting_object: 168.1863
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.0943
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 2.38s
                      Time elapsed: 01:21:37
                               ETA: 00:07:05

################################################################################
                     [1m Learning iteration 1841/2000 [0m                     

                       Computation: 41435 steps/s (collection: 2.242s, learning 0.130s)
             Mean action noise std: 4.12
          Mean value_function loss: 107.3620
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.0310
                       Mean reward: 867.44
               Mean episode length: 229.26
    Episode_Reward/reaching_object: 1.5726
     Episode_Reward/lifting_object: 170.8432
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0952
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 2.37s
                      Time elapsed: 01:21:40
                               ETA: 00:07:02

################################################################################
                     [1m Learning iteration 1842/2000 [0m                     

                       Computation: 41258 steps/s (collection: 2.266s, learning 0.117s)
             Mean action noise std: 4.13
          Mean value_function loss: 114.1705
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.0418
                       Mean reward: 886.29
               Mean episode length: 233.86
    Episode_Reward/reaching_object: 1.6203
     Episode_Reward/lifting_object: 176.1012
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0975
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 2.38s
                      Time elapsed: 01:21:42
                               ETA: 00:07:00

################################################################################
                     [1m Learning iteration 1843/2000 [0m                     

                       Computation: 37446 steps/s (collection: 2.492s, learning 0.134s)
             Mean action noise std: 4.13
          Mean value_function loss: 135.9731
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.0562
                       Mean reward: 880.41
               Mean episode length: 232.85
    Episode_Reward/reaching_object: 1.5939
     Episode_Reward/lifting_object: 172.8116
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0963
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 2.63s
                      Time elapsed: 01:21:45
                               ETA: 00:06:57

################################################################################
                     [1m Learning iteration 1844/2000 [0m                     

                       Computation: 38634 steps/s (collection: 2.353s, learning 0.191s)
             Mean action noise std: 4.13
          Mean value_function loss: 151.1614
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.0652
                       Mean reward: 821.66
               Mean episode length: 219.64
    Episode_Reward/reaching_object: 1.5620
     Episode_Reward/lifting_object: 168.7935
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0948
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 2.54s
                      Time elapsed: 01:21:47
                               ETA: 00:06:54

################################################################################
                     [1m Learning iteration 1845/2000 [0m                     

                       Computation: 31476 steps/s (collection: 2.846s, learning 0.278s)
             Mean action noise std: 4.13
          Mean value_function loss: 137.2144
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.0730
                       Mean reward: 865.77
               Mean episode length: 230.84
    Episode_Reward/reaching_object: 1.5736
     Episode_Reward/lifting_object: 169.9644
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0955
          Episode_Reward/joint_vel: -0.0638
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 3.12s
                      Time elapsed: 01:21:50
                               ETA: 00:06:52

################################################################################
                     [1m Learning iteration 1846/2000 [0m                     

                       Computation: 34898 steps/s (collection: 2.673s, learning 0.143s)
             Mean action noise std: 4.13
          Mean value_function loss: 140.1172
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.0838
                       Mean reward: 819.39
               Mean episode length: 218.45
    Episode_Reward/reaching_object: 1.5744
     Episode_Reward/lifting_object: 169.9902
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0953
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 2.82s
                      Time elapsed: 01:21:53
                               ETA: 00:06:49

################################################################################
                     [1m Learning iteration 1847/2000 [0m                     

                       Computation: 36096 steps/s (collection: 2.569s, learning 0.154s)
             Mean action noise std: 4.13
          Mean value_function loss: 126.4616
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 73.0937
                       Mean reward: 860.55
               Mean episode length: 228.87
    Episode_Reward/reaching_object: 1.5987
     Episode_Reward/lifting_object: 172.8605
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0971
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 2.72s
                      Time elapsed: 01:21:56
                               ETA: 00:06:47

################################################################################
                     [1m Learning iteration 1848/2000 [0m                     

                       Computation: 34972 steps/s (collection: 2.687s, learning 0.124s)
             Mean action noise std: 4.14
          Mean value_function loss: 148.1575
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.1051
                       Mean reward: 879.74
               Mean episode length: 232.66
    Episode_Reward/reaching_object: 1.5694
     Episode_Reward/lifting_object: 169.1510
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0952
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 2.81s
                      Time elapsed: 01:21:59
                               ETA: 00:06:44

################################################################################
                     [1m Learning iteration 1849/2000 [0m                     

                       Computation: 37177 steps/s (collection: 2.438s, learning 0.207s)
             Mean action noise std: 4.14
          Mean value_function loss: 142.8514
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.1154
                       Mean reward: 907.55
               Mean episode length: 240.17
    Episode_Reward/reaching_object: 1.5948
     Episode_Reward/lifting_object: 172.1572
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0969
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 2.64s
                      Time elapsed: 01:22:01
                               ETA: 00:06:41

################################################################################
                     [1m Learning iteration 1850/2000 [0m                     

                       Computation: 39864 steps/s (collection: 2.360s, learning 0.106s)
             Mean action noise std: 4.14
          Mean value_function loss: 118.2811
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.1271
                       Mean reward: 882.22
               Mean episode length: 232.99
    Episode_Reward/reaching_object: 1.5570
     Episode_Reward/lifting_object: 168.1432
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0951
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 2.47s
                      Time elapsed: 01:22:04
                               ETA: 00:06:39

################################################################################
                     [1m Learning iteration 1851/2000 [0m                     

                       Computation: 37376 steps/s (collection: 2.360s, learning 0.270s)
             Mean action noise std: 4.14
          Mean value_function loss: 141.7989
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.1371
                       Mean reward: 863.52
               Mean episode length: 228.39
    Episode_Reward/reaching_object: 1.5881
     Episode_Reward/lifting_object: 171.9308
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0965
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 2.63s
                      Time elapsed: 01:22:06
                               ETA: 00:06:36

################################################################################
                     [1m Learning iteration 1852/2000 [0m                     

                       Computation: 37591 steps/s (collection: 2.512s, learning 0.104s)
             Mean action noise std: 4.14
          Mean value_function loss: 167.0380
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 73.1494
                       Mean reward: 836.16
               Mean episode length: 225.30
    Episode_Reward/reaching_object: 1.5463
     Episode_Reward/lifting_object: 166.8070
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0945
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 2.62s
                      Time elapsed: 01:22:09
                               ETA: 00:06:33

################################################################################
                     [1m Learning iteration 1853/2000 [0m                     

                       Computation: 38079 steps/s (collection: 2.445s, learning 0.136s)
             Mean action noise std: 4.15
          Mean value_function loss: 140.2701
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 73.1579
                       Mean reward: 860.81
               Mean episode length: 227.79
    Episode_Reward/reaching_object: 1.5771
     Episode_Reward/lifting_object: 170.7604
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.0962
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 2.58s
                      Time elapsed: 01:22:12
                               ETA: 00:06:31

################################################################################
                     [1m Learning iteration 1854/2000 [0m                     

                       Computation: 41358 steps/s (collection: 2.276s, learning 0.101s)
             Mean action noise std: 4.15
          Mean value_function loss: 103.2647
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.1665
                       Mean reward: 902.79
               Mean episode length: 237.95
    Episode_Reward/reaching_object: 1.6018
     Episode_Reward/lifting_object: 173.6317
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0981
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 2.38s
                      Time elapsed: 01:22:14
                               ETA: 00:06:28

################################################################################
                     [1m Learning iteration 1855/2000 [0m                     

                       Computation: 36190 steps/s (collection: 2.584s, learning 0.133s)
             Mean action noise std: 4.15
          Mean value_function loss: 124.5986
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 73.1758
                       Mean reward: 864.36
               Mean episode length: 228.29
    Episode_Reward/reaching_object: 1.6018
     Episode_Reward/lifting_object: 173.7760
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0974
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 2.72s
                      Time elapsed: 01:22:17
                               ETA: 00:06:25

################################################################################
                     [1m Learning iteration 1856/2000 [0m                     

                       Computation: 33352 steps/s (collection: 2.596s, learning 0.352s)
             Mean action noise std: 4.15
          Mean value_function loss: 135.2482
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.1895
                       Mean reward: 879.70
               Mean episode length: 233.03
    Episode_Reward/reaching_object: 1.5648
     Episode_Reward/lifting_object: 169.5235
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0955
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 2.95s
                      Time elapsed: 01:22:20
                               ETA: 00:06:23

################################################################################
                     [1m Learning iteration 1857/2000 [0m                     

                       Computation: 39649 steps/s (collection: 2.345s, learning 0.135s)
             Mean action noise std: 4.15
          Mean value_function loss: 124.5550
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 73.1966
                       Mean reward: 883.49
               Mean episode length: 233.21
    Episode_Reward/reaching_object: 1.5988
     Episode_Reward/lifting_object: 173.4529
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0979
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 2.48s
                      Time elapsed: 01:22:22
                               ETA: 00:06:20

################################################################################
                     [1m Learning iteration 1858/2000 [0m                     

                       Computation: 42214 steps/s (collection: 2.232s, learning 0.097s)
             Mean action noise std: 4.15
          Mean value_function loss: 145.2091
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.2042
                       Mean reward: 875.85
               Mean episode length: 231.44
    Episode_Reward/reaching_object: 1.5496
     Episode_Reward/lifting_object: 167.7788
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0952
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 2.33s
                      Time elapsed: 01:22:24
                               ETA: 00:06:17

################################################################################
                     [1m Learning iteration 1859/2000 [0m                     

                       Computation: 40181 steps/s (collection: 2.290s, learning 0.157s)
             Mean action noise std: 4.15
          Mean value_function loss: 164.6750
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 73.2109
                       Mean reward: 839.86
               Mean episode length: 222.79
    Episode_Reward/reaching_object: 1.5736
     Episode_Reward/lifting_object: 170.2385
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0966
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 2.45s
                      Time elapsed: 01:22:27
                               ETA: 00:06:15

################################################################################
                     [1m Learning iteration 1860/2000 [0m                     

                       Computation: 41095 steps/s (collection: 2.271s, learning 0.121s)
             Mean action noise std: 4.16
          Mean value_function loss: 130.7299
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 73.2184
                       Mean reward: 840.71
               Mean episode length: 222.52
    Episode_Reward/reaching_object: 1.5924
     Episode_Reward/lifting_object: 172.6505
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 2.39s
                      Time elapsed: 01:22:29
                               ETA: 00:06:12

################################################################################
                     [1m Learning iteration 1861/2000 [0m                     

                       Computation: 39239 steps/s (collection: 2.320s, learning 0.185s)
             Mean action noise std: 4.16
          Mean value_function loss: 148.0477
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 73.2276
                       Mean reward: 846.21
               Mean episode length: 224.79
    Episode_Reward/reaching_object: 1.5660
     Episode_Reward/lifting_object: 169.2921
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0961
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 2.51s
                      Time elapsed: 01:22:32
                               ETA: 00:06:09

################################################################################
                     [1m Learning iteration 1862/2000 [0m                     

                       Computation: 41034 steps/s (collection: 2.294s, learning 0.101s)
             Mean action noise std: 4.16
          Mean value_function loss: 115.5932
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.2312
                       Mean reward: 858.43
               Mean episode length: 227.96
    Episode_Reward/reaching_object: 1.5608
     Episode_Reward/lifting_object: 168.8091
      Episode_Reward/object_height: 0.0246
        Episode_Reward/action_rate: -0.0961
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 2.40s
                      Time elapsed: 01:22:34
                               ETA: 00:06:07

################################################################################
                     [1m Learning iteration 1863/2000 [0m                     

                       Computation: 40618 steps/s (collection: 2.253s, learning 0.167s)
             Mean action noise std: 4.16
          Mean value_function loss: 127.9101
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.2384
                       Mean reward: 827.00
               Mean episode length: 220.77
    Episode_Reward/reaching_object: 1.5874
     Episode_Reward/lifting_object: 171.6841
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0980
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 2.42s
                      Time elapsed: 01:22:37
                               ETA: 00:06:04

################################################################################
                     [1m Learning iteration 1864/2000 [0m                     

                       Computation: 39307 steps/s (collection: 2.379s, learning 0.122s)
             Mean action noise std: 4.16
          Mean value_function loss: 136.9637
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 73.2468
                       Mean reward: 854.85
               Mean episode length: 226.90
    Episode_Reward/reaching_object: 1.5986
     Episode_Reward/lifting_object: 173.2378
      Episode_Reward/object_height: 0.0253
        Episode_Reward/action_rate: -0.0982
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 2.50s
                      Time elapsed: 01:22:39
                               ETA: 00:06:01

################################################################################
                     [1m Learning iteration 1865/2000 [0m                     

                       Computation: 39315 steps/s (collection: 2.349s, learning 0.151s)
             Mean action noise std: 4.16
          Mean value_function loss: 109.0165
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.2568
                       Mean reward: 880.12
               Mean episode length: 232.36
    Episode_Reward/reaching_object: 1.5856
     Episode_Reward/lifting_object: 171.5270
      Episode_Reward/object_height: 0.0254
        Episode_Reward/action_rate: -0.0975
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 2.50s
                      Time elapsed: 01:22:42
                               ETA: 00:05:58

################################################################################
                     [1m Learning iteration 1866/2000 [0m                     

                       Computation: 40220 steps/s (collection: 2.329s, learning 0.116s)
             Mean action noise std: 4.16
          Mean value_function loss: 102.1746
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.2649
                       Mean reward: 874.41
               Mean episode length: 231.81
    Episode_Reward/reaching_object: 1.5916
     Episode_Reward/lifting_object: 172.5437
      Episode_Reward/object_height: 0.0256
        Episode_Reward/action_rate: -0.0978
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 2.44s
                      Time elapsed: 01:22:44
                               ETA: 00:05:56

################################################################################
                     [1m Learning iteration 1867/2000 [0m                     

                       Computation: 37889 steps/s (collection: 2.486s, learning 0.109s)
             Mean action noise std: 4.16
          Mean value_function loss: 155.5643
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 73.2745
                       Mean reward: 873.79
               Mean episode length: 231.00
    Episode_Reward/reaching_object: 1.5870
     Episode_Reward/lifting_object: 171.9803
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0973
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 2.59s
                      Time elapsed: 01:22:47
                               ETA: 00:05:53

################################################################################
                     [1m Learning iteration 1868/2000 [0m                     

                       Computation: 33586 steps/s (collection: 2.739s, learning 0.188s)
             Mean action noise std: 4.17
          Mean value_function loss: 106.4080
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 73.2873
                       Mean reward: 867.59
               Mean episode length: 228.96
    Episode_Reward/reaching_object: 1.5419
     Episode_Reward/lifting_object: 166.7578
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.0951
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 2.93s
                      Time elapsed: 01:22:50
                               ETA: 00:05:51

################################################################################
                     [1m Learning iteration 1869/2000 [0m                     

                       Computation: 41618 steps/s (collection: 2.252s, learning 0.110s)
             Mean action noise std: 4.17
          Mean value_function loss: 120.3211
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 73.3005
                       Mean reward: 874.80
               Mean episode length: 230.72
    Episode_Reward/reaching_object: 1.5795
     Episode_Reward/lifting_object: 171.0007
      Episode_Reward/object_height: 0.0257
        Episode_Reward/action_rate: -0.0970
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 2.36s
                      Time elapsed: 01:22:52
                               ETA: 00:05:48

################################################################################
                     [1m Learning iteration 1870/2000 [0m                     

                       Computation: 37196 steps/s (collection: 2.454s, learning 0.189s)
             Mean action noise std: 4.17
          Mean value_function loss: 134.6546
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.3071
                       Mean reward: 889.63
               Mean episode length: 235.59
    Episode_Reward/reaching_object: 1.5912
     Episode_Reward/lifting_object: 172.1190
      Episode_Reward/object_height: 0.0255
        Episode_Reward/action_rate: -0.0980
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 2.64s
                      Time elapsed: 01:22:55
                               ETA: 00:05:45

################################################################################
                     [1m Learning iteration 1871/2000 [0m                     

                       Computation: 39839 steps/s (collection: 2.357s, learning 0.110s)
             Mean action noise std: 4.17
          Mean value_function loss: 167.5404
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.3209
                       Mean reward: 872.67
               Mean episode length: 230.84
    Episode_Reward/reaching_object: 1.6069
     Episode_Reward/lifting_object: 174.4835
      Episode_Reward/object_height: 0.0260
        Episode_Reward/action_rate: -0.0986
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 2.47s
                      Time elapsed: 01:22:57
                               ETA: 00:05:43

################################################################################
                     [1m Learning iteration 1872/2000 [0m                     

                       Computation: 34621 steps/s (collection: 2.680s, learning 0.160s)
             Mean action noise std: 4.18
          Mean value_function loss: 147.2304
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.3390
                       Mean reward: 838.17
               Mean episode length: 222.67
    Episode_Reward/reaching_object: 1.5555
     Episode_Reward/lifting_object: 168.2024
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0959
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 2.84s
                      Time elapsed: 01:23:00
                               ETA: 00:05:40

################################################################################
                     [1m Learning iteration 1873/2000 [0m                     

                       Computation: 39001 steps/s (collection: 2.343s, learning 0.178s)
             Mean action noise std: 4.18
          Mean value_function loss: 140.5168
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 73.3501
                       Mean reward: 889.55
               Mean episode length: 234.68
    Episode_Reward/reaching_object: 1.5513
     Episode_Reward/lifting_object: 166.8729
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0960
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 2.52s
                      Time elapsed: 01:23:02
                               ETA: 00:05:37

################################################################################
                     [1m Learning iteration 1874/2000 [0m                     

                       Computation: 40627 steps/s (collection: 2.305s, learning 0.115s)
             Mean action noise std: 4.18
          Mean value_function loss: 124.8777
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.3550
                       Mean reward: 846.72
               Mean episode length: 224.81
    Episode_Reward/reaching_object: 1.5755
     Episode_Reward/lifting_object: 170.7388
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.0976
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 2.42s
                      Time elapsed: 01:23:05
                               ETA: 00:05:35

################################################################################
                     [1m Learning iteration 1875/2000 [0m                     

                       Computation: 42231 steps/s (collection: 2.193s, learning 0.135s)
             Mean action noise std: 4.18
          Mean value_function loss: 129.9462
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.3630
                       Mean reward: 848.12
               Mean episode length: 225.27
    Episode_Reward/reaching_object: 1.5690
     Episode_Reward/lifting_object: 170.3036
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0972
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 2.33s
                      Time elapsed: 01:23:07
                               ETA: 00:05:32

################################################################################
                     [1m Learning iteration 1876/2000 [0m                     

                       Computation: 42886 steps/s (collection: 2.186s, learning 0.106s)
             Mean action noise std: 4.18
          Mean value_function loss: 127.2800
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 73.3765
                       Mean reward: 904.67
               Mean episode length: 238.52
    Episode_Reward/reaching_object: 1.6032
     Episode_Reward/lifting_object: 174.2341
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0993
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 2.29s
                      Time elapsed: 01:23:09
                               ETA: 00:05:29

################################################################################
                     [1m Learning iteration 1877/2000 [0m                     

                       Computation: 42005 steps/s (collection: 2.208s, learning 0.133s)
             Mean action noise std: 4.18
          Mean value_function loss: 149.5163
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.3870
                       Mean reward: 858.57
               Mean episode length: 227.27
    Episode_Reward/reaching_object: 1.5828
     Episode_Reward/lifting_object: 171.8107
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0983
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 2.34s
                      Time elapsed: 01:23:12
                               ETA: 00:05:26

################################################################################
                     [1m Learning iteration 1878/2000 [0m                     

                       Computation: 42456 steps/s (collection: 2.191s, learning 0.124s)
             Mean action noise std: 4.18
          Mean value_function loss: 153.4754
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 73.3992
                       Mean reward: 866.48
               Mean episode length: 230.02
    Episode_Reward/reaching_object: 1.5777
     Episode_Reward/lifting_object: 170.9842
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.0982
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 2.32s
                      Time elapsed: 01:23:14
                               ETA: 00:05:24

################################################################################
                     [1m Learning iteration 1879/2000 [0m                     

                       Computation: 42528 steps/s (collection: 2.183s, learning 0.128s)
             Mean action noise std: 4.19
          Mean value_function loss: 126.9255
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.4083
                       Mean reward: 877.64
               Mean episode length: 232.58
    Episode_Reward/reaching_object: 1.5807
     Episode_Reward/lifting_object: 171.7557
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0983
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 2.31s
                      Time elapsed: 01:23:16
                               ETA: 00:05:21

################################################################################
                     [1m Learning iteration 1880/2000 [0m                     

                       Computation: 41671 steps/s (collection: 2.223s, learning 0.136s)
             Mean action noise std: 4.19
          Mean value_function loss: 123.2648
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.4195
                       Mean reward: 908.64
               Mean episode length: 238.43
    Episode_Reward/reaching_object: 1.5808
     Episode_Reward/lifting_object: 171.7012
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0985
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 2.36s
                      Time elapsed: 01:23:19
                               ETA: 00:05:18

################################################################################
                     [1m Learning iteration 1881/2000 [0m                     

                       Computation: 41404 steps/s (collection: 2.227s, learning 0.148s)
             Mean action noise std: 4.19
          Mean value_function loss: 140.4235
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 73.4313
                       Mean reward: 859.37
               Mean episode length: 228.10
    Episode_Reward/reaching_object: 1.5503
     Episode_Reward/lifting_object: 168.1242
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0969
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 2.37s
                      Time elapsed: 01:23:21
                               ETA: 00:05:16

################################################################################
                     [1m Learning iteration 1882/2000 [0m                     

                       Computation: 40951 steps/s (collection: 2.256s, learning 0.144s)
             Mean action noise std: 4.19
          Mean value_function loss: 141.5792
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.4418
                       Mean reward: 891.28
               Mean episode length: 234.62
    Episode_Reward/reaching_object: 1.5923
     Episode_Reward/lifting_object: 173.0580
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.0992
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 2.40s
                      Time elapsed: 01:23:24
                               ETA: 00:05:13

################################################################################
                     [1m Learning iteration 1883/2000 [0m                     

                       Computation: 41472 steps/s (collection: 2.233s, learning 0.137s)
             Mean action noise std: 4.20
          Mean value_function loss: 138.8473
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.4562
                       Mean reward: 847.07
               Mean episode length: 223.97
    Episode_Reward/reaching_object: 1.5666
     Episode_Reward/lifting_object: 169.9109
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0982
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 2.37s
                      Time elapsed: 01:23:26
                               ETA: 00:05:10

################################################################################
                     [1m Learning iteration 1884/2000 [0m                     

                       Computation: 41817 steps/s (collection: 2.214s, learning 0.137s)
             Mean action noise std: 4.20
          Mean value_function loss: 166.0690
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 73.4686
                       Mean reward: 866.51
               Mean episode length: 230.09
    Episode_Reward/reaching_object: 1.5856
     Episode_Reward/lifting_object: 172.2023
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.0993
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 2.35s
                      Time elapsed: 01:23:28
                               ETA: 00:05:08

################################################################################
                     [1m Learning iteration 1885/2000 [0m                     

                       Computation: 41955 steps/s (collection: 2.223s, learning 0.121s)
             Mean action noise std: 4.20
          Mean value_function loss: 133.1627
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 73.4844
                       Mean reward: 824.74
               Mean episode length: 219.70
    Episode_Reward/reaching_object: 1.5500
     Episode_Reward/lifting_object: 167.7276
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0973
          Episode_Reward/joint_vel: -0.0617
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 2.34s
                      Time elapsed: 01:23:31
                               ETA: 00:05:05

################################################################################
                     [1m Learning iteration 1886/2000 [0m                     

                       Computation: 42287 steps/s (collection: 2.221s, learning 0.104s)
             Mean action noise std: 4.20
          Mean value_function loss: 153.7796
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 73.4946
                       Mean reward: 835.15
               Mean episode length: 222.26
    Episode_Reward/reaching_object: 1.5339
     Episode_Reward/lifting_object: 166.4614
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0966
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 2.32s
                      Time elapsed: 01:23:33
                               ETA: 00:05:02

################################################################################
                     [1m Learning iteration 1887/2000 [0m                     

                       Computation: 39719 steps/s (collection: 2.362s, learning 0.113s)
             Mean action noise std: 4.20
          Mean value_function loss: 146.3015
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 73.4989
                       Mean reward: 848.05
               Mean episode length: 224.83
    Episode_Reward/reaching_object: 1.5454
     Episode_Reward/lifting_object: 167.4850
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0971
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 2.47s
                      Time elapsed: 01:23:35
                               ETA: 00:05:00

################################################################################
                     [1m Learning iteration 1888/2000 [0m                     

                       Computation: 38718 steps/s (collection: 2.398s, learning 0.141s)
             Mean action noise std: 4.20
          Mean value_function loss: 135.3033
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 73.5020
                       Mean reward: 835.60
               Mean episode length: 223.98
    Episode_Reward/reaching_object: 1.5930
     Episode_Reward/lifting_object: 172.7815
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0999
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 2.54s
                      Time elapsed: 01:23:38
                               ETA: 00:04:57

################################################################################
                     [1m Learning iteration 1889/2000 [0m                     

                       Computation: 41030 steps/s (collection: 2.295s, learning 0.101s)
             Mean action noise std: 4.20
          Mean value_function loss: 120.9618
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.5089
                       Mean reward: 868.71
               Mean episode length: 230.31
    Episode_Reward/reaching_object: 1.5681
     Episode_Reward/lifting_object: 170.1365
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0988
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 2.40s
                      Time elapsed: 01:23:40
                               ETA: 00:04:54

################################################################################
                     [1m Learning iteration 1890/2000 [0m                     

                       Computation: 40154 steps/s (collection: 2.309s, learning 0.139s)
             Mean action noise std: 4.21
          Mean value_function loss: 136.3060
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.5205
                       Mean reward: 830.66
               Mean episode length: 220.45
    Episode_Reward/reaching_object: 1.5846
     Episode_Reward/lifting_object: 172.4422
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.0996
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 2.45s
                      Time elapsed: 01:23:43
                               ETA: 00:04:52

################################################################################
                     [1m Learning iteration 1891/2000 [0m                     

                       Computation: 41736 steps/s (collection: 2.245s, learning 0.110s)
             Mean action noise std: 4.21
          Mean value_function loss: 98.1040
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.5307
                       Mean reward: 883.47
               Mean episode length: 233.45
    Episode_Reward/reaching_object: 1.6094
     Episode_Reward/lifting_object: 175.0888
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.1009
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 2.36s
                      Time elapsed: 01:23:45
                               ETA: 00:04:49

################################################################################
                     [1m Learning iteration 1892/2000 [0m                     

                       Computation: 41904 steps/s (collection: 2.228s, learning 0.118s)
             Mean action noise std: 4.21
          Mean value_function loss: 160.9498
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 73.5409
                       Mean reward: 840.82
               Mean episode length: 222.58
    Episode_Reward/reaching_object: 1.5414
     Episode_Reward/lifting_object: 167.1210
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0977
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7500
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 2.35s
                      Time elapsed: 01:23:47
                               ETA: 00:04:46

################################################################################
                     [1m Learning iteration 1893/2000 [0m                     

                       Computation: 41117 steps/s (collection: 2.249s, learning 0.142s)
             Mean action noise std: 4.21
          Mean value_function loss: 120.3874
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 73.5535
                       Mean reward: 889.98
               Mean episode length: 234.57
    Episode_Reward/reaching_object: 1.5957
     Episode_Reward/lifting_object: 173.7073
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.1006
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 2.39s
                      Time elapsed: 01:23:50
                               ETA: 00:04:44

################################################################################
                     [1m Learning iteration 1894/2000 [0m                     

                       Computation: 41516 steps/s (collection: 2.225s, learning 0.143s)
             Mean action noise std: 4.21
          Mean value_function loss: 127.8386
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.5646
                       Mean reward: 871.75
               Mean episode length: 230.22
    Episode_Reward/reaching_object: 1.5700
     Episode_Reward/lifting_object: 170.7542
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0993
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 2.37s
                      Time elapsed: 01:23:52
                               ETA: 00:04:41

################################################################################
                     [1m Learning iteration 1895/2000 [0m                     

                       Computation: 40148 steps/s (collection: 2.318s, learning 0.130s)
             Mean action noise std: 4.22
          Mean value_function loss: 115.1557
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.5779
                       Mean reward: 895.09
               Mean episode length: 236.17
    Episode_Reward/reaching_object: 1.5866
     Episode_Reward/lifting_object: 172.8200
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.1005
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 2.45s
                      Time elapsed: 01:23:55
                               ETA: 00:04:38

################################################################################
                     [1m Learning iteration 1896/2000 [0m                     

                       Computation: 40811 steps/s (collection: 2.277s, learning 0.132s)
             Mean action noise std: 4.22
          Mean value_function loss: 147.8497
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 73.5922
                       Mean reward: 871.14
               Mean episode length: 230.66
    Episode_Reward/reaching_object: 1.5963
     Episode_Reward/lifting_object: 174.3796
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.1012
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 2.41s
                      Time elapsed: 01:23:57
                               ETA: 00:04:36

################################################################################
                     [1m Learning iteration 1897/2000 [0m                     

                       Computation: 41978 steps/s (collection: 2.232s, learning 0.110s)
             Mean action noise std: 4.22
          Mean value_function loss: 166.3501
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.6055
                       Mean reward: 828.89
               Mean episode length: 220.72
    Episode_Reward/reaching_object: 1.5237
     Episode_Reward/lifting_object: 165.4922
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0971
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 2.34s
                      Time elapsed: 01:23:59
                               ETA: 00:04:33

################################################################################
                     [1m Learning iteration 1898/2000 [0m                     

                       Computation: 41185 steps/s (collection: 2.241s, learning 0.146s)
             Mean action noise std: 4.22
          Mean value_function loss: 148.0608
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 73.6106
                       Mean reward: 862.52
               Mean episode length: 227.50
    Episode_Reward/reaching_object: 1.5391
     Episode_Reward/lifting_object: 167.7409
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0983
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 2.39s
                      Time elapsed: 01:24:02
                               ETA: 00:04:30

################################################################################
                     [1m Learning iteration 1899/2000 [0m                     

                       Computation: 41432 steps/s (collection: 2.242s, learning 0.131s)
             Mean action noise std: 4.22
          Mean value_function loss: 145.0001
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 73.6159
                       Mean reward: 866.31
               Mean episode length: 230.72
    Episode_Reward/reaching_object: 1.5709
     Episode_Reward/lifting_object: 171.3413
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.1002
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 2.37s
                      Time elapsed: 01:24:04
                               ETA: 00:04:28

################################################################################
                     [1m Learning iteration 1900/2000 [0m                     

                       Computation: 42022 steps/s (collection: 2.235s, learning 0.105s)
             Mean action noise std: 4.22
          Mean value_function loss: 106.6025
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 73.6223
                       Mean reward: 879.18
               Mean episode length: 232.32
    Episode_Reward/reaching_object: 1.5653
     Episode_Reward/lifting_object: 171.0167
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0998
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 2.34s
                      Time elapsed: 01:24:07
                               ETA: 00:04:25

################################################################################
                     [1m Learning iteration 1901/2000 [0m                     

                       Computation: 40443 steps/s (collection: 2.290s, learning 0.141s)
             Mean action noise std: 4.22
          Mean value_function loss: 114.0391
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.6320
                       Mean reward: 868.12
               Mean episode length: 230.47
    Episode_Reward/reaching_object: 1.5815
     Episode_Reward/lifting_object: 172.3550
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.1008
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.43s
                      Time elapsed: 01:24:09
                               ETA: 00:04:22

################################################################################
                     [1m Learning iteration 1902/2000 [0m                     

                       Computation: 41104 steps/s (collection: 2.260s, learning 0.131s)
             Mean action noise std: 4.23
          Mean value_function loss: 143.3355
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.6468
                       Mean reward: 892.63
               Mean episode length: 235.90
    Episode_Reward/reaching_object: 1.5676
     Episode_Reward/lifting_object: 170.7604
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.1001
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 2.39s
                      Time elapsed: 01:24:11
                               ETA: 00:04:20

################################################################################
                     [1m Learning iteration 1903/2000 [0m                     

                       Computation: 41352 steps/s (collection: 2.252s, learning 0.126s)
             Mean action noise std: 4.23
          Mean value_function loss: 138.7611
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 73.6598
                       Mean reward: 823.94
               Mean episode length: 220.61
    Episode_Reward/reaching_object: 1.5551
     Episode_Reward/lifting_object: 168.9878
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0993
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 2.38s
                      Time elapsed: 01:24:14
                               ETA: 00:04:17

################################################################################
                     [1m Learning iteration 1904/2000 [0m                     

                       Computation: 41026 steps/s (collection: 2.278s, learning 0.118s)
             Mean action noise std: 4.23
          Mean value_function loss: 115.0096
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.6666
                       Mean reward: 848.23
               Mean episode length: 226.26
    Episode_Reward/reaching_object: 1.5465
     Episode_Reward/lifting_object: 167.6529
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0990
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 2.40s
                      Time elapsed: 01:24:16
                               ETA: 00:04:14

################################################################################
                     [1m Learning iteration 1905/2000 [0m                     

                       Computation: 42312 steps/s (collection: 2.216s, learning 0.107s)
             Mean action noise std: 4.23
          Mean value_function loss: 147.4759
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.6737
                       Mean reward: 864.93
               Mean episode length: 229.24
    Episode_Reward/reaching_object: 1.5576
     Episode_Reward/lifting_object: 168.6108
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0998
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 2.32s
                      Time elapsed: 01:24:18
                               ETA: 00:04:12

################################################################################
                     [1m Learning iteration 1906/2000 [0m                     

                       Computation: 40491 steps/s (collection: 2.286s, learning 0.142s)
             Mean action noise std: 4.23
          Mean value_function loss: 116.6305
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.6849
                       Mean reward: 858.89
               Mean episode length: 228.61
    Episode_Reward/reaching_object: 1.5843
     Episode_Reward/lifting_object: 171.4014
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.1009
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.43s
                      Time elapsed: 01:24:21
                               ETA: 00:04:09

################################################################################
                     [1m Learning iteration 1907/2000 [0m                     

                       Computation: 41475 steps/s (collection: 2.241s, learning 0.129s)
             Mean action noise std: 4.23
          Mean value_function loss: 120.2488
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.6962
                       Mean reward: 874.06
               Mean episode length: 231.24
    Episode_Reward/reaching_object: 1.6000
     Episode_Reward/lifting_object: 173.3765
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.1015
          Episode_Reward/joint_vel: -0.0616
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 2.37s
                      Time elapsed: 01:24:23
                               ETA: 00:04:06

################################################################################
                     [1m Learning iteration 1908/2000 [0m                     

                       Computation: 41664 steps/s (collection: 2.249s, learning 0.110s)
             Mean action noise std: 4.24
          Mean value_function loss: 112.1304
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 73.7057
                       Mean reward: 876.60
               Mean episode length: 231.24
    Episode_Reward/reaching_object: 1.5851
     Episode_Reward/lifting_object: 170.9247
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.1007
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 2.36s
                      Time elapsed: 01:24:26
                               ETA: 00:04:04

################################################################################
                     [1m Learning iteration 1909/2000 [0m                     

                       Computation: 41727 steps/s (collection: 2.221s, learning 0.135s)
             Mean action noise std: 4.24
          Mean value_function loss: 129.5914
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.7192
                       Mean reward: 835.57
               Mean episode length: 222.37
    Episode_Reward/reaching_object: 1.5797
     Episode_Reward/lifting_object: 170.3675
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.1005
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 2.36s
                      Time elapsed: 01:24:28
                               ETA: 00:04:01

################################################################################
                     [1m Learning iteration 1910/2000 [0m                     

                       Computation: 41369 steps/s (collection: 2.247s, learning 0.129s)
             Mean action noise std: 4.24
          Mean value_function loss: 91.1894
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.7328
                       Mean reward: 866.82
               Mean episode length: 229.23
    Episode_Reward/reaching_object: 1.5766
     Episode_Reward/lifting_object: 170.1748
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.1002
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 2.38s
                      Time elapsed: 01:24:30
                               ETA: 00:03:58

################################################################################
                     [1m Learning iteration 1911/2000 [0m                     

                       Computation: 41922 steps/s (collection: 2.219s, learning 0.126s)
             Mean action noise std: 4.24
          Mean value_function loss: 102.3930
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 73.7408
                       Mean reward: 902.98
               Mean episode length: 237.91
    Episode_Reward/reaching_object: 1.6096
     Episode_Reward/lifting_object: 173.9295
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.1019
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 2.34s
                      Time elapsed: 01:24:33
                               ETA: 00:03:56

################################################################################
                     [1m Learning iteration 1912/2000 [0m                     

                       Computation: 40617 steps/s (collection: 2.286s, learning 0.135s)
             Mean action noise std: 4.24
          Mean value_function loss: 106.5949
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 73.7438
                       Mean reward: 837.27
               Mean episode length: 222.72
    Episode_Reward/reaching_object: 1.5567
     Episode_Reward/lifting_object: 167.6131
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0993
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 2.42s
                      Time elapsed: 01:24:35
                               ETA: 00:03:53

################################################################################
                     [1m Learning iteration 1913/2000 [0m                     

                       Computation: 42007 steps/s (collection: 2.220s, learning 0.120s)
             Mean action noise std: 4.24
          Mean value_function loss: 104.3799
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.7520
                       Mean reward: 904.70
               Mean episode length: 238.74
    Episode_Reward/reaching_object: 1.6049
     Episode_Reward/lifting_object: 173.4473
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.1023
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 2.34s
                      Time elapsed: 01:24:37
                               ETA: 00:03:50

################################################################################
                     [1m Learning iteration 1914/2000 [0m                     

                       Computation: 41139 steps/s (collection: 2.249s, learning 0.141s)
             Mean action noise std: 4.25
          Mean value_function loss: 117.6000
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.7656
                       Mean reward: 853.87
               Mean episode length: 227.37
    Episode_Reward/reaching_object: 1.6053
     Episode_Reward/lifting_object: 173.2555
      Episode_Reward/object_height: 0.0240
        Episode_Reward/action_rate: -0.1026
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 2.39s
                      Time elapsed: 01:24:40
                               ETA: 00:03:48

################################################################################
                     [1m Learning iteration 1915/2000 [0m                     

                       Computation: 40853 steps/s (collection: 2.269s, learning 0.138s)
             Mean action noise std: 4.25
          Mean value_function loss: 124.1871
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.7823
                       Mean reward: 819.45
               Mean episode length: 218.41
    Episode_Reward/reaching_object: 1.5626
     Episode_Reward/lifting_object: 168.6156
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0999
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 2.41s
                      Time elapsed: 01:24:42
                               ETA: 00:03:45

################################################################################
                     [1m Learning iteration 1916/2000 [0m                     

                       Computation: 41004 steps/s (collection: 2.300s, learning 0.098s)
             Mean action noise std: 4.25
          Mean value_function loss: 114.7495
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.7922
                       Mean reward: 895.58
               Mean episode length: 236.59
    Episode_Reward/reaching_object: 1.6146
     Episode_Reward/lifting_object: 174.8235
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.1027
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 2.40s
                      Time elapsed: 01:24:45
                               ETA: 00:03:42

################################################################################
                     [1m Learning iteration 1917/2000 [0m                     

                       Computation: 40231 steps/s (collection: 2.313s, learning 0.131s)
             Mean action noise std: 4.25
          Mean value_function loss: 141.6071
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.8017
                       Mean reward: 841.72
               Mean episode length: 223.71
    Episode_Reward/reaching_object: 1.5750
     Episode_Reward/lifting_object: 170.4395
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.1008
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.44s
                      Time elapsed: 01:24:47
                               ETA: 00:03:40

################################################################################
                     [1m Learning iteration 1918/2000 [0m                     

                       Computation: 41619 steps/s (collection: 2.230s, learning 0.132s)
             Mean action noise std: 4.25
          Mean value_function loss: 117.3472
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.8107
                       Mean reward: 904.41
               Mean episode length: 238.43
    Episode_Reward/reaching_object: 1.5835
     Episode_Reward/lifting_object: 171.3962
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.1013
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 2.36s
                      Time elapsed: 01:24:49
                               ETA: 00:03:37

################################################################################
                     [1m Learning iteration 1919/2000 [0m                     

                       Computation: 41684 steps/s (collection: 2.223s, learning 0.136s)
             Mean action noise std: 4.26
          Mean value_function loss: 110.8268
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 73.8219
                       Mean reward: 905.16
               Mean episode length: 238.03
    Episode_Reward/reaching_object: 1.5916
     Episode_Reward/lifting_object: 172.6712
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.1013
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 2.36s
                      Time elapsed: 01:24:52
                               ETA: 00:03:34

################################################################################
                     [1m Learning iteration 1920/2000 [0m                     

                       Computation: 41904 steps/s (collection: 2.210s, learning 0.136s)
             Mean action noise std: 4.26
          Mean value_function loss: 91.6194
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.8320
                       Mean reward: 858.98
               Mean episode length: 227.33
    Episode_Reward/reaching_object: 1.6309
     Episode_Reward/lifting_object: 177.1023
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.1037
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 2.35s
                      Time elapsed: 01:24:54
                               ETA: 00:03:32

################################################################################
                     [1m Learning iteration 1921/2000 [0m                     

                       Computation: 41339 steps/s (collection: 2.232s, learning 0.146s)
             Mean action noise std: 4.26
          Mean value_function loss: 139.7471
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.8431
                       Mean reward: 853.39
               Mean episode length: 226.65
    Episode_Reward/reaching_object: 1.5886
     Episode_Reward/lifting_object: 172.0243
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.1018
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 2.38s
                      Time elapsed: 01:24:57
                               ETA: 00:03:29

################################################################################
                     [1m Learning iteration 1922/2000 [0m                     

                       Computation: 40571 steps/s (collection: 2.273s, learning 0.150s)
             Mean action noise std: 4.26
          Mean value_function loss: 135.5104
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.8590
                       Mean reward: 878.90
               Mean episode length: 232.00
    Episode_Reward/reaching_object: 1.5817
     Episode_Reward/lifting_object: 171.2757
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.1015
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 2.42s
                      Time elapsed: 01:24:59
                               ETA: 00:03:26

################################################################################
                     [1m Learning iteration 1923/2000 [0m                     

                       Computation: 42075 steps/s (collection: 2.204s, learning 0.132s)
             Mean action noise std: 4.26
          Mean value_function loss: 144.0247
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.8716
                       Mean reward: 880.73
               Mean episode length: 232.51
    Episode_Reward/reaching_object: 1.5742
     Episode_Reward/lifting_object: 169.9508
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.1014
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 2.34s
                      Time elapsed: 01:25:01
                               ETA: 00:03:24

################################################################################
                     [1m Learning iteration 1924/2000 [0m                     

                       Computation: 40799 steps/s (collection: 2.285s, learning 0.124s)
             Mean action noise std: 4.27
          Mean value_function loss: 139.5152
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 73.8786
                       Mean reward: 818.36
               Mean episode length: 216.99
    Episode_Reward/reaching_object: 1.5516
     Episode_Reward/lifting_object: 167.5075
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.1000
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 2.41s
                      Time elapsed: 01:25:04
                               ETA: 00:03:21

################################################################################
                     [1m Learning iteration 1925/2000 [0m                     

                       Computation: 38998 steps/s (collection: 2.367s, learning 0.154s)
             Mean action noise std: 4.27
          Mean value_function loss: 117.4725
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 73.8873
                       Mean reward: 900.96
               Mean episode length: 237.75
    Episode_Reward/reaching_object: 1.6040
     Episode_Reward/lifting_object: 173.4100
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.1032
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 2.52s
                      Time elapsed: 01:25:06
                               ETA: 00:03:18

################################################################################
                     [1m Learning iteration 1926/2000 [0m                     

                       Computation: 39715 steps/s (collection: 2.371s, learning 0.105s)
             Mean action noise std: 4.27
          Mean value_function loss: 145.4815
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 73.8945
                       Mean reward: 847.30
               Mean episode length: 225.85
    Episode_Reward/reaching_object: 1.5818
     Episode_Reward/lifting_object: 170.9833
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.1020
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 2.48s
                      Time elapsed: 01:25:09
                               ETA: 00:03:16

################################################################################
                     [1m Learning iteration 1927/2000 [0m                     

                       Computation: 38391 steps/s (collection: 2.411s, learning 0.149s)
             Mean action noise std: 4.27
          Mean value_function loss: 146.7276
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 73.9033
                       Mean reward: 851.33
               Mean episode length: 225.40
    Episode_Reward/reaching_object: 1.5665
     Episode_Reward/lifting_object: 168.3348
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.1012
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 2.56s
                      Time elapsed: 01:25:11
                               ETA: 00:03:13

################################################################################
                     [1m Learning iteration 1928/2000 [0m                     

                       Computation: 39655 steps/s (collection: 2.374s, learning 0.105s)
             Mean action noise std: 4.27
          Mean value_function loss: 163.0259
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 73.9102
                       Mean reward: 841.68
               Mean episode length: 224.37
    Episode_Reward/reaching_object: 1.5384
     Episode_Reward/lifting_object: 165.9725
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0999
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 2.48s
                      Time elapsed: 01:25:14
                               ETA: 00:03:10

################################################################################
                     [1m Learning iteration 1929/2000 [0m                     

                       Computation: 41260 steps/s (collection: 2.245s, learning 0.138s)
             Mean action noise std: 4.27
          Mean value_function loss: 136.8528
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.9208
                       Mean reward: 886.30
               Mean episode length: 234.13
    Episode_Reward/reaching_object: 1.5672
     Episode_Reward/lifting_object: 169.5295
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.1016
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 2.38s
                      Time elapsed: 01:25:16
                               ETA: 00:03:08

################################################################################
                     [1m Learning iteration 1930/2000 [0m                     

                       Computation: 41540 steps/s (collection: 2.246s, learning 0.121s)
             Mean action noise std: 4.27
          Mean value_function loss: 138.7290
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 73.9330
                       Mean reward: 826.75
               Mean episode length: 220.55
    Episode_Reward/reaching_object: 1.5748
     Episode_Reward/lifting_object: 170.2200
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.1022
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 2.37s
                      Time elapsed: 01:25:18
                               ETA: 00:03:05

################################################################################
                     [1m Learning iteration 1931/2000 [0m                     

                       Computation: 41292 steps/s (collection: 2.246s, learning 0.135s)
             Mean action noise std: 4.28
          Mean value_function loss: 109.7643
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 73.9380
                       Mean reward: 911.34
               Mean episode length: 239.47
    Episode_Reward/reaching_object: 1.6126
     Episode_Reward/lifting_object: 174.6130
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.1045
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 2.38s
                      Time elapsed: 01:25:21
                               ETA: 00:03:02

################################################################################
                     [1m Learning iteration 1932/2000 [0m                     

                       Computation: 40433 steps/s (collection: 2.285s, learning 0.147s)
             Mean action noise std: 4.28
          Mean value_function loss: 118.6401
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 73.9408
                       Mean reward: 847.70
               Mean episode length: 224.39
    Episode_Reward/reaching_object: 1.5918
     Episode_Reward/lifting_object: 172.3777
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.1035
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 2.43s
                      Time elapsed: 01:25:23
                               ETA: 00:03:00

################################################################################
                     [1m Learning iteration 1933/2000 [0m                     

                       Computation: 41798 steps/s (collection: 2.216s, learning 0.136s)
             Mean action noise std: 4.28
          Mean value_function loss: 117.1516
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 73.9481
                       Mean reward: 873.89
               Mean episode length: 232.03
    Episode_Reward/reaching_object: 1.6127
     Episode_Reward/lifting_object: 174.8687
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.1046
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 2.35s
                      Time elapsed: 01:25:26
                               ETA: 00:02:57

################################################################################
                     [1m Learning iteration 1934/2000 [0m                     

                       Computation: 42530 steps/s (collection: 2.193s, learning 0.119s)
             Mean action noise std: 4.28
          Mean value_function loss: 126.1867
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.9596
                       Mean reward: 852.70
               Mean episode length: 225.39
    Episode_Reward/reaching_object: 1.6228
     Episode_Reward/lifting_object: 176.2461
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.1052
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 2.31s
                      Time elapsed: 01:25:28
                               ETA: 00:02:54

################################################################################
                     [1m Learning iteration 1935/2000 [0m                     

                       Computation: 41889 steps/s (collection: 2.217s, learning 0.130s)
             Mean action noise std: 4.28
          Mean value_function loss: 123.5313
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 73.9689
                       Mean reward: 864.43
               Mean episode length: 228.67
    Episode_Reward/reaching_object: 1.5764
     Episode_Reward/lifting_object: 171.0834
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.1028
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 2.35s
                      Time elapsed: 01:25:30
                               ETA: 00:02:52

################################################################################
                     [1m Learning iteration 1936/2000 [0m                     

                       Computation: 40941 steps/s (collection: 2.285s, learning 0.117s)
             Mean action noise std: 4.28
          Mean value_function loss: 130.9434
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.9735
                       Mean reward: 851.45
               Mean episode length: 225.31
    Episode_Reward/reaching_object: 1.5739
     Episode_Reward/lifting_object: 171.1451
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.1025
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 2.40s
                      Time elapsed: 01:25:33
                               ETA: 00:02:49

################################################################################
                     [1m Learning iteration 1937/2000 [0m                     

                       Computation: 41396 steps/s (collection: 2.241s, learning 0.134s)
             Mean action noise std: 4.28
          Mean value_function loss: 147.2518
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 73.9822
                       Mean reward: 878.23
               Mean episode length: 231.63
    Episode_Reward/reaching_object: 1.5765
     Episode_Reward/lifting_object: 171.4897
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.1025
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 2.37s
                      Time elapsed: 01:25:35
                               ETA: 00:02:46

################################################################################
                     [1m Learning iteration 1938/2000 [0m                     

                       Computation: 41859 steps/s (collection: 2.229s, learning 0.120s)
             Mean action noise std: 4.28
          Mean value_function loss: 117.7982
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 73.9876
                       Mean reward: 830.86
               Mean episode length: 222.41
    Episode_Reward/reaching_object: 1.5741
     Episode_Reward/lifting_object: 170.8109
      Episode_Reward/object_height: 0.0242
        Episode_Reward/action_rate: -0.1029
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 2.35s
                      Time elapsed: 01:25:37
                               ETA: 00:02:44

################################################################################
                     [1m Learning iteration 1939/2000 [0m                     

                       Computation: 41713 steps/s (collection: 2.236s, learning 0.121s)
             Mean action noise std: 4.29
          Mean value_function loss: 131.6128
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 73.9968
                       Mean reward: 846.39
               Mean episode length: 223.90
    Episode_Reward/reaching_object: 1.5681
     Episode_Reward/lifting_object: 170.8095
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.1026
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 2.36s
                      Time elapsed: 01:25:40
                               ETA: 00:02:41

################################################################################
                     [1m Learning iteration 1940/2000 [0m                     

                       Computation: 41034 steps/s (collection: 2.266s, learning 0.130s)
             Mean action noise std: 4.29
          Mean value_function loss: 119.0581
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 74.0057
                       Mean reward: 868.10
               Mean episode length: 230.57
    Episode_Reward/reaching_object: 1.6119
     Episode_Reward/lifting_object: 175.6648
      Episode_Reward/object_height: 0.0252
        Episode_Reward/action_rate: -0.1053
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 2.40s
                      Time elapsed: 01:25:42
                               ETA: 00:02:38

################################################################################
                     [1m Learning iteration 1941/2000 [0m                     

                       Computation: 40529 steps/s (collection: 2.293s, learning 0.133s)
             Mean action noise std: 4.29
          Mean value_function loss: 111.3570
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.0131
                       Mean reward: 898.26
               Mean episode length: 235.74
    Episode_Reward/reaching_object: 1.5937
     Episode_Reward/lifting_object: 173.7007
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.1040
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 2.43s
                      Time elapsed: 01:25:45
                               ETA: 00:02:36

################################################################################
                     [1m Learning iteration 1942/2000 [0m                     

                       Computation: 41187 steps/s (collection: 2.240s, learning 0.147s)
             Mean action noise std: 4.29
          Mean value_function loss: 123.3675
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 74.0219
                       Mean reward: 858.32
               Mean episode length: 228.59
    Episode_Reward/reaching_object: 1.5960
     Episode_Reward/lifting_object: 173.9685
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.1043
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 2.39s
                      Time elapsed: 01:25:47
                               ETA: 00:02:33

################################################################################
                     [1m Learning iteration 1943/2000 [0m                     

                       Computation: 40565 steps/s (collection: 2.282s, learning 0.142s)
             Mean action noise std: 4.29
          Mean value_function loss: 114.4219
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.0274
                       Mean reward: 850.00
               Mean episode length: 225.54
    Episode_Reward/reaching_object: 1.5712
     Episode_Reward/lifting_object: 171.1122
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.1030
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 2.42s
                      Time elapsed: 01:25:49
                               ETA: 00:02:31

################################################################################
                     [1m Learning iteration 1944/2000 [0m                     

                       Computation: 41805 steps/s (collection: 2.213s, learning 0.138s)
             Mean action noise std: 4.29
          Mean value_function loss: 163.1271
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.0390
                       Mean reward: 825.41
               Mean episode length: 219.22
    Episode_Reward/reaching_object: 1.5578
     Episode_Reward/lifting_object: 169.5091
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.1023
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 2.35s
                      Time elapsed: 01:25:52
                               ETA: 00:02:28

################################################################################
                     [1m Learning iteration 1945/2000 [0m                     

                       Computation: 40430 steps/s (collection: 2.277s, learning 0.154s)
             Mean action noise std: 4.30
          Mean value_function loss: 117.4552
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.0503
                       Mean reward: 855.37
               Mean episode length: 226.22
    Episode_Reward/reaching_object: 1.5855
     Episode_Reward/lifting_object: 172.4830
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.1037
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 2.43s
                      Time elapsed: 01:25:54
                               ETA: 00:02:25

################################################################################
                     [1m Learning iteration 1946/2000 [0m                     

                       Computation: 42162 steps/s (collection: 2.206s, learning 0.126s)
             Mean action noise std: 4.30
          Mean value_function loss: 107.0354
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.0607
                       Mean reward: 870.75
               Mean episode length: 230.32
    Episode_Reward/reaching_object: 1.5894
     Episode_Reward/lifting_object: 173.0772
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.1040
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 2.33s
                      Time elapsed: 01:25:57
                               ETA: 00:02:23

################################################################################
                     [1m Learning iteration 1947/2000 [0m                     

                       Computation: 42499 steps/s (collection: 2.200s, learning 0.113s)
             Mean action noise std: 4.30
          Mean value_function loss: 138.6823
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 74.0791
                       Mean reward: 875.14
               Mean episode length: 231.02
    Episode_Reward/reaching_object: 1.5981
     Episode_Reward/lifting_object: 174.3017
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.1050
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 2.31s
                      Time elapsed: 01:25:59
                               ETA: 00:02:20

################################################################################
                     [1m Learning iteration 1948/2000 [0m                     

                       Computation: 41644 steps/s (collection: 2.233s, learning 0.128s)
             Mean action noise std: 4.30
          Mean value_function loss: 129.0264
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 74.0937
                       Mean reward: 849.71
               Mean episode length: 225.94
    Episode_Reward/reaching_object: 1.5838
     Episode_Reward/lifting_object: 172.6110
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.1037
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 2.36s
                      Time elapsed: 01:26:01
                               ETA: 00:02:17

################################################################################
                     [1m Learning iteration 1949/2000 [0m                     

                       Computation: 41199 steps/s (collection: 2.255s, learning 0.131s)
             Mean action noise std: 4.31
          Mean value_function loss: 138.3252
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.1078
                       Mean reward: 846.81
               Mean episode length: 226.04
    Episode_Reward/reaching_object: 1.6132
     Episode_Reward/lifting_object: 175.9941
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.1057
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 2.39s
                      Time elapsed: 01:26:04
                               ETA: 00:02:15

################################################################################
                     [1m Learning iteration 1950/2000 [0m                     

                       Computation: 41549 steps/s (collection: 2.249s, learning 0.116s)
             Mean action noise std: 4.31
          Mean value_function loss: 135.1284
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 74.1202
                       Mean reward: 868.88
               Mean episode length: 230.28
    Episode_Reward/reaching_object: 1.5608
     Episode_Reward/lifting_object: 169.7072
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.1031
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 2.37s
                      Time elapsed: 01:26:06
                               ETA: 00:02:12

################################################################################
                     [1m Learning iteration 1951/2000 [0m                     

                       Computation: 41576 steps/s (collection: 2.229s, learning 0.135s)
             Mean action noise std: 4.31
          Mean value_function loss: 137.9663
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 74.1286
                       Mean reward: 852.05
               Mean episode length: 225.83
    Episode_Reward/reaching_object: 1.5658
     Episode_Reward/lifting_object: 170.7572
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.1033
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 2.36s
                      Time elapsed: 01:26:08
                               ETA: 00:02:09

################################################################################
                     [1m Learning iteration 1952/2000 [0m                     

                       Computation: 41480 steps/s (collection: 2.231s, learning 0.139s)
             Mean action noise std: 4.31
          Mean value_function loss: 114.0891
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.1391
                       Mean reward: 887.11
               Mean episode length: 234.83
    Episode_Reward/reaching_object: 1.5988
     Episode_Reward/lifting_object: 174.2857
      Episode_Reward/object_height: 0.0238
        Episode_Reward/action_rate: -0.1057
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 2.37s
                      Time elapsed: 01:26:11
                               ETA: 00:02:07

################################################################################
                     [1m Learning iteration 1953/2000 [0m                     

                       Computation: 41417 steps/s (collection: 2.256s, learning 0.117s)
             Mean action noise std: 4.31
          Mean value_function loss: 132.9163
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 74.1523
                       Mean reward: 845.17
               Mean episode length: 224.08
    Episode_Reward/reaching_object: 1.5770
     Episode_Reward/lifting_object: 171.8231
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.1043
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 2.37s
                      Time elapsed: 01:26:13
                               ETA: 00:02:04

################################################################################
                     [1m Learning iteration 1954/2000 [0m                     

                       Computation: 41903 steps/s (collection: 2.216s, learning 0.130s)
             Mean action noise std: 4.31
          Mean value_function loss: 103.6236
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 74.1644
                       Mean reward: 862.42
               Mean episode length: 227.64
    Episode_Reward/reaching_object: 1.5974
     Episode_Reward/lifting_object: 174.3567
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.1056
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 2.35s
                      Time elapsed: 01:26:15
                               ETA: 00:02:01

################################################################################
                     [1m Learning iteration 1955/2000 [0m                     

                       Computation: 41676 steps/s (collection: 2.225s, learning 0.134s)
             Mean action noise std: 4.32
          Mean value_function loss: 99.6896
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.1747
                       Mean reward: 877.84
               Mean episode length: 232.17
    Episode_Reward/reaching_object: 1.6154
     Episode_Reward/lifting_object: 175.7569
      Episode_Reward/object_height: 0.0247
        Episode_Reward/action_rate: -0.1070
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 2.36s
                      Time elapsed: 01:26:18
                               ETA: 00:01:59

################################################################################
                     [1m Learning iteration 1956/2000 [0m                     

                       Computation: 42157 steps/s (collection: 2.210s, learning 0.122s)
             Mean action noise std: 4.32
          Mean value_function loss: 120.8827
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.1866
                       Mean reward: 829.08
               Mean episode length: 220.50
    Episode_Reward/reaching_object: 1.5916
     Episode_Reward/lifting_object: 172.6509
      Episode_Reward/object_height: 0.0245
        Episode_Reward/action_rate: -0.1055
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 2.33s
                      Time elapsed: 01:26:20
                               ETA: 00:01:56

################################################################################
                     [1m Learning iteration 1957/2000 [0m                     

                       Computation: 42205 steps/s (collection: 2.221s, learning 0.109s)
             Mean action noise std: 4.32
          Mean value_function loss: 99.0230
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.1969
                       Mean reward: 911.14
               Mean episode length: 241.11
    Episode_Reward/reaching_object: 1.6382
     Episode_Reward/lifting_object: 178.0144
      Episode_Reward/object_height: 0.0248
        Episode_Reward/action_rate: -0.1085
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 2.33s
                      Time elapsed: 01:26:22
                               ETA: 00:01:53

################################################################################
                     [1m Learning iteration 1958/2000 [0m                     

                       Computation: 41549 steps/s (collection: 2.216s, learning 0.150s)
             Mean action noise std: 4.32
          Mean value_function loss: 94.4952
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.2056
                       Mean reward: 908.40
               Mean episode length: 240.14
    Episode_Reward/reaching_object: 1.6526
     Episode_Reward/lifting_object: 179.6032
      Episode_Reward/object_height: 0.0250
        Episode_Reward/action_rate: -0.1095
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 2.37s
                      Time elapsed: 01:26:25
                               ETA: 00:01:51

################################################################################
                     [1m Learning iteration 1959/2000 [0m                     

                       Computation: 41719 steps/s (collection: 2.216s, learning 0.140s)
             Mean action noise std: 4.32
          Mean value_function loss: 92.0302
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.2166
                       Mean reward: 896.81
               Mean episode length: 236.00
    Episode_Reward/reaching_object: 1.6155
     Episode_Reward/lifting_object: 175.2642
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.1072
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 2.36s
                      Time elapsed: 01:26:27
                               ETA: 00:01:48

################################################################################
                     [1m Learning iteration 1960/2000 [0m                     

                       Computation: 41039 steps/s (collection: 2.256s, learning 0.139s)
             Mean action noise std: 4.32
          Mean value_function loss: 132.5286
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 74.2278
                       Mean reward: 859.52
               Mean episode length: 227.46
    Episode_Reward/reaching_object: 1.5641
     Episode_Reward/lifting_object: 169.4494
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.1043
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 2.40s
                      Time elapsed: 01:26:30
                               ETA: 00:01:45

################################################################################
                     [1m Learning iteration 1961/2000 [0m                     

                       Computation: 41572 steps/s (collection: 2.233s, learning 0.132s)
             Mean action noise std: 4.33
          Mean value_function loss: 119.7124
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 74.2355
                       Mean reward: 879.46
               Mean episode length: 232.31
    Episode_Reward/reaching_object: 1.5866
     Episode_Reward/lifting_object: 172.0080
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.1056
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 2.36s
                      Time elapsed: 01:26:32
                               ETA: 00:01:43

################################################################################
                     [1m Learning iteration 1962/2000 [0m                     

                       Computation: 41853 steps/s (collection: 2.215s, learning 0.134s)
             Mean action noise std: 4.33
          Mean value_function loss: 147.0142
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 74.2404
                       Mean reward: 841.49
               Mean episode length: 224.39
    Episode_Reward/reaching_object: 1.5674
     Episode_Reward/lifting_object: 169.9525
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.1047
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 2.35s
                      Time elapsed: 01:26:34
                               ETA: 00:01:40

################################################################################
                     [1m Learning iteration 1963/2000 [0m                     

                       Computation: 41767 steps/s (collection: 2.226s, learning 0.128s)
             Mean action noise std: 4.33
          Mean value_function loss: 188.6526
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 74.2461
                       Mean reward: 828.74
               Mean episode length: 220.96
    Episode_Reward/reaching_object: 1.5327
     Episode_Reward/lifting_object: 166.2078
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.1028
          Episode_Reward/joint_vel: -0.0613
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 2.35s
                      Time elapsed: 01:26:37
                               ETA: 00:01:37

################################################################################
                     [1m Learning iteration 1964/2000 [0m                     

                       Computation: 41117 steps/s (collection: 2.258s, learning 0.133s)
             Mean action noise std: 4.33
          Mean value_function loss: 105.0891
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.2574
                       Mean reward: 850.47
               Mean episode length: 226.04
    Episode_Reward/reaching_object: 1.5915
     Episode_Reward/lifting_object: 172.3314
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.1066
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 2.39s
                      Time elapsed: 01:26:39
                               ETA: 00:01:35

################################################################################
                     [1m Learning iteration 1965/2000 [0m                     

                       Computation: 41951 steps/s (collection: 2.217s, learning 0.127s)
             Mean action noise std: 4.33
          Mean value_function loss: 109.8132
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.2723
                       Mean reward: 846.78
               Mean episode length: 224.96
    Episode_Reward/reaching_object: 1.5937
     Episode_Reward/lifting_object: 173.0988
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.1064
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 2.34s
                      Time elapsed: 01:26:41
                               ETA: 00:01:32

################################################################################
                     [1m Learning iteration 1966/2000 [0m                     

                       Computation: 42023 steps/s (collection: 2.215s, learning 0.124s)
             Mean action noise std: 4.33
          Mean value_function loss: 108.6869
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.2837
                       Mean reward: 906.33
               Mean episode length: 239.39
    Episode_Reward/reaching_object: 1.6089
     Episode_Reward/lifting_object: 174.7957
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.1076
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 2.34s
                      Time elapsed: 01:26:44
                               ETA: 00:01:29

################################################################################
                     [1m Learning iteration 1967/2000 [0m                     

                       Computation: 41812 steps/s (collection: 2.237s, learning 0.115s)
             Mean action noise std: 4.34
          Mean value_function loss: 126.1986
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.2904
                       Mean reward: 874.60
               Mean episode length: 231.84
    Episode_Reward/reaching_object: 1.6098
     Episode_Reward/lifting_object: 174.9449
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.1077
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 2.35s
                      Time elapsed: 01:26:46
                               ETA: 00:01:27

################################################################################
                     [1m Learning iteration 1968/2000 [0m                     

                       Computation: 41445 steps/s (collection: 2.236s, learning 0.136s)
             Mean action noise std: 4.34
          Mean value_function loss: 146.1165
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.3000
                       Mean reward: 841.60
               Mean episode length: 224.81
    Episode_Reward/reaching_object: 1.5471
     Episode_Reward/lifting_object: 167.5435
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.1043
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 2.37s
                      Time elapsed: 01:26:48
                               ETA: 00:01:24

################################################################################
                     [1m Learning iteration 1969/2000 [0m                     

                       Computation: 42040 steps/s (collection: 2.209s, learning 0.130s)
             Mean action noise std: 4.34
          Mean value_function loss: 122.4666
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 74.3092
                       Mean reward: 878.51
               Mean episode length: 231.85
    Episode_Reward/reaching_object: 1.5799
     Episode_Reward/lifting_object: 171.8523
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.1059
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 2.34s
                      Time elapsed: 01:26:51
                               ETA: 00:01:22

################################################################################
                     [1m Learning iteration 1970/2000 [0m                     

                       Computation: 41671 steps/s (collection: 2.250s, learning 0.109s)
             Mean action noise std: 4.34
          Mean value_function loss: 111.8105
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.3188
                       Mean reward: 884.36
               Mean episode length: 233.65
    Episode_Reward/reaching_object: 1.5725
     Episode_Reward/lifting_object: 170.5470
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.1059
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 2.36s
                      Time elapsed: 01:26:53
                               ETA: 00:01:19

################################################################################
                     [1m Learning iteration 1971/2000 [0m                     

                       Computation: 40171 steps/s (collection: 2.300s, learning 0.147s)
             Mean action noise std: 4.34
          Mean value_function loss: 115.1188
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.3272
                       Mean reward: 822.76
               Mean episode length: 220.48
    Episode_Reward/reaching_object: 1.5917
     Episode_Reward/lifting_object: 172.9843
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.1072
          Episode_Reward/joint_vel: -0.0632
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 2.45s
                      Time elapsed: 01:26:56
                               ETA: 00:01:16

################################################################################
                     [1m Learning iteration 1972/2000 [0m                     

                       Computation: 40446 steps/s (collection: 2.302s, learning 0.128s)
             Mean action noise std: 4.34
          Mean value_function loss: 148.4826
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.3354
                       Mean reward: 840.05
               Mean episode length: 223.82
    Episode_Reward/reaching_object: 1.5507
     Episode_Reward/lifting_object: 168.0771
      Episode_Reward/object_height: 0.0214
        Episode_Reward/action_rate: -0.1049
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 2.43s
                      Time elapsed: 01:26:58
                               ETA: 00:01:14

################################################################################
                     [1m Learning iteration 1973/2000 [0m                     

                       Computation: 38890 steps/s (collection: 2.408s, learning 0.120s)
             Mean action noise std: 4.35
          Mean value_function loss: 138.7219
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 74.3507
                       Mean reward: 910.80
               Mean episode length: 239.99
    Episode_Reward/reaching_object: 1.5644
     Episode_Reward/lifting_object: 169.7422
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.1058
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 2.53s
                      Time elapsed: 01:27:01
                               ETA: 00:01:11

################################################################################
                     [1m Learning iteration 1974/2000 [0m                     

                       Computation: 39664 steps/s (collection: 2.339s, learning 0.139s)
             Mean action noise std: 4.35
          Mean value_function loss: 81.2102
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.3651
                       Mean reward: 893.21
               Mean episode length: 236.83
    Episode_Reward/reaching_object: 1.5977
     Episode_Reward/lifting_object: 173.2885
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.1077
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 2.48s
                      Time elapsed: 01:27:03
                               ETA: 00:01:08

################################################################################
                     [1m Learning iteration 1975/2000 [0m                     

                       Computation: 39434 steps/s (collection: 2.367s, learning 0.126s)
             Mean action noise std: 4.35
          Mean value_function loss: 100.6217
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.3811
                       Mean reward: 879.54
               Mean episode length: 233.50
    Episode_Reward/reaching_object: 1.5919
     Episode_Reward/lifting_object: 172.4461
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.1077
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 2.49s
                      Time elapsed: 01:27:05
                               ETA: 00:01:06

################################################################################
                     [1m Learning iteration 1976/2000 [0m                     

                       Computation: 40536 steps/s (collection: 2.292s, learning 0.133s)
             Mean action noise std: 4.35
          Mean value_function loss: 122.0690
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 74.3940
                       Mean reward: 901.12
               Mean episode length: 237.35
    Episode_Reward/reaching_object: 1.6057
     Episode_Reward/lifting_object: 174.5418
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.1083
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 2.43s
                      Time elapsed: 01:27:08
                               ETA: 00:01:03

################################################################################
                     [1m Learning iteration 1977/2000 [0m                     

                       Computation: 41976 steps/s (collection: 2.200s, learning 0.142s)
             Mean action noise std: 4.36
          Mean value_function loss: 111.5204
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 74.4043
                       Mean reward: 870.96
               Mean episode length: 230.56
    Episode_Reward/reaching_object: 1.5981
     Episode_Reward/lifting_object: 173.5581
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.1080
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 2.34s
                      Time elapsed: 01:27:10
                               ETA: 00:01:00

################################################################################
                     [1m Learning iteration 1978/2000 [0m                     

                       Computation: 41751 steps/s (collection: 2.222s, learning 0.132s)
             Mean action noise std: 4.36
          Mean value_function loss: 139.4339
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 74.4096
                       Mean reward: 820.32
               Mean episode length: 218.65
    Episode_Reward/reaching_object: 1.5455
     Episode_Reward/lifting_object: 167.5829
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.1052
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 2.35s
                      Time elapsed: 01:27:13
                               ETA: 00:00:58

################################################################################
                     [1m Learning iteration 1979/2000 [0m                     

                       Computation: 42846 steps/s (collection: 2.191s, learning 0.103s)
             Mean action noise std: 4.36
          Mean value_function loss: 131.1016
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.4151
                       Mean reward: 884.08
               Mean episode length: 233.24
    Episode_Reward/reaching_object: 1.5662
     Episode_Reward/lifting_object: 169.9591
      Episode_Reward/object_height: 0.0210
        Episode_Reward/action_rate: -0.1065
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 2.29s
                      Time elapsed: 01:27:15
                               ETA: 00:00:55

################################################################################
                     [1m Learning iteration 1980/2000 [0m                     

                       Computation: 41579 steps/s (collection: 2.238s, learning 0.126s)
             Mean action noise std: 4.36
          Mean value_function loss: 132.0928
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 74.4256
                       Mean reward: 843.01
               Mean episode length: 226.50
    Episode_Reward/reaching_object: 1.5946
     Episode_Reward/lifting_object: 173.1924
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.1084
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 2.36s
                      Time elapsed: 01:27:17
                               ETA: 00:00:52

################################################################################
                     [1m Learning iteration 1981/2000 [0m                     

                       Computation: 41582 steps/s (collection: 2.229s, learning 0.135s)
             Mean action noise std: 4.36
          Mean value_function loss: 135.9225
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.4373
                       Mean reward: 840.55
               Mean episode length: 223.57
    Episode_Reward/reaching_object: 1.5790
     Episode_Reward/lifting_object: 171.5687
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.1079
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 2.36s
                      Time elapsed: 01:27:20
                               ETA: 00:00:50

################################################################################
                     [1m Learning iteration 1982/2000 [0m                     

                       Computation: 42894 steps/s (collection: 2.189s, learning 0.103s)
             Mean action noise std: 4.36
          Mean value_function loss: 105.9812
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.4512
                       Mean reward: 861.95
               Mean episode length: 229.36
    Episode_Reward/reaching_object: 1.5865
     Episode_Reward/lifting_object: 172.5040
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.1085
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 2.29s
                      Time elapsed: 01:27:22
                               ETA: 00:00:47

################################################################################
                     [1m Learning iteration 1983/2000 [0m                     

                       Computation: 41241 steps/s (collection: 2.244s, learning 0.140s)
             Mean action noise std: 4.37
          Mean value_function loss: 124.7612
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 74.4647
                       Mean reward: 845.61
               Mean episode length: 224.48
    Episode_Reward/reaching_object: 1.5936
     Episode_Reward/lifting_object: 173.4902
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.1090
          Episode_Reward/joint_vel: -0.0626
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 2.38s
                      Time elapsed: 01:27:24
                               ETA: 00:00:44

################################################################################
                     [1m Learning iteration 1984/2000 [0m                     

                       Computation: 41531 steps/s (collection: 2.229s, learning 0.138s)
             Mean action noise std: 4.37
          Mean value_function loss: 150.5495
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 74.4804
                       Mean reward: 874.74
               Mean episode length: 231.50
    Episode_Reward/reaching_object: 1.5732
     Episode_Reward/lifting_object: 171.3253
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.1076
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 2.37s
                      Time elapsed: 01:27:27
                               ETA: 00:00:42

################################################################################
                     [1m Learning iteration 1985/2000 [0m                     

                       Computation: 40404 steps/s (collection: 2.304s, learning 0.129s)
             Mean action noise std: 4.37
          Mean value_function loss: 142.4646
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.4924
                       Mean reward: 840.88
               Mean episode length: 224.59
    Episode_Reward/reaching_object: 1.5498
     Episode_Reward/lifting_object: 168.0569
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.1071
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 2.43s
                      Time elapsed: 01:27:29
                               ETA: 00:00:39

################################################################################
                     [1m Learning iteration 1986/2000 [0m                     

                       Computation: 39590 steps/s (collection: 2.347s, learning 0.136s)
             Mean action noise std: 4.37
          Mean value_function loss: 151.2111
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 74.5051
                       Mean reward: 874.59
               Mean episode length: 230.60
    Episode_Reward/reaching_object: 1.5730
     Episode_Reward/lifting_object: 171.5844
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.1083
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 2.48s
                      Time elapsed: 01:27:32
                               ETA: 00:00:37

################################################################################
                     [1m Learning iteration 1987/2000 [0m                     

                       Computation: 41621 steps/s (collection: 2.227s, learning 0.135s)
             Mean action noise std: 4.37
          Mean value_function loss: 179.2170
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 74.5151
                       Mean reward: 866.68
               Mean episode length: 228.45
    Episode_Reward/reaching_object: 1.5414
     Episode_Reward/lifting_object: 167.6615
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.1064
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 2.36s
                      Time elapsed: 01:27:34
                               ETA: 00:00:34

################################################################################
                     [1m Learning iteration 1988/2000 [0m                     

                       Computation: 42129 steps/s (collection: 2.233s, learning 0.101s)
             Mean action noise std: 4.38
          Mean value_function loss: 135.7052
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 74.5196
                       Mean reward: 891.86
               Mean episode length: 235.79
    Episode_Reward/reaching_object: 1.5554
     Episode_Reward/lifting_object: 169.4015
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.1074
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 2.33s
                      Time elapsed: 01:27:36
                               ETA: 00:00:31

################################################################################
                     [1m Learning iteration 1989/2000 [0m                     

                       Computation: 39909 steps/s (collection: 2.325s, learning 0.138s)
             Mean action noise std: 4.38
          Mean value_function loss: 120.7325
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 74.5244
                       Mean reward: 881.34
               Mean episode length: 233.39
    Episode_Reward/reaching_object: 1.6130
     Episode_Reward/lifting_object: 176.0097
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.1108
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 2.46s
                      Time elapsed: 01:27:39
                               ETA: 00:00:29

################################################################################
                     [1m Learning iteration 1990/2000 [0m                     

                       Computation: 40155 steps/s (collection: 2.332s, learning 0.116s)
             Mean action noise std: 4.38
          Mean value_function loss: 130.8886
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.5283
                       Mean reward: 866.96
               Mean episode length: 229.17
    Episode_Reward/reaching_object: 1.5907
     Episode_Reward/lifting_object: 173.8666
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.1098
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 2.45s
                      Time elapsed: 01:27:41
                               ETA: 00:00:26

################################################################################
                     [1m Learning iteration 1991/2000 [0m                     

                       Computation: 40997 steps/s (collection: 2.279s, learning 0.119s)
             Mean action noise std: 4.38
          Mean value_function loss: 133.7344
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.5336
                       Mean reward: 876.21
               Mean episode length: 232.71
    Episode_Reward/reaching_object: 1.5771
     Episode_Reward/lifting_object: 171.9733
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.1089
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 2.40s
                      Time elapsed: 01:27:44
                               ETA: 00:00:23

################################################################################
                     [1m Learning iteration 1992/2000 [0m                     

                       Computation: 40336 steps/s (collection: 2.327s, learning 0.110s)
             Mean action noise std: 4.38
          Mean value_function loss: 143.0368
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.5454
                       Mean reward: 878.79
               Mean episode length: 232.25
    Episode_Reward/reaching_object: 1.5783
     Episode_Reward/lifting_object: 171.8102
      Episode_Reward/object_height: 0.0225
        Episode_Reward/action_rate: -0.1091
          Episode_Reward/joint_vel: -0.0635
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 2.44s
                      Time elapsed: 01:27:46
                               ETA: 00:00:21

################################################################################
                     [1m Learning iteration 1993/2000 [0m                     

                       Computation: 39759 steps/s (collection: 2.349s, learning 0.123s)
             Mean action noise std: 4.38
          Mean value_function loss: 113.4673
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.5559
                       Mean reward: 898.99
               Mean episode length: 236.68
    Episode_Reward/reaching_object: 1.6147
     Episode_Reward/lifting_object: 176.3789
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.1113
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 2.47s
                      Time elapsed: 01:27:48
                               ETA: 00:00:18

################################################################################
                     [1m Learning iteration 1994/2000 [0m                     

                       Computation: 40750 steps/s (collection: 2.298s, learning 0.114s)
             Mean action noise std: 4.38
          Mean value_function loss: 107.7549
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 74.5627
                       Mean reward: 860.15
               Mean episode length: 228.75
    Episode_Reward/reaching_object: 1.5987
     Episode_Reward/lifting_object: 174.2412
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.1103
          Episode_Reward/joint_vel: -0.0645
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 2.41s
                      Time elapsed: 01:27:51
                               ETA: 00:00:15

################################################################################
                     [1m Learning iteration 1995/2000 [0m                     

                       Computation: 41174 steps/s (collection: 2.281s, learning 0.106s)
             Mean action noise std: 4.39
          Mean value_function loss: 164.9108
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 74.5739
                       Mean reward: 841.46
               Mean episode length: 223.39
    Episode_Reward/reaching_object: 1.5565
     Episode_Reward/lifting_object: 169.2746
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.1080
          Episode_Reward/joint_vel: -0.0642
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 2.39s
                      Time elapsed: 01:27:53
                               ETA: 00:00:13

################################################################################
                     [1m Learning iteration 1996/2000 [0m                     

                       Computation: 41045 steps/s (collection: 2.288s, learning 0.107s)
             Mean action noise std: 4.39
          Mean value_function loss: 113.4754
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.5855
                       Mean reward: 888.86
               Mean episode length: 234.01
    Episode_Reward/reaching_object: 1.6159
     Episode_Reward/lifting_object: 176.2994
      Episode_Reward/object_height: 0.0233
        Episode_Reward/action_rate: -0.1118
          Episode_Reward/joint_vel: -0.0663
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 2.39s
                      Time elapsed: 01:27:56
                               ETA: 00:00:10

################################################################################
                     [1m Learning iteration 1997/2000 [0m                     

                       Computation: 41206 steps/s (collection: 2.278s, learning 0.108s)
             Mean action noise std: 4.39
          Mean value_function loss: 119.8318
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.5966
                       Mean reward: 848.12
               Mean episode length: 226.85
    Episode_Reward/reaching_object: 1.5757
     Episode_Reward/lifting_object: 170.6763
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.1091
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 2.39s
                      Time elapsed: 01:27:58
                               ETA: 00:00:07

################################################################################
                     [1m Learning iteration 1998/2000 [0m                     

                       Computation: 40327 steps/s (collection: 2.316s, learning 0.121s)
             Mean action noise std: 4.39
          Mean value_function loss: 130.0239
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.6063
                       Mean reward: 891.35
               Mean episode length: 235.74
    Episode_Reward/reaching_object: 1.5868
     Episode_Reward/lifting_object: 172.3242
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.1100
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 2.44s
                      Time elapsed: 01:28:01
                               ETA: 00:00:05

################################################################################
                     [1m Learning iteration 1999/2000 [0m                     

                       Computation: 40520 steps/s (collection: 2.309s, learning 0.117s)
             Mean action noise std: 4.39
          Mean value_function loss: 103.8356
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 74.6177
                       Mean reward: 872.22
               Mean episode length: 230.54
    Episode_Reward/reaching_object: 1.6216
     Episode_Reward/lifting_object: 176.6335
      Episode_Reward/object_height: 0.0235
        Episode_Reward/action_rate: -0.1121
          Episode_Reward/joint_vel: -0.0662
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 2.43s
                      Time elapsed: 01:28:03
                               ETA: 00:00:02

