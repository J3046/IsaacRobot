################################################################################
                      [1m Learning iteration 0/2500 [0m                       

                       Computation: 10954 steps/s (collection: 8.693s, learning 0.281s)
             Mean action noise std: 1.00
          Mean value_function loss: 0.0046
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 44.0360
                       Mean reward: 0.01
               Mean episode length: 21.93
    Episode_Reward/reaching_object: 0.0014
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0001
        Episode_Reward/action_rate: -0.0003
          Episode_Reward/joint_vel: -0.0005
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 8.97s
                      Time elapsed: 00:00:08
                               ETA: 06:13:53

################################################################################
                      [1m Learning iteration 1/2500 [0m                       

                       Computation: 14500 steps/s (collection: 6.626s, learning 0.153s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 44.1571
                       Mean reward: 0.01
               Mean episode length: 45.42
    Episode_Reward/reaching_object: 0.0036
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0003
        Episode_Reward/action_rate: -0.0009
          Episode_Reward/joint_vel: -0.0014
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 6.78s
                      Time elapsed: 00:00:15
                               ETA: 05:28:02

################################################################################
                      [1m Learning iteration 2/2500 [0m                       

                       Computation: 14879 steps/s (collection: 6.463s, learning 0.143s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 44.2206
                       Mean reward: 0.01
               Mean episode length: 69.07
    Episode_Reward/reaching_object: 0.0057
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0005
        Episode_Reward/action_rate: -0.0015
          Episode_Reward/joint_vel: -0.0023
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 6.61s
                      Time elapsed: 00:00:22
                               ETA: 05:10:17

################################################################################
                      [1m Learning iteration 3/2500 [0m                       

                       Computation: 14740 steps/s (collection: 6.535s, learning 0.134s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 44.2697
                       Mean reward: 0.02
               Mean episode length: 93.12
    Episode_Reward/reaching_object: 0.0073
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0007
        Episode_Reward/action_rate: -0.0021
          Episode_Reward/joint_vel: -0.0031
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 6.67s
                      Time elapsed: 00:00:29
                               ETA: 05:02:00

################################################################################
                      [1m Learning iteration 4/2500 [0m                       

                       Computation: 14515 steps/s (collection: 6.607s, learning 0.166s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 44.2882
                       Mean reward: 0.02
               Mean episode length: 117.84
    Episode_Reward/reaching_object: 0.0095
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0009
        Episode_Reward/action_rate: -0.0027
          Episode_Reward/joint_vel: -0.0041
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 6.77s
                      Time elapsed: 00:00:35
                               ETA: 04:57:51

################################################################################
                      [1m Learning iteration 5/2500 [0m                       

                       Computation: 14697 steps/s (collection: 6.520s, learning 0.169s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0056
                 Mean entropy loss: 44.3084
                       Mean reward: 0.02
               Mean episode length: 141.19
    Episode_Reward/reaching_object: 0.0107
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0011
        Episode_Reward/action_rate: -0.0033
          Episode_Reward/joint_vel: -0.0050
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 6.69s
                      Time elapsed: 00:00:42
                               ETA: 04:54:28

################################################################################
                      [1m Learning iteration 6/2500 [0m                       

                       Computation: 14779 steps/s (collection: 6.512s, learning 0.139s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0062
                 Mean entropy loss: 44.3468
                       Mean reward: 0.02
               Mean episode length: 165.18
    Episode_Reward/reaching_object: 0.0126
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0013
        Episode_Reward/action_rate: -0.0040
          Episode_Reward/joint_vel: -0.0059
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 6.65s
                      Time elapsed: 00:00:49
                               ETA: 04:51:48

################################################################################
                      [1m Learning iteration 7/2500 [0m                       

                       Computation: 14999 steps/s (collection: 6.412s, learning 0.142s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0070
                 Mean entropy loss: 44.3626
                       Mean reward: 0.03
               Mean episode length: 189.88
    Episode_Reward/reaching_object: 0.0142
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0046
          Episode_Reward/joint_vel: -0.0068
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 6.55s
                      Time elapsed: 00:00:55
                               ETA: 04:49:15

################################################################################
                      [1m Learning iteration 8/2500 [0m                       

                       Computation: 18771 steps/s (collection: 5.123s, learning 0.114s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 44.3800
                       Mean reward: 0.03
               Mean episode length: 213.51
    Episode_Reward/reaching_object: 0.0157
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0052
          Episode_Reward/joint_vel: -0.0077
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 5.24s
                      Time elapsed: 00:01:00
                               ETA: 04:41:11

################################################################################
                      [1m Learning iteration 9/2500 [0m                       

                       Computation: 55389 steps/s (collection: 1.677s, learning 0.097s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0080
                 Mean entropy loss: 44.3873
                       Mean reward: 0.02
               Mean episode length: 236.90
    Episode_Reward/reaching_object: 0.0166
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0058
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 1.77s
                      Time elapsed: 00:01:02
                               ETA: 04:20:20

################################################################################
                      [1m Learning iteration 10/2500 [0m                      

                       Computation: 56935 steps/s (collection: 1.627s, learning 0.100s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 44.3827
                       Mean reward: 0.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0184
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0063
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 1.73s
                      Time elapsed: 00:01:04
                               ETA: 04:03:05

################################################################################
                      [1m Learning iteration 11/2500 [0m                      

                       Computation: 56057 steps/s (collection: 1.640s, learning 0.114s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 44.3750
                       Mean reward: 0.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0212
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 1.75s
                      Time elapsed: 00:01:06
                               ETA: 03:48:48

################################################################################
                      [1m Learning iteration 12/2500 [0m                      

                       Computation: 55788 steps/s (collection: 1.645s, learning 0.117s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 44.3501
                       Mean reward: 0.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0215
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 1.76s
                      Time elapsed: 00:01:07
                               ETA: 03:36:44

################################################################################
                      [1m Learning iteration 13/2500 [0m                      

                       Computation: 56913 steps/s (collection: 1.634s, learning 0.093s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 44.3483
                       Mean reward: 0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0259
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 1.73s
                      Time elapsed: 00:01:09
                               ETA: 03:26:17

################################################################################
                      [1m Learning iteration 14/2500 [0m                      

                       Computation: 55928 steps/s (collection: 1.639s, learning 0.119s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0096
                 Mean entropy loss: 44.3811
                       Mean reward: 0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0311
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 1.76s
                      Time elapsed: 00:01:11
                               ETA: 03:17:18

################################################################################
                      [1m Learning iteration 15/2500 [0m                      

                       Computation: 55126 steps/s (collection: 1.662s, learning 0.121s)
             Mean action noise std: 1.01
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0094
                 Mean entropy loss: 44.4104
                       Mean reward: 0.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0360
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 1.78s
                      Time elapsed: 00:01:13
                               ETA: 03:09:31

################################################################################
                      [1m Learning iteration 16/2500 [0m                      

                       Computation: 55342 steps/s (collection: 1.661s, learning 0.116s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 44.4618
                       Mean reward: 0.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0422
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 1.78s
                      Time elapsed: 00:01:14
                               ETA: 03:02:37

################################################################################
                      [1m Learning iteration 17/2500 [0m                      

                       Computation: 56427 steps/s (collection: 1.637s, learning 0.105s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 44.5294
                       Mean reward: 0.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0554
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 1.74s
                      Time elapsed: 00:01:16
                               ETA: 02:56:25

################################################################################
                      [1m Learning iteration 18/2500 [0m                      

                       Computation: 56160 steps/s (collection: 1.639s, learning 0.111s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 44.6131
                       Mean reward: 0.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0711
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 1.75s
                      Time elapsed: 00:01:18
                               ETA: 02:50:52

################################################################################
                      [1m Learning iteration 19/2500 [0m                      

                       Computation: 54863 steps/s (collection: 1.677s, learning 0.115s)
             Mean action noise std: 1.02
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 44.6986
                       Mean reward: 0.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0880
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 1.79s
                      Time elapsed: 00:01:20
                               ETA: 02:45:58

################################################################################
                      [1m Learning iteration 20/2500 [0m                      

                       Computation: 56512 steps/s (collection: 1.634s, learning 0.106s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 44.7555
                       Mean reward: 0.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1079
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0064
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 1.74s
                      Time elapsed: 00:01:22
                               ETA: 02:41:25

################################################################################
                      [1m Learning iteration 21/2500 [0m                      

                       Computation: 55889 steps/s (collection: 1.662s, learning 0.097s)
             Mean action noise std: 1.03
          Mean value_function loss: 0.0017
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 44.8209
                       Mean reward: 0.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1460
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 1.76s
                      Time elapsed: 00:01:23
                               ETA: 02:37:20

################################################################################
                      [1m Learning iteration 22/2500 [0m                      

                       Computation: 55047 steps/s (collection: 1.676s, learning 0.110s)
             Mean action noise std: 1.03
          Mean value_function loss: 4.8225
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 44.9296
                       Mean reward: -0.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1814
     Episode_Reward/lifting_object: -0.2262
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 1.79s
                      Time elapsed: 00:01:25
                               ETA: 02:33:38

################################################################################
                      [1m Learning iteration 23/2500 [0m                      

                       Computation: 53957 steps/s (collection: 1.718s, learning 0.104s)
             Mean action noise std: 1.04
          Mean value_function loss: 12.3345
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 45.0114
                       Mean reward: 0.45
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2266
     Episode_Reward/lifting_object: -0.2508
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 1.82s
                      Time elapsed: 00:01:27
                               ETA: 02:30:18

################################################################################
                      [1m Learning iteration 24/2500 [0m                      

                       Computation: 51591 steps/s (collection: 1.790s, learning 0.115s)
             Mean action noise std: 1.04
          Mean value_function loss: 18.2500
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 45.1913
                       Mean reward: -11.82
               Mean episode length: 249.97
    Episode_Reward/reaching_object: 0.2664
     Episode_Reward/lifting_object: -1.5638
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0065
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 1.91s
                      Time elapsed: 00:01:29
                               ETA: 02:27:23

################################################################################
                      [1m Learning iteration 25/2500 [0m                      

                       Computation: 51754 steps/s (collection: 1.799s, learning 0.100s)
             Mean action noise std: 1.05
          Mean value_function loss: 11.5982
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 45.3179
                       Mean reward: -2.22
               Mean episode length: 249.94
    Episode_Reward/reaching_object: 0.2963
     Episode_Reward/lifting_object: -0.9379
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 1.90s
                      Time elapsed: 00:01:31
                               ETA: 02:24:40

################################################################################
                      [1m Learning iteration 26/2500 [0m                      

                       Computation: 50348 steps/s (collection: 1.861s, learning 0.092s)
             Mean action noise std: 1.05
          Mean value_function loss: 15.4720
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 45.4850
                       Mean reward: -6.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3369
     Episode_Reward/lifting_object: -1.0664
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0066
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 1.95s
                      Time elapsed: 00:01:33
                               ETA: 02:22:14

################################################################################
                      [1m Learning iteration 27/2500 [0m                      

                       Computation: 51980 steps/s (collection: 1.802s, learning 0.090s)
             Mean action noise std: 1.06
          Mean value_function loss: 13.4279
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 45.6312
                       Mean reward: 0.40
               Mean episode length: 249.91
    Episode_Reward/reaching_object: 0.3662
     Episode_Reward/lifting_object: -0.9796
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0096
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 1.89s
                      Time elapsed: 00:01:35
                               ETA: 02:19:53

################################################################################
                      [1m Learning iteration 28/2500 [0m                      

                       Computation: 51885 steps/s (collection: 1.798s, learning 0.097s)
             Mean action noise std: 1.06
          Mean value_function loss: 15.0229
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 45.8008
                       Mean reward: -2.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3725
     Episode_Reward/lifting_object: -1.0696
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0067
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 1.89s
                      Time elapsed: 00:01:36
                               ETA: 02:17:42

################################################################################
                      [1m Learning iteration 29/2500 [0m                      

                       Computation: 52531 steps/s (collection: 1.783s, learning 0.089s)
             Mean action noise std: 1.07
          Mean value_function loss: 6.9255
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 45.9695
                       Mean reward: 0.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3825
     Episode_Reward/lifting_object: -0.9426
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 1.87s
                      Time elapsed: 00:01:38
                               ETA: 02:15:37

################################################################################
                      [1m Learning iteration 30/2500 [0m                      

                       Computation: 53436 steps/s (collection: 1.743s, learning 0.097s)
             Mean action noise std: 1.07
          Mean value_function loss: 0.4027
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 46.1221
                       Mean reward: 1.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3491
     Episode_Reward/lifting_object: -0.4599
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0068
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 1.84s
                      Time elapsed: 00:01:40
                               ETA: 02:13:38

################################################################################
                      [1m Learning iteration 31/2500 [0m                      

                       Computation: 50318 steps/s (collection: 1.844s, learning 0.110s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.0067
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 46.2676
                       Mean reward: 1.55
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3072
     Episode_Reward/lifting_object: -0.0111
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0069
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 1.95s
                      Time elapsed: 00:01:42
                               ETA: 02:11:55

################################################################################
                      [1m Learning iteration 32/2500 [0m                      

                       Computation: 51421 steps/s (collection: 1.820s, learning 0.092s)
             Mean action noise std: 1.08
          Mean value_function loss: 0.0241
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 46.3830
                       Mean reward: 1.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2836
     Episode_Reward/lifting_object: -0.0093
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0070
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 1.91s
                      Time elapsed: 00:01:44
                               ETA: 02:10:15

################################################################################
                      [1m Learning iteration 33/2500 [0m                      

                       Computation: 54976 steps/s (collection: 1.675s, learning 0.114s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0073
                 Mean entropy loss: 46.4804
                       Mean reward: 0.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2366
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0071
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 1.79s
                      Time elapsed: 00:01:46
                               ETA: 02:08:32

################################################################################
                      [1m Learning iteration 34/2500 [0m                      

                       Computation: 55630 steps/s (collection: 1.657s, learning 0.111s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0009
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 46.6386
                       Mean reward: 0.83
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1857
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 1.77s
                      Time elapsed: 00:01:48
                               ETA: 02:06:53

################################################################################
                      [1m Learning iteration 35/2500 [0m                      

                       Computation: 55055 steps/s (collection: 1.686s, learning 0.099s)
             Mean action noise std: 1.09
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0076
                 Mean entropy loss: 46.7694
                       Mean reward: 0.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1621
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0072
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 1.79s
                      Time elapsed: 00:01:49
                               ETA: 02:05:21

################################################################################
                      [1m Learning iteration 36/2500 [0m                      

                       Computation: 55740 steps/s (collection: 1.663s, learning 0.101s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0061
                 Mean entropy loss: 46.8816
                       Mean reward: 0.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1216
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0073
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 1.76s
                      Time elapsed: 00:01:51
                               ETA: 02:03:52

################################################################################
                      [1m Learning iteration 37/2500 [0m                      

                       Computation: 54681 steps/s (collection: 1.694s, learning 0.104s)
             Mean action noise std: 1.10
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0091
                 Mean entropy loss: 46.9736
                       Mean reward: 0.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1105
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0074
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 1.80s
                      Time elapsed: 00:01:53
                               ETA: 02:02:30

################################################################################
                      [1m Learning iteration 38/2500 [0m                      

                       Computation: 56007 steps/s (collection: 1.656s, learning 0.099s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 47.0954
                       Mean reward: 0.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1019
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 1.76s
                      Time elapsed: 00:01:55
                               ETA: 02:01:09

################################################################################
                      [1m Learning iteration 39/2500 [0m                      

                       Computation: 55538 steps/s (collection: 1.677s, learning 0.094s)
             Mean action noise std: 1.11
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 47.2083
                       Mean reward: 0.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0883
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0075
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 1.77s
                      Time elapsed: 00:01:56
                               ETA: 01:59:54

################################################################################
                      [1m Learning iteration 40/2500 [0m                      

                       Computation: 56986 steps/s (collection: 1.633s, learning 0.093s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 47.3354
                       Mean reward: 0.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0842
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0076
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 1.73s
                      Time elapsed: 00:01:58
                               ETA: 01:58:39

################################################################################
                      [1m Learning iteration 41/2500 [0m                      

                       Computation: 55258 steps/s (collection: 1.685s, learning 0.094s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0102
                 Mean entropy loss: 47.4058
                       Mean reward: 0.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0826
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 1.78s
                      Time elapsed: 00:02:00
                               ETA: 01:57:31

################################################################################
                      [1m Learning iteration 42/2500 [0m                      

                       Computation: 56402 steps/s (collection: 1.634s, learning 0.109s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0108
                 Mean entropy loss: 47.4763
                       Mean reward: 0.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0803
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0077
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 1.74s
                      Time elapsed: 00:02:02
                               ETA: 01:56:24

################################################################################
                      [1m Learning iteration 43/2500 [0m                      

                       Computation: 56195 steps/s (collection: 1.660s, learning 0.090s)
             Mean action noise std: 1.12
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 47.5359
                       Mean reward: 0.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0852
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 1.75s
                      Time elapsed: 00:02:03
                               ETA: 01:55:20

################################################################################
                      [1m Learning iteration 44/2500 [0m                      

                       Computation: 56328 steps/s (collection: 1.640s, learning 0.105s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0005
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 47.6090
                       Mean reward: 0.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0912
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0078
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 1.75s
                      Time elapsed: 00:02:05
                               ETA: 01:54:18

################################################################################
                      [1m Learning iteration 45/2500 [0m                      

                       Computation: 54978 steps/s (collection: 1.678s, learning 0.110s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0007
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 47.6665
                       Mean reward: 0.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1069
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 1.79s
                      Time elapsed: 00:02:07
                               ETA: 01:53:22

################################################################################
                      [1m Learning iteration 46/2500 [0m                      

                       Computation: 55658 steps/s (collection: 1.646s, learning 0.120s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0011
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 47.7076
                       Mean reward: 0.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1188
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0079
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 1.77s
                      Time elapsed: 00:02:09
                               ETA: 01:52:27

################################################################################
                      [1m Learning iteration 47/2500 [0m                      

                       Computation: 55352 steps/s (collection: 1.673s, learning 0.103s)
             Mean action noise std: 1.13
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0092
                 Mean entropy loss: 47.7498
                       Mean reward: 0.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1408
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 1.78s
                      Time elapsed: 00:02:11
                               ETA: 01:51:34

################################################################################
                      [1m Learning iteration 48/2500 [0m                      

                       Computation: 52097 steps/s (collection: 1.765s, learning 0.122s)
             Mean action noise std: 1.13
          Mean value_function loss: 10.7853
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 47.8190
                       Mean reward: 0.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1802
     Episode_Reward/lifting_object: -0.0444
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 1.89s
                      Time elapsed: 00:02:12
                               ETA: 01:50:49

################################################################################
                      [1m Learning iteration 49/2500 [0m                      

                       Computation: 50596 steps/s (collection: 1.822s, learning 0.121s)
             Mean action noise std: 1.14
          Mean value_function loss: 31.4493
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 47.8917
                       Mean reward: -7.16
               Mean episode length: 249.81
    Episode_Reward/reaching_object: 0.2250
     Episode_Reward/lifting_object: -1.1809
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0080
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 1.94s
                      Time elapsed: 00:02:14
                               ETA: 01:50:09

################################################################################
                      [1m Learning iteration 50/2500 [0m                      

                       Computation: 51427 steps/s (collection: 1.797s, learning 0.114s)
             Mean action noise std: 1.14
          Mean value_function loss: 36.7910
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 48.0424
                       Mean reward: -2.28
               Mean episode length: 249.94
    Episode_Reward/reaching_object: 0.2834
     Episode_Reward/lifting_object: -1.4328
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 1.91s
                      Time elapsed: 00:02:16
                               ETA: 01:49:29

################################################################################
                      [1m Learning iteration 51/2500 [0m                      

                       Computation: 50517 steps/s (collection: 1.834s, learning 0.112s)
             Mean action noise std: 1.15
          Mean value_function loss: 97.5636
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 48.1407
                       Mean reward: -15.66
               Mean episode length: 249.70
    Episode_Reward/reaching_object: 0.3737
     Episode_Reward/lifting_object: -2.7356
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0081
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 1.95s
                      Time elapsed: 00:02:18
                               ETA: 01:48:51

################################################################################
                      [1m Learning iteration 52/2500 [0m                      

                       Computation: 50901 steps/s (collection: 1.819s, learning 0.112s)
             Mean action noise std: 1.15
          Mean value_function loss: 75.0767
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 48.2671
                       Mean reward: -32.06
               Mean episode length: 248.89
    Episode_Reward/reaching_object: 0.4271
     Episode_Reward/lifting_object: -5.8495
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 1.93s
                      Time elapsed: 00:02:20
                               ETA: 01:48:15

################################################################################
                      [1m Learning iteration 53/2500 [0m                      

                       Computation: 49989 steps/s (collection: 1.854s, learning 0.112s)
             Mean action noise std: 1.15
          Mean value_function loss: 76.4803
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 48.3626
                       Mean reward: -14.95
               Mean episode length: 249.40
    Episode_Reward/reaching_object: 0.5072
     Episode_Reward/lifting_object: -5.5027
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0082
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7500
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 1.97s
                      Time elapsed: 00:02:22
                               ETA: 01:47:41

################################################################################
                      [1m Learning iteration 54/2500 [0m                      

                       Computation: 49767 steps/s (collection: 1.862s, learning 0.113s)
             Mean action noise std: 1.16
          Mean value_function loss: 70.4180
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 48.4739
                       Mean reward: -18.30
               Mean episode length: 248.80
    Episode_Reward/reaching_object: 0.5262
     Episode_Reward/lifting_object: -5.0899
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 1.98s
                      Time elapsed: 00:02:24
                               ETA: 01:47:09

################################################################################
                      [1m Learning iteration 55/2500 [0m                      

                       Computation: 51367 steps/s (collection: 1.813s, learning 0.101s)
             Mean action noise std: 1.16
          Mean value_function loss: 47.8791
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 48.5559
                       Mean reward: -25.15
               Mean episode length: 249.38
    Episode_Reward/reaching_object: 0.5165
     Episode_Reward/lifting_object: -5.1031
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0083
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 1.91s
                      Time elapsed: 00:02:26
                               ETA: 01:46:35

################################################################################
                      [1m Learning iteration 56/2500 [0m                      

                       Computation: 51451 steps/s (collection: 1.818s, learning 0.093s)
             Mean action noise std: 1.17
          Mean value_function loss: 14.6070
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 48.6693
                       Mean reward: -20.17
               Mean episode length: 249.24
    Episode_Reward/reaching_object: 0.5626
     Episode_Reward/lifting_object: -3.2985
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 1.91s
                      Time elapsed: 00:02:28
                               ETA: 01:46:02

################################################################################
                      [1m Learning iteration 57/2500 [0m                      

                       Computation: 52410 steps/s (collection: 1.747s, learning 0.129s)
             Mean action noise std: 1.17
          Mean value_function loss: 2.5728
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 48.7536
                       Mean reward: 2.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.5081
     Episode_Reward/lifting_object: -0.1737
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0084
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 1.88s
                      Time elapsed: 00:02:30
                               ETA: 01:45:29

################################################################################
                      [1m Learning iteration 58/2500 [0m                      

                       Computation: 53785 steps/s (collection: 1.715s, learning 0.113s)
             Mean action noise std: 1.17
          Mean value_function loss: 3.5981
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 48.8560
                       Mean reward: -0.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4577
     Episode_Reward/lifting_object: -0.5446
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0085
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 1.83s
                      Time elapsed: 00:02:32
                               ETA: 01:44:54

################################################################################
                      [1m Learning iteration 59/2500 [0m                      

                       Computation: 55287 steps/s (collection: 1.667s, learning 0.111s)
             Mean action noise std: 1.18
          Mean value_function loss: 1.0480
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 48.9545
                       Mean reward: 1.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3608
     Episode_Reward/lifting_object: -0.2664
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0086
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 1.78s
                      Time elapsed: 00:02:33
                               ETA: 01:44:19

################################################################################
                      [1m Learning iteration 60/2500 [0m                      

                       Computation: 56505 steps/s (collection: 1.649s, learning 0.091s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0148
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 49.0528
                       Mean reward: 0.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2487
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 1.74s
                      Time elapsed: 00:02:35
                               ETA: 01:43:44

################################################################################
                      [1m Learning iteration 61/2500 [0m                      

                       Computation: 56145 steps/s (collection: 1.635s, learning 0.116s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0092
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 49.1420
                       Mean reward: 0.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2024
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0087
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 1.75s
                      Time elapsed: 00:02:37
                               ETA: 01:43:10

################################################################################
                      [1m Learning iteration 62/2500 [0m                      

                       Computation: 55462 steps/s (collection: 1.668s, learning 0.105s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0053
               Mean surrogate loss: -0.0110
                 Mean entropy loss: 49.2345
                       Mean reward: 0.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1410
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 1.77s
                      Time elapsed: 00:02:39
                               ETA: 01:42:38

################################################################################
                      [1m Learning iteration 63/2500 [0m                      

                       Computation: 55045 steps/s (collection: 1.669s, learning 0.117s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 49.3485
                       Mean reward: 0.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0992
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0088
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 1.79s
                      Time elapsed: 00:02:40
                               ETA: 01:42:07

################################################################################
                      [1m Learning iteration 64/2500 [0m                      

                       Computation: 55544 steps/s (collection: 1.665s, learning 0.105s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0022
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 49.4799
                       Mean reward: 0.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0704
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0089
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 1.77s
                      Time elapsed: 00:02:42
                               ETA: 01:41:37

################################################################################
                      [1m Learning iteration 65/2500 [0m                      

                       Computation: 55666 steps/s (collection: 1.654s, learning 0.112s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0016
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 49.5936
                       Mean reward: 0.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0592
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0090
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 1.77s
                      Time elapsed: 00:02:44
                               ETA: 01:41:07

################################################################################
                      [1m Learning iteration 66/2500 [0m                      

                       Computation: 56089 steps/s (collection: 1.652s, learning 0.101s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0098
                 Mean entropy loss: 49.7148
                       Mean reward: 0.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0378
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 1.75s
                      Time elapsed: 00:02:46
                               ETA: 01:40:37

################################################################################
                      [1m Learning iteration 67/2500 [0m                      

                       Computation: 56746 steps/s (collection: 1.642s, learning 0.091s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0015
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 49.8036
                       Mean reward: 0.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0289
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 1.73s
                      Time elapsed: 00:02:47
                               ETA: 01:40:08

################################################################################
                      [1m Learning iteration 68/2500 [0m                      

                       Computation: 55746 steps/s (collection: 1.645s, learning 0.119s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0093
                 Mean entropy loss: 49.8911
                       Mean reward: -0.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0220
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 1.76s
                      Time elapsed: 00:02:49
                               ETA: 01:39:41

################################################################################
                      [1m Learning iteration 69/2500 [0m                      

                       Computation: 55783 steps/s (collection: 1.649s, learning 0.113s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0013
               Mean surrogate loss: -0.0068
                 Mean entropy loss: 50.0069
                       Mean reward: -0.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0172
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 1.76s
                      Time elapsed: 00:02:51
                               ETA: 01:39:14

################################################################################
                      [1m Learning iteration 70/2500 [0m                      

                       Computation: 55546 steps/s (collection: 1.655s, learning 0.115s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0008
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 50.0703
                       Mean reward: -0.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0129
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 1.77s
                      Time elapsed: 00:02:53
                               ETA: 01:38:48

################################################################################
                      [1m Learning iteration 71/2500 [0m                      

                       Computation: 55738 steps/s (collection: 1.663s, learning 0.100s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0085
                 Mean entropy loss: 50.1377
                       Mean reward: -0.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0104
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 1.76s
                      Time elapsed: 00:02:54
                               ETA: 01:38:23

################################################################################
                      [1m Learning iteration 72/2500 [0m                      

                       Computation: 55836 steps/s (collection: 1.655s, learning 0.106s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0084
                 Mean entropy loss: 50.1994
                       Mean reward: -0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0081
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 1.76s
                      Time elapsed: 00:02:56
                               ETA: 01:37:59

################################################################################
                      [1m Learning iteration 73/2500 [0m                      

                       Computation: 55433 steps/s (collection: 1.661s, learning 0.112s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0104
                 Mean entropy loss: 50.2389
                       Mean reward: -0.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0068
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 1.77s
                      Time elapsed: 00:02:58
                               ETA: 01:37:35

################################################################################
                      [1m Learning iteration 74/2500 [0m                      

                       Computation: 55440 steps/s (collection: 1.668s, learning 0.105s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0088
                 Mean entropy loss: 50.2902
                       Mean reward: -0.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0056
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 1.77s
                      Time elapsed: 00:03:00
                               ETA: 01:37:12

################################################################################
                      [1m Learning iteration 75/2500 [0m                      

                       Computation: 56374 steps/s (collection: 1.640s, learning 0.104s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0105
                 Mean entropy loss: 50.3215
                       Mean reward: -0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0049
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 1.74s
                      Time elapsed: 00:03:02
                               ETA: 01:36:48

################################################################################
                      [1m Learning iteration 76/2500 [0m                      

                       Computation: 55478 steps/s (collection: 1.658s, learning 0.114s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0089
                 Mean entropy loss: 50.3875
                       Mean reward: -0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0044
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 1.77s
                      Time elapsed: 00:03:03
                               ETA: 01:36:26

################################################################################
                      [1m Learning iteration 77/2500 [0m                      

                       Computation: 55141 steps/s (collection: 1.672s, learning 0.111s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 50.4157
                       Mean reward: -0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0038
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 1.78s
                      Time elapsed: 00:03:05
                               ETA: 01:36:05

################################################################################
                      [1m Learning iteration 78/2500 [0m                      

                       Computation: 56017 steps/s (collection: 1.665s, learning 0.090s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 50.4495
                       Mean reward: -0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0035
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 1.75s
                      Time elapsed: 00:03:07
                               ETA: 01:35:44

################################################################################
                      [1m Learning iteration 79/2500 [0m                      

                       Computation: 54173 steps/s (collection: 1.702s, learning 0.113s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 50.4559
                       Mean reward: -0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0034
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 1.81s
                      Time elapsed: 00:03:09
                               ETA: 01:35:24

################################################################################
                      [1m Learning iteration 80/2500 [0m                      

                       Computation: 55485 steps/s (collection: 1.656s, learning 0.116s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 50.4662
                       Mean reward: -0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0032
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 1.77s
                      Time elapsed: 00:03:10
                               ETA: 01:35:04

################################################################################
                      [1m Learning iteration 81/2500 [0m                      

                       Computation: 55363 steps/s (collection: 1.660s, learning 0.116s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 50.4491
                       Mean reward: -0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0031
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 1.78s
                      Time elapsed: 00:03:12
                               ETA: 01:34:45

################################################################################
                      [1m Learning iteration 82/2500 [0m                      

                       Computation: 56385 steps/s (collection: 1.644s, learning 0.100s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 50.4591
                       Mean reward: -0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0031
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 1.74s
                      Time elapsed: 00:03:14
                               ETA: 01:34:25

################################################################################
                      [1m Learning iteration 83/2500 [0m                      

                       Computation: 55297 steps/s (collection: 1.668s, learning 0.110s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 50.4445
                       Mean reward: -0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0031
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 1.78s
                      Time elapsed: 00:03:16
                               ETA: 01:34:06

################################################################################
                      [1m Learning iteration 84/2500 [0m                      

                       Computation: 52867 steps/s (collection: 1.759s, learning 0.100s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 50.4427
                       Mean reward: -0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0031
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 1.86s
                      Time elapsed: 00:03:18
                               ETA: 01:33:50

################################################################################
                      [1m Learning iteration 85/2500 [0m                      

                       Computation: 55063 steps/s (collection: 1.669s, learning 0.117s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 50.4029
                       Mean reward: -0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0034
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 1.79s
                      Time elapsed: 00:03:19
                               ETA: 01:33:33

################################################################################
                      [1m Learning iteration 86/2500 [0m                      

                       Computation: 55461 steps/s (collection: 1.661s, learning 0.112s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 50.4118
                       Mean reward: -0.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0031
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 1.77s
                      Time elapsed: 00:03:21
                               ETA: 01:33:15

################################################################################
                      [1m Learning iteration 87/2500 [0m                      

                       Computation: 55122 steps/s (collection: 1.669s, learning 0.115s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 50.3926
                       Mean reward: -0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0033
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 1.78s
                      Time elapsed: 00:03:23
                               ETA: 01:32:58

################################################################################
                      [1m Learning iteration 88/2500 [0m                      

                       Computation: 54960 steps/s (collection: 1.676s, learning 0.113s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 50.3608
                       Mean reward: -0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0033
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 1.79s
                      Time elapsed: 00:03:25
                               ETA: 01:32:41

################################################################################
                      [1m Learning iteration 89/2500 [0m                      

                       Computation: 55415 steps/s (collection: 1.662s, learning 0.112s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 50.3266
                       Mean reward: -0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0034
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 1.77s
                      Time elapsed: 00:03:27
                               ETA: 01:32:25

################################################################################
                      [1m Learning iteration 90/2500 [0m                      

                       Computation: 55194 steps/s (collection: 1.678s, learning 0.103s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0136
                 Mean entropy loss: 50.3199
                       Mean reward: -0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0035
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 1.78s
                      Time elapsed: 00:03:28
                               ETA: 01:32:09

################################################################################
                      [1m Learning iteration 91/2500 [0m                      

                       Computation: 55002 steps/s (collection: 1.676s, learning 0.112s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0142
                 Mean entropy loss: 50.2697
                       Mean reward: -0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0036
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 1.79s
                      Time elapsed: 00:03:30
                               ETA: 01:31:53

################################################################################
                      [1m Learning iteration 92/2500 [0m                      

                       Computation: 54925 steps/s (collection: 1.686s, learning 0.104s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 50.2476
                       Mean reward: -0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0036
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 1.79s
                      Time elapsed: 00:03:32
                               ETA: 01:31:38

################################################################################
                      [1m Learning iteration 93/2500 [0m                      

                       Computation: 54406 steps/s (collection: 1.693s, learning 0.114s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0144
                 Mean entropy loss: 50.1987
                       Mean reward: -0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0036
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 1.81s
                      Time elapsed: 00:03:34
                               ETA: 01:31:24

################################################################################
                      [1m Learning iteration 94/2500 [0m                      

                       Computation: 54898 steps/s (collection: 1.689s, learning 0.101s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 50.1439
                       Mean reward: -0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0037
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 1.79s
                      Time elapsed: 00:03:35
                               ETA: 01:31:09

################################################################################
                      [1m Learning iteration 95/2500 [0m                      

                       Computation: 55029 steps/s (collection: 1.674s, learning 0.113s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 50.0739
                       Mean reward: -0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0038
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 1.79s
                      Time elapsed: 00:03:37
                               ETA: 01:30:54

################################################################################
                      [1m Learning iteration 96/2500 [0m                      

                       Computation: 54412 steps/s (collection: 1.695s, learning 0.112s)
             Mean action noise std: 1.22
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 49.9886
                       Mean reward: -0.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0042
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 1.81s
                      Time elapsed: 00:03:39
                               ETA: 01:30:41

################################################################################
                      [1m Learning iteration 97/2500 [0m                      

                       Computation: 54621 steps/s (collection: 1.687s, learning 0.113s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0141
                 Mean entropy loss: 49.9275
                       Mean reward: -0.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0044
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 1.80s
                      Time elapsed: 00:03:41
                               ETA: 01:30:27

################################################################################
                      [1m Learning iteration 98/2500 [0m                      

                       Computation: 54565 steps/s (collection: 1.667s, learning 0.135s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0145
                 Mean entropy loss: 49.8623
                       Mean reward: -0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0048
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 1.80s
                      Time elapsed: 00:03:43
                               ETA: 01:30:14

################################################################################
                      [1m Learning iteration 99/2500 [0m                      

                       Computation: 54427 steps/s (collection: 1.688s, learning 0.119s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0140
                 Mean entropy loss: 49.7948
                       Mean reward: -0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0048
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 1.81s
                      Time elapsed: 00:03:44
                               ETA: 01:30:01

################################################################################
                     [1m Learning iteration 100/2500 [0m                      

                       Computation: 54296 steps/s (collection: 1.700s, learning 0.111s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 49.7254
                       Mean reward: -0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0054
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 1.81s
                      Time elapsed: 00:03:46
                               ETA: 01:29:48

################################################################################
                     [1m Learning iteration 101/2500 [0m                      

                       Computation: 54143 steps/s (collection: 1.702s, learning 0.114s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0143
                 Mean entropy loss: 49.6421
                       Mean reward: -0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0061
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 1.82s
                      Time elapsed: 00:03:48
                               ETA: 01:29:36

################################################################################
                     [1m Learning iteration 102/2500 [0m                      

                       Computation: 55391 steps/s (collection: 1.672s, learning 0.103s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 49.5913
                       Mean reward: -0.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0066
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 1.77s
                      Time elapsed: 00:03:50
                               ETA: 01:29:23

################################################################################
                     [1m Learning iteration 103/2500 [0m                      

                       Computation: 54931 steps/s (collection: 1.671s, learning 0.118s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0139
                 Mean entropy loss: 49.4970
                       Mean reward: -0.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0076
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 1.79s
                      Time elapsed: 00:03:52
                               ETA: 01:29:10

################################################################################
                     [1m Learning iteration 104/2500 [0m                      

                       Computation: 53638 steps/s (collection: 1.723s, learning 0.110s)
             Mean action noise std: 1.20
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 49.4607
                       Mean reward: -0.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0086
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 1.83s
                      Time elapsed: 00:03:53
                               ETA: 01:28:59

################################################################################
                     [1m Learning iteration 105/2500 [0m                      

                       Computation: 53857 steps/s (collection: 1.717s, learning 0.108s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 49.3809
                       Mean reward: -0.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0096
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 1.83s
                      Time elapsed: 00:03:55
                               ETA: 01:28:47

################################################################################
                     [1m Learning iteration 106/2500 [0m                      

                       Computation: 54677 steps/s (collection: 1.693s, learning 0.105s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0128
                 Mean entropy loss: 49.3124
                       Mean reward: -0.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0109
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 1.80s
                      Time elapsed: 00:03:57
                               ETA: 01:28:36

################################################################################
                     [1m Learning iteration 107/2500 [0m                      

                       Computation: 54891 steps/s (collection: 1.677s, learning 0.114s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 49.2444
                       Mean reward: -0.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0116
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 1.79s
                      Time elapsed: 00:03:59
                               ETA: 01:28:24

################################################################################
                     [1m Learning iteration 108/2500 [0m                      

                       Computation: 54727 steps/s (collection: 1.695s, learning 0.101s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 49.1753
                       Mean reward: -0.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0143
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 1.80s
                      Time elapsed: 00:04:01
                               ETA: 01:28:12

################################################################################
                     [1m Learning iteration 109/2500 [0m                      

                       Computation: 55041 steps/s (collection: 1.677s, learning 0.109s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0117
                 Mean entropy loss: 49.1486
                       Mean reward: -0.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0157
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 1.79s
                      Time elapsed: 00:04:02
                               ETA: 01:28:01

################################################################################
                     [1m Learning iteration 110/2500 [0m                      

                       Computation: 54815 steps/s (collection: 1.695s, learning 0.098s)
             Mean action noise std: 1.19
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0101
                 Mean entropy loss: 49.1360
                       Mean reward: 0.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0187
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 1.79s
                      Time elapsed: 00:04:04
                               ETA: 01:27:50

################################################################################
                     [1m Learning iteration 111/2500 [0m                      

                       Computation: 54818 steps/s (collection: 1.675s, learning 0.118s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0114
                 Mean entropy loss: 49.0950
                       Mean reward: 0.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0213
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 1.79s
                      Time elapsed: 00:04:06
                               ETA: 01:27:39

################################################################################
                     [1m Learning iteration 112/2500 [0m                      

                       Computation: 54461 steps/s (collection: 1.701s, learning 0.104s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 49.0751
                       Mean reward: 0.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0266
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 1.81s
                      Time elapsed: 00:04:08
                               ETA: 01:27:28

################################################################################
                     [1m Learning iteration 113/2500 [0m                      

                       Computation: 54419 steps/s (collection: 1.691s, learning 0.116s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0116
                 Mean entropy loss: 49.0582
                       Mean reward: 0.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0282
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 1.81s
                      Time elapsed: 00:04:10
                               ETA: 01:27:18

################################################################################
                     [1m Learning iteration 114/2500 [0m                      

                       Computation: 54145 steps/s (collection: 1.709s, learning 0.107s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0133
                 Mean entropy loss: 49.0227
                       Mean reward: 0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0337
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 1.82s
                      Time elapsed: 00:04:11
                               ETA: 01:27:08

################################################################################
                     [1m Learning iteration 115/2500 [0m                      

                       Computation: 54218 steps/s (collection: 1.693s, learning 0.120s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 48.9865
                       Mean reward: 0.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0347
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 1.81s
                      Time elapsed: 00:04:13
                               ETA: 01:26:58

################################################################################
                     [1m Learning iteration 116/2500 [0m                      

                       Computation: 55003 steps/s (collection: 1.670s, learning 0.117s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0000
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 48.9426
                       Mean reward: 0.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0398
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 1.79s
                      Time elapsed: 00:04:15
                               ETA: 01:26:47

################################################################################
                     [1m Learning iteration 117/2500 [0m                      

                       Computation: 54357 steps/s (collection: 1.695s, learning 0.113s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0137
                 Mean entropy loss: 48.9128
                       Mean reward: 0.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0455
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 1.81s
                      Time elapsed: 00:04:17
                               ETA: 01:26:38

################################################################################
                     [1m Learning iteration 118/2500 [0m                      

                       Computation: 54094 steps/s (collection: 1.701s, learning 0.117s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 48.8904
                       Mean reward: 0.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0478
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 1.82s
                      Time elapsed: 00:04:19
                               ETA: 01:26:28

################################################################################
                     [1m Learning iteration 119/2500 [0m                      

                       Computation: 54020 steps/s (collection: 1.714s, learning 0.106s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0129
                 Mean entropy loss: 48.8718
                       Mean reward: 0.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0518
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 1.82s
                      Time elapsed: 00:04:21
                               ETA: 01:26:19

################################################################################
                     [1m Learning iteration 120/2500 [0m                      

                       Computation: 55464 steps/s (collection: 1.658s, learning 0.114s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 48.8689
                       Mean reward: 0.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0615
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 1.77s
                      Time elapsed: 00:04:22
                               ETA: 01:26:09

################################################################################
                     [1m Learning iteration 121/2500 [0m                      

                       Computation: 54986 steps/s (collection: 1.675s, learning 0.112s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0001
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 48.8741
                       Mean reward: 0.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0664
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 1.79s
                      Time elapsed: 00:04:24
                               ETA: 01:25:59

################################################################################
                     [1m Learning iteration 122/2500 [0m                      

                       Computation: 55202 steps/s (collection: 1.678s, learning 0.103s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0002
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 48.8570
                       Mean reward: 0.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0765
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 1.78s
                      Time elapsed: 00:04:26
                               ETA: 01:25:50

################################################################################
                     [1m Learning iteration 123/2500 [0m                      

                       Computation: 54750 steps/s (collection: 1.689s, learning 0.107s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0003
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 48.8496
                       Mean reward: 0.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.0864
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 1.80s
                      Time elapsed: 00:04:28
                               ETA: 01:25:40

################################################################################
                     [1m Learning iteration 124/2500 [0m                      

                       Computation: 54742 steps/s (collection: 1.684s, learning 0.112s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0004
               Mean surrogate loss: -0.0120
                 Mean entropy loss: 48.8616
                       Mean reward: 0.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1062
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 1.80s
                      Time elapsed: 00:04:29
                               ETA: 01:25:31

################################################################################
                     [1m Learning iteration 125/2500 [0m                      

                       Computation: 54338 steps/s (collection: 1.691s, learning 0.118s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0006
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 48.8648
                       Mean reward: 0.60
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1247
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 1.81s
                      Time elapsed: 00:04:31
                               ETA: 01:25:22

################################################################################
                     [1m Learning iteration 126/2500 [0m                      

                       Computation: 54949 steps/s (collection: 1.676s, learning 0.113s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0009
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 48.8637
                       Mean reward: 0.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1482
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 1.79s
                      Time elapsed: 00:04:33
                               ETA: 01:25:13

################################################################################
                     [1m Learning iteration 127/2500 [0m                      

                       Computation: 54168 steps/s (collection: 1.694s, learning 0.121s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0014
               Mean surrogate loss: -0.0130
                 Mean entropy loss: 48.8749
                       Mean reward: 0.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.1845
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 1.81s
                      Time elapsed: 00:04:35
                               ETA: 01:25:05

################################################################################
                     [1m Learning iteration 128/2500 [0m                      

                       Computation: 54519 steps/s (collection: 1.689s, learning 0.114s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0124
                 Mean entropy loss: 48.8706
                       Mean reward: 1.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2284
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 1.80s
                      Time elapsed: 00:04:37
                               ETA: 01:24:56

################################################################################
                     [1m Learning iteration 129/2500 [0m                      

                       Computation: 54502 steps/s (collection: 1.692s, learning 0.112s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.0033
               Mean surrogate loss: -0.0126
                 Mean entropy loss: 48.8604
                       Mean reward: 1.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.2693
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 1.80s
                      Time elapsed: 00:04:38
                               ETA: 01:24:48

################################################################################
                     [1m Learning iteration 130/2500 [0m                      

                       Computation: 53093 steps/s (collection: 1.742s, learning 0.110s)
             Mean action noise std: 1.18
          Mean value_function loss: 0.3557
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 48.8841
                       Mean reward: 1.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3290
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0091
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 1.85s
                      Time elapsed: 00:04:40
                               ETA: 01:24:40

################################################################################
                     [1m Learning iteration 131/2500 [0m                      

                       Computation: 52113 steps/s (collection: 1.782s, learning 0.105s)
             Mean action noise std: 1.19
          Mean value_function loss: 6.1304
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 48.9358
                       Mean reward: -0.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.3938
     Episode_Reward/lifting_object: -0.1391
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 1.89s
                      Time elapsed: 00:04:42
                               ETA: 01:24:34

################################################################################
                     [1m Learning iteration 132/2500 [0m                      

                       Computation: 50521 steps/s (collection: 1.840s, learning 0.106s)
             Mean action noise std: 1.19
          Mean value_function loss: 11.8555
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 49.0653
                       Mean reward: 2.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.4894
     Episode_Reward/lifting_object: -0.3090
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 1.95s
                      Time elapsed: 00:04:44
                               ETA: 01:24:28

################################################################################
                     [1m Learning iteration 133/2500 [0m                      

                       Computation: 50629 steps/s (collection: 1.823s, learning 0.118s)
             Mean action noise std: 1.19
          Mean value_function loss: 17.6943
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 49.1487
                       Mean reward: -3.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.5919
     Episode_Reward/lifting_object: -0.6640
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0092
          Episode_Reward/joint_vel: -0.0084
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 1.94s
                      Time elapsed: 00:04:46
                               ETA: 01:24:22

################################################################################
                     [1m Learning iteration 134/2500 [0m                      

                       Computation: 51330 steps/s (collection: 1.800s, learning 0.115s)
             Mean action noise std: 1.20
          Mean value_function loss: 9.3995
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 49.2649
                       Mean reward: -2.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.6591
     Episode_Reward/lifting_object: -1.1197
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0084
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 1.92s
                      Time elapsed: 00:04:48
                               ETA: 01:24:16

################################################################################
                     [1m Learning iteration 135/2500 [0m                      

                       Computation: 52618 steps/s (collection: 1.766s, learning 0.102s)
             Mean action noise std: 1.20
          Mean value_function loss: 3.1838
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 49.3646
                       Mean reward: 3.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7092
     Episode_Reward/lifting_object: -1.0432
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0084
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 1.87s
                      Time elapsed: 00:04:50
                               ETA: 01:24:09

################################################################################
                     [1m Learning iteration 136/2500 [0m                      

                       Computation: 54276 steps/s (collection: 1.700s, learning 0.112s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.2547
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 49.5212
                       Mean reward: -4.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7216
     Episode_Reward/lifting_object: -0.5294
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0093
          Episode_Reward/joint_vel: -0.0085
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 1.81s
                      Time elapsed: 00:04:52
                               ETA: 01:24:02

################################################################################
                     [1m Learning iteration 137/2500 [0m                      

                       Computation: 53977 steps/s (collection: 1.725s, learning 0.097s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0028
               Mean surrogate loss: -0.0132
                 Mean entropy loss: 49.7521
                       Mean reward: 1.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7637
     Episode_Reward/lifting_object: -0.1083
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 1.82s
                      Time elapsed: 00:04:54
                               ETA: 01:23:54

################################################################################
                     [1m Learning iteration 138/2500 [0m                      

                       Computation: 54016 steps/s (collection: 1.694s, learning 0.126s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0154
                 Mean entropy loss: 49.6755
                       Mean reward: 3.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7180
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0094
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 1.82s
                      Time elapsed: 00:04:55
                               ETA: 01:23:47

################################################################################
                     [1m Learning iteration 139/2500 [0m                      

                       Computation: 54657 steps/s (collection: 1.684s, learning 0.114s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0019
               Mean surrogate loss: -0.0165
                 Mean entropy loss: 49.6713
                       Mean reward: 2.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.6592
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0086
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 1.80s
                      Time elapsed: 00:04:57
                               ETA: 01:23:39

################################################################################
                     [1m Learning iteration 140/2500 [0m                      

                       Computation: 54318 steps/s (collection: 1.702s, learning 0.108s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0146
                 Mean entropy loss: 49.6392
                       Mean reward: 2.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.6039
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0095
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 1.81s
                      Time elapsed: 00:04:59
                               ETA: 01:23:32

################################################################################
                     [1m Learning iteration 141/2500 [0m                      

                       Computation: 54480 steps/s (collection: 1.697s, learning 0.108s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.0035
               Mean surrogate loss: -0.0122
                 Mean entropy loss: 49.6091
                       Mean reward: 2.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.5444
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 1.80s
                      Time elapsed: 00:05:01
                               ETA: 01:23:24

################################################################################
                     [1m Learning iteration 142/2500 [0m                      

                       Computation: 52255 steps/s (collection: 1.776s, learning 0.105s)
             Mean action noise std: 1.21
          Mean value_function loss: 0.1069
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 49.6016
                       Mean reward: 2.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.5454
     Episode_Reward/lifting_object: -0.0167
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0096
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 1.88s
                      Time elapsed: 00:05:03
                               ETA: 01:23:18

################################################################################
                     [1m Learning iteration 143/2500 [0m                      

                       Computation: 50642 steps/s (collection: 1.825s, learning 0.116s)
             Mean action noise std: 1.21
          Mean value_function loss: 20.8579
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 49.6304
                       Mean reward: 2.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.5437
     Episode_Reward/lifting_object: -0.1253
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0097
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 1.94s
                      Time elapsed: 00:05:05
                               ETA: 01:23:13

################################################################################
                     [1m Learning iteration 144/2500 [0m                      

                       Computation: 50231 steps/s (collection: 1.845s, learning 0.112s)
             Mean action noise std: 1.22
          Mean value_function loss: 61.6842
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 49.7035
                       Mean reward: -16.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.5968
     Episode_Reward/lifting_object: -1.8750
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 1.96s
                      Time elapsed: 00:05:07
                               ETA: 01:23:08

################################################################################
                     [1m Learning iteration 145/2500 [0m                      

                       Computation: 49504 steps/s (collection: 1.878s, learning 0.108s)
             Mean action noise std: 1.22
          Mean value_function loss: 87.5883
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 49.8385
                       Mean reward: -11.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.6716
     Episode_Reward/lifting_object: -2.4145
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 1.99s
                      Time elapsed: 00:05:09
                               ETA: 01:23:04

################################################################################
                     [1m Learning iteration 146/2500 [0m                      

                       Computation: 50626 steps/s (collection: 1.830s, learning 0.112s)
             Mean action noise std: 1.23
          Mean value_function loss: 43.1910
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 49.9576
                       Mean reward: -29.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7293
     Episode_Reward/lifting_object: -5.1168
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 1.94s
                      Time elapsed: 00:05:10
                               ETA: 01:22:59

################################################################################
                     [1m Learning iteration 147/2500 [0m                      

                       Computation: 51370 steps/s (collection: 1.794s, learning 0.120s)
             Mean action noise std: 1.23
          Mean value_function loss: 8.3824
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 50.0564
                       Mean reward: -27.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7724
     Episode_Reward/lifting_object: -4.1628
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0098
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 1.91s
                      Time elapsed: 00:05:12
                               ETA: 01:22:54

################################################################################
                     [1m Learning iteration 148/2500 [0m                      

                       Computation: 53169 steps/s (collection: 1.733s, learning 0.116s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0901
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.1485
                       Mean reward: 0.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.8636
     Episode_Reward/lifting_object: -0.8950
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0087
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 1.85s
                      Time elapsed: 00:05:14
                               ETA: 01:22:48

################################################################################
                     [1m Learning iteration 149/2500 [0m                      

                       Computation: 54173 steps/s (collection: 1.719s, learning 0.096s)
             Mean action noise std: 1.24
          Mean value_function loss: 0.0038
               Mean surrogate loss: -0.0111
                 Mean entropy loss: 50.2856
                       Mean reward: 1.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.8327
     Episode_Reward/lifting_object: -0.9079
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 1.81s
                      Time elapsed: 00:05:16
                               ETA: 01:22:41

################################################################################
                     [1m Learning iteration 150/2500 [0m                      

                       Computation: 54427 steps/s (collection: 1.683s, learning 0.123s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0023
               Mean surrogate loss: -0.0138
                 Mean entropy loss: 50.2151
                       Mean reward: 3.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.8366
     Episode_Reward/lifting_object: -1.1900
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0099
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 1.81s
                      Time elapsed: 00:05:18
                               ETA: 01:22:34

################################################################################
                     [1m Learning iteration 151/2500 [0m                      

                       Computation: 53828 steps/s (collection: 1.713s, learning 0.113s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0021
               Mean surrogate loss: -0.0147
                 Mean entropy loss: 50.1790
                       Mean reward: 0.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7513
     Episode_Reward/lifting_object: -0.3701
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 1.83s
                      Time elapsed: 00:05:20
                               ETA: 01:22:28

################################################################################
                     [1m Learning iteration 152/2500 [0m                      

                       Computation: 53916 steps/s (collection: 1.714s, learning 0.110s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0025
               Mean surrogate loss: -0.0135
                 Mean entropy loss: 50.1138
                       Mean reward: 3.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7095
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 1.82s
                      Time elapsed: 00:05:22
                               ETA: 01:22:21

################################################################################
                     [1m Learning iteration 153/2500 [0m                      

                       Computation: 53784 steps/s (collection: 1.710s, learning 0.118s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.0036
               Mean surrogate loss: -0.0125
                 Mean entropy loss: 50.0707
                       Mean reward: 2.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.6526
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0100
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 1.83s
                      Time elapsed: 00:05:23
                               ETA: 01:22:15

################################################################################
                     [1m Learning iteration 154/2500 [0m                      

                       Computation: 53756 steps/s (collection: 1.735s, learning 0.094s)
             Mean action noise std: 1.23
          Mean value_function loss: 0.2534
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 50.0575
                       Mean reward: 3.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.6152
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 1.83s
                      Time elapsed: 00:05:25
                               ETA: 01:22:09

################################################################################
                     [1m Learning iteration 155/2500 [0m                      

                       Computation: 51347 steps/s (collection: 1.798s, learning 0.117s)
             Mean action noise std: 1.23
          Mean value_function loss: 1.7725
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 50.1040
                       Mean reward: 2.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.6126
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0101
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 1.91s
                      Time elapsed: 00:05:27
                               ETA: 01:22:04

################################################################################
                     [1m Learning iteration 156/2500 [0m                      

                       Computation: 51018 steps/s (collection: 1.815s, learning 0.112s)
             Mean action noise std: 1.24
          Mean value_function loss: 9.4253
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 50.1595
                       Mean reward: 3.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.6354
     Episode_Reward/lifting_object: -0.2207
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 1.93s
                      Time elapsed: 00:05:29
                               ETA: 01:21:59

################################################################################
                     [1m Learning iteration 157/2500 [0m                      

                       Computation: 50369 steps/s (collection: 1.837s, learning 0.115s)
             Mean action noise std: 1.24
          Mean value_function loss: 18.0130
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 50.2414
                       Mean reward: 1.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.6708
     Episode_Reward/lifting_object: -0.4378
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 1.95s
                      Time elapsed: 00:05:31
                               ETA: 01:21:55

################################################################################
                     [1m Learning iteration 158/2500 [0m                      

                       Computation: 50351 steps/s (collection: 1.838s, learning 0.114s)
             Mean action noise std: 1.24
          Mean value_function loss: 11.0152
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 50.3215
                       Mean reward: 0.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7336
     Episode_Reward/lifting_object: -0.6419
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 1.95s
                      Time elapsed: 00:05:33
                               ETA: 01:21:50

################################################################################
                     [1m Learning iteration 159/2500 [0m                      

                       Computation: 52359 steps/s (collection: 1.771s, learning 0.106s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.2348
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.4321
                       Mean reward: -0.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7683
     Episode_Reward/lifting_object: -0.9654
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0102
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 1.88s
                      Time elapsed: 00:05:35
                               ETA: 01:21:45

################################################################################
                     [1m Learning iteration 160/2500 [0m                      

                       Computation: 54222 steps/s (collection: 1.710s, learning 0.103s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0051
               Mean surrogate loss: -0.0119
                 Mean entropy loss: 50.5842
                       Mean reward: 1.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.8069
     Episode_Reward/lifting_object: -0.8700
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 1.81s
                      Time elapsed: 00:05:37
                               ETA: 01:21:39

################################################################################
                     [1m Learning iteration 161/2500 [0m                      

                       Computation: 54773 steps/s (collection: 1.683s, learning 0.112s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0032
               Mean surrogate loss: -0.0169
                 Mean entropy loss: 50.5060
                       Mean reward: 3.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.8237
     Episode_Reward/lifting_object: -0.4322
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 1.79s
                      Time elapsed: 00:05:38
                               ETA: 01:21:32

################################################################################
                     [1m Learning iteration 162/2500 [0m                      

                       Computation: 54232 steps/s (collection: 1.705s, learning 0.108s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.0333
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 50.4752
                       Mean reward: 3.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7937
     Episode_Reward/lifting_object: -0.1235
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0103
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 1.81s
                      Time elapsed: 00:05:40
                               ETA: 01:21:26

################################################################################
                     [1m Learning iteration 163/2500 [0m                      

                       Computation: 51142 steps/s (collection: 1.810s, learning 0.112s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.6311
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 50.5001
                       Mean reward: 3.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.8001
     Episode_Reward/lifting_object: -0.0833
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 1.92s
                      Time elapsed: 00:05:42
                               ETA: 01:21:22

################################################################################
                     [1m Learning iteration 164/2500 [0m                      

                       Computation: 51655 steps/s (collection: 1.791s, learning 0.112s)
             Mean action noise std: 1.25
          Mean value_function loss: 0.7510
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 50.5769
                       Mean reward: 3.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7785
     Episode_Reward/lifting_object: -0.0104
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 1.90s
                      Time elapsed: 00:05:44
                               ETA: 01:21:17

################################################################################
                     [1m Learning iteration 165/2500 [0m                      

                       Computation: 50685 steps/s (collection: 1.826s, learning 0.113s)
             Mean action noise std: 1.26
          Mean value_function loss: 13.2484
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.6530
                       Mean reward: 3.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7841
     Episode_Reward/lifting_object: -0.0167
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0104
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 1.94s
                      Time elapsed: 00:05:46
                               ETA: 01:21:13

################################################################################
                     [1m Learning iteration 166/2500 [0m                      

                       Computation: 51329 steps/s (collection: 1.808s, learning 0.108s)
             Mean action noise std: 1.26
          Mean value_function loss: 7.0618
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 50.7444
                       Mean reward: 0.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7754
     Episode_Reward/lifting_object: -0.1960
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0088
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 1.92s
                      Time elapsed: 00:05:48
                               ETA: 01:21:09

################################################################################
                     [1m Learning iteration 167/2500 [0m                      

                       Computation: 51371 steps/s (collection: 1.798s, learning 0.116s)
             Mean action noise std: 1.26
          Mean value_function loss: 2.7976
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 50.8328
                       Mean reward: -1.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7905
     Episode_Reward/lifting_object: -0.7293
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0105
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 1.91s
                      Time elapsed: 00:05:50
                               ETA: 01:21:04

################################################################################
                     [1m Learning iteration 168/2500 [0m                      

                       Computation: 52224 steps/s (collection: 1.762s, learning 0.121s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0090
               Mean surrogate loss: -0.0075
                 Mean entropy loss: 50.8956
                       Mean reward: 3.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.7820
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0089
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 1.88s
                      Time elapsed: 00:05:52
                               ETA: 01:20:59

################################################################################
                     [1m Learning iteration 169/2500 [0m                      

                       Computation: 53835 steps/s (collection: 1.712s, learning 0.114s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0051
               Mean surrogate loss: -0.0112
                 Mean entropy loss: 50.8903
                       Mean reward: 3.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.8129
     Episode_Reward/lifting_object: -0.2187
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 1.83s
                      Time elapsed: 00:05:54
                               ETA: 01:20:54

################################################################################
                     [1m Learning iteration 170/2500 [0m                      

                       Computation: 53415 steps/s (collection: 1.730s, learning 0.111s)
             Mean action noise std: 1.26
          Mean value_function loss: 0.0055
               Mean surrogate loss: -0.0103
                 Mean entropy loss: 50.8743
                       Mean reward: 4.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.8304
     Episode_Reward/lifting_object: -0.1321
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0106
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 1.84s
                      Time elapsed: 00:05:55
                               ETA: 01:20:48

################################################################################
                     [1m Learning iteration 171/2500 [0m                      

                       Computation: 51915 steps/s (collection: 1.783s, learning 0.111s)
             Mean action noise std: 1.27
          Mean value_function loss: 0.1770
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 50.8795
                       Mean reward: 4.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.8560
     Episode_Reward/lifting_object: -0.1415
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 1.89s
                      Time elapsed: 00:05:57
                               ETA: 01:20:44

################################################################################
                     [1m Learning iteration 172/2500 [0m                      

                       Computation: 50468 steps/s (collection: 1.836s, learning 0.112s)
             Mean action noise std: 1.27
          Mean value_function loss: 12.0368
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 50.9674
                       Mean reward: 4.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.8367
     Episode_Reward/lifting_object: -0.0139
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0107
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 1.95s
                      Time elapsed: 00:05:59
                               ETA: 01:20:40

################################################################################
                     [1m Learning iteration 173/2500 [0m                      

                       Computation: 50226 steps/s (collection: 1.849s, learning 0.109s)
             Mean action noise std: 1.27
          Mean value_function loss: 23.5021
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 51.0078
                       Mean reward: -2.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9118
     Episode_Reward/lifting_object: -0.5469
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0090
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 1.96s
                      Time elapsed: 00:06:01
                               ETA: 01:20:36

################################################################################
                     [1m Learning iteration 174/2500 [0m                      

                       Computation: 50270 steps/s (collection: 1.862s, learning 0.094s)
             Mean action noise std: 1.27
          Mean value_function loss: 26.3115
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 51.0850
                       Mean reward: 0.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.8750
     Episode_Reward/lifting_object: -1.5153
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0108
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 1.96s
                      Time elapsed: 00:06:03
                               ETA: 01:20:32

################################################################################
                     [1m Learning iteration 175/2500 [0m                      

                       Computation: 51793 steps/s (collection: 1.775s, learning 0.123s)
             Mean action noise std: 1.28
          Mean value_function loss: 0.7267
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.1581
                       Mean reward: -0.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9329
     Episode_Reward/lifting_object: -0.6078
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0091
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 1.90s
                      Time elapsed: 00:06:05
                               ETA: 01:20:28

################################################################################
                     [1m Learning iteration 176/2500 [0m                      

                       Computation: 52892 steps/s (collection: 1.748s, learning 0.111s)
             Mean action noise std: 1.28
          Mean value_function loss: 1.0791
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 51.3398
                       Mean reward: -8.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.8949
     Episode_Reward/lifting_object: -0.8312
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0109
          Episode_Reward/joint_vel: -0.0092
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 1.86s
                      Time elapsed: 00:06:07
                               ETA: 01:20:23

################################################################################
                     [1m Learning iteration 177/2500 [0m                      

                       Computation: 52508 steps/s (collection: 1.758s, learning 0.115s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.1506
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.3819
                       Mean reward: 2.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9033
     Episode_Reward/lifting_object: -0.4268
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 1.87s
                      Time elapsed: 00:06:09
                               ETA: 01:20:18

################################################################################
                     [1m Learning iteration 178/2500 [0m                      

                       Computation: 52543 steps/s (collection: 1.756s, learning 0.114s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.0430
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 51.4026
                       Mean reward: 3.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9119
     Episode_Reward/lifting_object: -0.1361
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0110
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 1.87s
                      Time elapsed: 00:06:11
                               ETA: 01:20:13

################################################################################
                     [1m Learning iteration 179/2500 [0m                      

                       Computation: 50660 steps/s (collection: 1.821s, learning 0.119s)
             Mean action noise std: 1.29
          Mean value_function loss: 0.5926
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 51.4319
                       Mean reward: 3.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.8912
     Episode_Reward/lifting_object: -0.5928
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0093
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 1.94s
                      Time elapsed: 00:06:13
                               ETA: 01:20:10

################################################################################
                     [1m Learning iteration 180/2500 [0m                      

                       Computation: 51665 steps/s (collection: 1.809s, learning 0.094s)
             Mean action noise std: 1.29
          Mean value_function loss: 4.8612
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 51.5219
                       Mean reward: 3.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9063
     Episode_Reward/lifting_object: -0.4311
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 1.90s
                      Time elapsed: 00:06:14
                               ETA: 01:20:05

################################################################################
                     [1m Learning iteration 181/2500 [0m                      

                       Computation: 49981 steps/s (collection: 1.878s, learning 0.089s)
             Mean action noise std: 1.29
          Mean value_function loss: 35.4652
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 51.5549
                       Mean reward: 3.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9237
     Episode_Reward/lifting_object: -0.1665
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0111
          Episode_Reward/joint_vel: -0.0094
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 1.97s
                      Time elapsed: 00:06:16
                               ETA: 01:20:02

################################################################################
                     [1m Learning iteration 182/2500 [0m                      

                       Computation: 49355 steps/s (collection: 1.886s, learning 0.106s)
             Mean action noise std: 1.30
          Mean value_function loss: 17.3760
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 51.6024
                       Mean reward: -2.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9354
     Episode_Reward/lifting_object: -0.8092
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 1.99s
                      Time elapsed: 00:06:18
                               ETA: 01:19:59

################################################################################
                     [1m Learning iteration 183/2500 [0m                      

                       Computation: 51125 steps/s (collection: 1.813s, learning 0.110s)
             Mean action noise std: 1.30
          Mean value_function loss: 3.4315
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 51.6647
                       Mean reward: 1.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9677
     Episode_Reward/lifting_object: -0.8043
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0112
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 1.92s
                      Time elapsed: 00:06:20
                               ETA: 01:19:55

################################################################################
                     [1m Learning iteration 184/2500 [0m                      

                       Computation: 51772 steps/s (collection: 1.791s, learning 0.108s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.1796
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 51.7071
                       Mean reward: 0.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9556
     Episode_Reward/lifting_object: -1.3870
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0095
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 1.90s
                      Time elapsed: 00:06:22
                               ETA: 01:19:51

################################################################################
                     [1m Learning iteration 185/2500 [0m                      

                       Computation: 52147 steps/s (collection: 1.775s, learning 0.111s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.0093
               Mean surrogate loss: -0.0121
                 Mean entropy loss: 51.7363
                       Mean reward: 4.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9629
     Episode_Reward/lifting_object: -0.2353
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0113
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 1.89s
                      Time elapsed: 00:06:24
                               ETA: 01:19:46

################################################################################
                     [1m Learning iteration 186/2500 [0m                      

                       Computation: 52028 steps/s (collection: 1.781s, learning 0.109s)
             Mean action noise std: 1.30
          Mean value_function loss: 0.8719
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 51.7287
                       Mean reward: 4.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9649
     Episode_Reward/lifting_object: -0.6495
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 1.89s
                      Time elapsed: 00:06:26
                               ETA: 01:19:42

################################################################################
                     [1m Learning iteration 187/2500 [0m                      

                       Computation: 50775 steps/s (collection: 1.829s, learning 0.107s)
             Mean action noise std: 1.30
          Mean value_function loss: 2.2286
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 51.7589
                       Mean reward: 0.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9539
     Episode_Reward/lifting_object: -0.3774
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0114
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 1.94s
                      Time elapsed: 00:06:28
                               ETA: 01:19:38

################################################################################
                     [1m Learning iteration 188/2500 [0m                      

                       Computation: 49533 steps/s (collection: 1.871s, learning 0.114s)
             Mean action noise std: 1.31
          Mean value_function loss: 8.3088
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 51.9438
                       Mean reward: 3.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9834
     Episode_Reward/lifting_object: -0.0996
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 1.98s
                      Time elapsed: 00:06:30
                               ETA: 01:19:35

################################################################################
                     [1m Learning iteration 189/2500 [0m                      

                       Computation: 49491 steps/s (collection: 1.874s, learning 0.113s)
             Mean action noise std: 1.31
          Mean value_function loss: 13.5395
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.0085
                       Mean reward: -1.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9897
     Episode_Reward/lifting_object: -0.4223
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0097
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 1.99s
                      Time elapsed: 00:06:32
                               ETA: 01:19:32

################################################################################
                     [1m Learning iteration 190/2500 [0m                      

                       Computation: 51094 steps/s (collection: 1.826s, learning 0.098s)
             Mean action noise std: 1.32
          Mean value_function loss: 1.9779
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.0872
                       Mean reward: 4.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9637
     Episode_Reward/lifting_object: -0.0769
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0115
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 1.92s
                      Time elapsed: 00:06:34
                               ETA: 01:19:29

################################################################################
                     [1m Learning iteration 191/2500 [0m                      

                       Computation: 51408 steps/s (collection: 1.797s, learning 0.116s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.2942
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.1887
                       Mean reward: -0.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9531
     Episode_Reward/lifting_object: -0.7074
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0098
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 1.91s
                      Time elapsed: 00:06:36
                               ETA: 01:19:25

################################################################################
                     [1m Learning iteration 192/2500 [0m                      

                       Computation: 52107 steps/s (collection: 1.772s, learning 0.115s)
             Mean action noise std: 1.32
          Mean value_function loss: 1.1937
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 52.2525
                       Mean reward: 3.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9141
     Episode_Reward/lifting_object: -0.1798
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0116
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 1.89s
                      Time elapsed: 00:06:38
                               ETA: 01:19:21

################################################################################
                     [1m Learning iteration 193/2500 [0m                      

                       Computation: 51677 steps/s (collection: 1.794s, learning 0.108s)
             Mean action noise std: 1.32
          Mean value_function loss: 0.8462
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 52.2932
                       Mean reward: 2.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9229
     Episode_Reward/lifting_object: -0.2200
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0099
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 1.90s
                      Time elapsed: 00:06:40
                               ETA: 01:19:17

################################################################################
                     [1m Learning iteration 194/2500 [0m                      

                       Computation: 50751 steps/s (collection: 1.826s, learning 0.111s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.5355
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 52.3771
                       Mean reward: 4.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9840
     Episode_Reward/lifting_object: -0.0921
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0117
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 1.94s
                      Time elapsed: 00:06:41
                               ETA: 01:19:13

################################################################################
                     [1m Learning iteration 195/2500 [0m                      

                       Computation: 50107 steps/s (collection: 1.852s, learning 0.110s)
             Mean action noise std: 1.33
          Mean value_function loss: 0.6298
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 52.5321
                       Mean reward: 1.89
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 0.9662
     Episode_Reward/lifting_object: -0.1901
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0118
          Episode_Reward/joint_vel: -0.0100
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 1.96s
                      Time elapsed: 00:06:43
                               ETA: 01:19:10

################################################################################
                     [1m Learning iteration 196/2500 [0m                      

                       Computation: 50139 steps/s (collection: 1.872s, learning 0.089s)
             Mean action noise std: 1.34
          Mean value_function loss: 10.1628
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 52.5739
                       Mean reward: 1.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0104
     Episode_Reward/lifting_object: -0.2764
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 1.96s
                      Time elapsed: 00:06:45
                               ETA: 01:19:07

################################################################################
                     [1m Learning iteration 197/2500 [0m                      

                       Computation: 50154 steps/s (collection: 1.842s, learning 0.118s)
             Mean action noise std: 1.34
          Mean value_function loss: 3.6390
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 52.6277
                       Mean reward: 4.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0044
     Episode_Reward/lifting_object: -0.1914
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0119
          Episode_Reward/joint_vel: -0.0101
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 1.96s
                      Time elapsed: 00:06:47
                               ETA: 01:19:03

################################################################################
                     [1m Learning iteration 198/2500 [0m                      

                       Computation: 50662 steps/s (collection: 1.832s, learning 0.109s)
             Mean action noise std: 1.34
          Mean value_function loss: 2.1900
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 52.6827
                       Mean reward: 4.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0309
     Episode_Reward/lifting_object: -0.3155
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0102
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 1.94s
                      Time elapsed: 00:06:49
                               ETA: 01:19:00

################################################################################
                     [1m Learning iteration 199/2500 [0m                      

                       Computation: 51704 steps/s (collection: 1.791s, learning 0.110s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.3428
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 52.7578
                       Mean reward: 4.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0081
     Episode_Reward/lifting_object: -0.0983
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0120
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 1.90s
                      Time elapsed: 00:06:51
                               ETA: 01:18:56

################################################################################
                     [1m Learning iteration 200/2500 [0m                      

                       Computation: 52075 steps/s (collection: 1.775s, learning 0.113s)
             Mean action noise std: 1.34
          Mean value_function loss: 0.0467
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 52.7947
                       Mean reward: 2.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0007
     Episode_Reward/lifting_object: -0.4760
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0121
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 1.89s
                      Time elapsed: 00:06:53
                               ETA: 01:18:52

################################################################################
                     [1m Learning iteration 201/2500 [0m                      

                       Computation: 50451 steps/s (collection: 1.833s, learning 0.115s)
             Mean action noise std: 1.35
          Mean value_function loss: 4.6947
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 52.8379
                       Mean reward: 3.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0134
     Episode_Reward/lifting_object: -0.3475
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 1.95s
                      Time elapsed: 00:06:55
                               ETA: 01:18:49

################################################################################
                     [1m Learning iteration 202/2500 [0m                      

                       Computation: 49666 steps/s (collection: 1.858s, learning 0.121s)
             Mean action noise std: 1.35
          Mean value_function loss: 4.9594
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 52.8629
                       Mean reward: 3.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0431
     Episode_Reward/lifting_object: -0.2812
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0122
          Episode_Reward/joint_vel: -0.0103
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 1.98s
                      Time elapsed: 00:06:57
                               ETA: 01:18:46

################################################################################
                     [1m Learning iteration 203/2500 [0m                      

                       Computation: 50659 steps/s (collection: 1.826s, learning 0.114s)
             Mean action noise std: 1.35
          Mean value_function loss: 0.9678
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 52.9115
                       Mean reward: 3.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0607
     Episode_Reward/lifting_object: -0.1980
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 1.94s
                      Time elapsed: 00:06:59
                               ETA: 01:18:42

################################################################################
                     [1m Learning iteration 204/2500 [0m                      

                       Computation: 50834 steps/s (collection: 1.826s, learning 0.107s)
             Mean action noise std: 1.35
          Mean value_function loss: 2.0829
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 52.9720
                       Mean reward: 5.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0741
     Episode_Reward/lifting_object: -0.1382
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 1.93s
                      Time elapsed: 00:07:01
                               ETA: 01:18:39

################################################################################
                     [1m Learning iteration 205/2500 [0m                      

                       Computation: 50312 steps/s (collection: 1.842s, learning 0.112s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.3357
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 53.0336
                       Mean reward: 2.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0285
     Episode_Reward/lifting_object: -0.4849
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0123
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 1.95s
                      Time elapsed: 00:07:03
                               ETA: 01:18:36

################################################################################
                     [1m Learning iteration 206/2500 [0m                      

                       Computation: 50547 steps/s (collection: 1.832s, learning 0.112s)
             Mean action noise std: 1.36
          Mean value_function loss: 0.1928
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 53.1492
                       Mean reward: 2.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0308
     Episode_Reward/lifting_object: -0.4859
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 1.94s
                      Time elapsed: 00:07:05
                               ETA: 01:18:32

################################################################################
                     [1m Learning iteration 207/2500 [0m                      

                       Computation: 49883 steps/s (collection: 1.860s, learning 0.111s)
             Mean action noise std: 1.36
          Mean value_function loss: 1.1330
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 53.1958
                       Mean reward: 4.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0409
     Episode_Reward/lifting_object: -0.0434
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0124
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 1.97s
                      Time elapsed: 00:07:07
                               ETA: 01:18:30

################################################################################
                     [1m Learning iteration 208/2500 [0m                      

                       Computation: 50060 steps/s (collection: 1.866s, learning 0.098s)
             Mean action noise std: 1.37
          Mean value_function loss: 1.7779
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 53.2577
                       Mean reward: 4.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0483
     Episode_Reward/lifting_object: -0.0742
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0104
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 1.96s
                      Time elapsed: 00:07:09
                               ETA: 01:18:26

################################################################################
                     [1m Learning iteration 209/2500 [0m                      

                       Computation: 50208 steps/s (collection: 1.867s, learning 0.090s)
             Mean action noise std: 1.37
          Mean value_function loss: 2.9331
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.3444
                       Mean reward: 4.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0602
     Episode_Reward/lifting_object: -0.0945
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 1.96s
                      Time elapsed: 00:07:11
                               ETA: 01:18:23

################################################################################
                     [1m Learning iteration 210/2500 [0m                      

                       Computation: 49396 steps/s (collection: 1.880s, learning 0.110s)
             Mean action noise std: 1.37
          Mean value_function loss: 3.2582
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.3909
                       Mean reward: 4.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0856
     Episode_Reward/lifting_object: -0.1465
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0125
          Episode_Reward/joint_vel: -0.0105
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 1.99s
                      Time elapsed: 00:07:13
                               ETA: 01:18:21

################################################################################
                     [1m Learning iteration 211/2500 [0m                      

                       Computation: 50661 steps/s (collection: 1.817s, learning 0.123s)
             Mean action noise std: 1.37
          Mean value_function loss: 0.0289
               Mean surrogate loss: -0.0069
                 Mean entropy loss: 53.4280
                       Mean reward: 4.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0805
     Episode_Reward/lifting_object: -0.0701
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0126
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 1.94s
                      Time elapsed: 00:07:15
                               ETA: 01:18:17

################################################################################
                     [1m Learning iteration 212/2500 [0m                      

                       Computation: 49958 steps/s (collection: 1.854s, learning 0.114s)
             Mean action noise std: 1.37
          Mean value_function loss: 2.6375
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.4385
                       Mean reward: 0.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0834
     Episode_Reward/lifting_object: -0.2569
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 1.97s
                      Time elapsed: 00:07:17
                               ETA: 01:18:14

################################################################################
                     [1m Learning iteration 213/2500 [0m                      

                       Computation: 50325 steps/s (collection: 1.839s, learning 0.115s)
             Mean action noise std: 1.38
          Mean value_function loss: 2.0017
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.4842
                       Mean reward: 5.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.0507
     Episode_Reward/lifting_object: -0.1782
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0127
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 1.95s
                      Time elapsed: 00:07:19
                               ETA: 01:18:11

################################################################################
                     [1m Learning iteration 214/2500 [0m                      

                       Computation: 50027 steps/s (collection: 1.855s, learning 0.110s)
             Mean action noise std: 1.38
          Mean value_function loss: 0.3555
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 53.5651
                       Mean reward: -0.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1454
     Episode_Reward/lifting_object: -0.3422
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0106
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 1.96s
                      Time elapsed: 00:07:20
                               ETA: 01:18:08

################################################################################
                     [1m Learning iteration 215/2500 [0m                      

                       Computation: 49717 steps/s (collection: 1.867s, learning 0.111s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0726
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 53.6989
                       Mean reward: 5.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1113
     Episode_Reward/lifting_object: -0.1998
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0128
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 1.98s
                      Time elapsed: 00:07:22
                               ETA: 01:18:05

################################################################################
                     [1m Learning iteration 216/2500 [0m                      

                       Computation: 49782 steps/s (collection: 1.863s, learning 0.112s)
             Mean action noise std: 1.39
          Mean value_function loss: 8.4229
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 53.7690
                       Mean reward: 5.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1126
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 1.97s
                      Time elapsed: 00:07:24
                               ETA: 01:18:03

################################################################################
                     [1m Learning iteration 217/2500 [0m                      

                       Computation: 49851 steps/s (collection: 1.861s, learning 0.111s)
             Mean action noise std: 1.39
          Mean value_function loss: 8.0697
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.7897
                       Mean reward: 0.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1617
     Episode_Reward/lifting_object: -0.3798
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0129
          Episode_Reward/joint_vel: -0.0107
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 1.97s
                      Time elapsed: 00:07:26
                               ETA: 01:18:00

################################################################################
                     [1m Learning iteration 218/2500 [0m                      

                       Computation: 49594 steps/s (collection: 1.857s, learning 0.125s)
             Mean action noise std: 1.39
          Mean value_function loss: 4.1889
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 53.8337
                       Mean reward: -1.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1343
     Episode_Reward/lifting_object: -0.5367
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 1.98s
                      Time elapsed: 00:07:28
                               ETA: 01:17:57

################################################################################
                     [1m Learning iteration 219/2500 [0m                      

                       Computation: 50921 steps/s (collection: 1.841s, learning 0.090s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.1434
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 53.8796
                       Mean reward: 4.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1072
     Episode_Reward/lifting_object: -0.2648
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0130
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 1.93s
                      Time elapsed: 00:07:30
                               ETA: 01:17:54

################################################################################
                     [1m Learning iteration 220/2500 [0m                      

                       Computation: 50993 steps/s (collection: 1.835s, learning 0.093s)
             Mean action noise std: 1.39
          Mean value_function loss: 0.0133
               Mean surrogate loss: -0.0097
                 Mean entropy loss: 53.8890
                       Mean reward: 4.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1258
     Episode_Reward/lifting_object: -0.2546
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 1.93s
                      Time elapsed: 00:07:32
                               ETA: 01:17:50

################################################################################
                     [1m Learning iteration 221/2500 [0m                      

                       Computation: 49848 steps/s (collection: 1.864s, learning 0.108s)
             Mean action noise std: 1.39
          Mean value_function loss: 2.8010
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 53.8836
                       Mean reward: 5.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1107
     Episode_Reward/lifting_object: -0.1322
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0131
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 1.97s
                      Time elapsed: 00:07:34
                               ETA: 01:17:48

################################################################################
                     [1m Learning iteration 222/2500 [0m                      

                       Computation: 49801 steps/s (collection: 1.872s, learning 0.101s)
             Mean action noise std: 1.39
          Mean value_function loss: 3.5718
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 53.9018
                       Mean reward: 3.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1044
     Episode_Reward/lifting_object: -0.1062
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 1.97s
                      Time elapsed: 00:07:36
                               ETA: 01:17:45

################################################################################
                     [1m Learning iteration 223/2500 [0m                      

                       Computation: 45583 steps/s (collection: 1.995s, learning 0.162s)
             Mean action noise std: 1.40
          Mean value_function loss: 7.9650
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 53.9498
                       Mean reward: 5.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1266
     Episode_Reward/lifting_object: -0.1056
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0108
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 2.16s
                      Time elapsed: 00:07:38
                               ETA: 01:17:44

################################################################################
                     [1m Learning iteration 224/2500 [0m                      

                       Computation: 48325 steps/s (collection: 1.934s, learning 0.100s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.2747
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 53.9886
                       Mean reward: 4.64
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1312
     Episode_Reward/lifting_object: -0.2689
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0132
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 2.03s
                      Time elapsed: 00:07:40
                               ETA: 01:17:42

################################################################################
                     [1m Learning iteration 225/2500 [0m                      

                       Computation: 49112 steps/s (collection: 1.903s, learning 0.099s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.2514
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.0547
                       Mean reward: 5.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1389
     Episode_Reward/lifting_object: -0.1061
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0133
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 2.00s
                      Time elapsed: 00:07:42
                               ETA: 01:17:39

################################################################################
                     [1m Learning iteration 226/2500 [0m                      

                       Computation: 49737 steps/s (collection: 1.874s, learning 0.103s)
             Mean action noise std: 1.40
          Mean value_function loss: 0.3769
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 54.1189
                       Mean reward: 5.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1178
     Episode_Reward/lifting_object: -0.0155
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 1.98s
                      Time elapsed: 00:07:44
                               ETA: 01:17:36

################################################################################
                     [1m Learning iteration 227/2500 [0m                      

                       Computation: 47761 steps/s (collection: 1.963s, learning 0.096s)
             Mean action noise std: 1.41
          Mean value_function loss: 0.8302
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 54.2295
                       Mean reward: 5.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1480
     Episode_Reward/lifting_object: -0.0288
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 2.06s
                      Time elapsed: 00:07:46
                               ETA: 01:17:34

################################################################################
                     [1m Learning iteration 228/2500 [0m                      

                       Computation: 46004 steps/s (collection: 2.030s, learning 0.107s)
             Mean action noise std: 1.41
          Mean value_function loss: 1.4423
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 54.2939
                       Mean reward: 5.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1704
     Episode_Reward/lifting_object: -0.1518
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0134
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 2.14s
                      Time elapsed: 00:07:49
                               ETA: 01:17:33

################################################################################
                     [1m Learning iteration 229/2500 [0m                      

                       Computation: 48205 steps/s (collection: 1.933s, learning 0.107s)
             Mean action noise std: 1.42
          Mean value_function loss: 3.9707
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 54.4066
                       Mean reward: 5.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1448
     Episode_Reward/lifting_object: -0.5679
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0109
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 2.04s
                      Time elapsed: 00:07:51
                               ETA: 01:17:31

################################################################################
                     [1m Learning iteration 230/2500 [0m                      

                       Computation: 47078 steps/s (collection: 1.970s, learning 0.119s)
             Mean action noise std: 1.42
          Mean value_function loss: 2.5554
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 54.4989
                       Mean reward: 5.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1578
     Episode_Reward/lifting_object: -0.1393
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0135
          Episode_Reward/joint_vel: -0.0110
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 2.09s
                      Time elapsed: 00:07:53
                               ETA: 01:17:29

################################################################################
                     [1m Learning iteration 231/2500 [0m                      

                       Computation: 48701 steps/s (collection: 1.910s, learning 0.109s)
             Mean action noise std: 1.42
          Mean value_function loss: 2.0223
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.5614
                       Mean reward: 5.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1945
     Episode_Reward/lifting_object: -0.2522
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0136
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 2.02s
                      Time elapsed: 00:07:55
                               ETA: 01:17:27

################################################################################
                     [1m Learning iteration 232/2500 [0m                      

                       Computation: 48801 steps/s (collection: 1.905s, learning 0.110s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.2904
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 54.5983
                       Mean reward: 4.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1856
     Episode_Reward/lifting_object: -0.2871
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 2.01s
                      Time elapsed: 00:07:57
                               ETA: 01:17:25

################################################################################
                     [1m Learning iteration 233/2500 [0m                      

                       Computation: 47751 steps/s (collection: 1.945s, learning 0.114s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.6459
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 54.6521
                       Mean reward: 5.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.2039
     Episode_Reward/lifting_object: -0.1905
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0137
          Episode_Reward/joint_vel: -0.0111
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 2.06s
                      Time elapsed: 00:07:59
                               ETA: 01:17:23

################################################################################
                     [1m Learning iteration 234/2500 [0m                      

                       Computation: 49835 steps/s (collection: 1.881s, learning 0.092s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.1219
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 54.6739
                       Mean reward: 4.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1690
     Episode_Reward/lifting_object: -0.1038
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 1.97s
                      Time elapsed: 00:08:01
                               ETA: 01:17:20

################################################################################
                     [1m Learning iteration 235/2500 [0m                      

                       Computation: 49800 steps/s (collection: 1.881s, learning 0.093s)
             Mean action noise std: 1.43
          Mean value_function loss: 0.1321
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 54.7472
                       Mean reward: 5.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.2067
     Episode_Reward/lifting_object: -0.2503
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0138
          Episode_Reward/joint_vel: -0.0112
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 1.97s
                      Time elapsed: 00:08:03
                               ETA: 01:17:17

################################################################################
                     [1m Learning iteration 236/2500 [0m                      

                       Computation: 49914 steps/s (collection: 1.872s, learning 0.097s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.4794
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.8591
                       Mean reward: 5.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1818
     Episode_Reward/lifting_object: -0.1082
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0139
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 1.97s
                      Time elapsed: 00:08:05
                               ETA: 01:17:14

################################################################################
                     [1m Learning iteration 237/2500 [0m                      

                       Computation: 48540 steps/s (collection: 1.929s, learning 0.096s)
             Mean action noise std: 1.44
          Mean value_function loss: 5.7686
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 54.8953
                       Mean reward: 5.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.2167
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 2.03s
                      Time elapsed: 00:08:07
                               ETA: 01:17:12

################################################################################
                     [1m Learning iteration 238/2500 [0m                      

                       Computation: 49796 steps/s (collection: 1.885s, learning 0.089s)
             Mean action noise std: 1.44
          Mean value_function loss: 0.7476
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 54.9252
                       Mean reward: 6.04
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.2053
     Episode_Reward/lifting_object: -0.0238
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0140
          Episode_Reward/joint_vel: -0.0113
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 1.97s
                      Time elapsed: 00:08:09
                               ETA: 01:17:09

################################################################################
                     [1m Learning iteration 239/2500 [0m                      

                       Computation: 47976 steps/s (collection: 1.935s, learning 0.114s)
             Mean action noise std: 1.44
          Mean value_function loss: 1.9437
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.0007
                       Mean reward: 5.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1842
     Episode_Reward/lifting_object: -0.1724
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0141
          Episode_Reward/joint_vel: -0.0114
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 2.05s
                      Time elapsed: 00:08:11
                               ETA: 01:17:07

################################################################################
                     [1m Learning iteration 240/2500 [0m                      

                       Computation: 46215 steps/s (collection: 2.029s, learning 0.098s)
             Mean action noise std: 1.45
          Mean value_function loss: 1.5570
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 55.0337
                       Mean reward: 5.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1502
     Episode_Reward/lifting_object: -0.1233
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 2.13s
                      Time elapsed: 00:08:13
                               ETA: 01:17:06

################################################################################
                     [1m Learning iteration 241/2500 [0m                      

                       Computation: 48966 steps/s (collection: 1.911s, learning 0.097s)
             Mean action noise std: 1.45
          Mean value_function loss: 3.0757
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 55.1181
                       Mean reward: 3.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.2310
     Episode_Reward/lifting_object: -0.1744
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0142
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 2.01s
                      Time elapsed: 00:08:15
                               ETA: 01:17:04

################################################################################
                     [1m Learning iteration 242/2500 [0m                      

                       Computation: 49899 steps/s (collection: 1.859s, learning 0.111s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.1725
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 55.1615
                       Mean reward: 5.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.2110
     Episode_Reward/lifting_object: -0.0451
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0143
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 1.97s
                      Time elapsed: 00:08:17
                               ETA: 01:17:01

################################################################################
                     [1m Learning iteration 243/2500 [0m                      

                       Computation: 49267 steps/s (collection: 1.897s, learning 0.099s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0342
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 55.1987
                       Mean reward: 5.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1986
     Episode_Reward/lifting_object: -0.4522
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 2.00s
                      Time elapsed: 00:08:19
                               ETA: 01:16:58

################################################################################
                     [1m Learning iteration 244/2500 [0m                      

                       Computation: 48769 steps/s (collection: 1.925s, learning 0.091s)
             Mean action noise std: 1.45
          Mean value_function loss: 1.3136
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 55.2134
                       Mean reward: 5.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.2530
     Episode_Reward/lifting_object: -0.1949
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0115
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 2.02s
                      Time elapsed: 00:08:21
                               ETA: 01:16:56

################################################################################
                     [1m Learning iteration 245/2500 [0m                      

                       Computation: 48771 steps/s (collection: 1.904s, learning 0.112s)
             Mean action noise std: 1.45
          Mean value_function loss: 0.0502
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 55.2259
                       Mean reward: 5.63
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.2183
     Episode_Reward/lifting_object: -0.0284
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0144
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 2.02s
                      Time elapsed: 00:08:23
                               ETA: 01:16:54

################################################################################
                     [1m Learning iteration 246/2500 [0m                      

                       Computation: 49635 steps/s (collection: 1.888s, learning 0.092s)
             Mean action noise std: 1.46
          Mean value_function loss: 3.5140
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.2621
                       Mean reward: 3.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1815
     Episode_Reward/lifting_object: -0.0915
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 1.98s
                      Time elapsed: 00:08:25
                               ETA: 01:16:51

################################################################################
                     [1m Learning iteration 247/2500 [0m                      

                       Computation: 47928 steps/s (collection: 1.937s, learning 0.115s)
             Mean action noise std: 1.46
          Mean value_function loss: 1.1216
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.2931
                       Mean reward: 4.05
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.2136
     Episode_Reward/lifting_object: -0.2140
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0145
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 2.05s
                      Time elapsed: 00:08:27
                               ETA: 01:16:49

################################################################################
                     [1m Learning iteration 248/2500 [0m                      

                       Computation: 47346 steps/s (collection: 1.984s, learning 0.093s)
             Mean action noise std: 1.46
          Mean value_function loss: 1.7670
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.3844
                       Mean reward: 5.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.2289
     Episode_Reward/lifting_object: -0.1465
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 2.08s
                      Time elapsed: 00:08:29
                               ETA: 01:16:47

################################################################################
                     [1m Learning iteration 249/2500 [0m                      

                       Computation: 48546 steps/s (collection: 1.928s, learning 0.097s)
             Mean action noise std: 1.46
          Mean value_function loss: 2.4029
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.4208
                       Mean reward: 4.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.1981
     Episode_Reward/lifting_object: -0.1541
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0146
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 2.02s
                      Time elapsed: 00:08:31
                               ETA: 01:16:45

################################################################################
                     [1m Learning iteration 250/2500 [0m                      

                       Computation: 49675 steps/s (collection: 1.890s, learning 0.089s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.4148
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 55.4534
                       Mean reward: 4.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.2381
     Episode_Reward/lifting_object: -0.1679
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0116
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 1.98s
                      Time elapsed: 00:08:33
                               ETA: 01:16:42

################################################################################
                     [1m Learning iteration 251/2500 [0m                      

                       Computation: 48228 steps/s (collection: 1.927s, learning 0.112s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.0642
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 55.5453
                       Mean reward: 6.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.2628
     Episode_Reward/lifting_object: -0.0302
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 2.04s
                      Time elapsed: 00:08:35
                               ETA: 01:16:40

################################################################################
                     [1m Learning iteration 252/2500 [0m                      

                       Computation: 49540 steps/s (collection: 1.892s, learning 0.092s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.4260
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 55.6168
                       Mean reward: 5.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.2806
     Episode_Reward/lifting_object: -0.0786
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0147
          Episode_Reward/joint_vel: -0.0117
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 1.98s
                      Time elapsed: 00:08:37
                               ETA: 01:16:38

################################################################################
                     [1m Learning iteration 253/2500 [0m                      

                       Computation: 49404 steps/s (collection: 1.903s, learning 0.087s)
             Mean action noise std: 1.47
          Mean value_function loss: 0.7951
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 55.6394
                       Mean reward: 6.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.2939
     Episode_Reward/lifting_object: -0.0909
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 1.99s
                      Time elapsed: 00:08:39
                               ETA: 01:16:35

################################################################################
                     [1m Learning iteration 254/2500 [0m                      

                       Computation: 48593 steps/s (collection: 1.936s, learning 0.087s)
             Mean action noise std: 1.48
          Mean value_function loss: 3.3104
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 55.7097
                       Mean reward: 6.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3113
     Episode_Reward/lifting_object: -0.2022
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 2.02s
                      Time elapsed: 00:08:41
                               ETA: 01:16:33

################################################################################
                     [1m Learning iteration 255/2500 [0m                      

                       Computation: 48466 steps/s (collection: 1.913s, learning 0.115s)
             Mean action noise std: 1.48
          Mean value_function loss: 2.2135
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 55.7407
                       Mean reward: 6.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3098
     Episode_Reward/lifting_object: -0.1744
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0148
          Episode_Reward/joint_vel: -0.0118
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 2.03s
                      Time elapsed: 00:08:43
                               ETA: 01:16:31

################################################################################
                     [1m Learning iteration 256/2500 [0m                      

                       Computation: 49592 steps/s (collection: 1.895s, learning 0.087s)
             Mean action noise std: 1.48
          Mean value_function loss: 0.3338
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 55.7723
                       Mean reward: 6.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3323
     Episode_Reward/lifting_object: -0.1623
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 1.98s
                      Time elapsed: 00:08:45
                               ETA: 01:16:28

################################################################################
                     [1m Learning iteration 257/2500 [0m                      

                       Computation: 50191 steps/s (collection: 1.841s, learning 0.118s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.0379
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 55.8519
                       Mean reward: 5.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.2969
     Episode_Reward/lifting_object: -0.1598
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0149
          Episode_Reward/joint_vel: -0.0119
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 1.96s
                      Time elapsed: 00:08:47
                               ETA: 01:16:25

################################################################################
                     [1m Learning iteration 258/2500 [0m                      

                       Computation: 48733 steps/s (collection: 1.908s, learning 0.110s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.6541
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 55.9166
                       Mean reward: 6.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.2988
     Episode_Reward/lifting_object: -0.2688
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 2.02s
                      Time elapsed: 00:08:49
                               ETA: 01:16:23

################################################################################
                     [1m Learning iteration 259/2500 [0m                      

                       Computation: 49104 steps/s (collection: 1.915s, learning 0.087s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.6869
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 55.9552
                       Mean reward: 5.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.2993
     Episode_Reward/lifting_object: -0.2595
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0150
          Episode_Reward/joint_vel: -0.0120
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 2.00s
                      Time elapsed: 00:08:51
                               ETA: 01:16:21

################################################################################
                     [1m Learning iteration 260/2500 [0m                      

                       Computation: 47938 steps/s (collection: 1.935s, learning 0.116s)
             Mean action noise std: 1.49
          Mean value_function loss: 4.2004
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 55.9902
                       Mean reward: 4.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3506
     Episode_Reward/lifting_object: -0.1672
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 2.05s
                      Time elapsed: 00:08:53
                               ETA: 01:16:19

################################################################################
                     [1m Learning iteration 261/2500 [0m                      

                       Computation: 46730 steps/s (collection: 1.994s, learning 0.110s)
             Mean action noise std: 1.49
          Mean value_function loss: 0.8540
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.0165
                       Mean reward: 6.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3290
     Episode_Reward/lifting_object: -0.2005
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 2.10s
                      Time elapsed: 00:08:55
                               ETA: 01:16:17

################################################################################
                     [1m Learning iteration 262/2500 [0m                      

                       Computation: 48255 steps/s (collection: 1.916s, learning 0.121s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.8301
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.0977
                       Mean reward: 6.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3785
     Episode_Reward/lifting_object: -0.0825
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0151
          Episode_Reward/joint_vel: -0.0121
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 2.04s
                      Time elapsed: 00:08:57
                               ETA: 01:16:15

################################################################################
                     [1m Learning iteration 263/2500 [0m                      

                       Computation: 49377 steps/s (collection: 1.895s, learning 0.096s)
             Mean action noise std: 1.50
          Mean value_function loss: 0.4118
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 56.1489
                       Mean reward: 6.23
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3589
     Episode_Reward/lifting_object: -0.0654
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0152
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 1.99s
                      Time elapsed: 00:08:59
                               ETA: 01:16:13

################################################################################
                     [1m Learning iteration 264/2500 [0m                      

                       Computation: 47950 steps/s (collection: 1.948s, learning 0.102s)
             Mean action noise std: 1.50
          Mean value_function loss: 1.4922
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 56.2231
                       Mean reward: 5.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3372
     Episode_Reward/lifting_object: -0.0989
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0123
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 2.05s
                      Time elapsed: 00:09:01
                               ETA: 01:16:11

################################################################################
                     [1m Learning iteration 265/2500 [0m                      

                       Computation: 47977 steps/s (collection: 1.941s, learning 0.108s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.2701
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.2852
                       Mean reward: 5.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3692
     Episode_Reward/lifting_object: -0.0767
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0153
          Episode_Reward/joint_vel: -0.0122
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 2.05s
                      Time elapsed: 00:09:03
                               ETA: 01:16:09

################################################################################
                     [1m Learning iteration 266/2500 [0m                      

                       Computation: 46807 steps/s (collection: 1.989s, learning 0.111s)
             Mean action noise std: 1.51
          Mean value_function loss: 0.4149
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.4004
                       Mean reward: 5.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3630
     Episode_Reward/lifting_object: -0.2133
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 2.10s
                      Time elapsed: 00:09:05
                               ETA: 01:16:07

################################################################################
                     [1m Learning iteration 267/2500 [0m                      

                       Computation: 36151 steps/s (collection: 2.526s, learning 0.194s)
             Mean action noise std: 1.51
          Mean value_function loss: 2.0752
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 56.4889
                       Mean reward: 6.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3746
     Episode_Reward/lifting_object: -0.0333
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0154
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 2.72s
                      Time elapsed: 00:09:08
                               ETA: 01:16:11

################################################################################
                     [1m Learning iteration 268/2500 [0m                      

                       Computation: 36451 steps/s (collection: 2.547s, learning 0.150s)
             Mean action noise std: 1.52
          Mean value_function loss: 4.5819
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 56.5129
                       Mean reward: 3.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3325
     Episode_Reward/lifting_object: -0.2479
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 2.70s
                      Time elapsed: 00:09:11
                               ETA: 01:16:14

################################################################################
                     [1m Learning iteration 269/2500 [0m                      

                       Computation: 43047 steps/s (collection: 2.195s, learning 0.089s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.1162
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.5360
                       Mean reward: 5.10
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3351
     Episode_Reward/lifting_object: -0.1701
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0155
          Episode_Reward/joint_vel: -0.0124
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 2.28s
                      Time elapsed: 00:09:13
                               ETA: 01:16:14

################################################################################
                     [1m Learning iteration 270/2500 [0m                      

                       Computation: 48533 steps/s (collection: 1.933s, learning 0.093s)
             Mean action noise std: 1.52
          Mean value_function loss: 0.0467
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 56.5870
                       Mean reward: 4.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3106
     Episode_Reward/lifting_object: -0.2701
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0156
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 2.03s
                      Time elapsed: 00:09:15
                               ETA: 01:16:12

################################################################################
                     [1m Learning iteration 271/2500 [0m                      

                       Computation: 48190 steps/s (collection: 1.927s, learning 0.113s)
             Mean action noise std: 1.52
          Mean value_function loss: 1.4777
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 56.6428
                       Mean reward: 5.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.2945
     Episode_Reward/lifting_object: -0.0363
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0125
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 2.04s
                      Time elapsed: 00:09:17
                               ETA: 01:16:09

################################################################################
                     [1m Learning iteration 272/2500 [0m                      

                       Computation: 47695 steps/s (collection: 1.952s, learning 0.110s)
             Mean action noise std: 1.52
          Mean value_function loss: 1.9817
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.6676
                       Mean reward: 6.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.2879
     Episode_Reward/lifting_object: -0.1886
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0157
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 2.06s
                      Time elapsed: 00:09:19
                               ETA: 01:16:07

################################################################################
                     [1m Learning iteration 273/2500 [0m                      

                       Computation: 46673 steps/s (collection: 1.972s, learning 0.134s)
             Mean action noise std: 1.53
          Mean value_function loss: 1.3857
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 56.7051
                       Mean reward: 5.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3755
     Episode_Reward/lifting_object: -0.0938
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0158
          Episode_Reward/joint_vel: -0.0126
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 2.11s
                      Time elapsed: 00:09:21
                               ETA: 01:16:06

################################################################################
                     [1m Learning iteration 274/2500 [0m                      

                       Computation: 47092 steps/s (collection: 1.943s, learning 0.145s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.3327
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 56.7941
                       Mean reward: 6.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3614
     Episode_Reward/lifting_object: -0.1930
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 2.09s
                      Time elapsed: 00:09:23
                               ETA: 01:16:04

################################################################################
                     [1m Learning iteration 275/2500 [0m                      

                       Computation: 46154 steps/s (collection: 1.982s, learning 0.148s)
             Mean action noise std: 1.53
          Mean value_function loss: 0.6982
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 56.8674
                       Mean reward: 6.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3477
     Episode_Reward/lifting_object: -0.0630
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0159
          Episode_Reward/joint_vel: -0.0127
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 2.13s
                      Time elapsed: 00:09:26
                               ETA: 01:16:03

################################################################################
                     [1m Learning iteration 276/2500 [0m                      

                       Computation: 39407 steps/s (collection: 2.287s, learning 0.208s)
             Mean action noise std: 1.53
          Mean value_function loss: 2.6663
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 56.8879
                       Mean reward: 6.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3750
     Episode_Reward/lifting_object: -0.1634
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 2.49s
                      Time elapsed: 00:09:28
                               ETA: 01:16:04

################################################################################
                     [1m Learning iteration 277/2500 [0m                      

                       Computation: 46661 steps/s (collection: 2.011s, learning 0.096s)
             Mean action noise std: 1.54
          Mean value_function loss: 1.1133
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 56.9523
                       Mean reward: 4.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3415
     Episode_Reward/lifting_object: -0.1484
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0128
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 2.11s
                      Time elapsed: 00:09:30
                               ETA: 01:16:03

################################################################################
                     [1m Learning iteration 278/2500 [0m                      

                       Computation: 48624 steps/s (collection: 1.926s, learning 0.096s)
             Mean action noise std: 1.54
          Mean value_function loss: 1.3515
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 57.0207
                       Mean reward: 4.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3889
     Episode_Reward/lifting_object: -0.1348
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0160
          Episode_Reward/joint_vel: -0.0129
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 2.02s
                      Time elapsed: 00:09:32
                               ETA: 01:16:00

################################################################################
                     [1m Learning iteration 279/2500 [0m                      

                       Computation: 48210 steps/s (collection: 1.930s, learning 0.109s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.2869
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 57.1339
                       Mean reward: 6.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4345
     Episode_Reward/lifting_object: -0.1862
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 2.04s
                      Time elapsed: 00:09:34
                               ETA: 01:15:58

################################################################################
                     [1m Learning iteration 280/2500 [0m                      

                       Computation: 48595 steps/s (collection: 1.911s, learning 0.112s)
             Mean action noise std: 1.55
          Mean value_function loss: 1.1842
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 57.2458
                       Mean reward: 7.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3930
     Episode_Reward/lifting_object: -0.2921
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0161
          Episode_Reward/joint_vel: -0.0130
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 2.02s
                      Time elapsed: 00:09:36
                               ETA: 01:15:56

################################################################################
                     [1m Learning iteration 281/2500 [0m                      

                       Computation: 48531 steps/s (collection: 1.913s, learning 0.113s)
             Mean action noise std: 1.55
          Mean value_function loss: 1.9705
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.2697
                       Mean reward: 6.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4161
     Episode_Reward/lifting_object: -0.0123
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0162
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 2.03s
                      Time elapsed: 00:09:38
                               ETA: 01:15:54

################################################################################
                     [1m Learning iteration 282/2500 [0m                      

                       Computation: 48846 steps/s (collection: 1.907s, learning 0.106s)
             Mean action noise std: 1.55
          Mean value_function loss: 0.0343
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 57.2912
                       Mean reward: 6.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3773
     Episode_Reward/lifting_object: -0.0449
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 2.01s
                      Time elapsed: 00:09:40
                               ETA: 01:15:51

################################################################################
                     [1m Learning iteration 283/2500 [0m                      

                       Computation: 48536 steps/s (collection: 1.918s, learning 0.108s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.6000
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.3378
                       Mean reward: 5.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3744
     Episode_Reward/lifting_object: -0.4139
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0163
          Episode_Reward/joint_vel: -0.0131
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 2.03s
                      Time elapsed: 00:09:42
                               ETA: 01:15:49

################################################################################
                     [1m Learning iteration 284/2500 [0m                      

                       Computation: 48471 steps/s (collection: 1.913s, learning 0.115s)
             Mean action noise std: 1.56
          Mean value_function loss: 0.0932
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 57.4088
                       Mean reward: 5.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3651
     Episode_Reward/lifting_object: -0.0461
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 2.03s
                      Time elapsed: 00:09:44
                               ETA: 01:15:47

################################################################################
                     [1m Learning iteration 285/2500 [0m                      

                       Computation: 47939 steps/s (collection: 1.931s, learning 0.120s)
             Mean action noise std: 1.56
          Mean value_function loss: 1.2966
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 57.4890
                       Mean reward: 6.79
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3986
     Episode_Reward/lifting_object: -0.2518
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0164
          Episode_Reward/joint_vel: -0.0132
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 2.05s
                      Time elapsed: 00:09:46
                               ETA: 01:15:45

################################################################################
                     [1m Learning iteration 286/2500 [0m                      

                       Computation: 47564 steps/s (collection: 1.941s, learning 0.126s)
             Mean action noise std: 1.57
          Mean value_function loss: 1.0111
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 57.5185
                       Mean reward: 5.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3862
     Episode_Reward/lifting_object: -0.1833
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0165
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 2.07s
                      Time elapsed: 00:09:48
                               ETA: 01:15:43

################################################################################
                     [1m Learning iteration 287/2500 [0m                      

                       Computation: 48469 steps/s (collection: 1.921s, learning 0.107s)
             Mean action noise std: 1.57
          Mean value_function loss: 0.3086
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 57.6130
                       Mean reward: 6.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4346
     Episode_Reward/lifting_object: -0.0943
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0166
          Episode_Reward/joint_vel: -0.0133
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 2.03s
                      Time elapsed: 00:09:50
                               ETA: 01:15:40

################################################################################
                     [1m Learning iteration 288/2500 [0m                      

                       Computation: 49060 steps/s (collection: 1.898s, learning 0.106s)
             Mean action noise std: 1.57
          Mean value_function loss: 2.3004
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 57.7087
                       Mean reward: 5.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.3598
     Episode_Reward/lifting_object: -0.2014
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 2.00s
                      Time elapsed: 00:09:52
                               ETA: 01:15:38

################################################################################
                     [1m Learning iteration 289/2500 [0m                      

                       Computation: 49134 steps/s (collection: 1.898s, learning 0.103s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.6714
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 57.7310
                       Mean reward: 6.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4178
     Episode_Reward/lifting_object: -0.0788
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0167
          Episode_Reward/joint_vel: -0.0134
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 2.00s
                      Time elapsed: 00:09:54
                               ETA: 01:15:36

################################################################################
                     [1m Learning iteration 290/2500 [0m                      

                       Computation: 48616 steps/s (collection: 1.905s, learning 0.117s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.5281
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 57.7865
                       Mean reward: 4.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4183
     Episode_Reward/lifting_object: -0.3195
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0168
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 2.02s
                      Time elapsed: 00:09:56
                               ETA: 01:15:33

################################################################################
                     [1m Learning iteration 291/2500 [0m                      

                       Computation: 48736 steps/s (collection: 1.911s, learning 0.107s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.1473
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.8142
                       Mean reward: 5.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4161
     Episode_Reward/lifting_object: -0.0996
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 2.02s
                      Time elapsed: 00:09:59
                               ETA: 01:15:31

################################################################################
                     [1m Learning iteration 292/2500 [0m                      

                       Computation: 48944 steps/s (collection: 1.917s, learning 0.092s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.9469
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 57.8529
                       Mean reward: 6.39
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4260
     Episode_Reward/lifting_object: -0.1003
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 2.01s
                      Time elapsed: 00:10:01
                               ETA: 01:15:29

################################################################################
                     [1m Learning iteration 293/2500 [0m                      

                       Computation: 49434 steps/s (collection: 1.893s, learning 0.095s)
             Mean action noise std: 1.58
          Mean value_function loss: 0.2070
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.8684
                       Mean reward: 7.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4470
     Episode_Reward/lifting_object: -0.0546
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0169
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 1.99s
                      Time elapsed: 00:10:03
                               ETA: 01:15:26

################################################################################
                     [1m Learning iteration 294/2500 [0m                      

                       Computation: 48796 steps/s (collection: 1.922s, learning 0.093s)
             Mean action noise std: 1.58
          Mean value_function loss: 1.6511
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 57.9077
                       Mean reward: 6.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4099
     Episode_Reward/lifting_object: -0.1351
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 2.01s
                      Time elapsed: 00:10:05
                               ETA: 01:15:24

################################################################################
                     [1m Learning iteration 295/2500 [0m                      

                       Computation: 48759 steps/s (collection: 1.901s, learning 0.115s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.8738
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 57.9394
                       Mean reward: 6.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4081
     Episode_Reward/lifting_object: -0.0579
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0170
          Episode_Reward/joint_vel: -0.0135
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 2.02s
                      Time elapsed: 00:10:07
                               ETA: 01:15:22

################################################################################
                     [1m Learning iteration 296/2500 [0m                      

                       Computation: 48788 steps/s (collection: 1.903s, learning 0.112s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.3291
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 57.9804
                       Mean reward: 4.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4055
     Episode_Reward/lifting_object: -0.1586
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0171
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 2.01s
                      Time elapsed: 00:10:09
                               ETA: 01:15:19

################################################################################
                     [1m Learning iteration 297/2500 [0m                      

                       Computation: 48112 steps/s (collection: 1.922s, learning 0.122s)
             Mean action noise std: 1.59
          Mean value_function loss: 1.3016
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 58.0486
                       Mean reward: 6.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4585
     Episode_Reward/lifting_object: -0.0820
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 2.04s
                      Time elapsed: 00:10:11
                               ETA: 01:15:17

################################################################################
                     [1m Learning iteration 298/2500 [0m                      

                       Computation: 48326 steps/s (collection: 1.915s, learning 0.119s)
             Mean action noise std: 1.59
          Mean value_function loss: 0.8808
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.0830
                       Mean reward: 5.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4352
     Episode_Reward/lifting_object: -0.1531
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 2.03s
                      Time elapsed: 00:10:13
                               ETA: 01:15:15

################################################################################
                     [1m Learning iteration 299/2500 [0m                      

                       Computation: 48572 steps/s (collection: 1.923s, learning 0.101s)
             Mean action noise std: 1.59
          Mean value_function loss: 1.7370
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.1105
                       Mean reward: 7.19
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4523
     Episode_Reward/lifting_object: -0.1066
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0172
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 17.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 2.02s
                      Time elapsed: 00:10:15
                               ETA: 01:15:13

################################################################################
                     [1m Learning iteration 300/2500 [0m                      

                       Computation: 48290 steps/s (collection: 1.914s, learning 0.122s)
             Mean action noise std: 1.60
          Mean value_function loss: 1.6388
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 58.1336
                       Mean reward: 6.74
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4526
     Episode_Reward/lifting_object: -0.1222
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0173
          Episode_Reward/joint_vel: -0.0136
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 2.04s
                      Time elapsed: 00:10:17
                               ETA: 01:15:11

################################################################################
                     [1m Learning iteration 301/2500 [0m                      

                       Computation: 48575 steps/s (collection: 1.899s, learning 0.125s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.0832
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 58.1564
                       Mean reward: 6.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4903
     Episode_Reward/lifting_object: -0.0713
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 2.02s
                      Time elapsed: 00:10:19
                               ETA: 01:15:08

################################################################################
                     [1m Learning iteration 302/2500 [0m                      

                       Computation: 49426 steps/s (collection: 1.877s, learning 0.112s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.0796
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 58.2143
                       Mean reward: 5.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4653
     Episode_Reward/lifting_object: -0.2727
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 1.99s
                      Time elapsed: 00:10:21
                               ETA: 01:15:06

################################################################################
                     [1m Learning iteration 303/2500 [0m                      

                       Computation: 48705 steps/s (collection: 1.902s, learning 0.116s)
             Mean action noise std: 1.60
          Mean value_function loss: 0.1498
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 58.2848
                       Mean reward: 6.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4111
     Episode_Reward/lifting_object: -0.0745
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 2.02s
                      Time elapsed: 00:10:23
                               ETA: 01:15:03

################################################################################
                     [1m Learning iteration 304/2500 [0m                      

                       Computation: 48298 steps/s (collection: 1.921s, learning 0.114s)
             Mean action noise std: 1.61
          Mean value_function loss: 1.4166
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 58.3230
                       Mean reward: 6.68
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4560
     Episode_Reward/lifting_object: -0.2935
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0174
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 2.04s
                      Time elapsed: 00:10:25
                               ETA: 01:15:01

################################################################################
                     [1m Learning iteration 305/2500 [0m                      

                       Computation: 47957 steps/s (collection: 1.937s, learning 0.113s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.5094
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.3384
                       Mean reward: 6.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4523
     Episode_Reward/lifting_object: -0.0745
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0137
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 2.05s
                      Time elapsed: 00:10:27
                               ETA: 01:14:59

################################################################################
                     [1m Learning iteration 306/2500 [0m                      

                       Computation: 48760 steps/s (collection: 1.907s, learning 0.109s)
             Mean action noise std: 1.61
          Mean value_function loss: 0.0165
               Mean surrogate loss: -0.0065
                 Mean entropy loss: 58.3763
                       Mean reward: 7.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4560
     Episode_Reward/lifting_object: -0.1162
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0175
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 2.02s
                      Time elapsed: 00:10:29
                               ETA: 01:14:57

################################################################################
                     [1m Learning iteration 307/2500 [0m                      

                       Computation: 49008 steps/s (collection: 1.893s, learning 0.113s)
             Mean action noise std: 1.61
          Mean value_function loss: 1.1119
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 58.4083
                       Mean reward: 6.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4199
     Episode_Reward/lifting_object: -0.1343
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 2.01s
                      Time elapsed: 00:10:31
                               ETA: 01:14:55

################################################################################
                     [1m Learning iteration 308/2500 [0m                      

                       Computation: 47814 steps/s (collection: 1.941s, learning 0.115s)
             Mean action noise std: 1.61
          Mean value_function loss: 1.1296
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.4238
                       Mean reward: 7.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4385
     Episode_Reward/lifting_object: -0.0556
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 2.06s
                      Time elapsed: 00:10:33
                               ETA: 01:14:53

################################################################################
                     [1m Learning iteration 309/2500 [0m                      

                       Computation: 47506 steps/s (collection: 1.957s, learning 0.112s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.2800
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 58.4836
                       Mean reward: 7.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4793
     Episode_Reward/lifting_object: -0.0305
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0176
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 2.07s
                      Time elapsed: 00:10:35
                               ETA: 01:14:51

################################################################################
                     [1m Learning iteration 310/2500 [0m                      

                       Computation: 47924 steps/s (collection: 1.939s, learning 0.112s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.7444
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.5657
                       Mean reward: 6.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4558
     Episode_Reward/lifting_object: -0.1465
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 2.05s
                      Time elapsed: 00:10:37
                               ETA: 01:14:49

################################################################################
                     [1m Learning iteration 311/2500 [0m                      

                       Computation: 47899 steps/s (collection: 1.940s, learning 0.113s)
             Mean action noise std: 1.62
          Mean value_function loss: 1.0086
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 58.5836
                       Mean reward: 6.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4373
     Episode_Reward/lifting_object: -0.0265
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0177
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 2.05s
                      Time elapsed: 00:10:39
                               ETA: 01:14:47

################################################################################
                     [1m Learning iteration 312/2500 [0m                      

                       Computation: 48747 steps/s (collection: 1.903s, learning 0.114s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.5621
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 58.6053
                       Mean reward: 6.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4733
     Episode_Reward/lifting_object: -0.0780
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0138
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 2.02s
                      Time elapsed: 00:10:41
                               ETA: 01:14:44

################################################################################
                     [1m Learning iteration 313/2500 [0m                      

                       Computation: 48189 steps/s (collection: 1.929s, learning 0.111s)
             Mean action noise std: 1.62
          Mean value_function loss: 0.1600
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 58.6679
                       Mean reward: 7.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4632
     Episode_Reward/lifting_object: -0.2841
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0178
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 2.04s
                      Time elapsed: 00:10:43
                               ETA: 01:14:42

################################################################################
                     [1m Learning iteration 314/2500 [0m                      

                       Computation: 47348 steps/s (collection: 1.960s, learning 0.116s)
             Mean action noise std: 1.63
          Mean value_function loss: 1.4630
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 58.7091
                       Mean reward: 7.30
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4910
     Episode_Reward/lifting_object: -0.0342
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 2.08s
                      Time elapsed: 00:10:45
                               ETA: 01:14:40

################################################################################
                     [1m Learning iteration 315/2500 [0m                      

                       Computation: 48240 steps/s (collection: 1.922s, learning 0.116s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.0955
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.7220
                       Mean reward: 7.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4705
     Episode_Reward/lifting_object: -0.2120
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 2.04s
                      Time elapsed: 00:10:47
                               ETA: 01:14:38

################################################################################
                     [1m Learning iteration 316/2500 [0m                      

                       Computation: 47872 steps/s (collection: 1.938s, learning 0.116s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.9815
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 58.7750
                       Mean reward: 7.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4840
     Episode_Reward/lifting_object: -0.1477
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0179
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 2.05s
                      Time elapsed: 00:10:49
                               ETA: 01:14:36

################################################################################
                     [1m Learning iteration 317/2500 [0m                      

                       Computation: 47916 steps/s (collection: 1.940s, learning 0.112s)
             Mean action noise std: 1.63
          Mean value_function loss: 0.1607
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.7957
                       Mean reward: 6.00
               Mean episode length: 249.41
    Episode_Reward/reaching_object: 1.4950
     Episode_Reward/lifting_object: -0.1048
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 2.05s
                      Time elapsed: 00:10:51
                               ETA: 01:14:34

################################################################################
                     [1m Learning iteration 318/2500 [0m                      

                       Computation: 48220 steps/s (collection: 1.928s, learning 0.111s)
             Mean action noise std: 1.63
          Mean value_function loss: 1.2467
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 58.8361
                       Mean reward: 5.18
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.5063
     Episode_Reward/lifting_object: -0.2004
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0180
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 2.04s
                      Time elapsed: 00:10:53
                               ETA: 01:14:32

################################################################################
                     [1m Learning iteration 319/2500 [0m                      

                       Computation: 48316 steps/s (collection: 1.927s, learning 0.108s)
             Mean action noise std: 1.63
          Mean value_function loss: 1.7471
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 58.8529
                       Mean reward: 6.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4710
     Episode_Reward/lifting_object: -0.0781
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0139
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 2.03s
                      Time elapsed: 00:10:55
                               ETA: 01:14:30

################################################################################
                     [1m Learning iteration 320/2500 [0m                      

                       Computation: 48514 steps/s (collection: 1.913s, learning 0.114s)
             Mean action noise std: 1.63
          Mean value_function loss: 2.9227
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 58.8741
                       Mean reward: 5.69
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.5004
     Episode_Reward/lifting_object: -0.2423
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0181
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 2.03s
                      Time elapsed: 00:10:57
                               ETA: 01:14:28

################################################################################
                     [1m Learning iteration 321/2500 [0m                      

                       Computation: 48213 steps/s (collection: 1.924s, learning 0.115s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.0115
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 58.8926
                       Mean reward: 7.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.5075
     Episode_Reward/lifting_object: -0.0762
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0182
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 2.04s
                      Time elapsed: 00:10:59
                               ETA: 01:14:26

################################################################################
                     [1m Learning iteration 322/2500 [0m                      

                       Computation: 48156 steps/s (collection: 1.929s, learning 0.112s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.0930
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 58.9200
                       Mean reward: 7.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.5062
     Episode_Reward/lifting_object: -0.1037
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 2.04s
                      Time elapsed: 00:11:02
                               ETA: 01:14:23

################################################################################
                     [1m Learning iteration 323/2500 [0m                      

                       Computation: 48387 steps/s (collection: 1.924s, learning 0.108s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.4076
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 58.9709
                       Mean reward: 7.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4841
     Episode_Reward/lifting_object: -0.1177
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 2.03s
                      Time elapsed: 00:11:04
                               ETA: 01:14:21

################################################################################
                     [1m Learning iteration 324/2500 [0m                      

                       Computation: 48187 steps/s (collection: 1.930s, learning 0.110s)
             Mean action noise std: 1.64
          Mean value_function loss: 2.6528
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 59.0159
                       Mean reward: 6.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4906
     Episode_Reward/lifting_object: -0.0679
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 2.04s
                      Time elapsed: 00:11:06
                               ETA: 01:14:19

################################################################################
                     [1m Learning iteration 325/2500 [0m                      

                       Computation: 49138 steps/s (collection: 1.888s, learning 0.113s)
             Mean action noise std: 1.64
          Mean value_function loss: 0.4313
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.0408
                       Mean reward: 7.41
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4925
     Episode_Reward/lifting_object: -0.2037
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0183
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 2.00s
                      Time elapsed: 00:11:08
                               ETA: 01:14:17

################################################################################
                     [1m Learning iteration 326/2500 [0m                      

                       Computation: 49198 steps/s (collection: 1.892s, learning 0.106s)
             Mean action noise std: 1.65
          Mean value_function loss: 1.1787
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 59.1032
                       Mean reward: 7.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4840
     Episode_Reward/lifting_object: -0.0305
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0140
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 2.00s
                      Time elapsed: 00:11:10
                               ETA: 01:14:14

################################################################################
                     [1m Learning iteration 327/2500 [0m                      

                       Computation: 48201 steps/s (collection: 1.924s, learning 0.115s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.0926
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.1204
                       Mean reward: 7.44
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.5046
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0184
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 2.04s
                      Time elapsed: 00:11:12
                               ETA: 01:14:12

################################################################################
                     [1m Learning iteration 328/2500 [0m                      

                       Computation: 47932 steps/s (collection: 1.936s, learning 0.115s)
             Mean action noise std: 1.65
          Mean value_function loss: 0.1954
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 59.1829
                       Mean reward: 7.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4923
     Episode_Reward/lifting_object: 0.0020
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 2.05s
                      Time elapsed: 00:11:14
                               ETA: 01:14:10

################################################################################
                     [1m Learning iteration 329/2500 [0m                      

                       Computation: 47320 steps/s (collection: 1.952s, learning 0.125s)
             Mean action noise std: 1.65
          Mean value_function loss: 1.3428
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 59.2398
                       Mean reward: 6.06
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4993
     Episode_Reward/lifting_object: -0.1340
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 2.08s
                      Time elapsed: 00:11:16
                               ETA: 01:14:08

################################################################################
                     [1m Learning iteration 330/2500 [0m                      

                       Computation: 47322 steps/s (collection: 1.965s, learning 0.112s)
             Mean action noise std: 1.65
          Mean value_function loss: 1.9043
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 59.2539
                       Mean reward: 7.49
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.5040
     Episode_Reward/lifting_object: -0.0965
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0141
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 2.08s
                      Time elapsed: 00:11:18
                               ETA: 01:14:07

################################################################################
                     [1m Learning iteration 331/2500 [0m                      

                       Computation: 48731 steps/s (collection: 1.907s, learning 0.111s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.4861
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 59.2735
                       Mean reward: 5.88
               Mean episode length: 248.20
    Episode_Reward/reaching_object: 1.5060
     Episode_Reward/lifting_object: -0.2683
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0185
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 2.02s
                      Time elapsed: 00:11:20
                               ETA: 01:14:04

################################################################################
                     [1m Learning iteration 332/2500 [0m                      

                       Computation: 48460 steps/s (collection: 1.918s, learning 0.111s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.0380
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 59.3244
                       Mean reward: 6.95
               Mean episode length: 249.57
    Episode_Reward/reaching_object: 1.5483
     Episode_Reward/lifting_object: -0.1693
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 2.03s
                      Time elapsed: 00:11:22
                               ETA: 01:14:02

################################################################################
                     [1m Learning iteration 333/2500 [0m                      

                       Computation: 19258 steps/s (collection: 4.949s, learning 0.156s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.0091
               Mean surrogate loss: -0.0090
                 Mean entropy loss: 59.3444
                       Mean reward: 5.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4938
     Episode_Reward/lifting_object: -0.2200
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0186
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 5.10s
                      Time elapsed: 00:11:27
                               ETA: 01:14:20

################################################################################
                     [1m Learning iteration 334/2500 [0m                      

                       Computation: 14401 steps/s (collection: 6.704s, learning 0.122s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.2864
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 59.3421
                       Mean reward: 6.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4656
     Episode_Reward/lifting_object: -0.0934
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0142
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 6.83s
                      Time elapsed: 00:11:34
                               ETA: 01:14:49

################################################################################
                     [1m Learning iteration 335/2500 [0m                      

                       Computation: 14481 steps/s (collection: 6.660s, learning 0.128s)
             Mean action noise std: 1.66
          Mean value_function loss: 0.6232
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 59.3895
                       Mean reward: 6.92
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4958
     Episode_Reward/lifting_object: -0.2344
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0187
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 6.79s
                      Time elapsed: 00:11:41
                               ETA: 01:15:17

################################################################################
                     [1m Learning iteration 336/2500 [0m                      

                       Computation: 14507 steps/s (collection: 6.662s, learning 0.114s)
             Mean action noise std: 1.67
          Mean value_function loss: 8.0718
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 59.4796
                       Mean reward: 6.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4941
     Episode_Reward/lifting_object: -0.0847
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0188
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 6.78s
                      Time elapsed: 00:11:47
                               ETA: 01:15:45

################################################################################
                     [1m Learning iteration 337/2500 [0m                      

                       Computation: 14587 steps/s (collection: 6.608s, learning 0.131s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.3557
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 59.5082
                       Mean reward: 5.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.4945
     Episode_Reward/lifting_object: -0.0587
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 6.74s
                      Time elapsed: 00:11:54
                               ETA: 01:16:13

################################################################################
                     [1m Learning iteration 338/2500 [0m                      

                       Computation: 14505 steps/s (collection: 6.667s, learning 0.110s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.7794
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 59.5604
                       Mean reward: 7.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.5088
     Episode_Reward/lifting_object: -0.0746
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0143
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 6.78s
                      Time elapsed: 00:12:01
                               ETA: 01:16:40

################################################################################
                     [1m Learning iteration 339/2500 [0m                      

                       Computation: 14678 steps/s (collection: 6.548s, learning 0.149s)
             Mean action noise std: 1.67
          Mean value_function loss: 0.3761
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 59.5778
                       Mean reward: 6.93
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.5222
     Episode_Reward/lifting_object: -0.0402
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0144
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 6.70s
                      Time elapsed: 00:12:08
                               ETA: 01:17:07

################################################################################
                     [1m Learning iteration 340/2500 [0m                      

                       Computation: 14147 steps/s (collection: 6.821s, learning 0.128s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.0862
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 59.6321
                       Mean reward: 7.13
               Mean episode length: 249.03
    Episode_Reward/reaching_object: 1.5187
     Episode_Reward/lifting_object: -0.0083
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0189
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 6.95s
                      Time elapsed: 00:12:15
                               ETA: 01:17:35

################################################################################
                     [1m Learning iteration 341/2500 [0m                      

                       Computation: 13295 steps/s (collection: 7.274s, learning 0.120s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.9521
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 59.6846
                       Mean reward: 6.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.5563
     Episode_Reward/lifting_object: -0.0224
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0190
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 7.39s
                      Time elapsed: 00:12:22
                               ETA: 01:18:06

################################################################################
                     [1m Learning iteration 342/2500 [0m                      

                       Computation: 47808 steps/s (collection: 1.946s, learning 0.111s)
             Mean action noise std: 1.68
          Mean value_function loss: 0.3112
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 59.7328
                       Mean reward: 7.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.5032
     Episode_Reward/lifting_object: -0.3220
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 2.06s
                      Time elapsed: 00:12:24
                               ETA: 01:18:03

################################################################################
                     [1m Learning iteration 343/2500 [0m                      

                       Computation: 47920 steps/s (collection: 1.933s, learning 0.119s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.6894
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 59.8138
                       Mean reward: 6.68
               Mean episode length: 248.92
    Episode_Reward/reaching_object: 1.5862
     Episode_Reward/lifting_object: -0.0925
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0191
          Episode_Reward/joint_vel: -0.0145
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 2.05s
                      Time elapsed: 00:12:26
                               ETA: 01:18:00

################################################################################
                     [1m Learning iteration 344/2500 [0m                      

                       Computation: 48498 steps/s (collection: 1.915s, learning 0.112s)
             Mean action noise std: 1.69
          Mean value_function loss: 1.0530
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 59.8550
                       Mean reward: 7.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.5466
     Episode_Reward/lifting_object: -0.1730
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 2.03s
                      Time elapsed: 00:12:28
                               ETA: 01:17:57

################################################################################
                     [1m Learning iteration 345/2500 [0m                      

                       Computation: 47952 steps/s (collection: 1.939s, learning 0.111s)
             Mean action noise std: 1.69
          Mean value_function loss: 0.6664
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 59.8926
                       Mean reward: 2.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.5683
     Episode_Reward/lifting_object: -0.3986
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0192
          Episode_Reward/joint_vel: -0.0146
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 2.05s
                      Time elapsed: 00:12:30
                               ETA: 01:17:55

################################################################################
                     [1m Learning iteration 346/2500 [0m                      

                       Computation: 48150 steps/s (collection: 1.924s, learning 0.117s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.4804
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 59.9944
                       Mean reward: 7.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.5636
     Episode_Reward/lifting_object: -0.2145
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 2.04s
                      Time elapsed: 00:12:32
                               ETA: 01:17:52

################################################################################
                     [1m Learning iteration 347/2500 [0m                      

                       Computation: 48304 steps/s (collection: 1.911s, learning 0.124s)
             Mean action noise std: 1.70
          Mean value_function loss: 0.6973
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 60.1109
                       Mean reward: 6.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.5607
     Episode_Reward/lifting_object: -0.0867
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0193
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 2.04s
                      Time elapsed: 00:12:34
                               ETA: 01:17:49

################################################################################
                     [1m Learning iteration 348/2500 [0m                      

                       Computation: 48387 steps/s (collection: 1.913s, learning 0.119s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.2078
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 60.1776
                       Mean reward: 7.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.5907
     Episode_Reward/lifting_object: -0.1267
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0194
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 2.03s
                      Time elapsed: 00:12:36
                               ETA: 01:17:46

################################################################################
                     [1m Learning iteration 349/2500 [0m                      

                       Computation: 48542 steps/s (collection: 1.911s, learning 0.115s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.3596
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 60.2560
                       Mean reward: 7.76
               Mean episode length: 249.24
    Episode_Reward/reaching_object: 1.5759
     Episode_Reward/lifting_object: -0.0196
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0195
          Episode_Reward/joint_vel: -0.0147
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 2.03s
                      Time elapsed: 00:12:38
                               ETA: 01:17:42

################################################################################
                     [1m Learning iteration 350/2500 [0m                      

                       Computation: 47913 steps/s (collection: 1.939s, learning 0.113s)
             Mean action noise std: 1.71
          Mean value_function loss: 1.5107
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 60.2729
                       Mean reward: 7.36
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.5617
     Episode_Reward/lifting_object: -0.0633
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 2.05s
                      Time elapsed: 00:12:40
                               ETA: 01:17:40

################################################################################
                     [1m Learning iteration 351/2500 [0m                      

                       Computation: 47835 steps/s (collection: 1.943s, learning 0.112s)
             Mean action noise std: 1.71
          Mean value_function loss: 2.3879
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 60.2999
                       Mean reward: 7.17
               Mean episode length: 249.19
    Episode_Reward/reaching_object: 1.5713
     Episode_Reward/lifting_object: -0.1549
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0196
          Episode_Reward/joint_vel: -0.0148
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 2.06s
                      Time elapsed: 00:12:42
                               ETA: 01:17:37

################################################################################
                     [1m Learning iteration 352/2500 [0m                      

                       Computation: 48294 steps/s (collection: 1.919s, learning 0.117s)
             Mean action noise std: 1.71
          Mean value_function loss: 0.3438
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 60.3272
                       Mean reward: 7.79
               Mean episode length: 249.33
    Episode_Reward/reaching_object: 1.5725
     Episode_Reward/lifting_object: -0.0684
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0197
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 2.04s
                      Time elapsed: 00:12:44
                               ETA: 01:17:34

################################################################################
                     [1m Learning iteration 353/2500 [0m                      

                       Computation: 48484 steps/s (collection: 1.913s, learning 0.115s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.3715
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 60.3888
                       Mean reward: 7.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.5690
     Episode_Reward/lifting_object: -0.1022
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 2.03s
                      Time elapsed: 00:12:46
                               ETA: 01:17:31

################################################################################
                     [1m Learning iteration 354/2500 [0m                      

                       Computation: 48319 steps/s (collection: 1.925s, learning 0.109s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.4894
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 60.4174
                       Mean reward: 7.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.5701
     Episode_Reward/lifting_object: -0.0844
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0198
          Episode_Reward/joint_vel: -0.0149
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 2.03s
                      Time elapsed: 00:12:48
                               ETA: 01:17:28

################################################################################
                     [1m Learning iteration 355/2500 [0m                      

                       Computation: 47474 steps/s (collection: 1.955s, learning 0.116s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.7679
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 60.4320
                       Mean reward: 7.94
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6040
     Episode_Reward/lifting_object: -0.1152
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0199
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 2.07s
                      Time elapsed: 00:12:51
                               ETA: 01:17:25

################################################################################
                     [1m Learning iteration 356/2500 [0m                      

                       Computation: 48411 steps/s (collection: 1.913s, learning 0.118s)
             Mean action noise std: 1.72
          Mean value_function loss: 0.1996
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 60.4916
                       Mean reward: 6.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6255
     Episode_Reward/lifting_object: -0.1405
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 2.03s
                      Time elapsed: 00:12:53
                               ETA: 01:17:22

################################################################################
                     [1m Learning iteration 357/2500 [0m                      

                       Computation: 47964 steps/s (collection: 1.940s, learning 0.110s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.2096
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 60.5861
                       Mean reward: 7.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6134
     Episode_Reward/lifting_object: -0.1097
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0200
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 2.05s
                      Time elapsed: 00:12:55
                               ETA: 01:17:19

################################################################################
                     [1m Learning iteration 358/2500 [0m                      

                       Computation: 47961 steps/s (collection: 1.936s, learning 0.114s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.6734
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 60.6521
                       Mean reward: 6.21
               Mean episode length: 248.23
    Episode_Reward/reaching_object: 1.6482
     Episode_Reward/lifting_object: -0.1401
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0201
          Episode_Reward/joint_vel: -0.0150
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 2.05s
                      Time elapsed: 00:12:57
                               ETA: 01:17:16

################################################################################
                     [1m Learning iteration 359/2500 [0m                      

                       Computation: 47671 steps/s (collection: 1.948s, learning 0.115s)
             Mean action noise std: 1.73
          Mean value_function loss: 1.7749
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 60.6707
                       Mean reward: 7.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6151
     Episode_Reward/lifting_object: -0.0755
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 2.06s
                      Time elapsed: 00:12:59
                               ETA: 01:17:14

################################################################################
                     [1m Learning iteration 360/2500 [0m                      

                       Computation: 48193 steps/s (collection: 1.920s, learning 0.120s)
             Mean action noise std: 1.73
          Mean value_function loss: 1.1892
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 60.6832
                       Mean reward: 8.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6763
     Episode_Reward/lifting_object: -0.1519
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0202
          Episode_Reward/joint_vel: -0.0151
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 2.04s
                      Time elapsed: 00:13:01
                               ETA: 01:17:11

################################################################################
                     [1m Learning iteration 361/2500 [0m                      

                       Computation: 47858 steps/s (collection: 1.937s, learning 0.117s)
             Mean action noise std: 1.73
          Mean value_function loss: 0.8119
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 60.7025
                       Mean reward: 7.86
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 1.6600
     Episode_Reward/lifting_object: -0.1082
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 2.05s
                      Time elapsed: 00:13:03
                               ETA: 01:17:08

################################################################################
                     [1m Learning iteration 362/2500 [0m                      

                       Computation: 48079 steps/s (collection: 1.932s, learning 0.113s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.1500
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 60.7520
                       Mean reward: 6.62
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6030
     Episode_Reward/lifting_object: -0.1482
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0204
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 2.04s
                      Time elapsed: 00:13:05
                               ETA: 01:17:05

################################################################################
                     [1m Learning iteration 363/2500 [0m                      

                       Computation: 47685 steps/s (collection: 1.948s, learning 0.114s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.0121
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 60.7922
                       Mean reward: 8.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6327
     Episode_Reward/lifting_object: 0.0000
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0203
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 2.06s
                      Time elapsed: 00:13:07
                               ETA: 01:17:02

################################################################################
                     [1m Learning iteration 364/2500 [0m                      

                       Computation: 47848 steps/s (collection: 1.946s, learning 0.108s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.8745
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 60.8088
                       Mean reward: 7.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6079
     Episode_Reward/lifting_object: -0.0996
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 2.05s
                      Time elapsed: 00:13:09
                               ETA: 01:16:59

################################################################################
                     [1m Learning iteration 365/2500 [0m                      

                       Computation: 47733 steps/s (collection: 1.955s, learning 0.104s)
             Mean action noise std: 1.74
          Mean value_function loss: 1.0514
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 60.8227
                       Mean reward: 7.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6213
     Episode_Reward/lifting_object: -0.1508
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 2.06s
                      Time elapsed: 00:13:11
                               ETA: 01:16:57

################################################################################
                     [1m Learning iteration 366/2500 [0m                      

                       Computation: 48080 steps/s (collection: 1.935s, learning 0.110s)
             Mean action noise std: 1.74
          Mean value_function loss: 6.3359
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 60.8571
                       Mean reward: 7.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6483
     Episode_Reward/lifting_object: -0.0143
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 2.04s
                      Time elapsed: 00:13:13
                               ETA: 01:16:54

################################################################################
                     [1m Learning iteration 367/2500 [0m                      

                       Computation: 48056 steps/s (collection: 1.935s, learning 0.111s)
             Mean action noise std: 1.74
          Mean value_function loss: 0.4081
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 60.8760
                       Mean reward: 6.34
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6340
     Episode_Reward/lifting_object: -0.2845
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0205
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 2.05s
                      Time elapsed: 00:13:15
                               ETA: 01:16:51

################################################################################
                     [1m Learning iteration 368/2500 [0m                      

                       Computation: 48211 steps/s (collection: 1.923s, learning 0.116s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.0128
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 60.9216
                       Mean reward: 7.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6321
     Episode_Reward/lifting_object: -0.1843
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 2.04s
                      Time elapsed: 00:13:17
                               ETA: 01:16:48

################################################################################
                     [1m Learning iteration 369/2500 [0m                      

                       Computation: 47726 steps/s (collection: 1.950s, learning 0.110s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.1606
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 60.9482
                       Mean reward: 7.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6571
     Episode_Reward/lifting_object: -0.0351
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 2.06s
                      Time elapsed: 00:13:19
                               ETA: 01:16:45

################################################################################
                     [1m Learning iteration 370/2500 [0m                      

                       Computation: 47293 steps/s (collection: 1.970s, learning 0.109s)
             Mean action noise std: 1.75
          Mean value_function loss: 0.7935
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 61.0056
                       Mean reward: 7.40
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 1.6411
     Episode_Reward/lifting_object: -0.0665
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0206
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 2.08s
                      Time elapsed: 00:13:21
                               ETA: 01:16:43

################################################################################
                     [1m Learning iteration 371/2500 [0m                      

                       Computation: 47038 steps/s (collection: 1.973s, learning 0.117s)
             Mean action noise std: 1.75
          Mean value_function loss: 2.8836
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 61.0676
                       Mean reward: 6.72
               Mean episode length: 249.40
    Episode_Reward/reaching_object: 1.6233
     Episode_Reward/lifting_object: -0.2095
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 2.09s
                      Time elapsed: 00:13:23
                               ETA: 01:16:40

################################################################################
                     [1m Learning iteration 372/2500 [0m                      

                       Computation: 47521 steps/s (collection: 1.953s, learning 0.116s)
             Mean action noise std: 1.75
          Mean value_function loss: 1.5487
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 61.0840
                       Mean reward: 8.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6707
     Episode_Reward/lifting_object: -0.1491
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0207
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 2.07s
                      Time elapsed: 00:13:25
                               ETA: 01:16:37

################################################################################
                     [1m Learning iteration 373/2500 [0m                      

                       Computation: 48042 steps/s (collection: 1.953s, learning 0.093s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.0676
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 61.1022
                       Mean reward: 8.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6756
     Episode_Reward/lifting_object: -0.1636
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 2.05s
                      Time elapsed: 00:13:27
                               ETA: 01:16:35

################################################################################
                     [1m Learning iteration 374/2500 [0m                      

                       Computation: 47635 steps/s (collection: 1.974s, learning 0.089s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.3857
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 61.1519
                       Mean reward: 8.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6734
     Episode_Reward/lifting_object: -0.0382
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 2.06s
                      Time elapsed: 00:13:30
                               ETA: 01:16:32

################################################################################
                     [1m Learning iteration 375/2500 [0m                      

                       Computation: 45447 steps/s (collection: 2.066s, learning 0.097s)
             Mean action noise std: 1.76
          Mean value_function loss: 0.3469
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 61.1949
                       Mean reward: 6.76
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6307
     Episode_Reward/lifting_object: -0.3625
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0208
          Episode_Reward/joint_vel: -0.0152
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 2.16s
                      Time elapsed: 00:13:32
                               ETA: 01:16:30

################################################################################
                     [1m Learning iteration 376/2500 [0m                      

                       Computation: 45856 steps/s (collection: 2.046s, learning 0.098s)
             Mean action noise std: 1.76
          Mean value_function loss: 1.3519
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 61.2602
                       Mean reward: 8.50
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6716
     Episode_Reward/lifting_object: -0.0916
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 2.14s
                      Time elapsed: 00:13:34
                               ETA: 01:16:28

################################################################################
                     [1m Learning iteration 377/2500 [0m                      

                       Computation: 46633 steps/s (collection: 2.013s, learning 0.095s)
             Mean action noise std: 1.77
          Mean value_function loss: 1.8132
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 61.2747
                       Mean reward: 7.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6557
     Episode_Reward/lifting_object: -0.2002
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 2.11s
                      Time elapsed: 00:13:36
                               ETA: 01:16:25

################################################################################
                     [1m Learning iteration 378/2500 [0m                      

                       Computation: 46217 steps/s (collection: 2.009s, learning 0.118s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.0702
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.2887
                       Mean reward: 8.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6671
     Episode_Reward/lifting_object: -0.1568
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0209
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 2.13s
                      Time elapsed: 00:13:38
                               ETA: 01:16:23

################################################################################
                     [1m Learning iteration 379/2500 [0m                      

                       Computation: 46524 steps/s (collection: 2.002s, learning 0.111s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.0309
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 61.3190
                       Mean reward: 8.07
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6733
     Episode_Reward/lifting_object: -0.1532
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0210
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 2.11s
                      Time elapsed: 00:13:40
                               ETA: 01:16:20

################################################################################
                     [1m Learning iteration 380/2500 [0m                      

                       Computation: 46182 steps/s (collection: 2.033s, learning 0.096s)
             Mean action noise std: 1.77
          Mean value_function loss: 1.1055
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 61.3474
                       Mean reward: 6.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6340
     Episode_Reward/lifting_object: -0.1285
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 2.13s
                      Time elapsed: 00:13:42
                               ETA: 01:16:18

################################################################################
                     [1m Learning iteration 381/2500 [0m                      

                       Computation: 46549 steps/s (collection: 2.017s, learning 0.095s)
             Mean action noise std: 1.77
          Mean value_function loss: 0.7439
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 61.3730
                       Mean reward: 7.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6695
     Episode_Reward/lifting_object: -0.2208
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0211
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 2.11s
                      Time elapsed: 00:13:44
                               ETA: 01:16:16

################################################################################
                     [1m Learning iteration 382/2500 [0m                      

                       Computation: 47338 steps/s (collection: 1.980s, learning 0.097s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.4182
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 61.4587
                       Mean reward: 8.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6745
     Episode_Reward/lifting_object: -0.0049
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 17.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 2.08s
                      Time elapsed: 00:13:47
                               ETA: 01:16:13

################################################################################
                     [1m Learning iteration 383/2500 [0m                      

                       Computation: 46744 steps/s (collection: 2.014s, learning 0.089s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.4029
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 61.5578
                       Mean reward: 7.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6585
     Episode_Reward/lifting_object: -0.0476
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0212
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 2.10s
                      Time elapsed: 00:13:49
                               ETA: 01:16:11

################################################################################
                     [1m Learning iteration 384/2500 [0m                      

                       Computation: 46059 steps/s (collection: 2.047s, learning 0.087s)
             Mean action noise std: 1.78
          Mean value_function loss: 0.7589
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 61.5913
                       Mean reward: 8.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6969
     Episode_Reward/lifting_object: -0.0980
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 2.13s
                      Time elapsed: 00:13:51
                               ETA: 01:16:08

################################################################################
                     [1m Learning iteration 385/2500 [0m                      

                       Computation: 45713 steps/s (collection: 2.058s, learning 0.092s)
             Mean action noise std: 1.79
          Mean value_function loss: 2.0435
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 61.6245
                       Mean reward: 8.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7340
     Episode_Reward/lifting_object: -0.1510
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0213
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 2.15s
                      Time elapsed: 00:13:53
                               ETA: 01:16:06

################################################################################
                     [1m Learning iteration 386/2500 [0m                      

                       Computation: 45938 steps/s (collection: 2.040s, learning 0.100s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.1619
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 61.6384
                       Mean reward: 7.92
               Mean episode length: 249.89
    Episode_Reward/reaching_object: 1.7221
     Episode_Reward/lifting_object: -0.0903
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0214
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 2.14s
                      Time elapsed: 00:13:55
                               ETA: 01:16:04

################################################################################
                     [1m Learning iteration 387/2500 [0m                      

                       Computation: 47499 steps/s (collection: 1.974s, learning 0.096s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.4127
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 61.6704
                       Mean reward: 6.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7424
     Episode_Reward/lifting_object: -0.1673
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 2.07s
                      Time elapsed: 00:13:57
                               ETA: 01:16:01

################################################################################
                     [1m Learning iteration 388/2500 [0m                      

                       Computation: 47188 steps/s (collection: 1.984s, learning 0.100s)
             Mean action noise std: 1.79
          Mean value_function loss: 1.0408
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 61.7007
                       Mean reward: 7.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7663
     Episode_Reward/lifting_object: -0.1703
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 2.08s
                      Time elapsed: 00:13:59
                               ETA: 01:15:59

################################################################################
                     [1m Learning iteration 389/2500 [0m                      

                       Computation: 47119 steps/s (collection: 1.979s, learning 0.107s)
             Mean action noise std: 1.79
          Mean value_function loss: 0.6077
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 61.7239
                       Mean reward: 8.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7635
     Episode_Reward/lifting_object: -0.0287
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0215
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 2.09s
                      Time elapsed: 00:14:01
                               ETA: 01:15:56

################################################################################
                     [1m Learning iteration 390/2500 [0m                      

                       Computation: 46942 steps/s (collection: 1.991s, learning 0.103s)
             Mean action noise std: 1.79
          Mean value_function loss: 2.8202
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 61.7656
                       Mean reward: 7.85
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7655
     Episode_Reward/lifting_object: -0.0995
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 2.09s
                      Time elapsed: 00:14:03
                               ETA: 01:15:53

################################################################################
                     [1m Learning iteration 391/2500 [0m                      

                       Computation: 47322 steps/s (collection: 1.961s, learning 0.117s)
             Mean action noise std: 1.80
          Mean value_function loss: 1.7730
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 61.7840
                       Mean reward: 6.78
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7633
     Episode_Reward/lifting_object: -0.1562
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 2.08s
                      Time elapsed: 00:14:05
                               ETA: 01:15:51

################################################################################
                     [1m Learning iteration 392/2500 [0m                      

                       Computation: 47101 steps/s (collection: 1.957s, learning 0.130s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.0623
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 61.8373
                       Mean reward: 7.95
               Mean episode length: 248.67
    Episode_Reward/reaching_object: 1.7491
     Episode_Reward/lifting_object: -0.0270
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0216
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 2.09s
                      Time elapsed: 00:14:08
                               ETA: 01:15:48

################################################################################
                     [1m Learning iteration 393/2500 [0m                      

                       Computation: 47405 steps/s (collection: 1.958s, learning 0.116s)
             Mean action noise std: 1.80
          Mean value_function loss: 0.0486
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 61.9014
                       Mean reward: 8.27
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6882
     Episode_Reward/lifting_object: -0.0374
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0217
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 2.07s
                      Time elapsed: 00:14:10
                               ETA: 01:15:46

################################################################################
                     [1m Learning iteration 394/2500 [0m                      

                       Computation: 47209 steps/s (collection: 1.962s, learning 0.120s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.5840
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 61.9748
                       Mean reward: 7.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7628
     Episode_Reward/lifting_object: -0.1476
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 2.08s
                      Time elapsed: 00:14:12
                               ETA: 01:15:43

################################################################################
                     [1m Learning iteration 395/2500 [0m                      

                       Computation: 46826 steps/s (collection: 1.990s, learning 0.110s)
             Mean action noise std: 1.81
          Mean value_function loss: 4.2183
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 62.0352
                       Mean reward: 7.08
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7061
     Episode_Reward/lifting_object: -0.2518
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0218
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 2.10s
                      Time elapsed: 00:14:14
                               ETA: 01:15:41

################################################################################
                     [1m Learning iteration 396/2500 [0m                      

                       Computation: 46979 steps/s (collection: 1.986s, learning 0.107s)
             Mean action noise std: 1.81
          Mean value_function loss: 3.0497
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 62.0487
                       Mean reward: 7.75
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7345
     Episode_Reward/lifting_object: -0.4034
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 2.09s
                      Time elapsed: 00:14:16
                               ETA: 01:15:38

################################################################################
                     [1m Learning iteration 397/2500 [0m                      

                       Computation: 46677 steps/s (collection: 1.990s, learning 0.116s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.1913
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.0617
                       Mean reward: 8.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7473
     Episode_Reward/lifting_object: -0.1821
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 2.11s
                      Time elapsed: 00:14:18
                               ETA: 01:15:36

################################################################################
                     [1m Learning iteration 398/2500 [0m                      

                       Computation: 46768 steps/s (collection: 1.996s, learning 0.106s)
             Mean action noise std: 1.81
          Mean value_function loss: 0.6994
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 62.0979
                       Mean reward: 7.87
               Mean episode length: 249.25
    Episode_Reward/reaching_object: 1.7453
     Episode_Reward/lifting_object: -0.1656
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0219
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 2.10s
                      Time elapsed: 00:14:20
                               ETA: 01:15:33

################################################################################
                     [1m Learning iteration 399/2500 [0m                      

                       Computation: 47804 steps/s (collection: 1.942s, learning 0.115s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.3409
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 62.1496
                       Mean reward: 6.73
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7280
     Episode_Reward/lifting_object: -0.1752
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0220
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 2.06s
                      Time elapsed: 00:14:22
                               ETA: 01:15:31

################################################################################
                     [1m Learning iteration 400/2500 [0m                      

                       Computation: 47676 steps/s (collection: 1.975s, learning 0.087s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.7197
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 62.1996
                       Mean reward: 8.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7141
     Episode_Reward/lifting_object: -0.3380
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 2.06s
                      Time elapsed: 00:14:24
                               ETA: 01:15:28

################################################################################
                     [1m Learning iteration 401/2500 [0m                      

                       Computation: 47054 steps/s (collection: 1.991s, learning 0.098s)
             Mean action noise std: 1.82
          Mean value_function loss: 0.4822
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 62.2364
                       Mean reward: 8.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7806
     Episode_Reward/lifting_object: -0.1139
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0221
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 2.09s
                      Time elapsed: 00:14:26
                               ETA: 01:15:25

################################################################################
                     [1m Learning iteration 402/2500 [0m                      

                       Computation: 46592 steps/s (collection: 2.015s, learning 0.095s)
             Mean action noise std: 1.82
          Mean value_function loss: 3.0032
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 62.2869
                       Mean reward: 8.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7295
     Episode_Reward/lifting_object: -0.1953
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0222
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 2.11s
                      Time elapsed: 00:14:28
                               ETA: 01:15:23

################################################################################
                     [1m Learning iteration 403/2500 [0m                      

                       Computation: 47883 steps/s (collection: 1.961s, learning 0.092s)
             Mean action noise std: 1.82
          Mean value_function loss: 2.0272
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 62.3009
                       Mean reward: 8.01
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7115
     Episode_Reward/lifting_object: -0.0938
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 2.05s
                      Time elapsed: 00:14:30
                               ETA: 01:15:20

################################################################################
                     [1m Learning iteration 404/2500 [0m                      

                       Computation: 48205 steps/s (collection: 1.934s, learning 0.106s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.0472
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 62.3175
                       Mean reward: 8.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7327
     Episode_Reward/lifting_object: -0.0968
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0223
          Episode_Reward/joint_vel: -0.0153
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 2.04s
                      Time elapsed: 00:14:33
                               ETA: 01:15:18

################################################################################
                     [1m Learning iteration 405/2500 [0m                      

                       Computation: 48557 steps/s (collection: 1.918s, learning 0.106s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.0848
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 62.3648
                       Mean reward: 8.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7264
     Episode_Reward/lifting_object: -0.0505
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 2.02s
                      Time elapsed: 00:14:35
                               ETA: 01:15:15

################################################################################
                     [1m Learning iteration 406/2500 [0m                      

                       Computation: 47681 steps/s (collection: 1.947s, learning 0.115s)
             Mean action noise std: 1.83
          Mean value_function loss: 0.1026
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 62.4341
                       Mean reward: 8.28
               Mean episode length: 248.63
    Episode_Reward/reaching_object: 1.7135
     Episode_Reward/lifting_object: -0.1518
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0224
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 2.06s
                      Time elapsed: 00:14:37
                               ETA: 01:15:12

################################################################################
                     [1m Learning iteration 407/2500 [0m                      

                       Computation: 47783 steps/s (collection: 1.959s, learning 0.099s)
             Mean action noise std: 1.84
          Mean value_function loss: 0.0894
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 62.5077
                       Mean reward: 7.26
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6689
     Episode_Reward/lifting_object: -0.0361
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 2.06s
                      Time elapsed: 00:14:39
                               ETA: 01:15:10

################################################################################
                     [1m Learning iteration 408/2500 [0m                      

                       Computation: 46355 steps/s (collection: 2.030s, learning 0.091s)
             Mean action noise std: 1.84
          Mean value_function loss: 1.2855
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 62.5761
                       Mean reward: 8.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7058
     Episode_Reward/lifting_object: 0.0066
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0225
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 2.12s
                      Time elapsed: 00:14:41
                               ETA: 01:15:07

################################################################################
                     [1m Learning iteration 409/2500 [0m                      

                       Computation: 47245 steps/s (collection: 1.971s, learning 0.110s)
             Mean action noise std: 1.84
          Mean value_function loss: 0.7270
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 62.5920
                       Mean reward: 8.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6826
     Episode_Reward/lifting_object: -0.0241
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0154
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 2.08s
                      Time elapsed: 00:14:43
                               ETA: 01:15:05

################################################################################
                     [1m Learning iteration 410/2500 [0m                      

                       Computation: 47799 steps/s (collection: 1.956s, learning 0.101s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.1251
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 62.6592
                       Mean reward: 8.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6781
     Episode_Reward/lifting_object: -0.2661
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 2.06s
                      Time elapsed: 00:14:45
                               ETA: 01:15:02

################################################################################
                     [1m Learning iteration 411/2500 [0m                      

                       Computation: 47672 steps/s (collection: 1.964s, learning 0.099s)
             Mean action noise std: 1.85
          Mean value_function loss: 0.5378
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 62.7386
                       Mean reward: 8.31
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.6841
     Episode_Reward/lifting_object: -0.1023
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0226
          Episode_Reward/joint_vel: -0.0155
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 2.06s
                      Time elapsed: 00:14:47
                               ETA: 01:14:59

################################################################################
                     [1m Learning iteration 412/2500 [0m                      

                       Computation: 46896 steps/s (collection: 1.985s, learning 0.111s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.3943
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.7864
                       Mean reward: 7.57
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7041
     Episode_Reward/lifting_object: -0.0182
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0228
          Episode_Reward/joint_vel: -0.0156
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 2.10s
                      Time elapsed: 00:14:49
                               ETA: 01:14:57

################################################################################
                     [1m Learning iteration 413/2500 [0m                      

                       Computation: 46936 steps/s (collection: 1.976s, learning 0.118s)
             Mean action noise std: 1.86
          Mean value_function loss: 1.6294
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 62.8529
                       Mean reward: 8.87
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7455
     Episode_Reward/lifting_object: 0.0224
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0229
          Episode_Reward/joint_vel: -0.0157
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 2.09s
                      Time elapsed: 00:14:51
                               ETA: 01:14:54

################################################################################
                     [1m Learning iteration 414/2500 [0m                      

                       Computation: 46848 steps/s (collection: 1.982s, learning 0.117s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.4984
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 62.8694
                       Mean reward: 8.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7241
     Episode_Reward/lifting_object: -0.0492
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0230
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 2.10s
                      Time elapsed: 00:14:53
                               ETA: 01:14:52

################################################################################
                     [1m Learning iteration 415/2500 [0m                      

                       Computation: 46474 steps/s (collection: 2.004s, learning 0.112s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.0797
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 62.9159
                       Mean reward: 9.09
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8035
     Episode_Reward/lifting_object: -0.0225
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0231
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 2.12s
                      Time elapsed: 00:14:55
                               ETA: 01:14:50

################################################################################
                     [1m Learning iteration 416/2500 [0m                      

                       Computation: 46189 steps/s (collection: 2.011s, learning 0.118s)
             Mean action noise std: 1.86
          Mean value_function loss: 0.5601
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 62.9658
                       Mean reward: 6.33
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8374
     Episode_Reward/lifting_object: -0.1425
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 2.13s
                      Time elapsed: 00:14:58
                               ETA: 01:14:47

################################################################################
                     [1m Learning iteration 417/2500 [0m                      

                       Computation: 46787 steps/s (collection: 1.987s, learning 0.115s)
             Mean action noise std: 1.86
          Mean value_function loss: 1.9939
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 62.9776
                       Mean reward: 8.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8009
     Episode_Reward/lifting_object: -0.1035
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0232
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 2.10s
                      Time elapsed: 00:15:00
                               ETA: 01:14:45

################################################################################
                     [1m Learning iteration 418/2500 [0m                      

                       Computation: 47908 steps/s (collection: 1.941s, learning 0.111s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.2690
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 62.9868
                       Mean reward: 8.20
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7483
     Episode_Reward/lifting_object: -0.1509
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0233
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 2.05s
                      Time elapsed: 00:15:02
                               ETA: 01:14:42

################################################################################
                     [1m Learning iteration 419/2500 [0m                      

                       Computation: 47146 steps/s (collection: 1.964s, learning 0.122s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.2613
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 63.0162
                       Mean reward: 8.84
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7724
     Episode_Reward/lifting_object: -0.0382
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 2.09s
                      Time elapsed: 00:15:04
                               ETA: 01:14:40

################################################################################
                     [1m Learning iteration 420/2500 [0m                      

                       Computation: 46689 steps/s (collection: 1.987s, learning 0.119s)
             Mean action noise std: 1.87
          Mean value_function loss: 0.2526
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 63.0902
                       Mean reward: 8.65
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8251
     Episode_Reward/lifting_object: -0.1055
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0234
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 2.11s
                      Time elapsed: 00:15:06
                               ETA: 01:14:37

################################################################################
                     [1m Learning iteration 421/2500 [0m                      

                       Computation: 46144 steps/s (collection: 2.010s, learning 0.120s)
             Mean action noise std: 1.88
          Mean value_function loss: 1.1085
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 63.1637
                       Mean reward: 8.77
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7848
     Episode_Reward/lifting_object: -0.0941
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 2.13s
                      Time elapsed: 00:15:08
                               ETA: 01:14:35

################################################################################
                     [1m Learning iteration 422/2500 [0m                      

                       Computation: 46466 steps/s (collection: 2.004s, learning 0.112s)
             Mean action noise std: 1.88
          Mean value_function loss: 4.9009
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 63.1739
                       Mean reward: 7.38
               Mean episode length: 248.65
    Episode_Reward/reaching_object: 1.7856
     Episode_Reward/lifting_object: -0.1305
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0235
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 2.12s
                      Time elapsed: 00:15:10
                               ETA: 01:14:33

################################################################################
                     [1m Learning iteration 423/2500 [0m                      

                       Computation: 47377 steps/s (collection: 1.962s, learning 0.113s)
             Mean action noise std: 1.88
          Mean value_function loss: 0.3831
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 63.1850
                       Mean reward: 8.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7813
     Episode_Reward/lifting_object: -0.1540
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 2.07s
                      Time elapsed: 00:15:12
                               ETA: 01:14:30

################################################################################
                     [1m Learning iteration 424/2500 [0m                      

                       Computation: 47276 steps/s (collection: 1.966s, learning 0.114s)
             Mean action noise std: 1.88
          Mean value_function loss: 0.1670
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 63.2291
                       Mean reward: 8.59
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8343
     Episode_Reward/lifting_object: -0.0708
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 2.08s
                      Time elapsed: 00:15:14
                               ETA: 01:14:28

################################################################################
                     [1m Learning iteration 425/2500 [0m                      

                       Computation: 47384 steps/s (collection: 1.959s, learning 0.115s)
             Mean action noise std: 1.88
          Mean value_function loss: 0.2545
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 63.2938
                       Mean reward: 9.22
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8500
     Episode_Reward/lifting_object: -0.0281
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0237
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 2.07s
                      Time elapsed: 00:15:16
                               ETA: 01:14:25

################################################################################
                     [1m Learning iteration 426/2500 [0m                      

                       Computation: 46729 steps/s (collection: 1.980s, learning 0.124s)
             Mean action noise std: 1.89
          Mean value_function loss: 0.1772
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 63.3219
                       Mean reward: 7.56
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7980
     Episode_Reward/lifting_object: -0.0918
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0238
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 2.10s
                      Time elapsed: 00:15:18
                               ETA: 01:14:23

################################################################################
                     [1m Learning iteration 427/2500 [0m                      

                       Computation: 46562 steps/s (collection: 1.987s, learning 0.124s)
             Mean action noise std: 1.89
          Mean value_function loss: 0.1768
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 63.3826
                       Mean reward: 8.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8009
     Episode_Reward/lifting_object: -0.1487
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 2.11s
                      Time elapsed: 00:15:21
                               ETA: 01:14:21

################################################################################
                     [1m Learning iteration 428/2500 [0m                      

                       Computation: 47384 steps/s (collection: 1.978s, learning 0.097s)
             Mean action noise std: 1.90
          Mean value_function loss: 0.3329
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 63.4662
                       Mean reward: 9.43
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8340
     Episode_Reward/lifting_object: -0.0620
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0239
          Episode_Reward/joint_vel: -0.0158
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 2.07s
                      Time elapsed: 00:15:23
                               ETA: 01:14:18

################################################################################
                     [1m Learning iteration 429/2500 [0m                      

                       Computation: 46894 steps/s (collection: 1.998s, learning 0.098s)
             Mean action noise std: 1.90
          Mean value_function loss: 1.2859
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 63.5415
                       Mean reward: 7.99
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8215
     Episode_Reward/lifting_object: -0.1347
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0240
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 2.10s
                      Time elapsed: 00:15:25
                               ETA: 01:14:16

################################################################################
                     [1m Learning iteration 430/2500 [0m                      

                       Computation: 47551 steps/s (collection: 1.968s, learning 0.100s)
             Mean action noise std: 1.90
          Mean value_function loss: 0.6386
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 63.5751
                       Mean reward: 7.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7721
     Episode_Reward/lifting_object: -0.3334
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 2.07s
                      Time elapsed: 00:15:27
                               ETA: 01:14:13

################################################################################
                     [1m Learning iteration 431/2500 [0m                      

                       Computation: 44864 steps/s (collection: 2.087s, learning 0.104s)
             Mean action noise std: 1.91
          Mean value_function loss: 0.4529
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 63.6291
                       Mean reward: 8.80
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8006
     Episode_Reward/lifting_object: -0.0709
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0241
          Episode_Reward/joint_vel: -0.0160
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 2.19s
                      Time elapsed: 00:15:29
                               ETA: 01:14:11

################################################################################
                     [1m Learning iteration 432/2500 [0m                      

                       Computation: 46311 steps/s (collection: 2.012s, learning 0.111s)
             Mean action noise std: 1.91
          Mean value_function loss: 0.7712
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 63.6770
                       Mean reward: 8.71
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.7570
     Episode_Reward/lifting_object: -0.0074
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0242
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 2.12s
                      Time elapsed: 00:15:31
                               ETA: 01:14:09

################################################################################
                     [1m Learning iteration 433/2500 [0m                      

                       Computation: 45861 steps/s (collection: 2.022s, learning 0.121s)
             Mean action noise std: 1.91
          Mean value_function loss: 2.4392
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 63.7438
                       Mean reward: 8.96
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8544
     Episode_Reward/lifting_object: 0.0214
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0243
          Episode_Reward/joint_vel: -0.0159
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 2.14s
                      Time elapsed: 00:15:33
                               ETA: 01:14:07

################################################################################
                     [1m Learning iteration 434/2500 [0m                      

                       Computation: 45600 steps/s (collection: 2.045s, learning 0.111s)
             Mean action noise std: 1.91
          Mean value_function loss: 2.0462
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 63.7640
                       Mean reward: 8.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8519
     Episode_Reward/lifting_object: -0.2092
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0244
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 2.16s
                      Time elapsed: 00:15:35
                               ETA: 01:14:04

################################################################################
                     [1m Learning iteration 435/2500 [0m                      

                       Computation: 46508 steps/s (collection: 1.996s, learning 0.118s)
             Mean action noise std: 1.91
          Mean value_function loss: 0.1954
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 63.7755
                       Mean reward: 9.14
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.9303
     Episode_Reward/lifting_object: -0.1045
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 2.11s
                      Time elapsed: 00:15:38
                               ETA: 01:14:02

################################################################################
                     [1m Learning iteration 436/2500 [0m                      

                       Computation: 46663 steps/s (collection: 1.994s, learning 0.113s)
             Mean action noise std: 1.92
          Mean value_function loss: 0.0433
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 63.8077
                       Mean reward: 7.90
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8685
     Episode_Reward/lifting_object: -0.2018
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 2.11s
                      Time elapsed: 00:15:40
                               ETA: 01:14:00

################################################################################
                     [1m Learning iteration 437/2500 [0m                      

                       Computation: 45374 steps/s (collection: 2.052s, learning 0.115s)
             Mean action noise std: 1.92
          Mean value_function loss: 0.8086
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 63.8528
                       Mean reward: 8.82
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8376
     Episode_Reward/lifting_object: -0.0660
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 2.17s
                      Time elapsed: 00:15:42
                               ETA: 01:13:58

################################################################################
                     [1m Learning iteration 438/2500 [0m                      

                       Computation: 46510 steps/s (collection: 2.001s, learning 0.113s)
             Mean action noise std: 1.92
          Mean value_function loss: 2.8528
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 63.8900
                       Mean reward: 8.03
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8541
     Episode_Reward/lifting_object: -0.1702
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0247
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 2.11s
                      Time elapsed: 00:15:44
                               ETA: 01:13:55

################################################################################
                     [1m Learning iteration 439/2500 [0m                      

                       Computation: 45623 steps/s (collection: 2.037s, learning 0.118s)
             Mean action noise std: 1.92
          Mean value_function loss: 1.6525
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 63.9015
                       Mean reward: 9.32
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8405
     Episode_Reward/lifting_object: -0.1050
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 2.15s
                      Time elapsed: 00:15:46
                               ETA: 01:13:53

################################################################################
                     [1m Learning iteration 440/2500 [0m                      

                       Computation: 46520 steps/s (collection: 1.995s, learning 0.119s)
             Mean action noise std: 1.92
          Mean value_function loss: 1.1001
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 63.9138
                       Mean reward: 8.98
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8836
     Episode_Reward/lifting_object: -0.0308
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 2.11s
                      Time elapsed: 00:15:48
                               ETA: 01:13:51

################################################################################
                     [1m Learning iteration 441/2500 [0m                      

                       Computation: 46877 steps/s (collection: 1.997s, learning 0.100s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.7697
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 63.9492
                       Mean reward: 8.69
               Mean episode length: 248.85
    Episode_Reward/reaching_object: 1.8815
     Episode_Reward/lifting_object: -0.2283
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0248
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 2.10s
                      Time elapsed: 00:15:50
                               ETA: 01:13:49

################################################################################
                     [1m Learning iteration 442/2500 [0m                      

                       Computation: 46992 steps/s (collection: 1.967s, learning 0.125s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.2389
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 64.0095
                       Mean reward: 8.15
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8340
     Episode_Reward/lifting_object: -0.2132
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 2.09s
                      Time elapsed: 00:15:52
                               ETA: 01:13:46

################################################################################
                     [1m Learning iteration 443/2500 [0m                      

                       Computation: 46213 steps/s (collection: 2.019s, learning 0.108s)
             Mean action noise std: 1.93
          Mean value_function loss: 0.4935
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 64.0504
                       Mean reward: 9.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8512
     Episode_Reward/lifting_object: -0.0721
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0249
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 2.13s
                      Time elapsed: 00:15:54
                               ETA: 01:13:44

################################################################################
                     [1m Learning iteration 444/2500 [0m                      

                       Computation: 46624 steps/s (collection: 2.012s, learning 0.096s)
             Mean action noise std: 1.93
          Mean value_function loss: 1.9823
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 64.1018
                       Mean reward: 8.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8546
     Episode_Reward/lifting_object: -0.2022
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0250
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 2.11s
                      Time elapsed: 00:15:57
                               ETA: 01:13:41

################################################################################
                     [1m Learning iteration 445/2500 [0m                      

                       Computation: 47093 steps/s (collection: 1.993s, learning 0.095s)
             Mean action noise std: 1.93
          Mean value_function loss: 2.0619
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 64.1246
                       Mean reward: 6.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8335
     Episode_Reward/lifting_object: -0.4143
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0251
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 2.09s
                      Time elapsed: 00:15:59
                               ETA: 01:13:39

################################################################################
                     [1m Learning iteration 446/2500 [0m                      

                       Computation: 46605 steps/s (collection: 1.991s, learning 0.118s)
             Mean action noise std: 1.94
          Mean value_function loss: 1.1469
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 64.1344
                       Mean reward: 7.66
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8142
     Episode_Reward/lifting_object: -0.0912
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 2.11s
                      Time elapsed: 00:16:01
                               ETA: 01:13:37

################################################################################
                     [1m Learning iteration 447/2500 [0m                      

                       Computation: 46759 steps/s (collection: 2.002s, learning 0.101s)
             Mean action noise std: 1.94
          Mean value_function loss: 0.1906
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.1656
                       Mean reward: 8.91
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8544
     Episode_Reward/lifting_object: -0.2475
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 2.10s
                      Time elapsed: 00:16:03
                               ETA: 01:13:34

################################################################################
                     [1m Learning iteration 448/2500 [0m                      

                       Computation: 42374 steps/s (collection: 2.197s, learning 0.123s)
             Mean action noise std: 1.94
          Mean value_function loss: 0.1331
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.2291
                       Mean reward: 8.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8742
     Episode_Reward/lifting_object: -0.0767
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 16.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 2.32s
                      Time elapsed: 00:16:05
                               ETA: 01:13:33

################################################################################
                     [1m Learning iteration 449/2500 [0m                      

                       Computation: 44594 steps/s (collection: 2.072s, learning 0.132s)
             Mean action noise std: 1.95
          Mean value_function loss: 0.1296
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 64.2980
                       Mean reward: 8.73
               Mean episode length: 248.97
    Episode_Reward/reaching_object: 1.8035
     Episode_Reward/lifting_object: -0.0224
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 2.20s
                      Time elapsed: 00:16:07
                               ETA: 01:13:31

################################################################################
                     [1m Learning iteration 450/2500 [0m                      

                       Computation: 40356 steps/s (collection: 2.280s, learning 0.156s)
             Mean action noise std: 1.95
          Mean value_function loss: 0.1821
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 64.3408
                       Mean reward: 9.13
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8093
     Episode_Reward/lifting_object: -0.0427
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 2.44s
                      Time elapsed: 00:16:10
                               ETA: 01:13:30

################################################################################
                     [1m Learning iteration 451/2500 [0m                      

                       Computation: 43268 steps/s (collection: 2.178s, learning 0.094s)
             Mean action noise std: 1.95
          Mean value_function loss: 0.6754
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 64.3945
                       Mean reward: 9.11
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8757
     Episode_Reward/lifting_object: 0.0554
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0161
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 2.27s
                      Time elapsed: 00:16:12
                               ETA: 01:13:29

################################################################################
                     [1m Learning iteration 452/2500 [0m                      

                       Computation: 40538 steps/s (collection: 2.267s, learning 0.158s)
             Mean action noise std: 1.95
          Mean value_function loss: 1.9656
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 64.4095
                       Mean reward: 9.33
               Mean episode length: 247.70
    Episode_Reward/reaching_object: 1.8368
     Episode_Reward/lifting_object: -0.0745
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 2.42s
                      Time elapsed: 00:16:15
                               ETA: 01:13:28

################################################################################
                     [1m Learning iteration 453/2500 [0m                      

                       Computation: 42053 steps/s (collection: 2.205s, learning 0.132s)
             Mean action noise std: 1.96
          Mean value_function loss: 0.4691
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 64.4449
                       Mean reward: 9.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8591
     Episode_Reward/lifting_object: -0.0405
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0162
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 2.34s
                      Time elapsed: 00:16:17
                               ETA: 01:13:26

################################################################################
                     [1m Learning iteration 454/2500 [0m                      

                       Computation: 41359 steps/s (collection: 2.282s, learning 0.095s)
             Mean action noise std: 1.96
          Mean value_function loss: 0.0813
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 64.5173
                       Mean reward: 9.12
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 1.8365
     Episode_Reward/lifting_object: -0.0013
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0163
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 2.38s
                      Time elapsed: 00:16:19
                               ETA: 01:13:25

################################################################################
                     [1m Learning iteration 455/2500 [0m                      

                       Computation: 42973 steps/s (collection: 2.193s, learning 0.095s)
             Mean action noise std: 1.96
          Mean value_function loss: 0.2755
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 64.5827
                       Mean reward: 9.29
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8565
     Episode_Reward/lifting_object: 0.0247
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 2.29s
                      Time elapsed: 00:16:22
                               ETA: 01:13:24

################################################################################
                     [1m Learning iteration 456/2500 [0m                      

                       Computation: 42002 steps/s (collection: 2.197s, learning 0.143s)
             Mean action noise std: 1.97
          Mean value_function loss: 0.6954
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 64.6303
                       Mean reward: 9.39
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 1.8800
     Episode_Reward/lifting_object: 0.0239
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0165
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 2.34s
                      Time elapsed: 00:16:24
                               ETA: 01:13:22

################################################################################
                     [1m Learning iteration 457/2500 [0m                      

                       Computation: 37162 steps/s (collection: 2.549s, learning 0.097s)
             Mean action noise std: 1.97
          Mean value_function loss: 2.6228
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 64.6555
                       Mean reward: 8.95
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8737
     Episode_Reward/lifting_object: -0.0368
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0164
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 2.65s
                      Time elapsed: 00:16:27
                               ETA: 01:13:22

################################################################################
                     [1m Learning iteration 458/2500 [0m                      

                       Computation: 44323 steps/s (collection: 2.111s, learning 0.107s)
             Mean action noise std: 1.97
          Mean value_function loss: 1.1262
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 64.6647
                       Mean reward: 9.04
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 1.8310
     Episode_Reward/lifting_object: -0.0552
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 2.22s
                      Time elapsed: 00:16:29
                               ETA: 01:13:20

################################################################################
                     [1m Learning iteration 459/2500 [0m                      

                       Computation: 47012 steps/s (collection: 2.001s, learning 0.090s)
             Mean action noise std: 1.97
          Mean value_function loss: 0.2728
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 64.6789
                       Mean reward: 8.47
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8847
     Episode_Reward/lifting_object: -0.0757
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0166
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 2.09s
                      Time elapsed: 00:16:31
                               ETA: 01:13:18

################################################################################
                     [1m Learning iteration 460/2500 [0m                      

                       Computation: 42812 steps/s (collection: 2.182s, learning 0.115s)
             Mean action noise std: 1.97
          Mean value_function loss: 0.2435
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 64.7233
                       Mean reward: 9.51
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.9633
     Episode_Reward/lifting_object: -0.1142
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 2.30s
                      Time elapsed: 00:16:33
                               ETA: 01:13:17

################################################################################
                     [1m Learning iteration 461/2500 [0m                      

                       Computation: 43524 steps/s (collection: 2.103s, learning 0.155s)
             Mean action noise std: 1.98
          Mean value_function loss: 0.3031
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 64.7846
                       Mean reward: 9.29
               Mean episode length: 248.55
    Episode_Reward/reaching_object: 1.8939
     Episode_Reward/lifting_object: -0.1415
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 2.26s
                      Time elapsed: 00:16:35
                               ETA: 01:13:15

################################################################################
                     [1m Learning iteration 462/2500 [0m                      

                       Computation: 44428 steps/s (collection: 2.118s, learning 0.095s)
             Mean action noise std: 1.98
          Mean value_function loss: 1.1911
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 64.8089
                       Mean reward: 7.81
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8815
     Episode_Reward/lifting_object: -0.0445
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 2.21s
                      Time elapsed: 00:16:38
                               ETA: 01:13:13

################################################################################
                     [1m Learning iteration 463/2500 [0m                      

                       Computation: 44815 steps/s (collection: 2.095s, learning 0.099s)
             Mean action noise std: 1.98
          Mean value_function loss: 1.5941
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 64.8392
                       Mean reward: 7.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.9050
     Episode_Reward/lifting_object: -0.1864
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0167
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 2.19s
                      Time elapsed: 00:16:40
                               ETA: 01:13:11

################################################################################
                     [1m Learning iteration 464/2500 [0m                      

                       Computation: 45068 steps/s (collection: 2.090s, learning 0.091s)
             Mean action noise std: 1.98
          Mean value_function loss: 0.5221
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 64.8604
                       Mean reward: 9.42
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.9338
     Episode_Reward/lifting_object: -0.1782
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 2.18s
                      Time elapsed: 00:16:42
                               ETA: 01:13:09

################################################################################
                     [1m Learning iteration 465/2500 [0m                      

                       Computation: 43532 steps/s (collection: 2.152s, learning 0.106s)
             Mean action noise std: 1.98
          Mean value_function loss: 1.8196
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 64.9003
                       Mean reward: 8.84
               Mean episode length: 248.85
    Episode_Reward/reaching_object: 1.9288
     Episode_Reward/lifting_object: -0.1798
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 2.26s
                      Time elapsed: 00:16:44
                               ETA: 01:13:07

################################################################################
                     [1m Learning iteration 466/2500 [0m                      

                       Computation: 42959 steps/s (collection: 2.170s, learning 0.119s)
             Mean action noise std: 1.99
          Mean value_function loss: 1.9435
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 64.9215
                       Mean reward: 9.22
               Mean episode length: 249.32
    Episode_Reward/reaching_object: 1.8897
     Episode_Reward/lifting_object: -0.0303
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 2.29s
                      Time elapsed: 00:16:47
                               ETA: 01:13:06

################################################################################
                     [1m Learning iteration 467/2500 [0m                      

                       Computation: 44787 steps/s (collection: 2.073s, learning 0.122s)
             Mean action noise std: 1.99
          Mean value_function loss: 2.6577
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 64.9522
                       Mean reward: 9.54
               Mean episode length: 249.19
    Episode_Reward/reaching_object: 1.9336
     Episode_Reward/lifting_object: -0.0673
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 2.19s
                      Time elapsed: 00:16:49
                               ETA: 01:13:04

################################################################################
                     [1m Learning iteration 468/2500 [0m                      

                       Computation: 36741 steps/s (collection: 2.528s, learning 0.148s)
             Mean action noise std: 1.99
          Mean value_function loss: 0.1353
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 64.9628
                       Mean reward: 7.67
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.9177
     Episode_Reward/lifting_object: -0.1191
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 2.68s
                      Time elapsed: 00:16:51
                               ETA: 01:13:04

################################################################################
                     [1m Learning iteration 469/2500 [0m                      

                       Computation: 40975 steps/s (collection: 2.239s, learning 0.161s)
             Mean action noise std: 1.99
          Mean value_function loss: 0.0849
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 64.9974
                       Mean reward: 8.35
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.9139
     Episode_Reward/lifting_object: -0.1069
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 2.40s
                      Time elapsed: 00:16:54
                               ETA: 01:13:03

################################################################################
                     [1m Learning iteration 470/2500 [0m                      

                       Computation: 44904 steps/s (collection: 2.091s, learning 0.098s)
             Mean action noise std: 1.99
          Mean value_function loss: 0.9892
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 65.0410
                       Mean reward: 9.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.8794
     Episode_Reward/lifting_object: 0.0095
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 2.19s
                      Time elapsed: 00:16:56
                               ETA: 01:13:01

################################################################################
                     [1m Learning iteration 471/2500 [0m                      

                       Computation: 46080 steps/s (collection: 2.021s, learning 0.112s)
             Mean action noise std: 2.00
          Mean value_function loss: 2.2675
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 65.0765
                       Mean reward: 10.24
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 2.0084
     Episode_Reward/lifting_object: -0.1052
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 2.13s
                      Time elapsed: 00:16:58
                               ETA: 01:12:58

################################################################################
                     [1m Learning iteration 472/2500 [0m                      

                       Computation: 46121 steps/s (collection: 2.020s, learning 0.111s)
             Mean action noise std: 2.00
          Mean value_function loss: 0.7110
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 65.1111
                       Mean reward: 8.84
               Mean episode length: 249.56
    Episode_Reward/reaching_object: 1.8649
     Episode_Reward/lifting_object: -0.1402
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 2.13s
                      Time elapsed: 00:17:00
                               ETA: 01:12:56

################################################################################
                     [1m Learning iteration 473/2500 [0m                      

                       Computation: 46427 steps/s (collection: 2.005s, learning 0.113s)
             Mean action noise std: 2.01
          Mean value_function loss: 0.2441
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 65.1902
                       Mean reward: 9.27
               Mean episode length: 248.76
    Episode_Reward/reaching_object: 1.9548
     Episode_Reward/lifting_object: -0.0717
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 2.12s
                      Time elapsed: 00:17:02
                               ETA: 01:12:54

################################################################################
                     [1m Learning iteration 474/2500 [0m                      

                       Computation: 45956 steps/s (collection: 2.025s, learning 0.114s)
             Mean action noise std: 2.01
          Mean value_function loss: 0.4188
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 65.2751
                       Mean reward: 9.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.9507
     Episode_Reward/lifting_object: -0.1449
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 2.14s
                      Time elapsed: 00:17:05
                               ETA: 01:12:51

################################################################################
                     [1m Learning iteration 475/2500 [0m                      

                       Computation: 45213 steps/s (collection: 2.053s, learning 0.122s)
             Mean action noise std: 2.01
          Mean value_function loss: 1.5570
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 65.2874
                       Mean reward: 4.71
               Mean episode length: 248.77
    Episode_Reward/reaching_object: 1.9327
     Episode_Reward/lifting_object: -0.2493
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 2.17s
                      Time elapsed: 00:17:07
                               ETA: 01:12:49

################################################################################
                     [1m Learning iteration 476/2500 [0m                      

                       Computation: 44879 steps/s (collection: 2.068s, learning 0.123s)
             Mean action noise std: 2.01
          Mean value_function loss: 4.0400
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 65.3135
                       Mean reward: 9.18
               Mean episode length: 249.24
    Episode_Reward/reaching_object: 1.9685
     Episode_Reward/lifting_object: -0.1373
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 2.19s
                      Time elapsed: 00:17:09
                               ETA: 01:12:47

################################################################################
                     [1m Learning iteration 477/2500 [0m                      

                       Computation: 45420 steps/s (collection: 2.050s, learning 0.114s)
             Mean action noise std: 2.02
          Mean value_function loss: 0.6600
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 65.3686
                       Mean reward: 5.85
               Mean episode length: 249.54
    Episode_Reward/reaching_object: 1.9782
     Episode_Reward/lifting_object: -0.2075
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0170
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 2.16s
                      Time elapsed: 00:17:11
                               ETA: 01:12:45

################################################################################
                     [1m Learning iteration 478/2500 [0m                      

                       Computation: 44586 steps/s (collection: 2.078s, learning 0.127s)
             Mean action noise std: 2.02
          Mean value_function loss: 0.1251
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.4376
                       Mean reward: 9.77
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 1.9086
     Episode_Reward/lifting_object: 0.0297
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 2.20s
                      Time elapsed: 00:17:13
                               ETA: 01:12:43

################################################################################
                     [1m Learning iteration 479/2500 [0m                      

                       Computation: 46260 steps/s (collection: 2.011s, learning 0.114s)
             Mean action noise std: 2.02
          Mean value_function loss: 0.3690
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 65.5083
                       Mean reward: 7.48
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.9004
     Episode_Reward/lifting_object: -0.1131
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 2.13s
                      Time elapsed: 00:17:15
                               ETA: 01:12:41

################################################################################
                     [1m Learning iteration 480/2500 [0m                      

                       Computation: 46854 steps/s (collection: 2.001s, learning 0.098s)
             Mean action noise std: 2.03
          Mean value_function loss: 0.1879
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 65.5400
                       Mean reward: 9.32
               Mean episode length: 249.27
    Episode_Reward/reaching_object: 1.9443
     Episode_Reward/lifting_object: 0.0173
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0168
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 2.10s
                      Time elapsed: 00:17:17
                               ETA: 01:12:39

################################################################################
                     [1m Learning iteration 481/2500 [0m                      

                       Computation: 46081 steps/s (collection: 2.024s, learning 0.109s)
             Mean action noise std: 2.03
          Mean value_function loss: 0.1107
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 65.5928
                       Mean reward: 9.68
               Mean episode length: 248.70
    Episode_Reward/reaching_object: 1.9128
     Episode_Reward/lifting_object: -0.1162
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0169
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 2.13s
                      Time elapsed: 00:17:20
                               ETA: 01:12:36

################################################################################
                     [1m Learning iteration 482/2500 [0m                      

                       Computation: 45713 steps/s (collection: 2.034s, learning 0.116s)
             Mean action noise std: 2.04
          Mean value_function loss: 0.1442
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 65.6765
                       Mean reward: 9.88
               Mean episode length: 249.53
    Episode_Reward/reaching_object: 1.9731
     Episode_Reward/lifting_object: -0.0599
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 2.15s
                      Time elapsed: 00:17:22
                               ETA: 01:12:34

################################################################################
                     [1m Learning iteration 483/2500 [0m                      

                       Computation: 45945 steps/s (collection: 2.024s, learning 0.116s)
             Mean action noise std: 2.04
          Mean value_function loss: 1.9626
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 65.7255
                       Mean reward: 9.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.9462
     Episode_Reward/lifting_object: -0.0411
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 2.14s
                      Time elapsed: 00:17:24
                               ETA: 01:12:32

################################################################################
                     [1m Learning iteration 484/2500 [0m                      

                       Computation: 45433 steps/s (collection: 2.045s, learning 0.119s)
             Mean action noise std: 2.04
          Mean value_function loss: 0.3745
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 65.7484
                       Mean reward: 9.58
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.9616
     Episode_Reward/lifting_object: -0.0301
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 2.16s
                      Time elapsed: 00:17:26
                               ETA: 01:12:30

################################################################################
                     [1m Learning iteration 485/2500 [0m                      

                       Computation: 45428 steps/s (collection: 2.041s, learning 0.123s)
             Mean action noise std: 2.04
          Mean value_function loss: 1.1880
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 65.7896
                       Mean reward: 10.01
               Mean episode length: 249.39
    Episode_Reward/reaching_object: 1.9143
     Episode_Reward/lifting_object: -0.0122
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 2.16s
                      Time elapsed: 00:17:28
                               ETA: 01:12:28

################################################################################
                     [1m Learning iteration 486/2500 [0m                      

                       Computation: 44703 steps/s (collection: 2.077s, learning 0.122s)
             Mean action noise std: 2.04
          Mean value_function loss: 2.2334
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 65.8179
                       Mean reward: 10.20
               Mean episode length: 247.72
    Episode_Reward/reaching_object: 1.9527
     Episode_Reward/lifting_object: 0.0311
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 2.20s
                      Time elapsed: 00:17:30
                               ETA: 01:12:26

################################################################################
                     [1m Learning iteration 487/2500 [0m                      

                       Computation: 45857 steps/s (collection: 2.032s, learning 0.112s)
             Mean action noise std: 2.05
          Mean value_function loss: 0.5015
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 65.8353
                       Mean reward: 9.95
               Mean episode length: 249.21
    Episode_Reward/reaching_object: 1.9842
     Episode_Reward/lifting_object: 0.0362
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 2.14s
                      Time elapsed: 00:17:33
                               ETA: 01:12:23

################################################################################
                     [1m Learning iteration 488/2500 [0m                      

                       Computation: 46141 steps/s (collection: 2.011s, learning 0.119s)
             Mean action noise std: 2.05
          Mean value_function loss: 0.2946
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 65.8862
                       Mean reward: 9.86
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.9779
     Episode_Reward/lifting_object: -0.0027
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 2.13s
                      Time elapsed: 00:17:35
                               ETA: 01:12:21

################################################################################
                     [1m Learning iteration 489/2500 [0m                      

                       Computation: 45287 steps/s (collection: 2.064s, learning 0.107s)
             Mean action noise std: 2.05
          Mean value_function loss: 0.2088
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 65.9597
                       Mean reward: 8.04
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 1.9447
     Episode_Reward/lifting_object: -0.0253
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 2.17s
                      Time elapsed: 00:17:37
                               ETA: 01:12:19

################################################################################
                     [1m Learning iteration 490/2500 [0m                      

                       Computation: 46424 steps/s (collection: 2.019s, learning 0.099s)
             Mean action noise std: 2.06
          Mean value_function loss: 0.3971
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 66.0231
                       Mean reward: 9.31
               Mean episode length: 249.99
    Episode_Reward/reaching_object: 2.0021
     Episode_Reward/lifting_object: -0.0678
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 2.12s
                      Time elapsed: 00:17:39
                               ETA: 01:12:17

################################################################################
                     [1m Learning iteration 491/2500 [0m                      

                       Computation: 46055 steps/s (collection: 2.034s, learning 0.100s)
             Mean action noise std: 2.06
          Mean value_function loss: 0.1728
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.0874
                       Mean reward: 9.42
               Mean episode length: 248.96
    Episode_Reward/reaching_object: 1.9395
     Episode_Reward/lifting_object: -0.0447
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 2.13s
                      Time elapsed: 00:17:41
                               ETA: 01:12:14

################################################################################
                     [1m Learning iteration 492/2500 [0m                      

                       Computation: 44919 steps/s (collection: 2.063s, learning 0.125s)
             Mean action noise std: 2.07
          Mean value_function loss: 0.3043
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 66.1534
                       Mean reward: 9.15
               Mean episode length: 247.62
    Episode_Reward/reaching_object: 1.9868
     Episode_Reward/lifting_object: -0.2349
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 2.19s
                      Time elapsed: 00:17:43
                               ETA: 01:12:12

################################################################################
                     [1m Learning iteration 493/2500 [0m                      

                       Computation: 45009 steps/s (collection: 2.054s, learning 0.130s)
             Mean action noise std: 2.07
          Mean value_function loss: 1.2570
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 66.2025
                       Mean reward: 9.10
               Mean episode length: 249.37
    Episode_Reward/reaching_object: 1.9614
     Episode_Reward/lifting_object: -0.1431
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 2.18s
                      Time elapsed: 00:17:45
                               ETA: 01:12:10

################################################################################
                     [1m Learning iteration 494/2500 [0m                      

                       Computation: 44315 steps/s (collection: 2.107s, learning 0.112s)
             Mean action noise std: 2.07
          Mean value_function loss: 0.3114
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.2300
                       Mean reward: 10.53
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.9465
     Episode_Reward/lifting_object: 0.0843
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 2.22s
                      Time elapsed: 00:17:48
                               ETA: 01:12:08

################################################################################
                     [1m Learning iteration 495/2500 [0m                      

                       Computation: 44803 steps/s (collection: 2.075s, learning 0.119s)
             Mean action noise std: 2.07
          Mean value_function loss: 0.8603
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 66.2805
                       Mean reward: 10.14
               Mean episode length: 249.18
    Episode_Reward/reaching_object: 1.9787
     Episode_Reward/lifting_object: 0.0729
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0171
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 2.19s
                      Time elapsed: 00:17:50
                               ETA: 01:12:06

################################################################################
                     [1m Learning iteration 496/2500 [0m                      

                       Computation: 45977 steps/s (collection: 2.042s, learning 0.097s)
             Mean action noise std: 2.08
          Mean value_function loss: 0.3080
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 66.3198
                       Mean reward: 9.97
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.9542
     Episode_Reward/lifting_object: 0.0326
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0172
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 2.14s
                      Time elapsed: 00:17:52
                               ETA: 01:12:04

################################################################################
                     [1m Learning iteration 497/2500 [0m                      

                       Computation: 45317 steps/s (collection: 2.067s, learning 0.103s)
             Mean action noise std: 2.08
          Mean value_function loss: 1.2482
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 66.3749
                       Mean reward: 9.76
               Mean episode length: 248.88
    Episode_Reward/reaching_object: 2.0030
     Episode_Reward/lifting_object: -0.0058
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 17.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 2.17s
                      Time elapsed: 00:17:54
                               ETA: 01:12:02

################################################################################
                     [1m Learning iteration 498/2500 [0m                      

                       Computation: 45188 steps/s (collection: 2.073s, learning 0.102s)
             Mean action noise std: 2.08
          Mean value_function loss: 0.6107
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 66.3897
                       Mean reward: 10.25
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 1.9811
     Episode_Reward/lifting_object: 0.0678
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0174
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 2.18s
                      Time elapsed: 00:17:56
                               ETA: 01:12:00

################################################################################
                     [1m Learning iteration 499/2500 [0m                      

                       Computation: 46057 steps/s (collection: 2.037s, learning 0.098s)
             Mean action noise std: 2.09
          Mean value_function loss: 0.5241
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.4299
                       Mean reward: 8.88
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.9408
     Episode_Reward/lifting_object: -0.0613
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0173
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 2.13s
                      Time elapsed: 00:17:59
                               ETA: 01:11:58

################################################################################
                     [1m Learning iteration 500/2500 [0m                      

                       Computation: 45303 steps/s (collection: 2.048s, learning 0.122s)
             Mean action noise std: 2.09
          Mean value_function loss: 0.3247
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 66.5074
                       Mean reward: 10.12
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.9805
     Episode_Reward/lifting_object: 0.0315
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 2.17s
                      Time elapsed: 00:18:01
                               ETA: 01:11:56

################################################################################
                     [1m Learning iteration 501/2500 [0m                      

                       Computation: 43911 steps/s (collection: 2.126s, learning 0.113s)
             Mean action noise std: 2.09
          Mean value_function loss: 2.2899
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 66.5650
                       Mean reward: 9.25
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.9879
     Episode_Reward/lifting_object: -0.0851
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0293
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 2.24s
                      Time elapsed: 00:18:03
                               ETA: 01:11:54

################################################################################
                     [1m Learning iteration 502/2500 [0m                      

                       Computation: 44581 steps/s (collection: 2.088s, learning 0.118s)
             Mean action noise std: 2.09
          Mean value_function loss: 0.3652
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 66.5790
                       Mean reward: 9.29
               Mean episode length: 248.50
    Episode_Reward/reaching_object: 1.9969
     Episode_Reward/lifting_object: -0.1235
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 2.21s
                      Time elapsed: 00:18:05
                               ETA: 01:11:52

################################################################################
                     [1m Learning iteration 503/2500 [0m                      

                       Computation: 44915 steps/s (collection: 2.060s, learning 0.129s)
             Mean action noise std: 2.10
          Mean value_function loss: 0.5271
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 66.6174
                       Mean reward: 9.93
               Mean episode length: 247.57
    Episode_Reward/reaching_object: 1.9943
     Episode_Reward/lifting_object: -0.0834
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0175
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 2.19s
                      Time elapsed: 00:18:07
                               ETA: 01:11:50

################################################################################
                     [1m Learning iteration 504/2500 [0m                      

                       Computation: 44331 steps/s (collection: 2.085s, learning 0.132s)
             Mean action noise std: 2.10
          Mean value_function loss: 0.2335
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 66.6651
                       Mean reward: 10.46
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.9624
     Episode_Reward/lifting_object: -0.0382
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0176
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 2.22s
                      Time elapsed: 00:18:10
                               ETA: 01:11:48

################################################################################
                     [1m Learning iteration 505/2500 [0m                      

                       Computation: 44993 steps/s (collection: 2.074s, learning 0.111s)
             Mean action noise std: 2.10
          Mean value_function loss: 2.7237
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 66.7182
                       Mean reward: 9.83
               Mean episode length: 249.02
    Episode_Reward/reaching_object: 2.0160
     Episode_Reward/lifting_object: -0.0392
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 2.18s
                      Time elapsed: 00:18:12
                               ETA: 01:11:46

################################################################################
                     [1m Learning iteration 506/2500 [0m                      

                       Computation: 44584 steps/s (collection: 2.081s, learning 0.124s)
             Mean action noise std: 2.11
          Mean value_function loss: 0.7400
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 66.7458
                       Mean reward: 9.86
               Mean episode length: 249.09
    Episode_Reward/reaching_object: 2.0654
     Episode_Reward/lifting_object: -0.1513
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 2.20s
                      Time elapsed: 00:18:14
                               ETA: 01:11:44

################################################################################
                     [1m Learning iteration 507/2500 [0m                      

                       Computation: 45132 steps/s (collection: 2.058s, learning 0.120s)
             Mean action noise std: 2.11
          Mean value_function loss: 1.7923
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.7880
                       Mean reward: 7.64
               Mean episode length: 247.37
    Episode_Reward/reaching_object: 1.9488
     Episode_Reward/lifting_object: -0.1682
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 2.18s
                      Time elapsed: 00:18:16
                               ETA: 01:11:42

################################################################################
                     [1m Learning iteration 508/2500 [0m                      

                       Computation: 45024 steps/s (collection: 2.061s, learning 0.122s)
             Mean action noise std: 2.11
          Mean value_function loss: 0.5315
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 66.8182
                       Mean reward: 9.70
               Mean episode length: 248.13
    Episode_Reward/reaching_object: 2.0501
     Episode_Reward/lifting_object: -0.0211
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 2.18s
                      Time elapsed: 00:18:18
                               ETA: 01:11:40

################################################################################
                     [1m Learning iteration 509/2500 [0m                      

                       Computation: 44364 steps/s (collection: 2.092s, learning 0.124s)
             Mean action noise std: 2.11
          Mean value_function loss: 1.1683
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 66.8511
                       Mean reward: 10.44
               Mean episode length: 246.07
    Episode_Reward/reaching_object: 2.0748
     Episode_Reward/lifting_object: -0.0783
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 2.22s
                      Time elapsed: 00:18:21
                               ETA: 01:11:38

################################################################################
                     [1m Learning iteration 510/2500 [0m                      

                       Computation: 44836 steps/s (collection: 2.070s, learning 0.122s)
             Mean action noise std: 2.11
          Mean value_function loss: 1.1526
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 66.8847
                       Mean reward: 9.52
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 2.0543
     Episode_Reward/lifting_object: -0.1491
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 2.19s
                      Time elapsed: 00:18:23
                               ETA: 01:11:36

################################################################################
                     [1m Learning iteration 511/2500 [0m                      

                       Computation: 44993 steps/s (collection: 2.060s, learning 0.125s)
             Mean action noise std: 2.12
          Mean value_function loss: 0.8252
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 66.9131
                       Mean reward: 8.65
               Mean episode length: 248.92
    Episode_Reward/reaching_object: 2.0006
     Episode_Reward/lifting_object: -0.1913
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 2.18s
                      Time elapsed: 00:18:25
                               ETA: 01:11:34

################################################################################
                     [1m Learning iteration 512/2500 [0m                      

                       Computation: 45293 steps/s (collection: 2.068s, learning 0.103s)
             Mean action noise std: 2.12
          Mean value_function loss: 0.2348
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 66.9429
                       Mean reward: 9.69
               Mean episode length: 248.38
    Episode_Reward/reaching_object: 2.0355
     Episode_Reward/lifting_object: -0.1562
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0177
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 2.17s
                      Time elapsed: 00:18:27
                               ETA: 01:11:32

################################################################################
                     [1m Learning iteration 513/2500 [0m                      

                       Computation: 45958 steps/s (collection: 2.032s, learning 0.107s)
             Mean action noise std: 2.12
          Mean value_function loss: 0.0560
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 66.9965
                       Mean reward: 9.65
               Mean episode length: 249.74
    Episode_Reward/reaching_object: 2.0977
     Episode_Reward/lifting_object: -0.0585
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 2.14s
                      Time elapsed: 00:18:29
                               ETA: 01:11:29

################################################################################
                     [1m Learning iteration 514/2500 [0m                      

                       Computation: 44920 steps/s (collection: 2.063s, learning 0.125s)
             Mean action noise std: 2.13
          Mean value_function loss: 0.3102
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 67.0490
                       Mean reward: 9.31
               Mean episode length: 247.76
    Episode_Reward/reaching_object: 2.0180
     Episode_Reward/lifting_object: -0.1450
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 2.19s
                      Time elapsed: 00:18:31
                               ETA: 01:11:27

################################################################################
                     [1m Learning iteration 515/2500 [0m                      

                       Computation: 43543 steps/s (collection: 2.139s, learning 0.118s)
             Mean action noise std: 2.13
          Mean value_function loss: 1.3979
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 67.0872
                       Mean reward: 9.32
               Mean episode length: 247.70
    Episode_Reward/reaching_object: 2.0466
     Episode_Reward/lifting_object: -0.0959
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 2.26s
                      Time elapsed: 00:18:34
                               ETA: 01:11:25

################################################################################
                     [1m Learning iteration 516/2500 [0m                      

                       Computation: 44373 steps/s (collection: 2.089s, learning 0.126s)
             Mean action noise std: 2.13
          Mean value_function loss: 0.9312
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 67.1014
                       Mean reward: 8.01
               Mean episode length: 248.73
    Episode_Reward/reaching_object: 2.0302
     Episode_Reward/lifting_object: -0.0955
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 2.22s
                      Time elapsed: 00:18:36
                               ETA: 01:11:24

################################################################################
                     [1m Learning iteration 517/2500 [0m                      

                       Computation: 44198 steps/s (collection: 2.099s, learning 0.126s)
             Mean action noise std: 2.13
          Mean value_function loss: 0.7785
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 67.1260
                       Mean reward: 9.86
               Mean episode length: 248.86
    Episode_Reward/reaching_object: 1.9985
     Episode_Reward/lifting_object: 0.0230
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 2.22s
                      Time elapsed: 00:18:38
                               ETA: 01:11:22

################################################################################
                     [1m Learning iteration 518/2500 [0m                      

                       Computation: 43777 steps/s (collection: 2.113s, learning 0.133s)
             Mean action noise std: 2.14
          Mean value_function loss: 0.1942
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 67.1691
                       Mean reward: 9.92
               Mean episode length: 248.90
    Episode_Reward/reaching_object: 2.0770
     Episode_Reward/lifting_object: -0.0435
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 2.25s
                      Time elapsed: 00:18:40
                               ETA: 01:11:20

################################################################################
                     [1m Learning iteration 519/2500 [0m                      

                       Computation: 43594 steps/s (collection: 2.140s, learning 0.115s)
             Mean action noise std: 2.14
          Mean value_function loss: 0.9352
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 67.2248
                       Mean reward: 9.34
               Mean episode length: 249.60
    Episode_Reward/reaching_object: 1.9842
     Episode_Reward/lifting_object: -0.1714
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 2.25s
                      Time elapsed: 00:18:43
                               ETA: 01:11:18

################################################################################
                     [1m Learning iteration 520/2500 [0m                      

                       Computation: 44198 steps/s (collection: 2.106s, learning 0.119s)
             Mean action noise std: 2.14
          Mean value_function loss: 0.2847
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 67.2374
                       Mean reward: 9.11
               Mean episode length: 247.39
    Episode_Reward/reaching_object: 2.0170
     Episode_Reward/lifting_object: -0.0063
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 2.22s
                      Time elapsed: 00:18:45
                               ETA: 01:11:16

################################################################################
                     [1m Learning iteration 521/2500 [0m                      

                       Computation: 45011 steps/s (collection: 2.064s, learning 0.120s)
             Mean action noise std: 2.14
          Mean value_function loss: 0.1250
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 67.2757
                       Mean reward: 10.96
               Mean episode length: 247.30
    Episode_Reward/reaching_object: 2.1307
     Episode_Reward/lifting_object: 0.0390
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 2.18s
                      Time elapsed: 00:18:47
                               ETA: 01:11:14

################################################################################
                     [1m Learning iteration 522/2500 [0m                      

                       Computation: 44743 steps/s (collection: 2.077s, learning 0.120s)
             Mean action noise std: 2.15
          Mean value_function loss: 0.4513
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 67.3345
                       Mean reward: 9.16
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 2.0298
     Episode_Reward/lifting_object: -0.1264
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 2.20s
                      Time elapsed: 00:18:49
                               ETA: 01:11:12

################################################################################
                     [1m Learning iteration 523/2500 [0m                      

                       Computation: 43970 steps/s (collection: 2.117s, learning 0.119s)
             Mean action noise std: 2.15
          Mean value_function loss: 0.1031
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 67.3964
                       Mean reward: 10.38
               Mean episode length: 248.66
    Episode_Reward/reaching_object: 2.0304
     Episode_Reward/lifting_object: 0.0245
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 2.24s
                      Time elapsed: 00:18:51
                               ETA: 01:11:10

################################################################################
                     [1m Learning iteration 524/2500 [0m                      

                       Computation: 42763 steps/s (collection: 2.193s, learning 0.106s)
             Mean action noise std: 2.15
          Mean value_function loss: 1.4704
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 67.4451
                       Mean reward: 10.12
               Mean episode length: 249.20
    Episode_Reward/reaching_object: 2.0733
     Episode_Reward/lifting_object: 0.0560
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 2.30s
                      Time elapsed: 00:18:54
                               ETA: 01:11:08

################################################################################
                     [1m Learning iteration 525/2500 [0m                      

                       Computation: 44756 steps/s (collection: 2.097s, learning 0.099s)
             Mean action noise std: 2.15
          Mean value_function loss: 0.3533
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 67.4591
                       Mean reward: 10.86
               Mean episode length: 249.61
    Episode_Reward/reaching_object: 2.0543
     Episode_Reward/lifting_object: -0.1053
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 2.20s
                      Time elapsed: 00:18:56
                               ETA: 01:11:06

################################################################################
                     [1m Learning iteration 526/2500 [0m                      

                       Computation: 44750 steps/s (collection: 2.079s, learning 0.118s)
             Mean action noise std: 2.16
          Mean value_function loss: 0.9240
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 67.4841
                       Mean reward: 11.28
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 1.9919
     Episode_Reward/lifting_object: 0.0595
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.0833
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 2.20s
                      Time elapsed: 00:18:58
                               ETA: 01:11:04

################################################################################
                     [1m Learning iteration 527/2500 [0m                      

                       Computation: 45454 steps/s (collection: 2.070s, learning 0.093s)
             Mean action noise std: 2.16
          Mean value_function loss: 0.1526
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 67.5250
                       Mean reward: 10.81
               Mean episode length: 246.74
    Episode_Reward/reaching_object: 2.0718
     Episode_Reward/lifting_object: 0.0298
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0178
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 2.16s
                      Time elapsed: 00:19:00
                               ETA: 01:11:02

################################################################################
                     [1m Learning iteration 528/2500 [0m                      

                       Computation: 44622 steps/s (collection: 2.081s, learning 0.122s)
             Mean action noise std: 2.16
          Mean value_function loss: 1.2919
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 67.5759
                       Mean reward: 9.46
               Mean episode length: 246.69
    Episode_Reward/reaching_object: 2.0350
     Episode_Reward/lifting_object: -0.0120
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0179
      Episode_Termination/time_out: 17.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 2.20s
                      Time elapsed: 00:19:02
                               ETA: 01:11:00

################################################################################
                     [1m Learning iteration 529/2500 [0m                      

                       Computation: 44741 steps/s (collection: 2.069s, learning 0.129s)
             Mean action noise std: 2.16
          Mean value_function loss: 0.3802
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 67.5873
                       Mean reward: 9.73
               Mean episode length: 248.77
    Episode_Reward/reaching_object: 2.0467
     Episode_Reward/lifting_object: 0.0330
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 2.20s
                      Time elapsed: 00:19:05
                               ETA: 01:10:58

################################################################################
                     [1m Learning iteration 530/2500 [0m                      

                       Computation: 44358 steps/s (collection: 2.076s, learning 0.140s)
             Mean action noise std: 2.17
          Mean value_function loss: 0.3044
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.6326
                       Mean reward: 10.19
               Mean episode length: 247.43
    Episode_Reward/reaching_object: 2.0365
     Episode_Reward/lifting_object: -0.0478
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 2.22s
                      Time elapsed: 00:19:07
                               ETA: 01:10:56

################################################################################
                     [1m Learning iteration 531/2500 [0m                      

                       Computation: 45223 steps/s (collection: 2.069s, learning 0.105s)
             Mean action noise std: 2.17
          Mean value_function loss: 0.2497
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 67.7078
                       Mean reward: 9.06
               Mean episode length: 249.51
    Episode_Reward/reaching_object: 2.0334
     Episode_Reward/lifting_object: -0.0423
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0180
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 2.17s
                      Time elapsed: 00:19:09
                               ETA: 01:10:54

################################################################################
                     [1m Learning iteration 532/2500 [0m                      

                       Computation: 44822 steps/s (collection: 2.099s, learning 0.095s)
             Mean action noise std: 2.18
          Mean value_function loss: 0.7339
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 67.7532
                       Mean reward: 10.45
               Mean episode length: 247.44
    Episode_Reward/reaching_object: 2.0878
     Episode_Reward/lifting_object: 0.0194
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 2.19s
                      Time elapsed: 00:19:11
                               ETA: 01:10:52

################################################################################
                     [1m Learning iteration 533/2500 [0m                      

                       Computation: 44966 steps/s (collection: 2.067s, learning 0.120s)
             Mean action noise std: 2.18
          Mean value_function loss: 0.4354
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.8073
                       Mean reward: 10.90
               Mean episode length: 249.01
    Episode_Reward/reaching_object: 2.1064
     Episode_Reward/lifting_object: 0.0190
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 2.19s
                      Time elapsed: 00:19:13
                               ETA: 01:10:50

################################################################################
                     [1m Learning iteration 534/2500 [0m                      

                       Computation: 44748 steps/s (collection: 2.075s, learning 0.122s)
             Mean action noise std: 2.18
          Mean value_function loss: 0.6599
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 67.8697
                       Mean reward: 10.62
               Mean episode length: 249.64
    Episode_Reward/reaching_object: 2.0706
     Episode_Reward/lifting_object: -0.0540
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0181
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 2.20s
                      Time elapsed: 00:19:16
                               ETA: 01:10:48

################################################################################
                     [1m Learning iteration 535/2500 [0m                      

                       Computation: 44581 steps/s (collection: 2.088s, learning 0.117s)
             Mean action noise std: 2.19
          Mean value_function loss: 1.6289
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 67.9112
                       Mean reward: 10.28
               Mean episode length: 249.24
    Episode_Reward/reaching_object: 2.0177
     Episode_Reward/lifting_object: 0.0181
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0182
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 2.21s
                      Time elapsed: 00:19:18
                               ETA: 01:10:46

################################################################################
                     [1m Learning iteration 536/2500 [0m                      

                       Computation: 45145 steps/s (collection: 2.059s, learning 0.118s)
             Mean action noise std: 2.19
          Mean value_function loss: 0.4099
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.9425
                       Mean reward: 10.71
               Mean episode length: 249.65
    Episode_Reward/reaching_object: 2.1717
     Episode_Reward/lifting_object: 0.0420
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 2.18s
                      Time elapsed: 00:19:20
                               ETA: 01:10:44

################################################################################
                     [1m Learning iteration 537/2500 [0m                      

                       Computation: 44631 steps/s (collection: 2.090s, learning 0.113s)
             Mean action noise std: 2.19
          Mean value_function loss: 0.2863
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 67.9991
                       Mean reward: 9.02
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 2.0538
     Episode_Reward/lifting_object: -0.0812
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0183
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 2.20s
                      Time elapsed: 00:19:22
                               ETA: 01:10:42

################################################################################
                     [1m Learning iteration 538/2500 [0m                      

                       Computation: 44695 steps/s (collection: 2.104s, learning 0.096s)
             Mean action noise std: 2.20
          Mean value_function loss: 0.3129
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 68.0772
                       Mean reward: 10.53
               Mean episode length: 249.45
    Episode_Reward/reaching_object: 2.1267
     Episode_Reward/lifting_object: 0.0419
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 17.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1250
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 2.20s
                      Time elapsed: 00:19:24
                               ETA: 01:10:40

################################################################################
                     [1m Learning iteration 539/2500 [0m                      

                       Computation: 44612 steps/s (collection: 2.089s, learning 0.115s)
             Mean action noise std: 2.20
          Mean value_function loss: 0.3972
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 68.1460
                       Mean reward: 10.28
               Mean episode length: 248.57
    Episode_Reward/reaching_object: 2.0885
     Episode_Reward/lifting_object: -0.0361
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0185
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 2.20s
                      Time elapsed: 00:19:27
                               ETA: 01:10:38

################################################################################
                     [1m Learning iteration 540/2500 [0m                      

                       Computation: 44059 steps/s (collection: 2.132s, learning 0.100s)
             Mean action noise std: 2.21
          Mean value_function loss: 0.5757
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 68.1929
                       Mean reward: 10.42
               Mean episode length: 248.22
    Episode_Reward/reaching_object: 2.0330
     Episode_Reward/lifting_object: 0.0693
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.0184
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 2.23s
                      Time elapsed: 00:19:29
                               ETA: 01:10:36

################################################################################
                     [1m Learning iteration 541/2500 [0m                      

                       Computation: 44046 steps/s (collection: 2.126s, learning 0.106s)
             Mean action noise std: 2.21
          Mean value_function loss: 0.8179
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 68.2507
                       Mean reward: 10.06
               Mean episode length: 249.54
    Episode_Reward/reaching_object: 2.0961
     Episode_Reward/lifting_object: -0.0122
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0189
      Episode_Termination/time_out: 16.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 2.23s
                      Time elapsed: 00:19:31
                               ETA: 01:10:34

################################################################################
                     [1m Learning iteration 542/2500 [0m                      

                       Computation: 43725 steps/s (collection: 2.142s, learning 0.106s)
             Mean action noise std: 2.21
          Mean value_function loss: 0.7837
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 68.3005
                       Mean reward: 10.94
               Mean episode length: 249.78
    Episode_Reward/reaching_object: 2.1276
     Episode_Reward/lifting_object: 0.1031
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.1667
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 2.25s
                      Time elapsed: 00:19:33
                               ETA: 01:10:32

################################################################################
                     [1m Learning iteration 543/2500 [0m                      

                       Computation: 43490 steps/s (collection: 2.156s, learning 0.105s)
             Mean action noise std: 2.22
          Mean value_function loss: 0.9637
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 68.3372
                       Mean reward: 9.17
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 2.1015
     Episode_Reward/lifting_object: -0.0978
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0190
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2500
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 2.26s
                      Time elapsed: 00:19:36
                               ETA: 01:10:30

################################################################################
                     [1m Learning iteration 544/2500 [0m                      

                       Computation: 43957 steps/s (collection: 2.128s, learning 0.108s)
             Mean action noise std: 2.22
          Mean value_function loss: 0.9454
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 68.3534
                       Mean reward: 9.92
               Mean episode length: 249.30
    Episode_Reward/reaching_object: 2.0747
     Episode_Reward/lifting_object: 0.0306
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0193
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 2.24s
                      Time elapsed: 00:19:38
                               ETA: 01:10:29

################################################################################
                     [1m Learning iteration 545/2500 [0m                      

                       Computation: 44498 steps/s (collection: 2.117s, learning 0.093s)
             Mean action noise std: 2.22
          Mean value_function loss: 1.0816
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 68.3863
                       Mean reward: 11.69
               Mean episode length: 245.67
    Episode_Reward/reaching_object: 2.0970
     Episode_Reward/lifting_object: -0.0087
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0195
      Episode_Termination/time_out: 17.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 2.21s
                      Time elapsed: 00:19:40
                               ETA: 01:10:27

################################################################################
                     [1m Learning iteration 546/2500 [0m                      

                       Computation: 43721 steps/s (collection: 2.147s, learning 0.101s)
             Mean action noise std: 2.22
          Mean value_function loss: 4.5052
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 68.4167
                       Mean reward: 9.72
               Mean episode length: 247.43
    Episode_Reward/reaching_object: 2.0909
     Episode_Reward/lifting_object: -0.1544
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0196
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 2.25s
                      Time elapsed: 00:19:42
                               ETA: 01:10:25

################################################################################
                     [1m Learning iteration 547/2500 [0m                      

                       Computation: 43915 steps/s (collection: 2.123s, learning 0.115s)
             Mean action noise std: 2.22
          Mean value_function loss: 1.3474
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 68.4356
                       Mean reward: 7.15
               Mean episode length: 245.00
    Episode_Reward/reaching_object: 2.1246
     Episode_Reward/lifting_object: -0.2605
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 2.24s
                      Time elapsed: 00:19:45
                               ETA: 01:10:23

################################################################################
                     [1m Learning iteration 548/2500 [0m                      

                       Computation: 43669 steps/s (collection: 2.141s, learning 0.110s)
             Mean action noise std: 2.23
          Mean value_function loss: 1.5608
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 68.4684
                       Mean reward: 9.72
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 2.1037
     Episode_Reward/lifting_object: -0.0662
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0329
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.7917
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 2.25s
                      Time elapsed: 00:19:47
                               ETA: 01:10:21

################################################################################
                     [1m Learning iteration 549/2500 [0m                      

                       Computation: 44172 steps/s (collection: 2.131s, learning 0.095s)
             Mean action noise std: 2.23
          Mean value_function loss: 0.7645
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 68.4955
                       Mean reward: 9.95
               Mean episode length: 248.79
    Episode_Reward/reaching_object: 2.1480
     Episode_Reward/lifting_object: -0.0059
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0333
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 2.23s
                      Time elapsed: 00:19:49
                               ETA: 01:10:19

################################################################################
                     [1m Learning iteration 550/2500 [0m                      

                       Computation: 44490 steps/s (collection: 2.107s, learning 0.102s)
             Mean action noise std: 2.23
          Mean value_function loss: 0.5899
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 68.5422
                       Mean reward: 10.60
               Mean episode length: 245.73
    Episode_Reward/reaching_object: 2.1034
     Episode_Reward/lifting_object: 0.0001
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 2.21s
                      Time elapsed: 00:19:51
                               ETA: 01:10:17

################################################################################
                     [1m Learning iteration 551/2500 [0m                      

                       Computation: 44045 steps/s (collection: 2.128s, learning 0.104s)
             Mean action noise std: 2.23
          Mean value_function loss: 0.5841
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 68.5839
                       Mean reward: 10.62
               Mean episode length: 247.49
    Episode_Reward/reaching_object: 2.1368
     Episode_Reward/lifting_object: -0.1716
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0198
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 2.23s
                      Time elapsed: 00:19:53
                               ETA: 01:10:15

################################################################################
                     [1m Learning iteration 552/2500 [0m                      

                       Computation: 44621 steps/s (collection: 2.086s, learning 0.118s)
             Mean action noise std: 2.24
          Mean value_function loss: 0.9547
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 68.6370
                       Mean reward: 7.38
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 2.1322
     Episode_Reward/lifting_object: -0.1142
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 2.20s
                      Time elapsed: 00:19:56
                               ETA: 01:10:13

################################################################################
                     [1m Learning iteration 553/2500 [0m                      

                       Computation: 45501 steps/s (collection: 2.064s, learning 0.096s)
             Mean action noise std: 2.24
          Mean value_function loss: 0.9271
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 68.6770
                       Mean reward: 10.65
               Mean episode length: 244.31
    Episode_Reward/reaching_object: 2.0609
     Episode_Reward/lifting_object: 0.0240
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0197
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5417
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 2.16s
                      Time elapsed: 00:19:58
                               ETA: 01:10:11

################################################################################
                     [1m Learning iteration 554/2500 [0m                      

                       Computation: 44589 steps/s (collection: 2.092s, learning 0.113s)
             Mean action noise std: 2.24
          Mean value_function loss: 1.8320
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 68.7372
                       Mean reward: 10.21
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 2.1040
     Episode_Reward/lifting_object: 0.0389
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 2.20s
                      Time elapsed: 00:20:00
                               ETA: 01:10:09

################################################################################
                     [1m Learning iteration 555/2500 [0m                      

                       Computation: 43513 steps/s (collection: 2.132s, learning 0.127s)
             Mean action noise std: 2.24
          Mean value_function loss: 1.0935
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 68.7484
                       Mean reward: 10.37
               Mean episode length: 250.00
    Episode_Reward/reaching_object: 2.0940
     Episode_Reward/lifting_object: -0.0791
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 2.26s
                      Time elapsed: 00:20:02
                               ETA: 01:10:07

################################################################################
                     [1m Learning iteration 556/2500 [0m                      

                       Computation: 44116 steps/s (collection: 2.123s, learning 0.106s)
             Mean action noise std: 2.25
          Mean value_function loss: 1.2877
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 68.7734
                       Mean reward: 10.87
               Mean episode length: 248.42
    Episode_Reward/reaching_object: 2.0705
     Episode_Reward/lifting_object: -0.0437
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0202
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2083
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 2.23s
                      Time elapsed: 00:20:05
                               ETA: 01:10:05

################################################################################
                     [1m Learning iteration 557/2500 [0m                      

                       Computation: 44415 steps/s (collection: 2.093s, learning 0.120s)
             Mean action noise std: 2.25
          Mean value_function loss: 0.5725
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 68.8220
                       Mean reward: 10.40
               Mean episode length: 244.50
    Episode_Reward/reaching_object: 2.0625
     Episode_Reward/lifting_object: 0.0768
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0200
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4583
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 2.21s
                      Time elapsed: 00:20:07
                               ETA: 01:10:03

################################################################################
                     [1m Learning iteration 558/2500 [0m                      

                       Computation: 42863 steps/s (collection: 2.168s, learning 0.126s)
             Mean action noise std: 2.25
          Mean value_function loss: 1.5048
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 68.8440
                       Mean reward: 11.83
               Mean episode length: 248.46
    Episode_Reward/reaching_object: 2.0677
     Episode_Reward/lifting_object: 0.1278
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0203
      Episode_Termination/time_out: 17.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 2.29s
                      Time elapsed: 00:20:09
                               ETA: 01:10:01

################################################################################
                     [1m Learning iteration 559/2500 [0m                      

                       Computation: 43805 steps/s (collection: 2.117s, learning 0.127s)
             Mean action noise std: 2.25
          Mean value_function loss: 1.2163
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 68.8806
                       Mean reward: 9.59
               Mean episode length: 246.19
    Episode_Reward/reaching_object: 2.0761
     Episode_Reward/lifting_object: 0.1041
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0201
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.4167
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 2.24s
                      Time elapsed: 00:20:11
                               ETA: 01:10:00

################################################################################
                     [1m Learning iteration 560/2500 [0m                      

                       Computation: 43711 steps/s (collection: 2.119s, learning 0.130s)
             Mean action noise std: 2.26
          Mean value_function loss: 2.9488
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 68.9090
                       Mean reward: 10.00
               Mean episode length: 245.35
    Episode_Reward/reaching_object: 2.0028
     Episode_Reward/lifting_object: -0.0638
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0199
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.3750
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 2.25s
                      Time elapsed: 00:20:14
                               ETA: 01:09:58

################################################################################
                     [1m Learning iteration 561/2500 [0m                      

                       Computation: 42644 steps/s (collection: 2.196s, learning 0.110s)
             Mean action noise std: 2.26
          Mean value_function loss: 3.1212
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 68.9237
                       Mean reward: 9.86
               Mean episode length: 247.77
    Episode_Reward/reaching_object: 2.0222
     Episode_Reward/lifting_object: 0.0035
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0209
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 2.31s
                      Time elapsed: 00:20:16
                               ETA: 01:09:56

################################################################################
                     [1m Learning iteration 562/2500 [0m                      

                       Computation: 44809 steps/s (collection: 2.085s, learning 0.109s)
             Mean action noise std: 2.26
          Mean value_function loss: 1.2487
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 68.9578
                       Mean reward: 10.48
               Mean episode length: 245.31
    Episode_Reward/reaching_object: 2.0719
     Episode_Reward/lifting_object: 0.0832
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 2.19s
                      Time elapsed: 00:20:18
                               ETA: 01:09:54

################################################################################
                     [1m Learning iteration 563/2500 [0m                      

                       Computation: 44672 steps/s (collection: 2.080s, learning 0.120s)
             Mean action noise std: 2.26
          Mean value_function loss: 1.5979
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 69.0073
                       Mean reward: 10.13
               Mean episode length: 248.75
    Episode_Reward/reaching_object: 2.0448
     Episode_Reward/lifting_object: 0.0265
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0210
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 2.20s
                      Time elapsed: 00:20:20
                               ETA: 01:09:52

################################################################################
                     [1m Learning iteration 564/2500 [0m                      

                       Computation: 44303 steps/s (collection: 2.104s, learning 0.115s)
             Mean action noise std: 2.27
          Mean value_function loss: 1.4036
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 69.0500
                       Mean reward: 10.49
               Mean episode length: 248.35
    Episode_Reward/reaching_object: 2.0311
     Episode_Reward/lifting_object: -0.0441
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0207
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 2.22s
                      Time elapsed: 00:20:22
                               ETA: 01:09:50

################################################################################
                     [1m Learning iteration 565/2500 [0m                      

                       Computation: 44294 steps/s (collection: 2.098s, learning 0.122s)
             Mean action noise std: 2.27
          Mean value_function loss: 1.3707
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.1073
                       Mean reward: 11.79
               Mean episode length: 245.95
    Episode_Reward/reaching_object: 2.0568
     Episode_Reward/lifting_object: 0.0942
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0215
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.2917
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 2.22s
                      Time elapsed: 00:20:25
                               ETA: 01:09:48

################################################################################
                     [1m Learning iteration 566/2500 [0m                      

                       Computation: 44353 steps/s (collection: 2.094s, learning 0.122s)
             Mean action noise std: 2.27
          Mean value_function loss: 1.9361
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 69.1344
                       Mean reward: 9.38
               Mean episode length: 248.45
    Episode_Reward/reaching_object: 2.0247
     Episode_Reward/lifting_object: -0.0525
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 17.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 2.22s
                      Time elapsed: 00:20:27
                               ETA: 01:09:46

################################################################################
                     [1m Learning iteration 567/2500 [0m                      

                       Computation: 45074 steps/s (collection: 2.087s, learning 0.094s)
             Mean action noise std: 2.28
          Mean value_function loss: 1.9112
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 69.1773
                       Mean reward: 11.37
               Mean episode length: 247.84
    Episode_Reward/reaching_object: 2.0494
     Episode_Reward/lifting_object: 0.0498
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0221
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.3333
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 2.18s
                      Time elapsed: 00:20:29
                               ETA: 01:09:44

################################################################################
                     [1m Learning iteration 568/2500 [0m                      

                       Computation: 44175 steps/s (collection: 2.110s, learning 0.116s)
             Mean action noise std: 2.28
          Mean value_function loss: 2.3391
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 69.2170
                       Mean reward: 8.69
               Mean episode length: 245.20
    Episode_Reward/reaching_object: 2.0374
     Episode_Reward/lifting_object: 0.1609
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0216
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.5000
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 2.23s
                      Time elapsed: 00:20:31
                               ETA: 01:09:42

################################################################################
                     [1m Learning iteration 569/2500 [0m                      

                       Computation: 43903 steps/s (collection: 2.113s, learning 0.127s)
             Mean action noise std: 2.28
          Mean value_function loss: 3.4783
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.2609
                       Mean reward: 9.98
               Mean episode length: 240.54
    Episode_Reward/reaching_object: 2.0669
     Episode_Reward/lifting_object: -0.0373
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0217
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 2.24s
                      Time elapsed: 00:20:34
                               ETA: 01:09:40

################################################################################
                     [1m Learning iteration 570/2500 [0m                      

                       Computation: 43536 steps/s (collection: 2.127s, learning 0.131s)
             Mean action noise std: 2.29
          Mean value_function loss: 4.6428
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 69.3104
                       Mean reward: 10.72
               Mean episode length: 246.37
    Episode_Reward/reaching_object: 2.1051
     Episode_Reward/lifting_object: 0.1487
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0224
      Episode_Termination/time_out: 17.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 0.7083
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 2.26s
                      Time elapsed: 00:20:36
                               ETA: 01:09:38

################################################################################
                     [1m Learning iteration 571/2500 [0m                      

                       Computation: 43458 steps/s (collection: 2.135s, learning 0.127s)
             Mean action noise std: 2.29
          Mean value_function loss: 5.3632
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 69.3434
                       Mean reward: 12.82
               Mean episode length: 245.07
    Episode_Reward/reaching_object: 2.0725
     Episode_Reward/lifting_object: 0.3063
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0227
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 2.26s
                      Time elapsed: 00:20:38
                               ETA: 01:09:36

################################################################################
                     [1m Learning iteration 572/2500 [0m                      

                       Computation: 43713 steps/s (collection: 2.132s, learning 0.117s)
             Mean action noise std: 2.29
          Mean value_function loss: 6.9540
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 69.3591
                       Mean reward: 12.28
               Mean episode length: 237.09
    Episode_Reward/reaching_object: 2.0277
     Episode_Reward/lifting_object: 0.4452
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0225
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 2.25s
                      Time elapsed: 00:20:40
                               ETA: 01:09:34

################################################################################
                     [1m Learning iteration 573/2500 [0m                      

                       Computation: 43955 steps/s (collection: 2.106s, learning 0.131s)
             Mean action noise std: 2.29
          Mean value_function loss: 6.1096
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 69.3657
                       Mean reward: 8.20
               Mean episode length: 245.19
    Episode_Reward/reaching_object: 2.0455
     Episode_Reward/lifting_object: -0.1307
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 2.24s
                      Time elapsed: 00:20:43
                               ETA: 01:09:32

################################################################################
                     [1m Learning iteration 574/2500 [0m                      

                       Computation: 44715 steps/s (collection: 2.077s, learning 0.121s)
             Mean action noise std: 2.29
          Mean value_function loss: 4.3167
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 69.3875
                       Mean reward: 11.05
               Mean episode length: 236.65
    Episode_Reward/reaching_object: 1.9624
     Episode_Reward/lifting_object: 0.3685
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0235
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 2.20s
                      Time elapsed: 00:20:45
                               ETA: 01:09:30

################################################################################
                     [1m Learning iteration 575/2500 [0m                      

                       Computation: 44812 steps/s (collection: 2.084s, learning 0.110s)
             Mean action noise std: 2.29
          Mean value_function loss: 4.2446
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 69.4277
                       Mean reward: 7.46
               Mean episode length: 246.35
    Episode_Reward/reaching_object: 2.0030
     Episode_Reward/lifting_object: 0.2653
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0239
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 0.5833
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 2.19s
                      Time elapsed: 00:20:47
                               ETA: 01:09:28

################################################################################
                     [1m Learning iteration 576/2500 [0m                      

                       Computation: 44103 steps/s (collection: 2.106s, learning 0.123s)
             Mean action noise std: 2.30
          Mean value_function loss: 6.7738
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 69.4609
                       Mean reward: 8.55
               Mean episode length: 244.89
    Episode_Reward/reaching_object: 1.9437
     Episode_Reward/lifting_object: 0.2182
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0250
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 2.23s
                      Time elapsed: 00:20:49
                               ETA: 01:09:26

################################################################################
                     [1m Learning iteration 577/2500 [0m                      

                       Computation: 42430 steps/s (collection: 2.191s, learning 0.126s)
             Mean action noise std: 2.30
          Mean value_function loss: 6.1805
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 69.5006
                       Mean reward: 12.51
               Mean episode length: 239.62
    Episode_Reward/reaching_object: 1.8712
     Episode_Reward/lifting_object: 0.1984
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0254
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 2.32s
                      Time elapsed: 00:20:51
                               ETA: 01:09:25

################################################################################
                     [1m Learning iteration 578/2500 [0m                      

                       Computation: 43716 steps/s (collection: 2.143s, learning 0.106s)
             Mean action noise std: 2.30
          Mean value_function loss: 9.3815
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 69.5523
                       Mean reward: 5.01
               Mean episode length: 237.65
    Episode_Reward/reaching_object: 1.7786
     Episode_Reward/lifting_object: 0.0687
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0252
      Episode_Termination/time_out: 13.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 2.25s
                      Time elapsed: 00:20:54
                               ETA: 01:09:23

################################################################################
                     [1m Learning iteration 579/2500 [0m                      

                       Computation: 44922 steps/s (collection: 2.075s, learning 0.114s)
             Mean action noise std: 2.31
          Mean value_function loss: 5.4094
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 69.6096
                       Mean reward: 12.11
               Mean episode length: 239.21
    Episode_Reward/reaching_object: 1.8153
     Episode_Reward/lifting_object: 0.3203
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0263
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 2.19s
                      Time elapsed: 00:20:56
                               ETA: 01:09:21

################################################################################
                     [1m Learning iteration 580/2500 [0m                      

                       Computation: 44479 steps/s (collection: 2.096s, learning 0.114s)
             Mean action noise std: 2.31
          Mean value_function loss: 7.9611
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 69.6432
                       Mean reward: 11.50
               Mean episode length: 244.14
    Episode_Reward/reaching_object: 1.7958
     Episode_Reward/lifting_object: 0.4202
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0265
      Episode_Termination/time_out: 17.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 2.21s
                      Time elapsed: 00:20:58
                               ETA: 01:09:19

################################################################################
                     [1m Learning iteration 581/2500 [0m                      

                       Computation: 44680 steps/s (collection: 2.087s, learning 0.113s)
             Mean action noise std: 2.31
          Mean value_function loss: 6.5860
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 69.6674
                       Mean reward: 8.70
               Mean episode length: 247.62
    Episode_Reward/reaching_object: 1.7568
     Episode_Reward/lifting_object: 0.2167
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 2.20s
                      Time elapsed: 00:21:00
                               ETA: 01:09:17

################################################################################
                     [1m Learning iteration 582/2500 [0m                      

                       Computation: 44447 steps/s (collection: 2.089s, learning 0.123s)
             Mean action noise std: 2.32
          Mean value_function loss: 9.0840
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 69.7096
                       Mean reward: 9.22
               Mean episode length: 240.81
    Episode_Reward/reaching_object: 1.7041
     Episode_Reward/lifting_object: 0.3112
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0261
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 2.21s
                      Time elapsed: 00:21:03
                               ETA: 01:09:15

################################################################################
                     [1m Learning iteration 583/2500 [0m                      

                       Computation: 45233 steps/s (collection: 2.061s, learning 0.112s)
             Mean action noise std: 2.32
          Mean value_function loss: 5.7518
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.7571
                       Mean reward: 13.46
               Mean episode length: 243.95
    Episode_Reward/reaching_object: 1.7273
     Episode_Reward/lifting_object: 0.4943
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0266
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 2.17s
                      Time elapsed: 00:21:05
                               ETA: 01:09:12

################################################################################
                     [1m Learning iteration 584/2500 [0m                      

                       Computation: 45110 steps/s (collection: 2.061s, learning 0.119s)
             Mean action noise std: 2.32
          Mean value_function loss: 6.7585
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 69.7968
                       Mean reward: 10.48
               Mean episode length: 243.84
    Episode_Reward/reaching_object: 1.7074
     Episode_Reward/lifting_object: 0.6123
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0267
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 2.18s
                      Time elapsed: 00:21:07
                               ETA: 01:09:10

################################################################################
                     [1m Learning iteration 585/2500 [0m                      

                       Computation: 44995 steps/s (collection: 2.092s, learning 0.093s)
             Mean action noise std: 2.33
          Mean value_function loss: 4.8759
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 69.8412
                       Mean reward: 10.16
               Mean episode length: 244.09
    Episode_Reward/reaching_object: 1.7245
     Episode_Reward/lifting_object: 0.7054
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0269
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 2.18s
                      Time elapsed: 00:21:09
                               ETA: 01:09:08

################################################################################
                     [1m Learning iteration 586/2500 [0m                      

                       Computation: 44722 steps/s (collection: 2.103s, learning 0.095s)
             Mean action noise std: 2.33
          Mean value_function loss: 7.8281
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 69.8833
                       Mean reward: 12.59
               Mean episode length: 244.84
    Episode_Reward/reaching_object: 1.7720
     Episode_Reward/lifting_object: 0.8373
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0278
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 2.20s
                      Time elapsed: 00:21:11
                               ETA: 01:09:06

################################################################################
                     [1m Learning iteration 587/2500 [0m                      

                       Computation: 45353 steps/s (collection: 2.049s, learning 0.119s)
             Mean action noise std: 2.33
          Mean value_function loss: 6.7007
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 69.9156
                       Mean reward: 14.06
               Mean episode length: 244.13
    Episode_Reward/reaching_object: 1.7637
     Episode_Reward/lifting_object: 0.9349
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 2.17s
                      Time elapsed: 00:21:13
                               ETA: 01:09:04

################################################################################
                     [1m Learning iteration 588/2500 [0m                      

                       Computation: 45481 steps/s (collection: 2.049s, learning 0.113s)
             Mean action noise std: 2.33
          Mean value_function loss: 9.1410
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 69.9454
                       Mean reward: 10.99
               Mean episode length: 233.97
    Episode_Reward/reaching_object: 1.7622
     Episode_Reward/lifting_object: 0.8545
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 2.16s
                      Time elapsed: 00:21:16
                               ETA: 01:09:02

################################################################################
                     [1m Learning iteration 589/2500 [0m                      

                       Computation: 45500 steps/s (collection: 2.041s, learning 0.119s)
             Mean action noise std: 2.34
          Mean value_function loss: 7.8783
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 69.9878
                       Mean reward: 15.81
               Mean episode length: 234.73
    Episode_Reward/reaching_object: 1.7682
     Episode_Reward/lifting_object: 1.2841
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0292
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 2.16s
                      Time elapsed: 00:21:18
                               ETA: 01:09:00

################################################################################
                     [1m Learning iteration 590/2500 [0m                      

                       Computation: 45693 steps/s (collection: 2.038s, learning 0.113s)
             Mean action noise std: 2.34
          Mean value_function loss: 10.0088
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 70.0265
                       Mean reward: 9.80
               Mean episode length: 238.27
    Episode_Reward/reaching_object: 1.6765
     Episode_Reward/lifting_object: 0.7741
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0277
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 2.15s
                      Time elapsed: 00:21:20
                               ETA: 01:08:57

################################################################################
                     [1m Learning iteration 591/2500 [0m                      

                       Computation: 45665 steps/s (collection: 2.044s, learning 0.109s)
             Mean action noise std: 2.34
          Mean value_function loss: 5.8431
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 70.0796
                       Mean reward: 12.31
               Mean episode length: 242.09
    Episode_Reward/reaching_object: 1.7240
     Episode_Reward/lifting_object: 1.0054
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0295
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 2.15s
                      Time elapsed: 00:21:22
                               ETA: 01:08:55

################################################################################
                     [1m Learning iteration 592/2500 [0m                      

                       Computation: 45518 steps/s (collection: 2.048s, learning 0.112s)
             Mean action noise std: 2.35
          Mean value_function loss: 7.7760
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 70.1326
                       Mean reward: 14.98
               Mean episode length: 237.31
    Episode_Reward/reaching_object: 1.6820
     Episode_Reward/lifting_object: 1.1262
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0268
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 2.16s
                      Time elapsed: 00:21:24
                               ETA: 01:08:53

################################################################################
                     [1m Learning iteration 593/2500 [0m                      

                       Computation: 45561 steps/s (collection: 2.060s, learning 0.097s)
             Mean action noise std: 2.35
          Mean value_function loss: 6.4854
               Mean surrogate loss: -0.0057
                 Mean entropy loss: 70.1747
                       Mean reward: 14.33
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 1.7484
     Episode_Reward/lifting_object: 1.0623
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 2.16s
                      Time elapsed: 00:21:26
                               ETA: 01:08:51

################################################################################
                     [1m Learning iteration 594/2500 [0m                      

                       Computation: 45716 steps/s (collection: 2.043s, learning 0.107s)
             Mean action noise std: 2.35
          Mean value_function loss: 8.3807
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 70.2182
                       Mean reward: 12.22
               Mean episode length: 235.01
    Episode_Reward/reaching_object: 1.6880
     Episode_Reward/lifting_object: 1.2280
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0294
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 2.15s
                      Time elapsed: 00:21:29
                               ETA: 01:08:49

################################################################################
                     [1m Learning iteration 595/2500 [0m                      

                       Computation: 45227 steps/s (collection: 2.060s, learning 0.114s)
             Mean action noise std: 2.36
          Mean value_function loss: 13.0239
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 70.2619
                       Mean reward: 15.97
               Mean episode length: 236.57
    Episode_Reward/reaching_object: 1.7481
     Episode_Reward/lifting_object: 1.3433
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0283
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 2.17s
                      Time elapsed: 00:21:31
                               ETA: 01:08:46

################################################################################
                     [1m Learning iteration 596/2500 [0m                      

                       Computation: 45874 steps/s (collection: 2.040s, learning 0.103s)
             Mean action noise std: 2.36
          Mean value_function loss: 12.2121
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 70.3019
                       Mean reward: 13.61
               Mean episode length: 235.61
    Episode_Reward/reaching_object: 1.6639
     Episode_Reward/lifting_object: 0.9064
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 2.14s
                      Time elapsed: 00:21:33
                               ETA: 01:08:44

################################################################################
                     [1m Learning iteration 597/2500 [0m                      

                       Computation: 45045 steps/s (collection: 2.068s, learning 0.115s)
             Mean action noise std: 2.36
          Mean value_function loss: 11.1965
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 70.3258
                       Mean reward: 12.50
               Mean episode length: 240.53
    Episode_Reward/reaching_object: 1.6839
     Episode_Reward/lifting_object: 1.2974
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0282
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 2.18s
                      Time elapsed: 00:21:35
                               ETA: 01:08:42

################################################################################
                     [1m Learning iteration 598/2500 [0m                      

                       Computation: 46130 steps/s (collection: 2.020s, learning 0.111s)
             Mean action noise std: 2.36
          Mean value_function loss: 9.6038
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 70.3636
                       Mean reward: 14.11
               Mean episode length: 232.97
    Episode_Reward/reaching_object: 1.6916
     Episode_Reward/lifting_object: 1.3153
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0290
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 2.13s
                      Time elapsed: 00:21:37
                               ETA: 01:08:40

################################################################################
                     [1m Learning iteration 599/2500 [0m                      

                       Computation: 45176 steps/s (collection: 2.062s, learning 0.114s)
             Mean action noise std: 2.37
          Mean value_function loss: 14.3193
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 70.4053
                       Mean reward: 17.09
               Mean episode length: 233.10
    Episode_Reward/reaching_object: 1.6218
     Episode_Reward/lifting_object: 1.7354
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0291
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 2.18s
                      Time elapsed: 00:21:39
                               ETA: 01:08:38

################################################################################
                     [1m Learning iteration 600/2500 [0m                      

                       Computation: 45607 steps/s (collection: 2.042s, learning 0.114s)
             Mean action noise std: 2.37
          Mean value_function loss: 12.6287
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 70.4536
                       Mean reward: 16.15
               Mean episode length: 235.65
    Episode_Reward/reaching_object: 1.6592
     Episode_Reward/lifting_object: 0.9959
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0310
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 2.16s
                      Time elapsed: 00:21:41
                               ETA: 01:08:36

################################################################################
                     [1m Learning iteration 601/2500 [0m                      

                       Computation: 45076 steps/s (collection: 2.060s, learning 0.121s)
             Mean action noise std: 2.38
          Mean value_function loss: 14.1679
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 70.5059
                       Mean reward: 15.84
               Mean episode length: 231.87
    Episode_Reward/reaching_object: 1.6119
     Episode_Reward/lifting_object: 1.2790
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0325
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 2.18s
                      Time elapsed: 00:21:44
                               ETA: 01:08:33

################################################################################
                     [1m Learning iteration 602/2500 [0m                      

                       Computation: 45097 steps/s (collection: 2.072s, learning 0.108s)
             Mean action noise std: 2.38
          Mean value_function loss: 15.8859
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 70.5441
                       Mean reward: 15.28
               Mean episode length: 222.26
    Episode_Reward/reaching_object: 1.5734
     Episode_Reward/lifting_object: 1.2906
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 2.18s
                      Time elapsed: 00:21:46
                               ETA: 01:08:31

################################################################################
                     [1m Learning iteration 603/2500 [0m                      

                       Computation: 45056 steps/s (collection: 2.064s, learning 0.117s)
             Mean action noise std: 2.38
          Mean value_function loss: 13.0899
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 70.5865
                       Mean reward: 13.93
               Mean episode length: 233.26
    Episode_Reward/reaching_object: 1.5822
     Episode_Reward/lifting_object: 1.7183
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 2.18s
                      Time elapsed: 00:21:48
                               ETA: 01:08:29

################################################################################
                     [1m Learning iteration 604/2500 [0m                      

                       Computation: 45044 steps/s (collection: 2.069s, learning 0.114s)
             Mean action noise std: 2.38
          Mean value_function loss: 15.2314
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 70.6221
                       Mean reward: 15.34
               Mean episode length: 217.82
    Episode_Reward/reaching_object: 1.5276
     Episode_Reward/lifting_object: 1.5347
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 2.18s
                      Time elapsed: 00:21:50
                               ETA: 01:08:27

################################################################################
                     [1m Learning iteration 605/2500 [0m                      

                       Computation: 45493 steps/s (collection: 2.049s, learning 0.112s)
             Mean action noise std: 2.39
          Mean value_function loss: 20.1439
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 70.6485
                       Mean reward: 16.98
               Mean episode length: 219.29
    Episode_Reward/reaching_object: 1.5108
     Episode_Reward/lifting_object: 1.3563
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0321
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 2.16s
                      Time elapsed: 00:21:52
                               ETA: 01:08:25

################################################################################
                     [1m Learning iteration 606/2500 [0m                      

                       Computation: 45364 steps/s (collection: 2.057s, learning 0.110s)
             Mean action noise std: 2.39
          Mean value_function loss: 22.7654
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.6793
                       Mean reward: 12.75
               Mean episode length: 213.99
    Episode_Reward/reaching_object: 1.4787
     Episode_Reward/lifting_object: 1.0694
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 4.6667
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 2.17s
                      Time elapsed: 00:21:55
                               ETA: 01:08:23

################################################################################
                     [1m Learning iteration 607/2500 [0m                      

                       Computation: 45645 steps/s (collection: 2.044s, learning 0.110s)
             Mean action noise std: 2.39
          Mean value_function loss: 19.0997
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 70.7023
                       Mean reward: 18.36
               Mean episode length: 220.19
    Episode_Reward/reaching_object: 1.5015
     Episode_Reward/lifting_object: 1.8744
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.7083
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 2.15s
                      Time elapsed: 00:21:57
                               ETA: 01:08:20

################################################################################
                     [1m Learning iteration 608/2500 [0m                      

                       Computation: 45807 steps/s (collection: 2.035s, learning 0.111s)
             Mean action noise std: 2.39
          Mean value_function loss: 16.6026
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 70.7392
                       Mean reward: 9.34
               Mean episode length: 215.92
    Episode_Reward/reaching_object: 1.5501
     Episode_Reward/lifting_object: 1.3217
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 4.3750
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 2.15s
                      Time elapsed: 00:21:59
                               ETA: 01:08:18

################################################################################
                     [1m Learning iteration 609/2500 [0m                      

                       Computation: 45617 steps/s (collection: 2.040s, learning 0.115s)
             Mean action noise std: 2.40
          Mean value_function loss: 16.7068
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 70.7752
                       Mean reward: 12.59
               Mean episode length: 220.31
    Episode_Reward/reaching_object: 1.4299
     Episode_Reward/lifting_object: 1.7342
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 2.15s
                      Time elapsed: 00:22:01
                               ETA: 01:08:16

################################################################################
                     [1m Learning iteration 610/2500 [0m                      

                       Computation: 45735 steps/s (collection: 2.047s, learning 0.103s)
             Mean action noise std: 2.40
          Mean value_function loss: 28.9802
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 70.7976
                       Mean reward: 7.63
               Mean episode length: 222.16
    Episode_Reward/reaching_object: 1.3859
     Episode_Reward/lifting_object: 1.2204
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 5.8333
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 2.15s
                      Time elapsed: 00:22:03
                               ETA: 01:08:14

################################################################################
                     [1m Learning iteration 611/2500 [0m                      

                       Computation: 45980 steps/s (collection: 2.040s, learning 0.098s)
             Mean action noise std: 2.40
          Mean value_function loss: 19.0132
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 70.8179
                       Mean reward: 18.03
               Mean episode length: 213.15
    Episode_Reward/reaching_object: 1.3724
     Episode_Reward/lifting_object: 1.5972
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 13.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 6.4583
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 2.14s
                      Time elapsed: 00:22:05
                               ETA: 01:08:12

################################################################################
                     [1m Learning iteration 612/2500 [0m                      

                       Computation: 44938 steps/s (collection: 2.093s, learning 0.094s)
             Mean action noise std: 2.40
          Mean value_function loss: 17.6852
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 70.8273
                       Mean reward: 16.35
               Mean episode length: 214.56
    Episode_Reward/reaching_object: 1.4244
     Episode_Reward/lifting_object: 1.3731
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 5.3333
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 2.19s
                      Time elapsed: 00:22:07
                               ETA: 01:08:09

################################################################################
                     [1m Learning iteration 613/2500 [0m                      

                       Computation: 45629 steps/s (collection: 2.035s, learning 0.120s)
             Mean action noise std: 2.40
          Mean value_function loss: 13.7071
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 70.8449
                       Mean reward: 22.16
               Mean episode length: 216.71
    Episode_Reward/reaching_object: 1.3625
     Episode_Reward/lifting_object: 1.9402
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 2.15s
                      Time elapsed: 00:22:10
                               ETA: 01:08:07

################################################################################
                     [1m Learning iteration 614/2500 [0m                      

                       Computation: 45248 steps/s (collection: 2.055s, learning 0.118s)
             Mean action noise std: 2.40
          Mean value_function loss: 13.3755
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 70.8742
                       Mean reward: 19.51
               Mean episode length: 221.99
    Episode_Reward/reaching_object: 1.2934
     Episode_Reward/lifting_object: 2.4035
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 5.6667
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 2.17s
                      Time elapsed: 00:22:12
                               ETA: 01:08:05

################################################################################
                     [1m Learning iteration 615/2500 [0m                      

                       Computation: 44941 steps/s (collection: 2.065s, learning 0.123s)
             Mean action noise std: 2.41
          Mean value_function loss: 23.8156
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 70.8988
                       Mean reward: 13.62
               Mean episode length: 213.61
    Episode_Reward/reaching_object: 1.2529
     Episode_Reward/lifting_object: 1.6757
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 2.19s
                      Time elapsed: 00:22:14
                               ETA: 01:08:03

################################################################################
                     [1m Learning iteration 616/2500 [0m                      

                       Computation: 45795 steps/s (collection: 2.033s, learning 0.113s)
             Mean action noise std: 2.41
          Mean value_function loss: 12.6587
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 70.9280
                       Mean reward: 17.55
               Mean episode length: 227.05
    Episode_Reward/reaching_object: 1.3080
     Episode_Reward/lifting_object: 2.2600
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 5.0000
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 2.15s
                      Time elapsed: 00:22:16
                               ETA: 01:08:01

################################################################################
                     [1m Learning iteration 617/2500 [0m                      

                       Computation: 46242 steps/s (collection: 2.016s, learning 0.110s)
             Mean action noise std: 2.41
          Mean value_function loss: 18.4993
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 70.9504
                       Mean reward: 11.78
               Mean episode length: 204.78
    Episode_Reward/reaching_object: 1.1799
     Episode_Reward/lifting_object: 2.2683
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 12.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 2.13s
                      Time elapsed: 00:22:18
                               ETA: 01:07:59

################################################################################
                     [1m Learning iteration 618/2500 [0m                      

                       Computation: 45869 steps/s (collection: 2.032s, learning 0.112s)
             Mean action noise std: 2.41
          Mean value_function loss: 16.1164
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 70.9693
                       Mean reward: 13.36
               Mean episode length: 222.18
    Episode_Reward/reaching_object: 1.2234
     Episode_Reward/lifting_object: 2.4005
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 2.14s
                      Time elapsed: 00:22:20
                               ETA: 01:07:56

################################################################################
                     [1m Learning iteration 619/2500 [0m                      

                       Computation: 45932 steps/s (collection: 2.022s, learning 0.118s)
             Mean action noise std: 2.41
          Mean value_function loss: 13.8845
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 70.9975
                       Mean reward: 18.57
               Mean episode length: 220.36
    Episode_Reward/reaching_object: 1.1765
     Episode_Reward/lifting_object: 2.6017
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 5.5417
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 2.14s
                      Time elapsed: 00:22:23
                               ETA: 01:07:54

################################################################################
                     [1m Learning iteration 620/2500 [0m                      

                       Computation: 46290 steps/s (collection: 2.012s, learning 0.112s)
             Mean action noise std: 2.42
          Mean value_function loss: 14.7306
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 71.0350
                       Mean reward: 23.90
               Mean episode length: 224.82
    Episode_Reward/reaching_object: 1.1283
     Episode_Reward/lifting_object: 2.9366
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 2.12s
                      Time elapsed: 00:22:25
                               ETA: 01:07:52

################################################################################
                     [1m Learning iteration 621/2500 [0m                      

                       Computation: 46685 steps/s (collection: 2.001s, learning 0.105s)
             Mean action noise std: 2.42
          Mean value_function loss: 11.3662
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 71.0773
                       Mean reward: 16.71
               Mean episode length: 212.26
    Episode_Reward/reaching_object: 1.1817
     Episode_Reward/lifting_object: 2.5686
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 5.0417
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 2.11s
                      Time elapsed: 00:22:27
                               ETA: 01:07:49

################################################################################
                     [1m Learning iteration 622/2500 [0m                      

                       Computation: 46412 steps/s (collection: 2.025s, learning 0.093s)
             Mean action noise std: 2.42
          Mean value_function loss: 14.7432
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 71.1118
                       Mean reward: 17.72
               Mean episode length: 210.23
    Episode_Reward/reaching_object: 1.1251
     Episode_Reward/lifting_object: 2.9302
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0327
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 6.1250
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 2.12s
                      Time elapsed: 00:22:29
                               ETA: 01:07:47

################################################################################
                     [1m Learning iteration 623/2500 [0m                      

                       Computation: 45534 steps/s (collection: 2.024s, learning 0.135s)
             Mean action noise std: 2.43
          Mean value_function loss: 16.1161
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 71.1473
                       Mean reward: 18.51
               Mean episode length: 207.52
    Episode_Reward/reaching_object: 1.1428
     Episode_Reward/lifting_object: 2.4634
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0340
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 5.1667
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 2.16s
                      Time elapsed: 00:22:31
                               ETA: 01:07:45

################################################################################
                     [1m Learning iteration 624/2500 [0m                      

                       Computation: 44149 steps/s (collection: 2.120s, learning 0.107s)
             Mean action noise std: 2.43
          Mean value_function loss: 14.5362
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 71.1854
                       Mean reward: 17.26
               Mean episode length: 215.24
    Episode_Reward/reaching_object: 1.0988
     Episode_Reward/lifting_object: 3.0394
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0301
      Episode_Termination/time_out: 12.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 2.23s
                      Time elapsed: 00:22:33
                               ETA: 01:07:43

################################################################################
                     [1m Learning iteration 625/2500 [0m                      

                       Computation: 45005 steps/s (collection: 2.069s, learning 0.115s)
             Mean action noise std: 2.43
          Mean value_function loss: 27.5991
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 71.2151
                       Mean reward: 18.09
               Mean episode length: 209.44
    Episode_Reward/reaching_object: 1.1009
     Episode_Reward/lifting_object: 2.5733
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 2.18s
                      Time elapsed: 00:22:35
                               ETA: 01:07:41

################################################################################
                     [1m Learning iteration 626/2500 [0m                      

                       Computation: 45939 steps/s (collection: 2.023s, learning 0.117s)
             Mean action noise std: 2.43
          Mean value_function loss: 17.1088
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 71.2401
                       Mean reward: 13.91
               Mean episode length: 204.63
    Episode_Reward/reaching_object: 1.0643
     Episode_Reward/lifting_object: 2.4093
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0306
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 2.14s
                      Time elapsed: 00:22:38
                               ETA: 01:07:39

################################################################################
                     [1m Learning iteration 627/2500 [0m                      

                       Computation: 45028 steps/s (collection: 2.068s, learning 0.115s)
             Mean action noise std: 2.43
          Mean value_function loss: 15.5528
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 71.2674
                       Mean reward: 19.44
               Mean episode length: 216.45
    Episode_Reward/reaching_object: 1.0768
     Episode_Reward/lifting_object: 2.9484
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0307
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 2.18s
                      Time elapsed: 00:22:40
                               ETA: 01:07:36

################################################################################
                     [1m Learning iteration 628/2500 [0m                      

                       Computation: 45011 steps/s (collection: 2.072s, learning 0.112s)
             Mean action noise std: 2.44
          Mean value_function loss: 20.7490
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 71.2863
                       Mean reward: 16.06
               Mean episode length: 199.26
    Episode_Reward/reaching_object: 1.0831
     Episode_Reward/lifting_object: 2.1095
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 12.9583
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 2.18s
                      Time elapsed: 00:22:42
                               ETA: 01:07:34

################################################################################
                     [1m Learning iteration 629/2500 [0m                      

                       Computation: 44940 steps/s (collection: 2.096s, learning 0.092s)
             Mean action noise std: 2.44
          Mean value_function loss: 19.6658
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 71.2982
                       Mean reward: 22.52
               Mean episode length: 202.63
    Episode_Reward/reaching_object: 1.1511
     Episode_Reward/lifting_object: 2.9178
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 2.19s
                      Time elapsed: 00:22:44
                               ETA: 01:07:32

################################################################################
                     [1m Learning iteration 630/2500 [0m                      

                       Computation: 45589 steps/s (collection: 2.039s, learning 0.118s)
             Mean action noise std: 2.44
          Mean value_function loss: 17.4943
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 71.3204
                       Mean reward: 16.72
               Mean episode length: 200.57
    Episode_Reward/reaching_object: 1.0834
     Episode_Reward/lifting_object: 2.6620
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 10.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 2.16s
                      Time elapsed: 00:22:46
                               ETA: 01:07:30

################################################################################
                     [1m Learning iteration 631/2500 [0m                      

                       Computation: 45401 steps/s (collection: 2.057s, learning 0.108s)
             Mean action noise std: 2.44
          Mean value_function loss: 18.7925
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 71.3433
                       Mean reward: 20.92
               Mean episode length: 189.27
    Episode_Reward/reaching_object: 1.0975
     Episode_Reward/lifting_object: 3.0154
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 2.17s
                      Time elapsed: 00:22:48
                               ETA: 01:07:28

################################################################################
                     [1m Learning iteration 632/2500 [0m                      

                       Computation: 45921 steps/s (collection: 2.028s, learning 0.113s)
             Mean action noise std: 2.44
          Mean value_function loss: 21.5633
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 71.3693
                       Mean reward: 11.81
               Mean episode length: 203.91
    Episode_Reward/reaching_object: 1.0375
     Episode_Reward/lifting_object: 2.3476
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 2.14s
                      Time elapsed: 00:22:51
                               ETA: 01:07:26

################################################################################
                     [1m Learning iteration 633/2500 [0m                      

                       Computation: 45472 steps/s (collection: 2.056s, learning 0.106s)
             Mean action noise std: 2.44
          Mean value_function loss: 20.6695
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 71.3961
                       Mean reward: 18.63
               Mean episode length: 192.26
    Episode_Reward/reaching_object: 1.0709
     Episode_Reward/lifting_object: 2.8195
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.7500
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 2.16s
                      Time elapsed: 00:22:53
                               ETA: 01:07:23

################################################################################
                     [1m Learning iteration 634/2500 [0m                      

                       Computation: 46056 steps/s (collection: 2.027s, learning 0.107s)
             Mean action noise std: 2.45
          Mean value_function loss: 19.1694
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 71.4160
                       Mean reward: 20.61
               Mean episode length: 199.37
    Episode_Reward/reaching_object: 1.0711
     Episode_Reward/lifting_object: 2.6560
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.8750
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 2.13s
                      Time elapsed: 00:22:55
                               ETA: 01:07:21

################################################################################
                     [1m Learning iteration 635/2500 [0m                      

                       Computation: 45315 steps/s (collection: 2.055s, learning 0.114s)
             Mean action noise std: 2.45
          Mean value_function loss: 13.5308
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 71.4490
                       Mean reward: 21.42
               Mean episode length: 201.05
    Episode_Reward/reaching_object: 1.0537
     Episode_Reward/lifting_object: 3.0527
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 2.17s
                      Time elapsed: 00:22:57
                               ETA: 01:07:19

################################################################################
                     [1m Learning iteration 636/2500 [0m                      

                       Computation: 44211 steps/s (collection: 2.100s, learning 0.123s)
             Mean action noise std: 2.45
          Mean value_function loss: 23.9460
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 71.4831
                       Mean reward: 20.27
               Mean episode length: 192.90
    Episode_Reward/reaching_object: 1.0234
     Episode_Reward/lifting_object: 3.0014
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 2.22s
                      Time elapsed: 00:22:59
                               ETA: 01:07:17

################################################################################
                     [1m Learning iteration 637/2500 [0m                      

                       Computation: 45187 steps/s (collection: 2.045s, learning 0.131s)
             Mean action noise std: 2.45
          Mean value_function loss: 25.3205
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 71.5112
                       Mean reward: 21.07
               Mean episode length: 194.11
    Episode_Reward/reaching_object: 0.9851
     Episode_Reward/lifting_object: 2.5910
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0342
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.4167
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 2.18s
                      Time elapsed: 00:23:01
                               ETA: 01:07:15

################################################################################
                     [1m Learning iteration 638/2500 [0m                      

                       Computation: 45665 steps/s (collection: 2.031s, learning 0.122s)
             Mean action noise std: 2.45
          Mean value_function loss: 17.5184
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 71.5303
                       Mean reward: 18.86
               Mean episode length: 201.71
    Episode_Reward/reaching_object: 0.9882
     Episode_Reward/lifting_object: 3.2627
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 2.15s
                      Time elapsed: 00:23:04
                               ETA: 01:07:13

################################################################################
                     [1m Learning iteration 639/2500 [0m                      

                       Computation: 45571 steps/s (collection: 2.053s, learning 0.104s)
             Mean action noise std: 2.46
          Mean value_function loss: 18.3837
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 71.5495
                       Mean reward: 21.71
               Mean episode length: 216.26
    Episode_Reward/reaching_object: 1.0166
     Episode_Reward/lifting_object: 2.8277
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 2.16s
                      Time elapsed: 00:23:06
                               ETA: 01:07:10

################################################################################
                     [1m Learning iteration 640/2500 [0m                      

                       Computation: 45549 steps/s (collection: 2.049s, learning 0.109s)
             Mean action noise std: 2.46
          Mean value_function loss: 29.6850
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 71.5696
                       Mean reward: 18.30
               Mean episode length: 191.46
    Episode_Reward/reaching_object: 0.9859
     Episode_Reward/lifting_object: 2.8811
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 8.2917
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 2.16s
                      Time elapsed: 00:23:08
                               ETA: 01:07:08

################################################################################
                     [1m Learning iteration 641/2500 [0m                      

                       Computation: 45514 steps/s (collection: 2.044s, learning 0.116s)
             Mean action noise std: 2.46
          Mean value_function loss: 22.6535
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 71.5925
                       Mean reward: 18.77
               Mean episode length: 187.68
    Episode_Reward/reaching_object: 0.9305
     Episode_Reward/lifting_object: 2.4358
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 8.3333
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 2.16s
                      Time elapsed: 00:23:10
                               ETA: 01:07:06

################################################################################
                     [1m Learning iteration 642/2500 [0m                      

                       Computation: 45000 steps/s (collection: 2.063s, learning 0.122s)
             Mean action noise std: 2.46
          Mean value_function loss: 18.0666
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 71.6034
                       Mean reward: 17.99
               Mean episode length: 211.51
    Episode_Reward/reaching_object: 1.0251
     Episode_Reward/lifting_object: 2.9655
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 13.3750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 7.0833
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 2.18s
                      Time elapsed: 00:23:12
                               ETA: 01:07:04

################################################################################
                     [1m Learning iteration 643/2500 [0m                      

                       Computation: 45107 steps/s (collection: 2.061s, learning 0.119s)
             Mean action noise std: 2.46
          Mean value_function loss: 18.4792
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 71.6242
                       Mean reward: 21.34
               Mean episode length: 212.18
    Episode_Reward/reaching_object: 1.0254
     Episode_Reward/lifting_object: 3.2233
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.9167
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 2.18s
                      Time elapsed: 00:23:14
                               ETA: 01:07:02

################################################################################
                     [1m Learning iteration 644/2500 [0m                      

                       Computation: 45286 steps/s (collection: 2.059s, learning 0.112s)
             Mean action noise std: 2.47
          Mean value_function loss: 19.1927
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 71.6649
                       Mean reward: 19.67
               Mean episode length: 201.32
    Episode_Reward/reaching_object: 1.0424
     Episode_Reward/lifting_object: 3.1739
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 11.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 2.17s
                      Time elapsed: 00:23:17
                               ETA: 01:07:00

################################################################################
                     [1m Learning iteration 645/2500 [0m                      

                       Computation: 46111 steps/s (collection: 2.020s, learning 0.112s)
             Mean action noise std: 2.47
          Mean value_function loss: 19.8644
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 71.7029
                       Mean reward: 21.82
               Mean episode length: 202.09
    Episode_Reward/reaching_object: 1.0382
     Episode_Reward/lifting_object: 3.5067
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 2.13s
                      Time elapsed: 00:23:19
                               ETA: 01:06:57

################################################################################
                     [1m Learning iteration 646/2500 [0m                      

                       Computation: 46013 steps/s (collection: 2.020s, learning 0.116s)
             Mean action noise std: 2.47
          Mean value_function loss: 16.8974
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 71.7264
                       Mean reward: 25.06
               Mean episode length: 196.19
    Episode_Reward/reaching_object: 1.0464
     Episode_Reward/lifting_object: 3.4599
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.8333
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 2.14s
                      Time elapsed: 00:23:21
                               ETA: 01:06:55

################################################################################
                     [1m Learning iteration 647/2500 [0m                      

                       Computation: 43854 steps/s (collection: 2.139s, learning 0.103s)
             Mean action noise std: 2.47
          Mean value_function loss: 15.7658
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 71.7458
                       Mean reward: 22.60
               Mean episode length: 201.47
    Episode_Reward/reaching_object: 1.0576
     Episode_Reward/lifting_object: 4.1913
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0367
          Episode_Reward/joint_vel: -0.0347
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 2.24s
                      Time elapsed: 00:23:23
                               ETA: 01:06:53

################################################################################
                     [1m Learning iteration 648/2500 [0m                      

                       Computation: 44464 steps/s (collection: 2.092s, learning 0.119s)
             Mean action noise std: 2.47
          Mean value_function loss: 23.9252
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 71.7682
                       Mean reward: 21.93
               Mean episode length: 203.36
    Episode_Reward/reaching_object: 1.0053
     Episode_Reward/lifting_object: 3.3030
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0351
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 7.5833
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 2.21s
                      Time elapsed: 00:23:25
                               ETA: 01:06:51

################################################################################
                     [1m Learning iteration 649/2500 [0m                      

                       Computation: 44827 steps/s (collection: 2.081s, learning 0.112s)
             Mean action noise std: 2.48
          Mean value_function loss: 16.3613
               Mean surrogate loss: -0.0063
                 Mean entropy loss: 71.7947
                       Mean reward: 26.15
               Mean episode length: 202.49
    Episode_Reward/reaching_object: 0.9863
     Episode_Reward/lifting_object: 3.4660
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0344
      Episode_Termination/time_out: 13.0000
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 8.0000
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 2.19s
                      Time elapsed: 00:23:28
                               ETA: 01:06:49

################################################################################
                     [1m Learning iteration 650/2500 [0m                      

                       Computation: 45253 steps/s (collection: 2.064s, learning 0.108s)
             Mean action noise std: 2.48
          Mean value_function loss: 21.0457
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 71.8191
                       Mean reward: 17.99
               Mean episode length: 201.33
    Episode_Reward/reaching_object: 0.9230
     Episode_Reward/lifting_object: 3.0595
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0350
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 8.7917
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 2.17s
                      Time elapsed: 00:23:30
                               ETA: 01:06:47

################################################################################
                     [1m Learning iteration 651/2500 [0m                      

                       Computation: 44773 steps/s (collection: 2.082s, learning 0.114s)
             Mean action noise std: 2.48
          Mean value_function loss: 20.3332
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 71.8417
                       Mean reward: 24.19
               Mean episode length: 198.54
    Episode_Reward/reaching_object: 0.9743
     Episode_Reward/lifting_object: 3.5247
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0362
          Episode_Reward/joint_vel: -0.0353
      Episode_Termination/time_out: 13.2917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 8.5833
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 2.20s
                      Time elapsed: 00:23:32
                               ETA: 01:06:45

################################################################################
                     [1m Learning iteration 652/2500 [0m                      

                       Computation: 43671 steps/s (collection: 2.131s, learning 0.120s)
             Mean action noise std: 2.48
          Mean value_function loss: 27.2310
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 71.8613
                       Mean reward: 19.96
               Mean episode length: 197.59
    Episode_Reward/reaching_object: 0.9524
     Episode_Reward/lifting_object: 3.3860
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0329
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 2.25s
                      Time elapsed: 00:23:34
                               ETA: 01:06:43

################################################################################
                     [1m Learning iteration 653/2500 [0m                      

                       Computation: 45338 steps/s (collection: 2.042s, learning 0.126s)
             Mean action noise std: 2.48
          Mean value_function loss: 17.4553
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 71.8741
                       Mean reward: 20.02
               Mean episode length: 193.66
    Episode_Reward/reaching_object: 0.8895
     Episode_Reward/lifting_object: 3.4031
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0316
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 7.9583
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 2.17s
                      Time elapsed: 00:23:36
                               ETA: 01:06:41

################################################################################
                     [1m Learning iteration 654/2500 [0m                      

                       Computation: 45905 steps/s (collection: 2.039s, learning 0.102s)
             Mean action noise std: 2.48
          Mean value_function loss: 18.8854
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 71.8964
                       Mean reward: 17.57
               Mean episode length: 201.66
    Episode_Reward/reaching_object: 0.9204
     Episode_Reward/lifting_object: 3.8467
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0315
      Episode_Termination/time_out: 12.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 2.14s
                      Time elapsed: 00:23:38
                               ETA: 01:06:39

################################################################################
                     [1m Learning iteration 655/2500 [0m                      

                       Computation: 45928 steps/s (collection: 2.051s, learning 0.090s)
             Mean action noise std: 2.49
          Mean value_function loss: 25.6774
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 71.9222
                       Mean reward: 20.62
               Mean episode length: 189.88
    Episode_Reward/reaching_object: 0.9425
     Episode_Reward/lifting_object: 4.2174
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0364
          Episode_Reward/joint_vel: -0.0330
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 2.14s
                      Time elapsed: 00:23:41
                               ETA: 01:06:36

################################################################################
                     [1m Learning iteration 656/2500 [0m                      

                       Computation: 46220 steps/s (collection: 2.020s, learning 0.107s)
             Mean action noise std: 2.49
          Mean value_function loss: 16.3649
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 71.9433
                       Mean reward: 25.61
               Mean episode length: 188.97
    Episode_Reward/reaching_object: 0.9479
     Episode_Reward/lifting_object: 4.3000
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0361
          Episode_Reward/joint_vel: -0.0323
      Episode_Termination/time_out: 11.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 2.13s
                      Time elapsed: 00:23:43
                               ETA: 01:06:34

################################################################################
                     [1m Learning iteration 657/2500 [0m                      

                       Computation: 46107 steps/s (collection: 2.023s, learning 0.110s)
             Mean action noise std: 2.49
          Mean value_function loss: 26.2517
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 71.9677
                       Mean reward: 23.72
               Mean episode length: 210.68
    Episode_Reward/reaching_object: 0.9286
     Episode_Reward/lifting_object: 3.8124
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0337
      Episode_Termination/time_out: 12.8750
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 2.13s
                      Time elapsed: 00:23:45
                               ETA: 01:06:32

################################################################################
                     [1m Learning iteration 658/2500 [0m                      

                       Computation: 45558 steps/s (collection: 2.043s, learning 0.115s)
             Mean action noise std: 2.49
          Mean value_function loss: 15.3244
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 71.9959
                       Mean reward: 28.12
               Mean episode length: 212.81
    Episode_Reward/reaching_object: 0.9242
     Episode_Reward/lifting_object: 3.8902
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0366
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 11.5833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 2.16s
                      Time elapsed: 00:23:47
                               ETA: 01:06:30

################################################################################
                     [1m Learning iteration 659/2500 [0m                      

                       Computation: 45901 steps/s (collection: 2.026s, learning 0.115s)
             Mean action noise std: 2.49
          Mean value_function loss: 16.0436
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 72.0161
                       Mean reward: 26.26
               Mean episode length: 213.68
    Episode_Reward/reaching_object: 0.9421
     Episode_Reward/lifting_object: 4.2138
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0309
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.2500
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 2.14s
                      Time elapsed: 00:23:49
                               ETA: 01:06:27

################################################################################
                     [1m Learning iteration 660/2500 [0m                      

                       Computation: 45042 steps/s (collection: 2.064s, learning 0.119s)
             Mean action noise std: 2.50
          Mean value_function loss: 14.9375
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.0385
                       Mean reward: 22.77
               Mean episode length: 203.81
    Episode_Reward/reaching_object: 0.9319
     Episode_Reward/lifting_object: 4.1638
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0373
          Episode_Reward/joint_vel: -0.0319
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 8.2083
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 2.18s
                      Time elapsed: 00:23:51
                               ETA: 01:06:25

################################################################################
                     [1m Learning iteration 661/2500 [0m                      

                       Computation: 44609 steps/s (collection: 2.085s, learning 0.118s)
             Mean action noise std: 2.50
          Mean value_function loss: 14.7267
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 72.0613
                       Mean reward: 28.29
               Mean episode length: 199.88
    Episode_Reward/reaching_object: 0.8974
     Episode_Reward/lifting_object: 3.8456
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0379
          Episode_Reward/joint_vel: -0.0314
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 7.4167
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 2.20s
                      Time elapsed: 00:23:54
                               ETA: 01:06:23

################################################################################
                     [1m Learning iteration 662/2500 [0m                      

                       Computation: 45476 steps/s (collection: 2.053s, learning 0.109s)
             Mean action noise std: 2.50
          Mean value_function loss: 19.7797
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 72.0810
                       Mean reward: 24.70
               Mean episode length: 206.49
    Episode_Reward/reaching_object: 0.9475
     Episode_Reward/lifting_object: 3.8560
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0322
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 7.1667
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 2.16s
                      Time elapsed: 00:23:56
                               ETA: 01:06:21

################################################################################
                     [1m Learning iteration 663/2500 [0m                      

                       Computation: 45269 steps/s (collection: 2.060s, learning 0.111s)
             Mean action noise std: 2.50
          Mean value_function loss: 20.3915
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 72.0922
                       Mean reward: 23.22
               Mean episode length: 202.97
    Episode_Reward/reaching_object: 0.9122
     Episode_Reward/lifting_object: 4.2296
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0303
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 8.1667
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 2.17s
                      Time elapsed: 00:23:58
                               ETA: 01:06:19

################################################################################
                     [1m Learning iteration 664/2500 [0m                      

                       Computation: 45378 steps/s (collection: 2.073s, learning 0.094s)
             Mean action noise std: 2.50
          Mean value_function loss: 23.9963
               Mean surrogate loss: -0.0058
                 Mean entropy loss: 72.1111
                       Mean reward: 24.79
               Mean episode length: 198.18
    Episode_Reward/reaching_object: 0.8998
     Episode_Reward/lifting_object: 4.1630
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0318
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 2.17s
                      Time elapsed: 00:24:00
                               ETA: 01:06:17

################################################################################
                     [1m Learning iteration 665/2500 [0m                      

                       Computation: 45093 steps/s (collection: 2.065s, learning 0.115s)
             Mean action noise std: 2.50
          Mean value_function loss: 26.0969
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 72.1347
                       Mean reward: 31.49
               Mean episode length: 184.88
    Episode_Reward/reaching_object: 0.9878
     Episode_Reward/lifting_object: 4.3415
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 11.6250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 9.7500
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 2.18s
                      Time elapsed: 00:24:02
                               ETA: 01:06:15

################################################################################
                     [1m Learning iteration 666/2500 [0m                      

                       Computation: 26669 steps/s (collection: 3.559s, learning 0.127s)
             Mean action noise std: 2.51
          Mean value_function loss: 35.1117
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 72.1515
                       Mean reward: 20.48
               Mean episode length: 191.67
    Episode_Reward/reaching_object: 0.9953
     Episode_Reward/lifting_object: 4.1775
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0336
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 3.69s
                      Time elapsed: 00:24:06
                               ETA: 01:06:17

################################################################################
                     [1m Learning iteration 667/2500 [0m                      

                       Computation: 14167 steps/s (collection: 6.812s, learning 0.127s)
             Mean action noise std: 2.51
          Mean value_function loss: 34.1016
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 72.1696
                       Mean reward: 27.42
               Mean episode length: 185.30
    Episode_Reward/reaching_object: 0.9929
     Episode_Reward/lifting_object: 4.5690
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0353
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 6.94s
                      Time elapsed: 00:24:13
                               ETA: 01:06:27

################################################################################
                     [1m Learning iteration 668/2500 [0m                      

                       Computation: 13760 steps/s (collection: 6.959s, learning 0.185s)
             Mean action noise std: 2.51
          Mean value_function loss: 22.5694
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 72.1845
                       Mean reward: 24.76
               Mean episode length: 176.74
    Episode_Reward/reaching_object: 0.9165
     Episode_Reward/lifting_object: 4.0730
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0326
      Episode_Termination/time_out: 9.9167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 10.7083
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 7.14s
                      Time elapsed: 00:24:20
                               ETA: 01:06:39

################################################################################
                     [1m Learning iteration 669/2500 [0m                      

                       Computation: 14091 steps/s (collection: 6.859s, learning 0.117s)
             Mean action noise std: 2.51
          Mean value_function loss: 34.9933
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 72.2002
                       Mean reward: 26.22
               Mean episode length: 177.25
    Episode_Reward/reaching_object: 0.9448
     Episode_Reward/lifting_object: 4.6766
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 6.98s
                      Time elapsed: 00:24:27
                               ETA: 01:06:50

################################################################################
                     [1m Learning iteration 670/2500 [0m                      

                       Computation: 13811 steps/s (collection: 6.984s, learning 0.134s)
             Mean action noise std: 2.51
          Mean value_function loss: 41.9658
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 72.2120
                       Mean reward: 24.47
               Mean episode length: 185.23
    Episode_Reward/reaching_object: 0.9388
     Episode_Reward/lifting_object: 3.9645
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0335
      Episode_Termination/time_out: 12.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 7.12s
                      Time elapsed: 00:24:34
                               ETA: 01:07:01

################################################################################
                     [1m Learning iteration 671/2500 [0m                      

                       Computation: 14053 steps/s (collection: 6.867s, learning 0.128s)
             Mean action noise std: 2.51
          Mean value_function loss: 22.4940
               Mean surrogate loss: -0.0060
                 Mean entropy loss: 72.2307
                       Mean reward: 28.76
               Mean episode length: 204.94
    Episode_Reward/reaching_object: 0.9856
     Episode_Reward/lifting_object: 4.0867
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0358
      Episode_Termination/time_out: 11.8333
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 6.99s
                      Time elapsed: 00:24:41
                               ETA: 01:07:12

################################################################################
                     [1m Learning iteration 672/2500 [0m                      

                       Computation: 14038 steps/s (collection: 6.844s, learning 0.159s)
             Mean action noise std: 2.51
          Mean value_function loss: 22.7720
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 72.2515
                       Mean reward: 25.14
               Mean episode length: 190.62
    Episode_Reward/reaching_object: 0.9209
     Episode_Reward/lifting_object: 4.4401
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 10.7917
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 10.9167
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 7.00s
                      Time elapsed: 00:24:48
                               ETA: 01:07:23

################################################################################
                     [1m Learning iteration 673/2500 [0m                      

                       Computation: 13741 steps/s (collection: 7.035s, learning 0.119s)
             Mean action noise std: 2.51
          Mean value_function loss: 29.2938
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 72.2682
                       Mean reward: 25.19
               Mean episode length: 185.67
    Episode_Reward/reaching_object: 0.8832
     Episode_Reward/lifting_object: 4.5862
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.0833
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 7.15s
                      Time elapsed: 00:24:55
                               ETA: 01:07:34

################################################################################
                     [1m Learning iteration 674/2500 [0m                      

                       Computation: 13847 steps/s (collection: 6.976s, learning 0.123s)
             Mean action noise std: 2.52
          Mean value_function loss: 28.4488
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.2826
                       Mean reward: 30.51
               Mean episode length: 190.43
    Episode_Reward/reaching_object: 0.8659
     Episode_Reward/lifting_object: 4.6003
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0356
      Episode_Termination/time_out: 11.1667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 11.0000
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 7.10s
                      Time elapsed: 00:25:02
                               ETA: 01:07:45

################################################################################
                     [1m Learning iteration 675/2500 [0m                      

                       Computation: 20363 steps/s (collection: 4.718s, learning 0.109s)
             Mean action noise std: 2.52
          Mean value_function loss: 38.8590
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 72.3058
                       Mean reward: 25.56
               Mean episode length: 182.68
    Episode_Reward/reaching_object: 0.7687
     Episode_Reward/lifting_object: 4.0848
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0331
      Episode_Termination/time_out: 10.8333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 12.7500
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 4.83s
                      Time elapsed: 00:25:07
                               ETA: 01:07:50

################################################################################
                     [1m Learning iteration 676/2500 [0m                      

                       Computation: 43582 steps/s (collection: 2.124s, learning 0.132s)
             Mean action noise std: 2.52
          Mean value_function loss: 26.8885
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 72.3242
                       Mean reward: 18.93
               Mean episode length: 179.68
    Episode_Reward/reaching_object: 0.7985
     Episode_Reward/lifting_object: 4.0868
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 2.26s
                      Time elapsed: 00:25:09
                               ETA: 01:07:48

################################################################################
                     [1m Learning iteration 677/2500 [0m                      

                       Computation: 39178 steps/s (collection: 2.360s, learning 0.149s)
             Mean action noise std: 2.52
          Mean value_function loss: 32.8944
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 72.3447
                       Mean reward: 26.31
               Mean episode length: 181.64
    Episode_Reward/reaching_object: 0.7862
     Episode_Reward/lifting_object: 4.5259
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0339
          Episode_Reward/joint_vel: -0.0354
      Episode_Termination/time_out: 11.0000
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 66650112
                    Iteration time: 2.51s
                      Time elapsed: 00:25:12
                               ETA: 01:07:46

################################################################################
                     [1m Learning iteration 678/2500 [0m                      

                       Computation: 41892 steps/s (collection: 2.219s, learning 0.128s)
             Mean action noise std: 2.52
          Mean value_function loss: 25.7871
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 72.3596
                       Mean reward: 30.74
               Mean episode length: 169.59
    Episode_Reward/reaching_object: 0.7927
     Episode_Reward/lifting_object: 4.6859
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 12.7917
--------------------------------------------------------------------------------
                   Total timesteps: 66748416
                    Iteration time: 2.35s
                      Time elapsed: 00:25:14
                               ETA: 01:07:44

################################################################################
                     [1m Learning iteration 679/2500 [0m                      

                       Computation: 43819 steps/s (collection: 2.085s, learning 0.159s)
             Mean action noise std: 2.52
          Mean value_function loss: 34.9805
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 72.3800
                       Mean reward: 28.73
               Mean episode length: 174.07
    Episode_Reward/reaching_object: 0.8028
     Episode_Reward/lifting_object: 4.5006
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0346
          Episode_Reward/joint_vel: -0.0343
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 66846720
                    Iteration time: 2.24s
                      Time elapsed: 00:25:17
                               ETA: 01:07:42

################################################################################
                     [1m Learning iteration 680/2500 [0m                      

                       Computation: 43591 steps/s (collection: 2.139s, learning 0.117s)
             Mean action noise std: 2.53
          Mean value_function loss: 40.4814
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 72.3963
                       Mean reward: 21.03
               Mean episode length: 181.75
    Episode_Reward/reaching_object: 0.7886
     Episode_Reward/lifting_object: 3.9437
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0346
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 12.3750
--------------------------------------------------------------------------------
                   Total timesteps: 66945024
                    Iteration time: 2.26s
                      Time elapsed: 00:25:19
                               ETA: 01:07:40

################################################################################
                     [1m Learning iteration 681/2500 [0m                      

                       Computation: 44316 steps/s (collection: 2.107s, learning 0.112s)
             Mean action noise std: 2.53
          Mean value_function loss: 32.6117
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 72.4171
                       Mean reward: 27.48
               Mean episode length: 176.56
    Episode_Reward/reaching_object: 0.8101
     Episode_Reward/lifting_object: 4.1421
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0355
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 67043328
                    Iteration time: 2.22s
                      Time elapsed: 00:25:21
                               ETA: 01:07:38

################################################################################
                     [1m Learning iteration 682/2500 [0m                      

                       Computation: 44583 steps/s (collection: 2.091s, learning 0.114s)
             Mean action noise std: 2.53
          Mean value_function loss: 23.3827
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.4402
                       Mean reward: 26.14
               Mean episode length: 161.84
    Episode_Reward/reaching_object: 0.8082
     Episode_Reward/lifting_object: 4.4802
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 11.5000
--------------------------------------------------------------------------------
                   Total timesteps: 67141632
                    Iteration time: 2.20s
                      Time elapsed: 00:25:23
                               ETA: 01:07:35

################################################################################
                     [1m Learning iteration 683/2500 [0m                      

                       Computation: 45778 steps/s (collection: 2.059s, learning 0.088s)
             Mean action noise std: 2.53
          Mean value_function loss: 26.4940
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 72.4614
                       Mean reward: 20.17
               Mean episode length: 189.25
    Episode_Reward/reaching_object: 0.8218
     Episode_Reward/lifting_object: 5.2528
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 11.5833
--------------------------------------------------------------------------------
                   Total timesteps: 67239936
                    Iteration time: 2.15s
                      Time elapsed: 00:25:25
                               ETA: 01:07:33

################################################################################
                     [1m Learning iteration 684/2500 [0m                      

                       Computation: 43742 steps/s (collection: 2.111s, learning 0.136s)
             Mean action noise std: 2.53
          Mean value_function loss: 33.3843
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.4766
                       Mean reward: 32.62
               Mean episode length: 182.93
    Episode_Reward/reaching_object: 0.7975
     Episode_Reward/lifting_object: 5.0825
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0358
          Episode_Reward/joint_vel: -0.0339
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 9.4167
--------------------------------------------------------------------------------
                   Total timesteps: 67338240
                    Iteration time: 2.25s
                      Time elapsed: 00:25:28
                               ETA: 01:07:31

################################################################################
                     [1m Learning iteration 685/2500 [0m                      

                       Computation: 44535 steps/s (collection: 2.118s, learning 0.090s)
             Mean action noise std: 2.53
          Mean value_function loss: 30.2515
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 72.4940
                       Mean reward: 27.66
               Mean episode length: 195.77
    Episode_Reward/reaching_object: 0.8338
     Episode_Reward/lifting_object: 4.8656
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0365
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 67436544
                    Iteration time: 2.21s
                      Time elapsed: 00:25:30
                               ETA: 01:07:28

################################################################################
                     [1m Learning iteration 686/2500 [0m                      

                       Computation: 45138 steps/s (collection: 2.090s, learning 0.088s)
             Mean action noise std: 2.54
          Mean value_function loss: 32.3040
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 72.5150
                       Mean reward: 34.31
               Mean episode length: 189.27
    Episode_Reward/reaching_object: 0.8245
     Episode_Reward/lifting_object: 5.1028
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0341
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 67534848
                    Iteration time: 2.18s
                      Time elapsed: 00:25:32
                               ETA: 01:07:26

################################################################################
                     [1m Learning iteration 687/2500 [0m                      

                       Computation: 43727 steps/s (collection: 2.159s, learning 0.089s)
             Mean action noise std: 2.54
          Mean value_function loss: 38.0144
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.5349
                       Mean reward: 28.60
               Mean episode length: 189.73
    Episode_Reward/reaching_object: 0.8419
     Episode_Reward/lifting_object: 5.1192
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0333
      Episode_Termination/time_out: 10.7083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 67633152
                    Iteration time: 2.25s
                      Time elapsed: 00:25:34
                               ETA: 01:07:24

################################################################################
                     [1m Learning iteration 688/2500 [0m                      

                       Computation: 43700 steps/s (collection: 2.141s, learning 0.108s)
             Mean action noise std: 2.54
          Mean value_function loss: 31.9520
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 72.5558
                       Mean reward: 23.89
               Mean episode length: 182.12
    Episode_Reward/reaching_object: 0.8747
     Episode_Reward/lifting_object: 4.7055
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0345
      Episode_Termination/time_out: 11.6667
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 12.1667
--------------------------------------------------------------------------------
                   Total timesteps: 67731456
                    Iteration time: 2.25s
                      Time elapsed: 00:25:36
                               ETA: 01:07:22

################################################################################
                     [1m Learning iteration 689/2500 [0m                      

                       Computation: 43668 steps/s (collection: 2.144s, learning 0.108s)
             Mean action noise std: 2.54
          Mean value_function loss: 44.3361
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 72.5660
                       Mean reward: 31.90
               Mean episode length: 168.74
    Episode_Reward/reaching_object: 0.8552
     Episode_Reward/lifting_object: 5.6122
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0338
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 15.0000
--------------------------------------------------------------------------------
                   Total timesteps: 67829760
                    Iteration time: 2.25s
                      Time elapsed: 00:25:39
                               ETA: 01:07:19

################################################################################
                     [1m Learning iteration 690/2500 [0m                      

                       Computation: 43639 steps/s (collection: 2.149s, learning 0.104s)
             Mean action noise std: 2.54
          Mean value_function loss: 49.9820
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 72.5854
                       Mean reward: 20.91
               Mean episode length: 179.75
    Episode_Reward/reaching_object: 0.9363
     Episode_Reward/lifting_object: 5.3277
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0354
          Episode_Reward/joint_vel: -0.0357
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 13.5000
--------------------------------------------------------------------------------
                   Total timesteps: 67928064
                    Iteration time: 2.25s
                      Time elapsed: 00:25:41
                               ETA: 01:07:17

################################################################################
                     [1m Learning iteration 691/2500 [0m                      

                       Computation: 44631 steps/s (collection: 2.100s, learning 0.103s)
             Mean action noise std: 2.54
          Mean value_function loss: 42.5612
               Mean surrogate loss: -0.0055
                 Mean entropy loss: 72.6105
                       Mean reward: 34.49
               Mean episode length: 168.48
    Episode_Reward/reaching_object: 0.9476
     Episode_Reward/lifting_object: 5.5094
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0348
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 68026368
                    Iteration time: 2.20s
                      Time elapsed: 00:25:43
                               ETA: 01:07:15

################################################################################
                     [1m Learning iteration 692/2500 [0m                      

                       Computation: 44552 steps/s (collection: 2.101s, learning 0.106s)
             Mean action noise std: 2.55
          Mean value_function loss: 38.2865
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 72.6364
                       Mean reward: 43.14
               Mean episode length: 177.29
    Episode_Reward/reaching_object: 0.9646
     Episode_Reward/lifting_object: 6.2680
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0349
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 13.5417
--------------------------------------------------------------------------------
                   Total timesteps: 68124672
                    Iteration time: 2.21s
                      Time elapsed: 00:25:45
                               ETA: 01:07:13

################################################################################
                     [1m Learning iteration 693/2500 [0m                      

                       Computation: 44054 steps/s (collection: 2.107s, learning 0.125s)
             Mean action noise std: 2.55
          Mean value_function loss: 42.6647
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 72.6569
                       Mean reward: 39.17
               Mean episode length: 158.04
    Episode_Reward/reaching_object: 1.0112
     Episode_Reward/lifting_object: 6.7866
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0347
          Episode_Reward/joint_vel: -0.0369
      Episode_Termination/time_out: 9.5833
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 68222976
                    Iteration time: 2.23s
                      Time elapsed: 00:25:48
                               ETA: 01:07:10

################################################################################
                     [1m Learning iteration 694/2500 [0m                      

                       Computation: 44105 steps/s (collection: 2.132s, learning 0.097s)
             Mean action noise std: 2.55
          Mean value_function loss: 47.2207
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 72.6807
                       Mean reward: 37.20
               Mean episode length: 163.83
    Episode_Reward/reaching_object: 0.9393
     Episode_Reward/lifting_object: 6.3159
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0350
      Episode_Termination/time_out: 8.0000
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 14.2500
--------------------------------------------------------------------------------
                   Total timesteps: 68321280
                    Iteration time: 2.23s
                      Time elapsed: 00:25:50
                               ETA: 01:07:08

################################################################################
                     [1m Learning iteration 695/2500 [0m                      

                       Computation: 42813 steps/s (collection: 2.198s, learning 0.098s)
             Mean action noise std: 2.55
          Mean value_function loss: 42.1402
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 72.7017
                       Mean reward: 37.40
               Mean episode length: 175.99
    Episode_Reward/reaching_object: 0.9262
     Episode_Reward/lifting_object: 5.8315
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0349
      Episode_Termination/time_out: 8.6667
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 15.9167
--------------------------------------------------------------------------------
                   Total timesteps: 68419584
                    Iteration time: 2.30s
                      Time elapsed: 00:25:52
                               ETA: 01:07:06

################################################################################
                     [1m Learning iteration 696/2500 [0m                      

                       Computation: 43849 steps/s (collection: 2.151s, learning 0.091s)
             Mean action noise std: 2.55
          Mean value_function loss: 35.7003
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 72.7174
                       Mean reward: 34.77
               Mean episode length: 160.90
    Episode_Reward/reaching_object: 0.9770
     Episode_Reward/lifting_object: 6.2561
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0340
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 68517888
                    Iteration time: 2.24s
                      Time elapsed: 00:25:54
                               ETA: 01:07:04

################################################################################
                     [1m Learning iteration 697/2500 [0m                      

                       Computation: 39574 steps/s (collection: 2.338s, learning 0.146s)
             Mean action noise std: 2.55
          Mean value_function loss: 70.0573
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 72.7387
                       Mean reward: 37.26
               Mean episode length: 164.16
    Episode_Reward/reaching_object: 0.9483
     Episode_Reward/lifting_object: 6.6042
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 8.2917
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 16.3750
--------------------------------------------------------------------------------
                   Total timesteps: 68616192
                    Iteration time: 2.48s
                      Time elapsed: 00:25:57
                               ETA: 01:07:02

################################################################################
                     [1m Learning iteration 698/2500 [0m                      

                       Computation: 40588 steps/s (collection: 2.283s, learning 0.139s)
             Mean action noise std: 2.56
          Mean value_function loss: 72.8420
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 72.7674
                       Mean reward: 29.49
               Mean episode length: 169.88
    Episode_Reward/reaching_object: 0.9033
     Episode_Reward/lifting_object: 5.3587
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0334
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 8.2083
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 16.5000
--------------------------------------------------------------------------------
                   Total timesteps: 68714496
                    Iteration time: 2.42s
                      Time elapsed: 00:25:59
                               ETA: 01:07:01

################################################################################
                     [1m Learning iteration 699/2500 [0m                      

                       Computation: 40275 steps/s (collection: 2.246s, learning 0.195s)
             Mean action noise std: 2.56
          Mean value_function loss: 59.5739
               Mean surrogate loss: -0.0059
                 Mean entropy loss: 72.7917
                       Mean reward: 32.08
               Mean episode length: 162.86
    Episode_Reward/reaching_object: 0.9960
     Episode_Reward/lifting_object: 6.8950
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0343
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 16.8333
--------------------------------------------------------------------------------
                   Total timesteps: 68812800
                    Iteration time: 2.44s
                      Time elapsed: 00:26:02
                               ETA: 01:06:59

################################################################################
                     [1m Learning iteration 700/2500 [0m                      

                       Computation: 41514 steps/s (collection: 2.246s, learning 0.122s)
             Mean action noise std: 2.56
          Mean value_function loss: 48.0677
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 72.8117
                       Mean reward: 34.22
               Mean episode length: 161.03
    Episode_Reward/reaching_object: 0.9291
     Episode_Reward/lifting_object: 6.4565
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 7.7500
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 18.5833
--------------------------------------------------------------------------------
                   Total timesteps: 68911104
                    Iteration time: 2.37s
                      Time elapsed: 00:26:04
                               ETA: 01:06:57

################################################################################
                     [1m Learning iteration 701/2500 [0m                      

                       Computation: 41302 steps/s (collection: 2.228s, learning 0.152s)
             Mean action noise std: 2.56
          Mean value_function loss: 48.0512
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 72.8276
                       Mean reward: 45.32
               Mean episode length: 157.58
    Episode_Reward/reaching_object: 0.9492
     Episode_Reward/lifting_object: 6.2281
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 17.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69009408
                    Iteration time: 2.38s
                      Time elapsed: 00:26:06
                               ETA: 01:06:55

################################################################################
                     [1m Learning iteration 702/2500 [0m                      

                       Computation: 41980 steps/s (collection: 2.216s, learning 0.126s)
             Mean action noise std: 2.56
          Mean value_function loss: 47.8890
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 72.8459
                       Mean reward: 44.28
               Mean episode length: 150.97
    Episode_Reward/reaching_object: 0.9720
     Episode_Reward/lifting_object: 7.1016
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 7.0833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 18.2500
--------------------------------------------------------------------------------
                   Total timesteps: 69107712
                    Iteration time: 2.34s
                      Time elapsed: 00:26:09
                               ETA: 01:06:53

################################################################################
                     [1m Learning iteration 703/2500 [0m                      

                       Computation: 42579 steps/s (collection: 2.200s, learning 0.109s)
             Mean action noise std: 2.56
          Mean value_function loss: 67.9273
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 72.8611
                       Mean reward: 38.47
               Mean episode length: 143.63
    Episode_Reward/reaching_object: 1.0135
     Episode_Reward/lifting_object: 7.3045
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 7.3333
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 17.1250
--------------------------------------------------------------------------------
                   Total timesteps: 69206016
                    Iteration time: 2.31s
                      Time elapsed: 00:26:11
                               ETA: 01:06:51

################################################################################
                     [1m Learning iteration 704/2500 [0m                      

                       Computation: 43014 steps/s (collection: 2.171s, learning 0.114s)
             Mean action noise std: 2.57
          Mean value_function loss: 56.8559
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 72.8864
                       Mean reward: 36.24
               Mean episode length: 153.18
    Episode_Reward/reaching_object: 0.9482
     Episode_Reward/lifting_object: 6.6179
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 6.4167
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 18.3750
--------------------------------------------------------------------------------
                   Total timesteps: 69304320
                    Iteration time: 2.29s
                      Time elapsed: 00:26:13
                               ETA: 01:06:49

################################################################################
                     [1m Learning iteration 705/2500 [0m                      

                       Computation: 43369 steps/s (collection: 2.152s, learning 0.115s)
             Mean action noise std: 2.57
          Mean value_function loss: 45.4917
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 72.9108
                       Mean reward: 47.04
               Mean episode length: 151.98
    Episode_Reward/reaching_object: 1.0187
     Episode_Reward/lifting_object: 7.2973
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 17.7500
--------------------------------------------------------------------------------
                   Total timesteps: 69402624
                    Iteration time: 2.27s
                      Time elapsed: 00:26:16
                               ETA: 01:06:47

################################################################################
                     [1m Learning iteration 706/2500 [0m                      

                       Computation: 43439 steps/s (collection: 2.136s, learning 0.127s)
             Mean action noise std: 2.57
          Mean value_function loss: 45.0321
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 72.9227
                       Mean reward: 45.21
               Mean episode length: 144.84
    Episode_Reward/reaching_object: 0.9865
     Episode_Reward/lifting_object: 7.5417
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 6.2500
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 17.1250
--------------------------------------------------------------------------------
                   Total timesteps: 69500928
                    Iteration time: 2.26s
                      Time elapsed: 00:26:18
                               ETA: 01:06:45

################################################################################
                     [1m Learning iteration 707/2500 [0m                      

                       Computation: 43596 steps/s (collection: 2.154s, learning 0.101s)
             Mean action noise std: 2.57
          Mean value_function loss: 49.5542
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 72.9300
                       Mean reward: 43.35
               Mean episode length: 165.58
    Episode_Reward/reaching_object: 1.0175
     Episode_Reward/lifting_object: 7.7906
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0338
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 18.8750
--------------------------------------------------------------------------------
                   Total timesteps: 69599232
                    Iteration time: 2.25s
                      Time elapsed: 00:26:20
                               ETA: 01:06:43

################################################################################
                     [1m Learning iteration 708/2500 [0m                      

                       Computation: 43481 steps/s (collection: 2.145s, learning 0.116s)
             Mean action noise std: 2.57
          Mean value_function loss: 56.4314
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 72.9357
                       Mean reward: 41.54
               Mean episode length: 148.79
    Episode_Reward/reaching_object: 0.9687
     Episode_Reward/lifting_object: 7.5057
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 5.9167
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 19.9167
--------------------------------------------------------------------------------
                   Total timesteps: 69697536
                    Iteration time: 2.26s
                      Time elapsed: 00:26:22
                               ETA: 01:06:40

################################################################################
                     [1m Learning iteration 709/2500 [0m                      

                       Computation: 43021 steps/s (collection: 2.157s, learning 0.128s)
             Mean action noise std: 2.57
          Mean value_function loss: 50.2916
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 72.9480
                       Mean reward: 40.40
               Mean episode length: 137.16
    Episode_Reward/reaching_object: 1.0083
     Episode_Reward/lifting_object: 7.5941
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0336
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 6.6667
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 19.0833
--------------------------------------------------------------------------------
                   Total timesteps: 69795840
                    Iteration time: 2.29s
                      Time elapsed: 00:26:25
                               ETA: 01:06:38

################################################################################
                     [1m Learning iteration 710/2500 [0m                      

                       Computation: 42970 steps/s (collection: 2.158s, learning 0.130s)
             Mean action noise std: 2.57
          Mean value_function loss: 53.2294
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 72.9615
                       Mean reward: 46.34
               Mean episode length: 158.39
    Episode_Reward/reaching_object: 1.0631
     Episode_Reward/lifting_object: 8.1736
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0344
          Episode_Reward/joint_vel: -0.0438
      Episode_Termination/time_out: 7.2917
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 18.2917
--------------------------------------------------------------------------------
                   Total timesteps: 69894144
                    Iteration time: 2.29s
                      Time elapsed: 00:26:27
                               ETA: 01:06:36

################################################################################
                     [1m Learning iteration 711/2500 [0m                      

                       Computation: 43946 steps/s (collection: 2.140s, learning 0.097s)
             Mean action noise std: 2.57
          Mean value_function loss: 65.3098
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 72.9681
                       Mean reward: 34.69
               Mean episode length: 141.99
    Episode_Reward/reaching_object: 1.0120
     Episode_Reward/lifting_object: 7.5492
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 7.4167
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 18.7917
--------------------------------------------------------------------------------
                   Total timesteps: 69992448
                    Iteration time: 2.24s
                      Time elapsed: 00:26:29
                               ETA: 01:06:34

################################################################################
                     [1m Learning iteration 712/2500 [0m                      

                       Computation: 43766 steps/s (collection: 2.155s, learning 0.091s)
             Mean action noise std: 2.57
          Mean value_function loss: 59.4429
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 72.9700
                       Mean reward: 45.35
               Mean episode length: 150.67
    Episode_Reward/reaching_object: 1.0111
     Episode_Reward/lifting_object: 7.7240
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0332
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 7.4583
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 21.4583
--------------------------------------------------------------------------------
                   Total timesteps: 70090752
                    Iteration time: 2.25s
                      Time elapsed: 00:26:32
                               ETA: 01:06:32

################################################################################
                     [1m Learning iteration 713/2500 [0m                      

                       Computation: 43918 steps/s (collection: 2.147s, learning 0.092s)
             Mean action noise std: 2.57
          Mean value_function loss: 58.6263
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 72.9773
                       Mean reward: 38.71
               Mean episode length: 157.23
    Episode_Reward/reaching_object: 0.9388
     Episode_Reward/lifting_object: 7.2095
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 5.4167
Episode_Termination/object_dropping: 1.8333
     Episode_Termination/robot_out: 21.7917
--------------------------------------------------------------------------------
                   Total timesteps: 70189056
                    Iteration time: 2.24s
                      Time elapsed: 00:26:34
                               ETA: 01:06:30

################################################################################
                     [1m Learning iteration 714/2500 [0m                      

                       Computation: 43930 steps/s (collection: 2.137s, learning 0.101s)
             Mean action noise std: 2.58
          Mean value_function loss: 49.2587
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 72.9896
                       Mean reward: 36.54
               Mean episode length: 133.71
    Episode_Reward/reaching_object: 0.9524
     Episode_Reward/lifting_object: 7.4845
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 5.7917
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 21.3750
--------------------------------------------------------------------------------
                   Total timesteps: 70287360
                    Iteration time: 2.24s
                      Time elapsed: 00:26:36
                               ETA: 01:06:27

################################################################################
                     [1m Learning iteration 715/2500 [0m                      

                       Computation: 42718 steps/s (collection: 2.149s, learning 0.152s)
             Mean action noise std: 2.58
          Mean value_function loss: 95.4036
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 72.9989
                       Mean reward: 41.50
               Mean episode length: 147.88
    Episode_Reward/reaching_object: 0.9420
     Episode_Reward/lifting_object: 7.7491
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 5.2083
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 23.3750
--------------------------------------------------------------------------------
                   Total timesteps: 70385664
                    Iteration time: 2.30s
                      Time elapsed: 00:26:38
                               ETA: 01:06:25

################################################################################
                     [1m Learning iteration 716/2500 [0m                      

                       Computation: 43458 steps/s (collection: 2.147s, learning 0.115s)
             Mean action noise std: 2.58
          Mean value_function loss: 68.3495
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.0163
                       Mean reward: 42.35
               Mean episode length: 131.97
    Episode_Reward/reaching_object: 0.9559
     Episode_Reward/lifting_object: 7.3535
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 5.0833
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 70483968
                    Iteration time: 2.26s
                      Time elapsed: 00:26:41
                               ETA: 01:06:23

################################################################################
                     [1m Learning iteration 717/2500 [0m                      

                       Computation: 43821 steps/s (collection: 2.152s, learning 0.091s)
             Mean action noise std: 2.58
          Mean value_function loss: 70.4688
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 73.0329
                       Mean reward: 44.93
               Mean episode length: 144.46
    Episode_Reward/reaching_object: 0.9612
     Episode_Reward/lifting_object: 8.3390
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 5.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 22.9583
--------------------------------------------------------------------------------
                   Total timesteps: 70582272
                    Iteration time: 2.24s
                      Time elapsed: 00:26:43
                               ETA: 01:06:21

################################################################################
                     [1m Learning iteration 718/2500 [0m                      

                       Computation: 43019 steps/s (collection: 2.177s, learning 0.108s)
             Mean action noise std: 2.58
          Mean value_function loss: 122.6883
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 73.0445
                       Mean reward: 47.08
               Mean episode length: 145.77
    Episode_Reward/reaching_object: 0.9485
     Episode_Reward/lifting_object: 8.1427
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 4.1250
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 26.4583
--------------------------------------------------------------------------------
                   Total timesteps: 70680576
                    Iteration time: 2.29s
                      Time elapsed: 00:26:45
                               ETA: 01:06:19

################################################################################
                     [1m Learning iteration 719/2500 [0m                      

                       Computation: 44148 steps/s (collection: 2.130s, learning 0.097s)
             Mean action noise std: 2.58
          Mean value_function loss: 60.1023
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 73.0586
                       Mean reward: 54.37
               Mean episode length: 146.18
    Episode_Reward/reaching_object: 0.9734
     Episode_Reward/lifting_object: 8.3987
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0427
      Episode_Termination/time_out: 4.7500
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 23.0833
--------------------------------------------------------------------------------
                   Total timesteps: 70778880
                    Iteration time: 2.23s
                      Time elapsed: 00:26:47
                               ETA: 01:06:17

################################################################################
                     [1m Learning iteration 720/2500 [0m                      

                       Computation: 43728 steps/s (collection: 2.148s, learning 0.100s)
             Mean action noise std: 2.58
          Mean value_function loss: 55.7461
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 73.0695
                       Mean reward: 45.21
               Mean episode length: 132.31
    Episode_Reward/reaching_object: 0.9495
     Episode_Reward/lifting_object: 9.1129
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 4.4583
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 25.4167
--------------------------------------------------------------------------------
                   Total timesteps: 70877184
                    Iteration time: 2.25s
                      Time elapsed: 00:26:50
                               ETA: 01:06:14

################################################################################
                     [1m Learning iteration 721/2500 [0m                      

                       Computation: 42084 steps/s (collection: 2.240s, learning 0.096s)
             Mean action noise std: 2.58
          Mean value_function loss: 72.4052
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 73.0790
                       Mean reward: 49.25
               Mean episode length: 132.48
    Episode_Reward/reaching_object: 0.9378
     Episode_Reward/lifting_object: 8.8924
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 3.7500
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 26.9167
--------------------------------------------------------------------------------
                   Total timesteps: 70975488
                    Iteration time: 2.34s
                      Time elapsed: 00:26:52
                               ETA: 01:06:12

################################################################################
                     [1m Learning iteration 722/2500 [0m                      

                       Computation: 43361 steps/s (collection: 2.180s, learning 0.087s)
             Mean action noise std: 2.58
          Mean value_function loss: 98.3719
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.0880
                       Mean reward: 49.85
               Mean episode length: 142.56
    Episode_Reward/reaching_object: 0.9118
     Episode_Reward/lifting_object: 8.0829
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 3.8333
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 27.1250
--------------------------------------------------------------------------------
                   Total timesteps: 71073792
                    Iteration time: 2.27s
                      Time elapsed: 00:26:54
                               ETA: 01:06:10

################################################################################
                     [1m Learning iteration 723/2500 [0m                      

                       Computation: 40082 steps/s (collection: 2.343s, learning 0.110s)
             Mean action noise std: 2.58
          Mean value_function loss: 93.3734
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 73.1017
                       Mean reward: 46.96
               Mean episode length: 129.24
    Episode_Reward/reaching_object: 0.9222
     Episode_Reward/lifting_object: 7.9086
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0399
      Episode_Termination/time_out: 4.3333
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 27.8750
--------------------------------------------------------------------------------
                   Total timesteps: 71172096
                    Iteration time: 2.45s
                      Time elapsed: 00:26:57
                               ETA: 01:06:09

################################################################################
                     [1m Learning iteration 724/2500 [0m                      

                       Computation: 42332 steps/s (collection: 2.200s, learning 0.122s)
             Mean action noise std: 2.59
          Mean value_function loss: 73.5331
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 73.1176
                       Mean reward: 43.57
               Mean episode length: 119.17
    Episode_Reward/reaching_object: 0.8950
     Episode_Reward/lifting_object: 8.3658
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 1.6250
     Episode_Termination/robot_out: 28.2917
--------------------------------------------------------------------------------
                   Total timesteps: 71270400
                    Iteration time: 2.32s
                      Time elapsed: 00:26:59
                               ETA: 01:06:07

################################################################################
                     [1m Learning iteration 725/2500 [0m                      

                       Computation: 43072 steps/s (collection: 2.166s, learning 0.116s)
             Mean action noise std: 2.59
          Mean value_function loss: 89.1921
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 73.1270
                       Mean reward: 41.32
               Mean episode length: 136.17
    Episode_Reward/reaching_object: 0.9293
     Episode_Reward/lifting_object: 8.7384
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 3.8750
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 27.4167
--------------------------------------------------------------------------------
                   Total timesteps: 71368704
                    Iteration time: 2.28s
                      Time elapsed: 00:27:01
                               ETA: 01:06:04

################################################################################
                     [1m Learning iteration 726/2500 [0m                      

                       Computation: 42423 steps/s (collection: 2.172s, learning 0.145s)
             Mean action noise std: 2.59
          Mean value_function loss: 78.4689
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 73.1393
                       Mean reward: 55.53
               Mean episode length: 133.36
    Episode_Reward/reaching_object: 0.9073
     Episode_Reward/lifting_object: 8.9262
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 26.7500
--------------------------------------------------------------------------------
                   Total timesteps: 71467008
                    Iteration time: 2.32s
                      Time elapsed: 00:27:04
                               ETA: 01:06:02

################################################################################
                     [1m Learning iteration 727/2500 [0m                      

                       Computation: 42625 steps/s (collection: 2.216s, learning 0.090s)
             Mean action noise std: 2.59
          Mean value_function loss: 80.4850
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 73.1495
                       Mean reward: 42.40
               Mean episode length: 115.77
    Episode_Reward/reaching_object: 0.8608
     Episode_Reward/lifting_object: 8.6232
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 28.5417
--------------------------------------------------------------------------------
                   Total timesteps: 71565312
                    Iteration time: 2.31s
                      Time elapsed: 00:27:06
                               ETA: 01:06:00

################################################################################
                     [1m Learning iteration 728/2500 [0m                      

                       Computation: 43592 steps/s (collection: 2.136s, learning 0.119s)
             Mean action noise std: 2.59
          Mean value_function loss: 66.8258
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 73.1543
                       Mean reward: 55.37
               Mean episode length: 141.57
    Episode_Reward/reaching_object: 0.8664
     Episode_Reward/lifting_object: 8.9832
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 3.0000
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 28.5833
--------------------------------------------------------------------------------
                   Total timesteps: 71663616
                    Iteration time: 2.26s
                      Time elapsed: 00:27:08
                               ETA: 01:05:58

################################################################################
                     [1m Learning iteration 729/2500 [0m                      

                       Computation: 43333 steps/s (collection: 2.159s, learning 0.109s)
             Mean action noise std: 2.59
          Mean value_function loss: 70.9712
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 73.1604
                       Mean reward: 56.07
               Mean episode length: 136.48
    Episode_Reward/reaching_object: 0.8812
     Episode_Reward/lifting_object: 9.7124
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 3.0417
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 28.6667
--------------------------------------------------------------------------------
                   Total timesteps: 71761920
                    Iteration time: 2.27s
                      Time elapsed: 00:27:10
                               ETA: 01:05:56

################################################################################
                     [1m Learning iteration 730/2500 [0m                      

                       Computation: 42619 steps/s (collection: 2.171s, learning 0.136s)
             Mean action noise std: 2.59
          Mean value_function loss: 104.6915
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.1713
                       Mean reward: 52.65
               Mean episode length: 123.69
    Episode_Reward/reaching_object: 0.8891
     Episode_Reward/lifting_object: 9.5576
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 31.7500
--------------------------------------------------------------------------------
                   Total timesteps: 71860224
                    Iteration time: 2.31s
                      Time elapsed: 00:27:13
                               ETA: 01:05:54

################################################################################
                     [1m Learning iteration 731/2500 [0m                      

                       Computation: 43505 steps/s (collection: 2.167s, learning 0.093s)
             Mean action noise std: 2.59
          Mean value_function loss: 107.5592
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 73.1829
                       Mean reward: 58.87
               Mean episode length: 126.67
    Episode_Reward/reaching_object: 0.8524
     Episode_Reward/lifting_object: 9.4705
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 30.6250
--------------------------------------------------------------------------------
                   Total timesteps: 71958528
                    Iteration time: 2.26s
                      Time elapsed: 00:27:15
                               ETA: 01:05:52

################################################################################
                     [1m Learning iteration 732/2500 [0m                      

                       Computation: 42785 steps/s (collection: 2.174s, learning 0.124s)
             Mean action noise std: 2.59
          Mean value_function loss: 96.9467
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 73.1962
                       Mean reward: 62.48
               Mean episode length: 136.87
    Episode_Reward/reaching_object: 0.8774
     Episode_Reward/lifting_object: 9.7437
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 31.2083
--------------------------------------------------------------------------------
                   Total timesteps: 72056832
                    Iteration time: 2.30s
                      Time elapsed: 00:27:17
                               ETA: 01:05:50

################################################################################
                     [1m Learning iteration 733/2500 [0m                      

                       Computation: 43570 steps/s (collection: 2.166s, learning 0.091s)
             Mean action noise std: 2.59
          Mean value_function loss: 86.9001
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 73.2069
                       Mean reward: 45.97
               Mean episode length: 113.46
    Episode_Reward/reaching_object: 0.8302
     Episode_Reward/lifting_object: 9.0090
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0384
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 32.1667
--------------------------------------------------------------------------------
                   Total timesteps: 72155136
                    Iteration time: 2.26s
                      Time elapsed: 00:27:19
                               ETA: 01:05:48

################################################################################
                     [1m Learning iteration 734/2500 [0m                      

                       Computation: 39291 steps/s (collection: 2.414s, learning 0.088s)
             Mean action noise std: 2.59
          Mean value_function loss: 85.0044
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 73.2167
                       Mean reward: 50.19
               Mean episode length: 110.81
    Episode_Reward/reaching_object: 0.8276
     Episode_Reward/lifting_object: 9.1598
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 33.2083
--------------------------------------------------------------------------------
                   Total timesteps: 72253440
                    Iteration time: 2.50s
                      Time elapsed: 00:27:22
                               ETA: 01:05:46

################################################################################
                     [1m Learning iteration 735/2500 [0m                      

                       Computation: 43815 steps/s (collection: 2.151s, learning 0.093s)
             Mean action noise std: 2.60
          Mean value_function loss: 93.8463
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 73.2260
                       Mean reward: 52.11
               Mean episode length: 122.18
    Episode_Reward/reaching_object: 0.8250
     Episode_Reward/lifting_object: 9.7112
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0382
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 31.3333
--------------------------------------------------------------------------------
                   Total timesteps: 72351744
                    Iteration time: 2.24s
                      Time elapsed: 00:27:24
                               ETA: 01:05:44

################################################################################
                     [1m Learning iteration 736/2500 [0m                      

                       Computation: 43562 steps/s (collection: 2.165s, learning 0.092s)
             Mean action noise std: 2.60
          Mean value_function loss: 107.2201
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 73.2377
                       Mean reward: 56.63
               Mean episode length: 111.02
    Episode_Reward/reaching_object: 0.8306
     Episode_Reward/lifting_object: 9.8658
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0385
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 32.3750
--------------------------------------------------------------------------------
                   Total timesteps: 72450048
                    Iteration time: 2.26s
                      Time elapsed: 00:27:26
                               ETA: 01:05:42

################################################################################
                     [1m Learning iteration 737/2500 [0m                      

                       Computation: 41700 steps/s (collection: 2.179s, learning 0.178s)
             Mean action noise std: 2.60
          Mean value_function loss: 98.3076
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 73.2522
                       Mean reward: 44.13
               Mean episode length: 113.44
    Episode_Reward/reaching_object: 0.8105
     Episode_Reward/lifting_object: 9.6860
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0361
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 32.4167
--------------------------------------------------------------------------------
                   Total timesteps: 72548352
                    Iteration time: 2.36s
                      Time elapsed: 00:27:29
                               ETA: 01:05:40

################################################################################
                     [1m Learning iteration 738/2500 [0m                      

                       Computation: 39572 steps/s (collection: 2.379s, learning 0.105s)
             Mean action noise std: 2.60
          Mean value_function loss: 100.6485
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 73.2590
                       Mean reward: 53.86
               Mean episode length: 122.46
    Episode_Reward/reaching_object: 0.8464
     Episode_Reward/lifting_object: 9.8685
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0371
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 31.1250
--------------------------------------------------------------------------------
                   Total timesteps: 72646656
                    Iteration time: 2.48s
                      Time elapsed: 00:27:31
                               ETA: 01:05:38

################################################################################
                     [1m Learning iteration 739/2500 [0m                      

                       Computation: 42889 steps/s (collection: 2.197s, learning 0.095s)
             Mean action noise std: 2.60
          Mean value_function loss: 103.8064
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 73.2633
                       Mean reward: 48.60
               Mean episode length: 120.65
    Episode_Reward/reaching_object: 0.8498
     Episode_Reward/lifting_object: 10.2899
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0383
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 34.2083
--------------------------------------------------------------------------------
                   Total timesteps: 72744960
                    Iteration time: 2.29s
                      Time elapsed: 00:27:34
                               ETA: 01:05:36

################################################################################
                     [1m Learning iteration 740/2500 [0m                      

                       Computation: 41402 steps/s (collection: 2.253s, learning 0.121s)
             Mean action noise std: 2.60
          Mean value_function loss: 81.7069
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.2709
                       Mean reward: 67.81
               Mean episode length: 129.67
    Episode_Reward/reaching_object: 0.8614
     Episode_Reward/lifting_object: 10.3507
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0391
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 31.4167
--------------------------------------------------------------------------------
                   Total timesteps: 72843264
                    Iteration time: 2.37s
                      Time elapsed: 00:27:36
                               ETA: 01:05:34

################################################################################
                     [1m Learning iteration 741/2500 [0m                      

                       Computation: 41721 steps/s (collection: 2.249s, learning 0.107s)
             Mean action noise std: 2.60
          Mean value_function loss: 85.3285
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 73.2766
                       Mean reward: 61.08
               Mean episode length: 120.56
    Episode_Reward/reaching_object: 0.8347
     Episode_Reward/lifting_object: 10.1331
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0373
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 32.9583
--------------------------------------------------------------------------------
                   Total timesteps: 72941568
                    Iteration time: 2.36s
                      Time elapsed: 00:27:38
                               ETA: 01:05:32

################################################################################
                     [1m Learning iteration 742/2500 [0m                      

                       Computation: 43213 steps/s (collection: 2.187s, learning 0.088s)
             Mean action noise std: 2.60
          Mean value_function loss: 97.4029
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 73.2877
                       Mean reward: 57.74
               Mean episode length: 123.10
    Episode_Reward/reaching_object: 0.8571
     Episode_Reward/lifting_object: 10.4666
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 31.9583
--------------------------------------------------------------------------------
                   Total timesteps: 73039872
                    Iteration time: 2.27s
                      Time elapsed: 00:27:41
                               ETA: 01:05:30

################################################################################
                     [1m Learning iteration 743/2500 [0m                      

                       Computation: 42719 steps/s (collection: 2.188s, learning 0.113s)
             Mean action noise std: 2.60
          Mean value_function loss: 92.3720
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.3031
                       Mean reward: 50.10
               Mean episode length: 112.72
    Episode_Reward/reaching_object: 0.8260
     Episode_Reward/lifting_object: 10.1308
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0374
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 31.5417
--------------------------------------------------------------------------------
                   Total timesteps: 73138176
                    Iteration time: 2.30s
                      Time elapsed: 00:27:43
                               ETA: 01:05:28

################################################################################
                     [1m Learning iteration 744/2500 [0m                      

                       Computation: 40613 steps/s (collection: 2.296s, learning 0.125s)
             Mean action noise std: 2.60
          Mean value_function loss: 88.1921
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.3180
                       Mean reward: 51.58
               Mean episode length: 103.51
    Episode_Reward/reaching_object: 0.8285
     Episode_Reward/lifting_object: 10.3082
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0376
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 31.4167
--------------------------------------------------------------------------------
                   Total timesteps: 73236480
                    Iteration time: 2.42s
                      Time elapsed: 00:27:45
                               ETA: 01:05:26

################################################################################
                     [1m Learning iteration 745/2500 [0m                      

                       Computation: 39692 steps/s (collection: 2.361s, learning 0.116s)
             Mean action noise std: 2.60
          Mean value_function loss: 100.6743
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 73.3306
                       Mean reward: 63.53
               Mean episode length: 119.79
    Episode_Reward/reaching_object: 0.8595
     Episode_Reward/lifting_object: 10.9522
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 33.5000
--------------------------------------------------------------------------------
                   Total timesteps: 73334784
                    Iteration time: 2.48s
                      Time elapsed: 00:27:48
                               ETA: 01:05:24

################################################################################
                     [1m Learning iteration 746/2500 [0m                      

                       Computation: 43222 steps/s (collection: 2.168s, learning 0.106s)
             Mean action noise std: 2.61
          Mean value_function loss: 90.9093
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 73.3425
                       Mean reward: 57.10
               Mean episode length: 124.71
    Episode_Reward/reaching_object: 0.8453
     Episode_Reward/lifting_object: 10.6445
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 34.3333
--------------------------------------------------------------------------------
                   Total timesteps: 73433088
                    Iteration time: 2.27s
                      Time elapsed: 00:27:50
                               ETA: 01:05:22

################################################################################
                     [1m Learning iteration 747/2500 [0m                      

                       Computation: 43518 steps/s (collection: 2.153s, learning 0.106s)
             Mean action noise std: 2.61
          Mean value_function loss: 95.9642
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 73.3511
                       Mean reward: 56.57
               Mean episode length: 115.33
    Episode_Reward/reaching_object: 0.8504
     Episode_Reward/lifting_object: 11.2775
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0397
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 31.9167
--------------------------------------------------------------------------------
                   Total timesteps: 73531392
                    Iteration time: 2.26s
                      Time elapsed: 00:27:52
                               ETA: 01:05:20

################################################################################
                     [1m Learning iteration 748/2500 [0m                      

                       Computation: 43142 steps/s (collection: 2.165s, learning 0.114s)
             Mean action noise std: 2.61
          Mean value_function loss: 92.3262
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 73.3548
                       Mean reward: 69.05
               Mean episode length: 120.98
    Episode_Reward/reaching_object: 0.8441
     Episode_Reward/lifting_object: 11.1983
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0377
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 32.8750
--------------------------------------------------------------------------------
                   Total timesteps: 73629696
                    Iteration time: 2.28s
                      Time elapsed: 00:27:55
                               ETA: 01:05:18

################################################################################
                     [1m Learning iteration 749/2500 [0m                      

                       Computation: 43477 steps/s (collection: 2.167s, learning 0.094s)
             Mean action noise std: 2.61
          Mean value_function loss: 123.7737
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 73.3631
                       Mean reward: 54.40
               Mean episode length: 113.51
    Episode_Reward/reaching_object: 0.8455
     Episode_Reward/lifting_object: 10.9555
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0380
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 33.7917
--------------------------------------------------------------------------------
                   Total timesteps: 73728000
                    Iteration time: 2.26s
                      Time elapsed: 00:27:57
                               ETA: 01:05:16

################################################################################
                     [1m Learning iteration 750/2500 [0m                      

                       Computation: 43354 steps/s (collection: 2.147s, learning 0.120s)
             Mean action noise std: 2.61
          Mean value_function loss: 127.7422
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 73.3732
                       Mean reward: 56.47
               Mean episode length: 109.52
    Episode_Reward/reaching_object: 0.8314
     Episode_Reward/lifting_object: 10.6019
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 32.8333
--------------------------------------------------------------------------------
                   Total timesteps: 73826304
                    Iteration time: 2.27s
                      Time elapsed: 00:27:59
                               ETA: 01:05:13

################################################################################
                     [1m Learning iteration 751/2500 [0m                      

                       Computation: 43069 steps/s (collection: 2.168s, learning 0.115s)
             Mean action noise std: 2.61
          Mean value_function loss: 118.8089
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.3865
                       Mean reward: 57.47
               Mean episode length: 113.89
    Episode_Reward/reaching_object: 0.8200
     Episode_Reward/lifting_object: 11.0296
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0366
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 36.6667
--------------------------------------------------------------------------------
                   Total timesteps: 73924608
                    Iteration time: 2.28s
                      Time elapsed: 00:28:01
                               ETA: 01:05:11

################################################################################
                     [1m Learning iteration 752/2500 [0m                      

                       Computation: 43064 steps/s (collection: 2.163s, learning 0.120s)
             Mean action noise std: 2.61
          Mean value_function loss: 145.7701
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 73.4003
                       Mean reward: 51.61
               Mean episode length: 109.11
    Episode_Reward/reaching_object: 0.7953
     Episode_Reward/lifting_object: 10.6658
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0370
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 35.6250
--------------------------------------------------------------------------------
                   Total timesteps: 74022912
                    Iteration time: 2.28s
                      Time elapsed: 00:28:04
                               ETA: 01:05:09

################################################################################
                     [1m Learning iteration 753/2500 [0m                      

                       Computation: 42683 steps/s (collection: 2.187s, learning 0.116s)
             Mean action noise std: 2.61
          Mean value_function loss: 91.8437
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 73.4124
                       Mean reward: 53.90
               Mean episode length: 113.60
    Episode_Reward/reaching_object: 0.8004
     Episode_Reward/lifting_object: 10.6938
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0363
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 33.0417
--------------------------------------------------------------------------------
                   Total timesteps: 74121216
                    Iteration time: 2.30s
                      Time elapsed: 00:28:06
                               ETA: 01:05:07

################################################################################
                     [1m Learning iteration 754/2500 [0m                      

                       Computation: 43190 steps/s (collection: 2.158s, learning 0.119s)
             Mean action noise std: 2.61
          Mean value_function loss: 117.2114
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 73.4247
                       Mean reward: 53.33
               Mean episode length: 113.35
    Episode_Reward/reaching_object: 0.8021
     Episode_Reward/lifting_object: 10.9263
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0372
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 35.2917
--------------------------------------------------------------------------------
                   Total timesteps: 74219520
                    Iteration time: 2.28s
                      Time elapsed: 00:28:08
                               ETA: 01:05:05

################################################################################
                     [1m Learning iteration 755/2500 [0m                      

                       Computation: 43516 steps/s (collection: 2.154s, learning 0.105s)
             Mean action noise std: 2.61
          Mean value_function loss: 93.6133
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 73.4392
                       Mean reward: 46.97
               Mean episode length: 114.73
    Episode_Reward/reaching_object: 0.8139
     Episode_Reward/lifting_object: 11.3663
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0379
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 34.8333
--------------------------------------------------------------------------------
                   Total timesteps: 74317824
                    Iteration time: 2.26s
                      Time elapsed: 00:28:11
                               ETA: 01:05:03

################################################################################
                     [1m Learning iteration 756/2500 [0m                      

                       Computation: 41659 steps/s (collection: 2.241s, learning 0.118s)
             Mean action noise std: 2.62
          Mean value_function loss: 99.2339
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 73.4495
                       Mean reward: 55.93
               Mean episode length: 110.06
    Episode_Reward/reaching_object: 0.8200
     Episode_Reward/lifting_object: 11.4673
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0378
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 34.3750
--------------------------------------------------------------------------------
                   Total timesteps: 74416128
                    Iteration time: 2.36s
                      Time elapsed: 00:28:13
                               ETA: 01:05:01

################################################################################
                     [1m Learning iteration 757/2500 [0m                      

                       Computation: 41435 steps/s (collection: 2.271s, learning 0.102s)
             Mean action noise std: 2.62
          Mean value_function loss: 100.3182
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 73.4605
                       Mean reward: 63.25
               Mean episode length: 116.90
    Episode_Reward/reaching_object: 0.8211
     Episode_Reward/lifting_object: 11.6474
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 32.3333
--------------------------------------------------------------------------------
                   Total timesteps: 74514432
                    Iteration time: 2.37s
                      Time elapsed: 00:28:15
                               ETA: 01:04:59

################################################################################
                     [1m Learning iteration 758/2500 [0m                      

                       Computation: 43355 steps/s (collection: 2.159s, learning 0.108s)
             Mean action noise std: 2.62
          Mean value_function loss: 114.8791
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 73.4663
                       Mean reward: 59.61
               Mean episode length: 112.37
    Episode_Reward/reaching_object: 0.8061
     Episode_Reward/lifting_object: 10.8392
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0381
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 34.0000
--------------------------------------------------------------------------------
                   Total timesteps: 74612736
                    Iteration time: 2.27s
                      Time elapsed: 00:28:18
                               ETA: 01:04:57

################################################################################
                     [1m Learning iteration 759/2500 [0m                      

                       Computation: 41686 steps/s (collection: 2.243s, learning 0.115s)
             Mean action noise std: 2.62
          Mean value_function loss: 106.3722
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.4786
                       Mean reward: 60.81
               Mean episode length: 110.61
    Episode_Reward/reaching_object: 0.8440
     Episode_Reward/lifting_object: 12.3424
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 34.3750
--------------------------------------------------------------------------------
                   Total timesteps: 74711040
                    Iteration time: 2.36s
                      Time elapsed: 00:28:20
                               ETA: 01:04:55

################################################################################
                     [1m Learning iteration 760/2500 [0m                      

                       Computation: 42341 steps/s (collection: 2.203s, learning 0.119s)
             Mean action noise std: 2.62
          Mean value_function loss: 126.3328
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 73.4888
                       Mean reward: 63.31
               Mean episode length: 104.88
    Episode_Reward/reaching_object: 0.8442
     Episode_Reward/lifting_object: 11.9164
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 1.9583
     Episode_Termination/robot_out: 34.0833
--------------------------------------------------------------------------------
                   Total timesteps: 74809344
                    Iteration time: 2.32s
                      Time elapsed: 00:28:22
                               ETA: 01:04:53

################################################################################
                     [1m Learning iteration 761/2500 [0m                      

                       Computation: 42986 steps/s (collection: 2.197s, learning 0.090s)
             Mean action noise std: 2.62
          Mean value_function loss: 132.3924
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 73.4996
                       Mean reward: 59.53
               Mean episode length: 110.47
    Episode_Reward/reaching_object: 0.8110
     Episode_Reward/lifting_object: 11.2264
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0253
          Episode_Reward/joint_vel: -0.0387
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 1.7500
     Episode_Termination/robot_out: 34.7500
--------------------------------------------------------------------------------
                   Total timesteps: 74907648
                    Iteration time: 2.29s
                      Time elapsed: 00:28:25
                               ETA: 01:04:51

################################################################################
                     [1m Learning iteration 762/2500 [0m                      

                       Computation: 43162 steps/s (collection: 2.170s, learning 0.108s)
             Mean action noise std: 2.62
          Mean value_function loss: 118.6775
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 73.5129
                       Mean reward: 60.19
               Mean episode length: 116.56
    Episode_Reward/reaching_object: 0.8440
     Episode_Reward/lifting_object: 11.7183
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 34.0833
--------------------------------------------------------------------------------
                   Total timesteps: 75005952
                    Iteration time: 2.28s
                      Time elapsed: 00:28:27
                               ETA: 01:04:48

################################################################################
                     [1m Learning iteration 763/2500 [0m                      

                       Computation: 42678 steps/s (collection: 2.186s, learning 0.117s)
             Mean action noise std: 2.62
          Mean value_function loss: 120.9722
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 73.5199
                       Mean reward: 58.00
               Mean episode length: 107.94
    Episode_Reward/reaching_object: 0.8312
     Episode_Reward/lifting_object: 11.5305
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0258
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 1.3750
     Episode_Termination/robot_out: 34.0000
--------------------------------------------------------------------------------
                   Total timesteps: 75104256
                    Iteration time: 2.30s
                      Time elapsed: 00:28:29
                               ETA: 01:04:46

################################################################################
                     [1m Learning iteration 764/2500 [0m                      

                       Computation: 42332 steps/s (collection: 2.207s, learning 0.115s)
             Mean action noise std: 2.62
          Mean value_function loss: 116.9040
               Mean surrogate loss: -0.0054
                 Mean entropy loss: 73.5324
                       Mean reward: 71.31
               Mean episode length: 117.94
    Episode_Reward/reaching_object: 0.8762
     Episode_Reward/lifting_object: 12.4047
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 32.6250
--------------------------------------------------------------------------------
                   Total timesteps: 75202560
                    Iteration time: 2.32s
                      Time elapsed: 00:28:31
                               ETA: 01:04:44

################################################################################
                     [1m Learning iteration 765/2500 [0m                      

                       Computation: 42750 steps/s (collection: 2.195s, learning 0.104s)
             Mean action noise std: 2.62
          Mean value_function loss: 120.8952
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 73.5454
                       Mean reward: 71.59
               Mean episode length: 110.94
    Episode_Reward/reaching_object: 0.8348
     Episode_Reward/lifting_object: 12.7960
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 35.2083
--------------------------------------------------------------------------------
                   Total timesteps: 75300864
                    Iteration time: 2.30s
                      Time elapsed: 00:28:34
                               ETA: 01:04:42

################################################################################
                     [1m Learning iteration 766/2500 [0m                      

                       Computation: 41376 steps/s (collection: 2.265s, learning 0.111s)
             Mean action noise std: 2.62
          Mean value_function loss: 110.8256
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 73.5544
                       Mean reward: 68.18
               Mean episode length: 112.25
    Episode_Reward/reaching_object: 0.8437
     Episode_Reward/lifting_object: 12.4187
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0395
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 32.0833
--------------------------------------------------------------------------------
                   Total timesteps: 75399168
                    Iteration time: 2.38s
                      Time elapsed: 00:28:36
                               ETA: 01:04:40

################################################################################
                     [1m Learning iteration 767/2500 [0m                      

                       Computation: 42962 steps/s (collection: 2.177s, learning 0.111s)
             Mean action noise std: 2.63
          Mean value_function loss: 125.3043
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 73.5600
                       Mean reward: 67.10
               Mean episode length: 106.24
    Episode_Reward/reaching_object: 0.8706
     Episode_Reward/lifting_object: 13.5562
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 35.7917
--------------------------------------------------------------------------------
                   Total timesteps: 75497472
                    Iteration time: 2.29s
                      Time elapsed: 00:28:38
                               ETA: 01:04:38

################################################################################
                     [1m Learning iteration 768/2500 [0m                      

                       Computation: 42756 steps/s (collection: 2.186s, learning 0.113s)
             Mean action noise std: 2.63
          Mean value_function loss: 99.4853
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 73.5665
                       Mean reward: 68.37
               Mean episode length: 107.61
    Episode_Reward/reaching_object: 0.8629
     Episode_Reward/lifting_object: 12.9032
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0420
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 35.1250
--------------------------------------------------------------------------------
                   Total timesteps: 75595776
                    Iteration time: 2.30s
                      Time elapsed: 00:28:41
                               ETA: 01:04:36

################################################################################
                     [1m Learning iteration 769/2500 [0m                      

                       Computation: 42739 steps/s (collection: 2.204s, learning 0.097s)
             Mean action noise std: 2.63
          Mean value_function loss: 123.0649
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 73.5723
                       Mean reward: 69.39
               Mean episode length: 110.74
    Episode_Reward/reaching_object: 0.8466
     Episode_Reward/lifting_object: 13.0888
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0264
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 35.2917
--------------------------------------------------------------------------------
                   Total timesteps: 75694080
                    Iteration time: 2.30s
                      Time elapsed: 00:28:43
                               ETA: 01:04:34

################################################################################
                     [1m Learning iteration 770/2500 [0m                      

                       Computation: 42876 steps/s (collection: 2.182s, learning 0.111s)
             Mean action noise std: 2.63
          Mean value_function loss: 124.4493
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 73.5797
                       Mean reward: 73.24
               Mean episode length: 104.94
    Episode_Reward/reaching_object: 0.8162
     Episode_Reward/lifting_object: 12.7479
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 1.5000
     Episode_Termination/robot_out: 36.7500
--------------------------------------------------------------------------------
                   Total timesteps: 75792384
                    Iteration time: 2.29s
                      Time elapsed: 00:28:45
                               ETA: 01:04:32

################################################################################
                     [1m Learning iteration 771/2500 [0m                      

                       Computation: 42721 steps/s (collection: 2.197s, learning 0.104s)
             Mean action noise std: 2.63
          Mean value_function loss: 116.2227
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 73.5854
                       Mean reward: 70.34
               Mean episode length: 109.74
    Episode_Reward/reaching_object: 0.8341
     Episode_Reward/lifting_object: 13.2386
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 34.9583
--------------------------------------------------------------------------------
                   Total timesteps: 75890688
                    Iteration time: 2.30s
                      Time elapsed: 00:28:48
                               ETA: 01:04:30

################################################################################
                     [1m Learning iteration 772/2500 [0m                      

                       Computation: 40415 steps/s (collection: 2.318s, learning 0.114s)
             Mean action noise std: 2.63
          Mean value_function loss: 103.6822
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 73.5922
                       Mean reward: 81.94
               Mean episode length: 114.34
    Episode_Reward/reaching_object: 0.8313
     Episode_Reward/lifting_object: 13.4371
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 33.8750
--------------------------------------------------------------------------------
                   Total timesteps: 75988992
                    Iteration time: 2.43s
                      Time elapsed: 00:28:50
                               ETA: 01:04:28

################################################################################
                     [1m Learning iteration 773/2500 [0m                      

                       Computation: 42171 steps/s (collection: 2.219s, learning 0.112s)
             Mean action noise std: 2.63
          Mean value_function loss: 121.4596
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 73.5989
                       Mean reward: 67.14
               Mean episode length: 105.55
    Episode_Reward/reaching_object: 0.8347
     Episode_Reward/lifting_object: 13.3193
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 35.2917
--------------------------------------------------------------------------------
                   Total timesteps: 76087296
                    Iteration time: 2.33s
                      Time elapsed: 00:28:52
                               ETA: 01:04:26

################################################################################
                     [1m Learning iteration 774/2500 [0m                      

                       Computation: 43132 steps/s (collection: 2.165s, learning 0.114s)
             Mean action noise std: 2.63
          Mean value_function loss: 131.5251
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 73.6065
                       Mean reward: 66.68
               Mean episode length: 104.15
    Episode_Reward/reaching_object: 0.8169
     Episode_Reward/lifting_object: 13.2785
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 36.2083
--------------------------------------------------------------------------------
                   Total timesteps: 76185600
                    Iteration time: 2.28s
                      Time elapsed: 00:28:55
                               ETA: 01:04:24

################################################################################
                     [1m Learning iteration 775/2500 [0m                      

                       Computation: 43227 steps/s (collection: 2.161s, learning 0.114s)
             Mean action noise std: 2.63
          Mean value_function loss: 131.2914
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 73.6167
                       Mean reward: 66.78
               Mean episode length: 103.52
    Episode_Reward/reaching_object: 0.8067
     Episode_Reward/lifting_object: 13.1638
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 1.5417
     Episode_Termination/robot_out: 36.6667
--------------------------------------------------------------------------------
                   Total timesteps: 76283904
                    Iteration time: 2.27s
                      Time elapsed: 00:28:57
                               ETA: 01:04:22

################################################################################
                     [1m Learning iteration 776/2500 [0m                      

                       Computation: 42706 steps/s (collection: 2.187s, learning 0.115s)
             Mean action noise std: 2.63
          Mean value_function loss: 120.5556
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.6261
                       Mean reward: 69.70
               Mean episode length: 108.56
    Episode_Reward/reaching_object: 0.8039
     Episode_Reward/lifting_object: 13.0199
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 36.9583
--------------------------------------------------------------------------------
                   Total timesteps: 76382208
                    Iteration time: 2.30s
                      Time elapsed: 00:28:59
                               ETA: 01:04:20

################################################################################
                     [1m Learning iteration 777/2500 [0m                      

                       Computation: 40513 steps/s (collection: 2.324s, learning 0.103s)
             Mean action noise std: 2.63
          Mean value_function loss: 134.0402
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 73.6373
                       Mean reward: 69.01
               Mean episode length: 105.13
    Episode_Reward/reaching_object: 0.7851
     Episode_Reward/lifting_object: 12.9699
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0398
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 38.4167
--------------------------------------------------------------------------------
                   Total timesteps: 76480512
                    Iteration time: 2.43s
                      Time elapsed: 00:29:02
                               ETA: 01:04:18

################################################################################
                     [1m Learning iteration 778/2500 [0m                      

                       Computation: 43482 steps/s (collection: 2.152s, learning 0.109s)
             Mean action noise std: 2.63
          Mean value_function loss: 127.2339
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 73.6466
                       Mean reward: 67.05
               Mean episode length: 109.45
    Episode_Reward/reaching_object: 0.8190
     Episode_Reward/lifting_object: 13.1685
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0404
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 36.5000
--------------------------------------------------------------------------------
                   Total timesteps: 76578816
                    Iteration time: 2.26s
                      Time elapsed: 00:29:04
                               ETA: 01:04:16

################################################################################
                     [1m Learning iteration 779/2500 [0m                      

                       Computation: 40909 steps/s (collection: 2.290s, learning 0.113s)
             Mean action noise std: 2.63
          Mean value_function loss: 136.2268
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.6560
                       Mean reward: 69.77
               Mean episode length: 103.70
    Episode_Reward/reaching_object: 0.8009
     Episode_Reward/lifting_object: 13.7646
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 1.1667
     Episode_Termination/robot_out: 36.9583
--------------------------------------------------------------------------------
                   Total timesteps: 76677120
                    Iteration time: 2.40s
                      Time elapsed: 00:29:06
                               ETA: 01:04:14

################################################################################
                     [1m Learning iteration 780/2500 [0m                      

                       Computation: 42385 steps/s (collection: 2.212s, learning 0.107s)
             Mean action noise std: 2.63
          Mean value_function loss: 126.2401
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 73.6646
                       Mean reward: 73.70
               Mean episode length: 114.56
    Episode_Reward/reaching_object: 0.8011
     Episode_Reward/lifting_object: 12.9460
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 38.4167
--------------------------------------------------------------------------------
                   Total timesteps: 76775424
                    Iteration time: 2.32s
                      Time elapsed: 00:29:09
                               ETA: 01:04:12

################################################################################
                     [1m Learning iteration 781/2500 [0m                      

                       Computation: 43322 steps/s (collection: 2.160s, learning 0.110s)
             Mean action noise std: 2.64
          Mean value_function loss: 144.4362
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 73.6749
                       Mean reward: 73.41
               Mean episode length: 102.68
    Episode_Reward/reaching_object: 0.7857
     Episode_Reward/lifting_object: 13.2545
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0252
          Episode_Reward/joint_vel: -0.0402
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 36.9167
--------------------------------------------------------------------------------
                   Total timesteps: 76873728
                    Iteration time: 2.27s
                      Time elapsed: 00:29:11
                               ETA: 01:04:09

################################################################################
                     [1m Learning iteration 782/2500 [0m                      

                       Computation: 42354 steps/s (collection: 2.200s, learning 0.121s)
             Mean action noise std: 2.64
          Mean value_function loss: 139.7273
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.6797
                       Mean reward: 63.52
               Mean episode length: 103.93
    Episode_Reward/reaching_object: 0.7607
     Episode_Reward/lifting_object: 12.7286
      Episode_Reward/object_height: 0.0014
        Episode_Reward/action_rate: -0.0245
          Episode_Reward/joint_vel: -0.0393
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 35.7917
--------------------------------------------------------------------------------
                   Total timesteps: 76972032
                    Iteration time: 2.32s
                      Time elapsed: 00:29:13
                               ETA: 01:04:07

################################################################################
                     [1m Learning iteration 783/2500 [0m                      

                       Computation: 41524 steps/s (collection: 2.248s, learning 0.119s)
             Mean action noise std: 2.64
          Mean value_function loss: 113.1819
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 73.6874
                       Mean reward: 73.10
               Mean episode length: 114.23
    Episode_Reward/reaching_object: 0.8057
     Episode_Reward/lifting_object: 13.5447
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 36.7500
--------------------------------------------------------------------------------
                   Total timesteps: 77070336
                    Iteration time: 2.37s
                      Time elapsed: 00:29:16
                               ETA: 01:04:05

################################################################################
                     [1m Learning iteration 784/2500 [0m                      

                       Computation: 43046 steps/s (collection: 2.192s, learning 0.092s)
             Mean action noise std: 2.64
          Mean value_function loss: 133.0756
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 73.6973
                       Mean reward: 72.22
               Mean episode length: 113.66
    Episode_Reward/reaching_object: 0.8035
     Episode_Reward/lifting_object: 13.3830
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 37.4583
--------------------------------------------------------------------------------
                   Total timesteps: 77168640
                    Iteration time: 2.28s
                      Time elapsed: 00:29:18
                               ETA: 01:04:03

################################################################################
                     [1m Learning iteration 785/2500 [0m                      

                       Computation: 42606 steps/s (collection: 2.188s, learning 0.120s)
             Mean action noise std: 2.64
          Mean value_function loss: 136.7705
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 73.7070
                       Mean reward: 73.14
               Mean episode length: 103.46
    Episode_Reward/reaching_object: 0.7986
     Episode_Reward/lifting_object: 13.4888
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 1.6667
     Episode_Termination/robot_out: 37.3333
--------------------------------------------------------------------------------
                   Total timesteps: 77266944
                    Iteration time: 2.31s
                      Time elapsed: 00:29:20
                               ETA: 01:04:01

################################################################################
                     [1m Learning iteration 786/2500 [0m                      

                       Computation: 42154 steps/s (collection: 2.214s, learning 0.118s)
             Mean action noise std: 2.64
          Mean value_function loss: 135.2537
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 73.7167
                       Mean reward: 73.57
               Mean episode length: 108.84
    Episode_Reward/reaching_object: 0.8183
     Episode_Reward/lifting_object: 13.8363
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 1.4583
     Episode_Termination/robot_out: 37.6250
--------------------------------------------------------------------------------
                   Total timesteps: 77365248
                    Iteration time: 2.33s
                      Time elapsed: 00:29:23
                               ETA: 01:03:59

################################################################################
                     [1m Learning iteration 787/2500 [0m                      

                       Computation: 42941 steps/s (collection: 2.194s, learning 0.095s)
             Mean action noise std: 2.64
          Mean value_function loss: 134.3669
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 73.7223
                       Mean reward: 78.41
               Mean episode length: 111.28
    Episode_Reward/reaching_object: 0.8113
     Episode_Reward/lifting_object: 14.4909
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 35.8333
--------------------------------------------------------------------------------
                   Total timesteps: 77463552
                    Iteration time: 2.29s
                      Time elapsed: 00:29:25
                               ETA: 01:03:57

################################################################################
                     [1m Learning iteration 788/2500 [0m                      

                       Computation: 41477 steps/s (collection: 2.262s, learning 0.108s)
             Mean action noise std: 2.64
          Mean value_function loss: 131.2781
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 73.7270
                       Mean reward: 73.59
               Mean episode length: 105.91
    Episode_Reward/reaching_object: 0.7939
     Episode_Reward/lifting_object: 14.4643
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0255
          Episode_Reward/joint_vel: -0.0403
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 36.6667
--------------------------------------------------------------------------------
                   Total timesteps: 77561856
                    Iteration time: 2.37s
                      Time elapsed: 00:29:27
                               ETA: 01:03:55

################################################################################
                     [1m Learning iteration 789/2500 [0m                      

                       Computation: 42950 steps/s (collection: 2.172s, learning 0.117s)
             Mean action noise std: 2.64
          Mean value_function loss: 149.2709
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 73.7322
                       Mean reward: 76.22
               Mean episode length: 113.05
    Episode_Reward/reaching_object: 0.7993
     Episode_Reward/lifting_object: 14.8139
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0256
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 34.5000
--------------------------------------------------------------------------------
                   Total timesteps: 77660160
                    Iteration time: 2.29s
                      Time elapsed: 00:29:29
                               ETA: 01:03:53

################################################################################
                     [1m Learning iteration 790/2500 [0m                      

                       Computation: 42719 steps/s (collection: 2.182s, learning 0.119s)
             Mean action noise std: 2.64
          Mean value_function loss: 133.8987
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 73.7401
                       Mean reward: 92.50
               Mean episode length: 111.96
    Episode_Reward/reaching_object: 0.8171
     Episode_Reward/lifting_object: 14.6501
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0421
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 37.8750
--------------------------------------------------------------------------------
                   Total timesteps: 77758464
                    Iteration time: 2.30s
                      Time elapsed: 00:29:32
                               ETA: 01:03:51

################################################################################
                     [1m Learning iteration 791/2500 [0m                      

                       Computation: 42940 steps/s (collection: 2.173s, learning 0.117s)
             Mean action noise std: 2.64
          Mean value_function loss: 124.7973
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 73.7466
                       Mean reward: 79.35
               Mean episode length: 108.30
    Episode_Reward/reaching_object: 0.8320
     Episode_Reward/lifting_object: 14.7866
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 1.4167
     Episode_Termination/robot_out: 35.2083
--------------------------------------------------------------------------------
                   Total timesteps: 77856768
                    Iteration time: 2.29s
                      Time elapsed: 00:29:34
                               ETA: 01:03:49

################################################################################
                     [1m Learning iteration 792/2500 [0m                      

                       Computation: 42929 steps/s (collection: 2.179s, learning 0.111s)
             Mean action noise std: 2.64
          Mean value_function loss: 137.7587
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.7498
                       Mean reward: 84.37
               Mean episode length: 113.25
    Episode_Reward/reaching_object: 0.7979
     Episode_Reward/lifting_object: 15.0048
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 38.8750
--------------------------------------------------------------------------------
                   Total timesteps: 77955072
                    Iteration time: 2.29s
                      Time elapsed: 00:29:36
                               ETA: 01:03:47

################################################################################
                     [1m Learning iteration 793/2500 [0m                      

                       Computation: 43202 steps/s (collection: 2.160s, learning 0.115s)
             Mean action noise std: 2.64
          Mean value_function loss: 119.7714
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 73.7574
                       Mean reward: 80.37
               Mean episode length: 111.53
    Episode_Reward/reaching_object: 0.8048
     Episode_Reward/lifting_object: 14.6721
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 35.5833
--------------------------------------------------------------------------------
                   Total timesteps: 78053376
                    Iteration time: 2.28s
                      Time elapsed: 00:29:39
                               ETA: 01:03:44

################################################################################
                     [1m Learning iteration 794/2500 [0m                      

                       Computation: 43249 steps/s (collection: 2.180s, learning 0.093s)
             Mean action noise std: 2.64
          Mean value_function loss: 166.4885
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 73.7656
                       Mean reward: 70.85
               Mean episode length: 104.52
    Episode_Reward/reaching_object: 0.7959
     Episode_Reward/lifting_object: 14.9902
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 37.4167
--------------------------------------------------------------------------------
                   Total timesteps: 78151680
                    Iteration time: 2.27s
                      Time elapsed: 00:29:41
                               ETA: 01:03:42

################################################################################
                     [1m Learning iteration 795/2500 [0m                      

                       Computation: 42765 steps/s (collection: 2.183s, learning 0.116s)
             Mean action noise std: 2.64
          Mean value_function loss: 129.7475
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 73.7741
                       Mean reward: 79.66
               Mean episode length: 110.65
    Episode_Reward/reaching_object: 0.7994
     Episode_Reward/lifting_object: 15.0293
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0259
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 39.3333
--------------------------------------------------------------------------------
                   Total timesteps: 78249984
                    Iteration time: 2.30s
                      Time elapsed: 00:29:43
                               ETA: 01:03:40

################################################################################
                     [1m Learning iteration 796/2500 [0m                      

                       Computation: 43199 steps/s (collection: 2.167s, learning 0.109s)
             Mean action noise std: 2.65
          Mean value_function loss: 148.7726
               Mean surrogate loss: -0.0051
                 Mean entropy loss: 73.7814
                       Mean reward: 84.84
               Mean episode length: 105.01
    Episode_Reward/reaching_object: 0.8076
     Episode_Reward/lifting_object: 15.3808
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 37.2083
--------------------------------------------------------------------------------
                   Total timesteps: 78348288
                    Iteration time: 2.28s
                      Time elapsed: 00:29:45
                               ETA: 01:03:38

################################################################################
                     [1m Learning iteration 797/2500 [0m                      

                       Computation: 43092 steps/s (collection: 2.167s, learning 0.114s)
             Mean action noise std: 2.65
          Mean value_function loss: 137.7926
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.7875
                       Mean reward: 82.67
               Mean episode length: 107.11
    Episode_Reward/reaching_object: 0.7727
     Episode_Reward/lifting_object: 14.7538
      Episode_Reward/object_height: 0.0015
        Episode_Reward/action_rate: -0.0254
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 37.1250
--------------------------------------------------------------------------------
                   Total timesteps: 78446592
                    Iteration time: 2.28s
                      Time elapsed: 00:29:48
                               ETA: 01:03:36

################################################################################
                     [1m Learning iteration 798/2500 [0m                      

                       Computation: 43220 steps/s (collection: 2.165s, learning 0.110s)
             Mean action noise std: 2.65
          Mean value_function loss: 129.4329
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 73.7921
                       Mean reward: 85.05
               Mean episode length: 106.79
    Episode_Reward/reaching_object: 0.8019
     Episode_Reward/lifting_object: 15.4630
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 1.7083
     Episode_Termination/robot_out: 38.1250
--------------------------------------------------------------------------------
                   Total timesteps: 78544896
                    Iteration time: 2.27s
                      Time elapsed: 00:29:50
                               ETA: 01:03:34

################################################################################
                     [1m Learning iteration 799/2500 [0m                      

                       Computation: 42802 steps/s (collection: 2.188s, learning 0.109s)
             Mean action noise std: 2.65
          Mean value_function loss: 125.7842
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 73.7966
                       Mean reward: 66.90
               Mean episode length: 91.97
    Episode_Reward/reaching_object: 0.7937
     Episode_Reward/lifting_object: 15.7059
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 35.0417
--------------------------------------------------------------------------------
                   Total timesteps: 78643200
                    Iteration time: 2.30s
                      Time elapsed: 00:29:52
                               ETA: 01:03:31

################################################################################
                     [1m Learning iteration 800/2500 [0m                      

                       Computation: 43266 steps/s (collection: 2.176s, learning 0.097s)
             Mean action noise std: 2.65
          Mean value_function loss: 130.3611
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 73.8014
                       Mean reward: 91.17
               Mean episode length: 105.55
    Episode_Reward/reaching_object: 0.8039
     Episode_Reward/lifting_object: 15.9767
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 36.6250
--------------------------------------------------------------------------------
                   Total timesteps: 78741504
                    Iteration time: 2.27s
                      Time elapsed: 00:29:55
                               ETA: 01:03:29

################################################################################
                     [1m Learning iteration 801/2500 [0m                      

                       Computation: 42700 steps/s (collection: 2.194s, learning 0.108s)
             Mean action noise std: 2.65
          Mean value_function loss: 125.2943
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 73.8062
                       Mean reward: 89.38
               Mean episode length: 103.32
    Episode_Reward/reaching_object: 0.8112
     Episode_Reward/lifting_object: 16.3949
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 34.7917
--------------------------------------------------------------------------------
                   Total timesteps: 78839808
                    Iteration time: 2.30s
                      Time elapsed: 00:29:57
                               ETA: 01:03:27

################################################################################
                     [1m Learning iteration 802/2500 [0m                      

                       Computation: 43166 steps/s (collection: 2.165s, learning 0.113s)
             Mean action noise std: 2.65
          Mean value_function loss: 115.0162
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 73.8117
                       Mean reward: 90.94
               Mean episode length: 106.21
    Episode_Reward/reaching_object: 0.8085
     Episode_Reward/lifting_object: 16.8770
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0396
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 36.1667
--------------------------------------------------------------------------------
                   Total timesteps: 78938112
                    Iteration time: 2.28s
                      Time elapsed: 00:29:59
                               ETA: 01:03:25

################################################################################
                     [1m Learning iteration 803/2500 [0m                      

                       Computation: 42049 steps/s (collection: 2.223s, learning 0.115s)
             Mean action noise std: 2.65
          Mean value_function loss: 145.5806
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 73.8154
                       Mean reward: 85.72
               Mean episode length: 112.10
    Episode_Reward/reaching_object: 0.8324
     Episode_Reward/lifting_object: 16.6323
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0415
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 38.7083
--------------------------------------------------------------------------------
                   Total timesteps: 79036416
                    Iteration time: 2.34s
                      Time elapsed: 00:30:01
                               ETA: 01:03:23

################################################################################
                     [1m Learning iteration 804/2500 [0m                      

                       Computation: 42731 steps/s (collection: 2.192s, learning 0.109s)
             Mean action noise std: 2.65
          Mean value_function loss: 160.5200
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 73.8205
                       Mean reward: 95.31
               Mean episode length: 113.96
    Episode_Reward/reaching_object: 0.8560
     Episode_Reward/lifting_object: 17.3119
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0431
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 36.0000
--------------------------------------------------------------------------------
                   Total timesteps: 79134720
                    Iteration time: 2.30s
                      Time elapsed: 00:30:04
                               ETA: 01:03:21

################################################################################
                     [1m Learning iteration 805/2500 [0m                      

                       Computation: 43105 steps/s (collection: 2.157s, learning 0.123s)
             Mean action noise std: 2.65
          Mean value_function loss: 156.9149
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 73.8253
                       Mean reward: 102.52
               Mean episode length: 122.13
    Episode_Reward/reaching_object: 0.8445
     Episode_Reward/lifting_object: 16.7818
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 1.2917
     Episode_Termination/robot_out: 37.1667
--------------------------------------------------------------------------------
                   Total timesteps: 79233024
                    Iteration time: 2.28s
                      Time elapsed: 00:30:06
                               ETA: 01:03:19

################################################################################
                     [1m Learning iteration 806/2500 [0m                      

                       Computation: 42882 steps/s (collection: 2.178s, learning 0.114s)
             Mean action noise std: 2.65
          Mean value_function loss: 154.9400
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 73.8274
                       Mean reward: 83.82
               Mean episode length: 104.55
    Episode_Reward/reaching_object: 0.8211
     Episode_Reward/lifting_object: 16.3246
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 35.1667
--------------------------------------------------------------------------------
                   Total timesteps: 79331328
                    Iteration time: 2.29s
                      Time elapsed: 00:30:08
                               ETA: 01:03:17

################################################################################
                     [1m Learning iteration 807/2500 [0m                      

                       Computation: 43153 steps/s (collection: 2.163s, learning 0.115s)
             Mean action noise std: 2.65
          Mean value_function loss: 138.2053
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 73.8307
                       Mean reward: 79.16
               Mean episode length: 97.22
    Episode_Reward/reaching_object: 0.8036
     Episode_Reward/lifting_object: 15.9608
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 1.2500
     Episode_Termination/robot_out: 37.5833
--------------------------------------------------------------------------------
                   Total timesteps: 79429632
                    Iteration time: 2.28s
                      Time elapsed: 00:30:11
                               ETA: 01:03:14

################################################################################
                     [1m Learning iteration 808/2500 [0m                      

                       Computation: 43048 steps/s (collection: 2.171s, learning 0.113s)
             Mean action noise std: 2.65
          Mean value_function loss: 149.2048
               Mean surrogate loss: 0.0121
                 Mean entropy loss: 73.8332
                       Mean reward: 71.67
               Mean episode length: 94.75
    Episode_Reward/reaching_object: 0.8067
     Episode_Reward/lifting_object: 16.2787
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0407
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 39.5417
--------------------------------------------------------------------------------
                   Total timesteps: 79527936
                    Iteration time: 2.28s
                      Time elapsed: 00:30:13
                               ETA: 01:03:12

################################################################################
                     [1m Learning iteration 809/2500 [0m                      

                       Computation: 43706 steps/s (collection: 2.152s, learning 0.098s)
             Mean action noise std: 2.65
          Mean value_function loss: 159.0787
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 73.8336
                       Mean reward: 95.50
               Mean episode length: 107.96
    Episode_Reward/reaching_object: 0.7987
     Episode_Reward/lifting_object: 16.2265
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 38.3333
--------------------------------------------------------------------------------
                   Total timesteps: 79626240
                    Iteration time: 2.25s
                      Time elapsed: 00:30:15
                               ETA: 01:03:10

################################################################################
                     [1m Learning iteration 810/2500 [0m                      

                       Computation: 43449 steps/s (collection: 2.156s, learning 0.107s)
             Mean action noise std: 2.65
          Mean value_function loss: 136.5764
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 73.8353
                       Mean reward: 84.53
               Mean episode length: 101.04
    Episode_Reward/reaching_object: 0.7886
     Episode_Reward/lifting_object: 16.0040
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0260
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 1.0417
     Episode_Termination/robot_out: 38.5000
--------------------------------------------------------------------------------
                   Total timesteps: 79724544
                    Iteration time: 2.26s
                      Time elapsed: 00:30:17
                               ETA: 01:03:08

################################################################################
                     [1m Learning iteration 811/2500 [0m                      

                       Computation: 42300 steps/s (collection: 2.202s, learning 0.122s)
             Mean action noise std: 2.65
          Mean value_function loss: 170.9441
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.8401
                       Mean reward: 78.40
               Mean episode length: 99.79
    Episode_Reward/reaching_object: 0.8010
     Episode_Reward/lifting_object: 16.7716
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 1.3333
     Episode_Termination/robot_out: 37.0833
--------------------------------------------------------------------------------
                   Total timesteps: 79822848
                    Iteration time: 2.32s
                      Time elapsed: 00:30:20
                               ETA: 01:03:06

################################################################################
                     [1m Learning iteration 812/2500 [0m                      

                       Computation: 42736 steps/s (collection: 2.196s, learning 0.104s)
             Mean action noise std: 2.65
          Mean value_function loss: 139.4976
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 73.8487
                       Mean reward: 83.01
               Mean episode length: 113.00
    Episode_Reward/reaching_object: 0.7918
     Episode_Reward/lifting_object: 16.4696
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 37.4583
--------------------------------------------------------------------------------
                   Total timesteps: 79921152
                    Iteration time: 2.30s
                      Time elapsed: 00:30:22
                               ETA: 01:03:04

################################################################################
                     [1m Learning iteration 813/2500 [0m                      

                       Computation: 43106 steps/s (collection: 2.168s, learning 0.113s)
             Mean action noise std: 2.65
          Mean value_function loss: 124.0719
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 73.8558
                       Mean reward: 85.23
               Mean episode length: 107.02
    Episode_Reward/reaching_object: 0.7891
     Episode_Reward/lifting_object: 16.8231
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0257
          Episode_Reward/joint_vel: -0.0390
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 36.7500
--------------------------------------------------------------------------------
                   Total timesteps: 80019456
                    Iteration time: 2.28s
                      Time elapsed: 00:30:24
                               ETA: 01:03:01

################################################################################
                     [1m Learning iteration 814/2500 [0m                      

                       Computation: 42668 steps/s (collection: 2.193s, learning 0.111s)
             Mean action noise std: 2.65
          Mean value_function loss: 149.3916
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 73.8653
                       Mean reward: 86.60
               Mean episode length: 99.33
    Episode_Reward/reaching_object: 0.8058
     Episode_Reward/lifting_object: 17.3170
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0261
          Episode_Reward/joint_vel: -0.0401
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 36.6250
--------------------------------------------------------------------------------
                   Total timesteps: 80117760
                    Iteration time: 2.30s
                      Time elapsed: 00:30:27
                               ETA: 01:02:59

################################################################################
                     [1m Learning iteration 815/2500 [0m                      

                       Computation: 43096 steps/s (collection: 2.171s, learning 0.110s)
             Mean action noise std: 2.65
          Mean value_function loss: 136.7503
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 73.8724
                       Mean reward: 101.77
               Mean episode length: 112.55
    Episode_Reward/reaching_object: 0.8333
     Episode_Reward/lifting_object: 17.6826
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 36.3750
--------------------------------------------------------------------------------
                   Total timesteps: 80216064
                    Iteration time: 2.28s
                      Time elapsed: 00:30:29
                               ETA: 01:02:57

################################################################################
                     [1m Learning iteration 816/2500 [0m                      

                       Computation: 42807 steps/s (collection: 2.196s, learning 0.100s)
             Mean action noise std: 2.65
          Mean value_function loss: 154.4450
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.8802
                       Mean reward: 95.34
               Mean episode length: 110.03
    Episode_Reward/reaching_object: 0.8325
     Episode_Reward/lifting_object: 17.6957
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 1.2083
     Episode_Termination/robot_out: 37.4167
--------------------------------------------------------------------------------
                   Total timesteps: 80314368
                    Iteration time: 2.30s
                      Time elapsed: 00:30:31
                               ETA: 01:02:55

################################################################################
                     [1m Learning iteration 817/2500 [0m                      

                       Computation: 43156 steps/s (collection: 2.174s, learning 0.104s)
             Mean action noise std: 2.66
          Mean value_function loss: 139.2971
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 73.8914
                       Mean reward: 84.01
               Mean episode length: 103.38
    Episode_Reward/reaching_object: 0.8343
     Episode_Reward/lifting_object: 18.0120
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0424
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 36.6667
--------------------------------------------------------------------------------
                   Total timesteps: 80412672
                    Iteration time: 2.28s
                      Time elapsed: 00:30:34
                               ETA: 01:02:53

################################################################################
                     [1m Learning iteration 818/2500 [0m                      

                       Computation: 42724 steps/s (collection: 2.177s, learning 0.124s)
             Mean action noise std: 2.66
          Mean value_function loss: 138.6428
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 73.8973
                       Mean reward: 88.79
               Mean episode length: 104.92
    Episode_Reward/reaching_object: 0.8118
     Episode_Reward/lifting_object: 17.6020
      Episode_Reward/object_height: 0.0016
        Episode_Reward/action_rate: -0.0262
          Episode_Reward/joint_vel: -0.0392
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 36.5000
--------------------------------------------------------------------------------
                   Total timesteps: 80510976
                    Iteration time: 2.30s
                      Time elapsed: 00:30:36
                               ETA: 01:02:51

################################################################################
                     [1m Learning iteration 819/2500 [0m                      

                       Computation: 43230 steps/s (collection: 2.163s, learning 0.111s)
             Mean action noise std: 2.66
          Mean value_function loss: 146.6099
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 73.9054
                       Mean reward: 105.00
               Mean episode length: 113.46
    Episode_Reward/reaching_object: 0.8430
     Episode_Reward/lifting_object: 18.7567
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 35.4167
--------------------------------------------------------------------------------
                   Total timesteps: 80609280
                    Iteration time: 2.27s
                      Time elapsed: 00:30:38
                               ETA: 01:02:49

################################################################################
                     [1m Learning iteration 820/2500 [0m                      

                       Computation: 43034 steps/s (collection: 2.173s, learning 0.112s)
             Mean action noise std: 2.66
          Mean value_function loss: 154.2992
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 73.9135
                       Mean reward: 101.97
               Mean episode length: 114.04
    Episode_Reward/reaching_object: 0.8602
     Episode_Reward/lifting_object: 18.7695
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 35.7083
--------------------------------------------------------------------------------
                   Total timesteps: 80707584
                    Iteration time: 2.28s
                      Time elapsed: 00:30:40
                               ETA: 01:02:46

################################################################################
                     [1m Learning iteration 821/2500 [0m                      

                       Computation: 43347 steps/s (collection: 2.165s, learning 0.103s)
             Mean action noise std: 2.66
          Mean value_function loss: 128.5443
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 73.9212
                       Mean reward: 104.72
               Mean episode length: 112.83
    Episode_Reward/reaching_object: 0.8504
     Episode_Reward/lifting_object: 18.5477
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0419
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 39.0833
--------------------------------------------------------------------------------
                   Total timesteps: 80805888
                    Iteration time: 2.27s
                      Time elapsed: 00:30:43
                               ETA: 01:02:44

################################################################################
                     [1m Learning iteration 822/2500 [0m                      

                       Computation: 43596 steps/s (collection: 2.161s, learning 0.094s)
             Mean action noise std: 2.66
          Mean value_function loss: 162.2471
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 73.9276
                       Mean reward: 94.87
               Mean episode length: 107.97
    Episode_Reward/reaching_object: 0.8492
     Episode_Reward/lifting_object: 18.6273
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0411
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 36.7500
--------------------------------------------------------------------------------
                   Total timesteps: 80904192
                    Iteration time: 2.25s
                      Time elapsed: 00:30:45
                               ETA: 01:02:42

################################################################################
                     [1m Learning iteration 823/2500 [0m                      

                       Computation: 43164 steps/s (collection: 2.184s, learning 0.093s)
             Mean action noise std: 2.66
          Mean value_function loss: 174.8071
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 73.9312
                       Mean reward: 91.91
               Mean episode length: 103.22
    Episode_Reward/reaching_object: 0.8076
     Episode_Reward/lifting_object: 17.9589
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0263
          Episode_Reward/joint_vel: -0.0400
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 37.2917
--------------------------------------------------------------------------------
                   Total timesteps: 81002496
                    Iteration time: 2.28s
                      Time elapsed: 00:30:47
                               ETA: 01:02:40

################################################################################
                     [1m Learning iteration 824/2500 [0m                      

                       Computation: 43478 steps/s (collection: 2.148s, learning 0.113s)
             Mean action noise std: 2.66
          Mean value_function loss: 145.6128
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 73.9359
                       Mean reward: 98.84
               Mean episode length: 108.63
    Episode_Reward/reaching_object: 0.8418
     Episode_Reward/lifting_object: 18.8550
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0414
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 38.1667
--------------------------------------------------------------------------------
                   Total timesteps: 81100800
                    Iteration time: 2.26s
                      Time elapsed: 00:30:49
                               ETA: 01:02:38

################################################################################
                     [1m Learning iteration 825/2500 [0m                      

                       Computation: 43559 steps/s (collection: 2.152s, learning 0.105s)
             Mean action noise std: 2.66
          Mean value_function loss: 154.1448
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 73.9409
                       Mean reward: 87.55
               Mean episode length: 101.19
    Episode_Reward/reaching_object: 0.8053
     Episode_Reward/lifting_object: 18.3225
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0265
          Episode_Reward/joint_vel: -0.0406
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 36.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81199104
                    Iteration time: 2.26s
                      Time elapsed: 00:30:52
                               ETA: 01:02:35

################################################################################
                     [1m Learning iteration 826/2500 [0m                      

                       Computation: 43490 steps/s (collection: 2.147s, learning 0.113s)
             Mean action noise std: 2.66
          Mean value_function loss: 147.7282
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 73.9454
                       Mean reward: 113.78
               Mean episode length: 120.45
    Episode_Reward/reaching_object: 0.8366
     Episode_Reward/lifting_object: 18.7076
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 36.8333
--------------------------------------------------------------------------------
                   Total timesteps: 81297408
                    Iteration time: 2.26s
                      Time elapsed: 00:30:54
                               ETA: 01:02:33

################################################################################
                     [1m Learning iteration 827/2500 [0m                      

                       Computation: 43622 steps/s (collection: 2.144s, learning 0.109s)
             Mean action noise std: 2.66
          Mean value_function loss: 127.7601
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 73.9492
                       Mean reward: 92.75
               Mean episode length: 104.99
    Episode_Reward/reaching_object: 0.8239
     Episode_Reward/lifting_object: 18.8939
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 37.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81395712
                    Iteration time: 2.25s
                      Time elapsed: 00:30:56
                               ETA: 01:02:31

################################################################################
                     [1m Learning iteration 828/2500 [0m                      

                       Computation: 42320 steps/s (collection: 2.204s, learning 0.119s)
             Mean action noise std: 2.66
          Mean value_function loss: 138.4111
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 73.9523
                       Mean reward: 91.68
               Mean episode length: 103.99
    Episode_Reward/reaching_object: 0.8172
     Episode_Reward/lifting_object: 18.4691
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 38.2083
--------------------------------------------------------------------------------
                   Total timesteps: 81494016
                    Iteration time: 2.32s
                      Time elapsed: 00:30:59
                               ETA: 01:02:29

################################################################################
                     [1m Learning iteration 829/2500 [0m                      

                       Computation: 42890 steps/s (collection: 2.180s, learning 0.112s)
             Mean action noise std: 2.66
          Mean value_function loss: 127.2458
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 73.9531
                       Mean reward: 110.69
               Mean episode length: 106.46
    Episode_Reward/reaching_object: 0.8232
     Episode_Reward/lifting_object: 19.5778
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0405
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 37.4583
--------------------------------------------------------------------------------
                   Total timesteps: 81592320
                    Iteration time: 2.29s
                      Time elapsed: 00:31:01
                               ETA: 01:02:27

################################################################################
                     [1m Learning iteration 830/2500 [0m                      

                       Computation: 42630 steps/s (collection: 2.175s, learning 0.131s)
             Mean action noise std: 2.66
          Mean value_function loss: 138.8733
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 73.9539
                       Mean reward: 100.67
               Mean episode length: 107.66
    Episode_Reward/reaching_object: 0.8283
     Episode_Reward/lifting_object: 19.2117
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0267
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 38.1250
--------------------------------------------------------------------------------
                   Total timesteps: 81690624
                    Iteration time: 2.31s
                      Time elapsed: 00:31:03
                               ETA: 01:02:25

################################################################################
                     [1m Learning iteration 831/2500 [0m                      

                       Computation: 43347 steps/s (collection: 2.170s, learning 0.098s)
             Mean action noise std: 2.66
          Mean value_function loss: 162.4904
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 73.9567
                       Mean reward: 107.12
               Mean episode length: 105.67
    Episode_Reward/reaching_object: 0.8404
     Episode_Reward/lifting_object: 20.0397
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0417
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 35.7917
--------------------------------------------------------------------------------
                   Total timesteps: 81788928
                    Iteration time: 2.27s
                      Time elapsed: 00:31:05
                               ETA: 01:02:22

################################################################################
                     [1m Learning iteration 832/2500 [0m                      

                       Computation: 43135 steps/s (collection: 2.171s, learning 0.108s)
             Mean action noise std: 2.66
          Mean value_function loss: 149.5352
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 73.9609
                       Mean reward: 99.13
               Mean episode length: 106.67
    Episode_Reward/reaching_object: 0.8311
     Episode_Reward/lifting_object: 18.9004
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 39.3750
--------------------------------------------------------------------------------
                   Total timesteps: 81887232
                    Iteration time: 2.28s
                      Time elapsed: 00:31:08
                               ETA: 01:02:20

################################################################################
                     [1m Learning iteration 833/2500 [0m                      

                       Computation: 43018 steps/s (collection: 2.163s, learning 0.122s)
             Mean action noise std: 2.66
          Mean value_function loss: 155.7951
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 73.9648
                       Mean reward: 112.02
               Mean episode length: 109.47
    Episode_Reward/reaching_object: 0.8331
     Episode_Reward/lifting_object: 19.4150
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 35.4583
--------------------------------------------------------------------------------
                   Total timesteps: 81985536
                    Iteration time: 2.29s
                      Time elapsed: 00:31:10
                               ETA: 01:02:18

################################################################################
                     [1m Learning iteration 834/2500 [0m                      

                       Computation: 42745 steps/s (collection: 2.189s, learning 0.111s)
             Mean action noise std: 2.66
          Mean value_function loss: 137.9468
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 73.9677
                       Mean reward: 109.68
               Mean episode length: 109.22
    Episode_Reward/reaching_object: 0.8453
     Episode_Reward/lifting_object: 20.1333
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0412
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 37.9167
--------------------------------------------------------------------------------
                   Total timesteps: 82083840
                    Iteration time: 2.30s
                      Time elapsed: 00:31:12
                               ETA: 01:02:16

################################################################################
                     [1m Learning iteration 835/2500 [0m                      

                       Computation: 43481 steps/s (collection: 2.167s, learning 0.094s)
             Mean action noise std: 2.66
          Mean value_function loss: 138.1766
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 73.9721
                       Mean reward: 100.64
               Mean episode length: 109.52
    Episode_Reward/reaching_object: 0.8464
     Episode_Reward/lifting_object: 20.1644
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0413
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 37.7083
--------------------------------------------------------------------------------
                   Total timesteps: 82182144
                    Iteration time: 2.26s
                      Time elapsed: 00:31:15
                               ETA: 01:02:14

################################################################################
                     [1m Learning iteration 836/2500 [0m                      

                       Computation: 42345 steps/s (collection: 2.198s, learning 0.124s)
             Mean action noise std: 2.66
          Mean value_function loss: 158.8753
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 73.9773
                       Mean reward: 113.35
               Mean episode length: 108.80
    Episode_Reward/reaching_object: 0.8336
     Episode_Reward/lifting_object: 20.2276
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0408
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 36.7500
--------------------------------------------------------------------------------
                   Total timesteps: 82280448
                    Iteration time: 2.32s
                      Time elapsed: 00:31:17
                               ETA: 01:02:12

################################################################################
                     [1m Learning iteration 837/2500 [0m                      

                       Computation: 42240 steps/s (collection: 2.208s, learning 0.120s)
             Mean action noise std: 2.66
          Mean value_function loss: 134.7061
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 73.9820
                       Mean reward: 107.57
               Mean episode length: 108.33
    Episode_Reward/reaching_object: 0.8339
     Episode_Reward/lifting_object: 19.6153
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0409
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 36.1667
--------------------------------------------------------------------------------
                   Total timesteps: 82378752
                    Iteration time: 2.33s
                      Time elapsed: 00:31:19
                               ETA: 01:02:10

################################################################################
                     [1m Learning iteration 838/2500 [0m                      

                       Computation: 42706 steps/s (collection: 2.186s, learning 0.116s)
             Mean action noise std: 2.66
          Mean value_function loss: 165.7090
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 73.9852
                       Mean reward: 112.01
               Mean episode length: 112.92
    Episode_Reward/reaching_object: 0.8473
     Episode_Reward/lifting_object: 20.2245
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0416
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 37.8750
--------------------------------------------------------------------------------
                   Total timesteps: 82477056
                    Iteration time: 2.30s
                      Time elapsed: 00:31:21
                               ETA: 01:02:08

################################################################################
                     [1m Learning iteration 839/2500 [0m                      

                       Computation: 41747 steps/s (collection: 2.241s, learning 0.114s)
             Mean action noise std: 2.66
          Mean value_function loss: 142.2158
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 73.9896
                       Mean reward: 107.62
               Mean episode length: 113.00
    Episode_Reward/reaching_object: 0.8384
     Episode_Reward/lifting_object: 19.4922
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0410
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 37.0417
--------------------------------------------------------------------------------
                   Total timesteps: 82575360
                    Iteration time: 2.35s
                      Time elapsed: 00:31:24
                               ETA: 01:02:06

################################################################################
                     [1m Learning iteration 840/2500 [0m                      

                       Computation: 42693 steps/s (collection: 2.204s, learning 0.098s)
             Mean action noise std: 2.66
          Mean value_function loss: 173.1574
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 73.9916
                       Mean reward: 94.71
               Mean episode length: 100.66
    Episode_Reward/reaching_object: 0.8355
     Episode_Reward/lifting_object: 19.7093
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0422
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 37.9167
--------------------------------------------------------------------------------
                   Total timesteps: 82673664
                    Iteration time: 2.30s
                      Time elapsed: 00:31:26
                               ETA: 01:02:03

################################################################################
                     [1m Learning iteration 841/2500 [0m                      

                       Computation: 42433 steps/s (collection: 2.222s, learning 0.095s)
             Mean action noise std: 2.67
          Mean value_function loss: 163.5344
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 73.9952
                       Mean reward: 104.82
               Mean episode length: 107.68
    Episode_Reward/reaching_object: 0.8497
     Episode_Reward/lifting_object: 20.0197
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0271
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 38.7083
--------------------------------------------------------------------------------
                   Total timesteps: 82771968
                    Iteration time: 2.32s
                      Time elapsed: 00:31:28
                               ETA: 01:02:01

################################################################################
                     [1m Learning iteration 842/2500 [0m                      

                       Computation: 42137 steps/s (collection: 2.231s, learning 0.102s)
             Mean action noise std: 2.67
          Mean value_function loss: 159.8725
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.0009
                       Mean reward: 96.25
               Mean episode length: 98.20
    Episode_Reward/reaching_object: 0.8418
     Episode_Reward/lifting_object: 20.1957
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.8333
     Episode_Termination/robot_out: 39.4583
--------------------------------------------------------------------------------
                   Total timesteps: 82870272
                    Iteration time: 2.33s
                      Time elapsed: 00:31:31
                               ETA: 01:01:59

################################################################################
                     [1m Learning iteration 843/2500 [0m                      

                       Computation: 42110 steps/s (collection: 2.241s, learning 0.093s)
             Mean action noise std: 2.67
          Mean value_function loss: 142.0074
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 74.0074
                       Mean reward: 100.42
               Mean episode length: 99.30
    Episode_Reward/reaching_object: 0.8379
     Episode_Reward/lifting_object: 19.8203
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0428
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 35.2917
--------------------------------------------------------------------------------
                   Total timesteps: 82968576
                    Iteration time: 2.33s
                      Time elapsed: 00:31:33
                               ETA: 01:01:57

################################################################################
                     [1m Learning iteration 844/2500 [0m                      

                       Computation: 42171 steps/s (collection: 2.239s, learning 0.092s)
             Mean action noise std: 2.67
          Mean value_function loss: 159.7625
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 74.0119
                       Mean reward: 103.57
               Mean episode length: 107.92
    Episode_Reward/reaching_object: 0.8662
     Episode_Reward/lifting_object: 20.5891
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0276
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 37.5000
--------------------------------------------------------------------------------
                   Total timesteps: 83066880
                    Iteration time: 2.33s
                      Time elapsed: 00:31:35
                               ETA: 01:01:55

################################################################################
                     [1m Learning iteration 845/2500 [0m                      

                       Computation: 42209 steps/s (collection: 2.232s, learning 0.097s)
             Mean action noise std: 2.67
          Mean value_function loss: 174.1817
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 74.0154
                       Mean reward: 116.00
               Mean episode length: 110.97
    Episode_Reward/reaching_object: 0.8376
     Episode_Reward/lifting_object: 20.1633
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0433
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 36.8750
--------------------------------------------------------------------------------
                   Total timesteps: 83165184
                    Iteration time: 2.33s
                      Time elapsed: 00:31:38
                               ETA: 01:01:53

################################################################################
                     [1m Learning iteration 846/2500 [0m                      

                       Computation: 41511 steps/s (collection: 2.275s, learning 0.093s)
             Mean action noise std: 2.67
          Mean value_function loss: 150.8519
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 74.0211
                       Mean reward: 104.98
               Mean episode length: 95.47
    Episode_Reward/reaching_object: 0.8269
     Episode_Reward/lifting_object: 20.3110
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0266
          Episode_Reward/joint_vel: -0.0425
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 40.0833
--------------------------------------------------------------------------------
                   Total timesteps: 83263488
                    Iteration time: 2.37s
                      Time elapsed: 00:31:40
                               ETA: 01:01:51

################################################################################
                     [1m Learning iteration 847/2500 [0m                      

                       Computation: 42208 steps/s (collection: 2.220s, learning 0.109s)
             Mean action noise std: 2.67
          Mean value_function loss: 155.2203
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 74.0265
                       Mean reward: 102.21
               Mean episode length: 101.87
    Episode_Reward/reaching_object: 0.8415
     Episode_Reward/lifting_object: 20.6806
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0270
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 38.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83361792
                    Iteration time: 2.33s
                      Time elapsed: 00:31:42
                               ETA: 01:01:49

################################################################################
                     [1m Learning iteration 848/2500 [0m                      

                       Computation: 42014 steps/s (collection: 2.245s, learning 0.095s)
             Mean action noise std: 2.67
          Mean value_function loss: 155.1836
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 74.0313
                       Mean reward: 107.62
               Mean episode length: 100.08
    Episode_Reward/reaching_object: 0.8345
     Episode_Reward/lifting_object: 20.6209
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0268
          Episode_Reward/joint_vel: -0.0436
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 35.8750
--------------------------------------------------------------------------------
                   Total timesteps: 83460096
                    Iteration time: 2.34s
                      Time elapsed: 00:31:45
                               ETA: 01:01:47

################################################################################
                     [1m Learning iteration 849/2500 [0m                      

                       Computation: 37613 steps/s (collection: 2.448s, learning 0.166s)
             Mean action noise std: 2.67
          Mean value_function loss: 163.5343
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 74.0356
                       Mean reward: 96.25
               Mean episode length: 97.65
    Episode_Reward/reaching_object: 0.8368
     Episode_Reward/lifting_object: 21.1947
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0269
          Episode_Reward/joint_vel: -0.0429
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 37.1667
--------------------------------------------------------------------------------
                   Total timesteps: 83558400
                    Iteration time: 2.61s
                      Time elapsed: 00:31:47
                               ETA: 01:01:45

################################################################################
                     [1m Learning iteration 850/2500 [0m                      

                       Computation: 38071 steps/s (collection: 2.457s, learning 0.125s)
             Mean action noise std: 2.67
          Mean value_function loss: 141.4550
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 74.0377
                       Mean reward: 105.57
               Mean episode length: 105.31
    Episode_Reward/reaching_object: 0.8735
     Episode_Reward/lifting_object: 21.4571
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0278
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 35.5833
--------------------------------------------------------------------------------
                   Total timesteps: 83656704
                    Iteration time: 2.58s
                      Time elapsed: 00:31:50
                               ETA: 01:01:44

################################################################################
                     [1m Learning iteration 851/2500 [0m                      

                       Computation: 39174 steps/s (collection: 2.418s, learning 0.092s)
             Mean action noise std: 2.67
          Mean value_function loss: 174.1704
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 74.0383
                       Mean reward: 126.44
               Mean episode length: 117.99
    Episode_Reward/reaching_object: 0.8744
     Episode_Reward/lifting_object: 22.0882
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 37.5000
--------------------------------------------------------------------------------
                   Total timesteps: 83755008
                    Iteration time: 2.51s
                      Time elapsed: 00:31:53
                               ETA: 01:01:42

################################################################################
                     [1m Learning iteration 852/2500 [0m                      

                       Computation: 38923 steps/s (collection: 2.356s, learning 0.170s)
             Mean action noise std: 2.67
          Mean value_function loss: 156.2424
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 74.0392
                       Mean reward: 107.72
               Mean episode length: 103.74
    Episode_Reward/reaching_object: 0.8777
     Episode_Reward/lifting_object: 21.7290
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 37.3333
--------------------------------------------------------------------------------
                   Total timesteps: 83853312
                    Iteration time: 2.53s
                      Time elapsed: 00:31:55
                               ETA: 01:01:40

################################################################################
                     [1m Learning iteration 853/2500 [0m                      

                       Computation: 39185 steps/s (collection: 2.397s, learning 0.111s)
             Mean action noise std: 2.67
          Mean value_function loss: 172.5838
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 74.0414
                       Mean reward: 98.87
               Mean episode length: 106.13
    Episode_Reward/reaching_object: 0.8688
     Episode_Reward/lifting_object: 21.2003
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 38.6667
--------------------------------------------------------------------------------
                   Total timesteps: 83951616
                    Iteration time: 2.51s
                      Time elapsed: 00:31:58
                               ETA: 01:01:39

################################################################################
                     [1m Learning iteration 854/2500 [0m                      

                       Computation: 39596 steps/s (collection: 2.348s, learning 0.135s)
             Mean action noise std: 2.67
          Mean value_function loss: 177.3733
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 74.0442
                       Mean reward: 97.41
               Mean episode length: 105.67
    Episode_Reward/reaching_object: 0.8743
     Episode_Reward/lifting_object: 20.4053
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0495
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 1.0000
     Episode_Termination/robot_out: 36.4583
--------------------------------------------------------------------------------
                   Total timesteps: 84049920
                    Iteration time: 2.48s
                      Time elapsed: 00:32:00
                               ETA: 01:01:37

################################################################################
                     [1m Learning iteration 855/2500 [0m                      

                       Computation: 39511 steps/s (collection: 2.317s, learning 0.171s)
             Mean action noise std: 2.67
          Mean value_function loss: 165.1496
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.0478
                       Mean reward: 100.72
               Mean episode length: 104.20
    Episode_Reward/reaching_object: 0.8339
     Episode_Reward/lifting_object: 19.3791
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 35.2500
--------------------------------------------------------------------------------
                   Total timesteps: 84148224
                    Iteration time: 2.49s
                      Time elapsed: 00:32:03
                               ETA: 01:01:35

################################################################################
                     [1m Learning iteration 856/2500 [0m                      

                       Computation: 38834 steps/s (collection: 2.402s, learning 0.129s)
             Mean action noise std: 2.67
          Mean value_function loss: 167.5364
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 74.0523
                       Mean reward: 105.05
               Mean episode length: 104.97
    Episode_Reward/reaching_object: 0.8481
     Episode_Reward/lifting_object: 19.8891
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0465
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 35.6250
--------------------------------------------------------------------------------
                   Total timesteps: 84246528
                    Iteration time: 2.53s
                      Time elapsed: 00:32:05
                               ETA: 01:01:33

################################################################################
                     [1m Learning iteration 857/2500 [0m                      

                       Computation: 39531 steps/s (collection: 2.321s, learning 0.166s)
             Mean action noise std: 2.67
          Mean value_function loss: 165.1268
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 74.0542
                       Mean reward: 113.72
               Mean episode length: 109.20
    Episode_Reward/reaching_object: 0.9056
     Episode_Reward/lifting_object: 21.4806
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 37.8333
--------------------------------------------------------------------------------
                   Total timesteps: 84344832
                    Iteration time: 2.49s
                      Time elapsed: 00:32:08
                               ETA: 01:01:32

################################################################################
                     [1m Learning iteration 858/2500 [0m                      

                       Computation: 40301 steps/s (collection: 2.309s, learning 0.131s)
             Mean action noise std: 2.67
          Mean value_function loss: 172.4070
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 74.0552
                       Mean reward: 105.64
               Mean episode length: 107.84
    Episode_Reward/reaching_object: 0.8810
     Episode_Reward/lifting_object: 20.5658
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 38.1667
--------------------------------------------------------------------------------
                   Total timesteps: 84443136
                    Iteration time: 2.44s
                      Time elapsed: 00:32:10
                               ETA: 01:01:30

################################################################################
                     [1m Learning iteration 859/2500 [0m                      

                       Computation: 39838 steps/s (collection: 2.336s, learning 0.132s)
             Mean action noise std: 2.67
          Mean value_function loss: 162.9101
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.0593
                       Mean reward: 119.18
               Mean episode length: 116.18
    Episode_Reward/reaching_object: 0.8846
     Episode_Reward/lifting_object: 21.1317
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 38.0833
--------------------------------------------------------------------------------
                   Total timesteps: 84541440
                    Iteration time: 2.47s
                      Time elapsed: 00:32:12
                               ETA: 01:01:28

################################################################################
                     [1m Learning iteration 860/2500 [0m                      

                       Computation: 38678 steps/s (collection: 2.416s, learning 0.126s)
             Mean action noise std: 2.67
          Mean value_function loss: 162.0022
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 74.0650
                       Mean reward: 109.55
               Mean episode length: 105.83
    Episode_Reward/reaching_object: 0.8630
     Episode_Reward/lifting_object: 20.9958
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 36.0833
--------------------------------------------------------------------------------
                   Total timesteps: 84639744
                    Iteration time: 2.54s
                      Time elapsed: 00:32:15
                               ETA: 01:01:26

################################################################################
                     [1m Learning iteration 861/2500 [0m                      

                       Computation: 39134 steps/s (collection: 2.376s, learning 0.136s)
             Mean action noise std: 2.67
          Mean value_function loss: 176.3824
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 74.0709
                       Mean reward: 120.65
               Mean episode length: 110.91
    Episode_Reward/reaching_object: 0.8816
     Episode_Reward/lifting_object: 21.5283
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0484
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.9583
     Episode_Termination/robot_out: 37.9583
--------------------------------------------------------------------------------
                   Total timesteps: 84738048
                    Iteration time: 2.51s
                      Time elapsed: 00:32:17
                               ETA: 01:01:24

################################################################################
                     [1m Learning iteration 862/2500 [0m                      

                       Computation: 40588 steps/s (collection: 2.301s, learning 0.121s)
             Mean action noise std: 2.67
          Mean value_function loss: 170.1214
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 74.0778
                       Mean reward: 109.69
               Mean episode length: 112.38
    Episode_Reward/reaching_object: 0.8963
     Episode_Reward/lifting_object: 22.2741
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0483
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 1.1250
     Episode_Termination/robot_out: 37.7083
--------------------------------------------------------------------------------
                   Total timesteps: 84836352
                    Iteration time: 2.42s
                      Time elapsed: 00:32:20
                               ETA: 01:01:22

################################################################################
                     [1m Learning iteration 863/2500 [0m                      

                       Computation: 40209 steps/s (collection: 2.299s, learning 0.146s)
             Mean action noise std: 2.67
          Mean value_function loss: 162.4884
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 74.0819
                       Mean reward: 115.29
               Mean episode length: 110.49
    Episode_Reward/reaching_object: 0.8855
     Episode_Reward/lifting_object: 22.2572
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0485
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 34.7500
--------------------------------------------------------------------------------
                   Total timesteps: 84934656
                    Iteration time: 2.44s
                      Time elapsed: 00:32:22
                               ETA: 01:01:21

################################################################################
                     [1m Learning iteration 864/2500 [0m                      

                       Computation: 40508 steps/s (collection: 2.325s, learning 0.102s)
             Mean action noise std: 2.67
          Mean value_function loss: 177.9248
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 74.0879
                       Mean reward: 121.10
               Mean episode length: 101.90
    Episode_Reward/reaching_object: 0.8889
     Episode_Reward/lifting_object: 23.2128
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 36.9167
--------------------------------------------------------------------------------
                   Total timesteps: 85032960
                    Iteration time: 2.43s
                      Time elapsed: 00:32:25
                               ETA: 01:01:19

################################################################################
                     [1m Learning iteration 865/2500 [0m                      

                       Computation: 40794 steps/s (collection: 2.310s, learning 0.100s)
             Mean action noise std: 2.67
          Mean value_function loss: 198.0500
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 74.0924
                       Mean reward: 114.52
               Mean episode length: 110.45
    Episode_Reward/reaching_object: 0.8601
     Episode_Reward/lifting_object: 21.7712
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 38.9583
--------------------------------------------------------------------------------
                   Total timesteps: 85131264
                    Iteration time: 2.41s
                      Time elapsed: 00:32:27
                               ETA: 01:01:17

################################################################################
                     [1m Learning iteration 866/2500 [0m                      

                       Computation: 40253 steps/s (collection: 2.339s, learning 0.103s)
             Mean action noise std: 2.68
          Mean value_function loss: 158.2751
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 74.0959
                       Mean reward: 124.36
               Mean episode length: 104.64
    Episode_Reward/reaching_object: 0.8621
     Episode_Reward/lifting_object: 22.0930
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0466
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 38.1667
--------------------------------------------------------------------------------
                   Total timesteps: 85229568
                    Iteration time: 2.44s
                      Time elapsed: 00:32:30
                               ETA: 01:01:15

################################################################################
                     [1m Learning iteration 867/2500 [0m                      

                       Computation: 41587 steps/s (collection: 2.267s, learning 0.097s)
             Mean action noise std: 2.68
          Mean value_function loss: 174.7827
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 74.0995
                       Mean reward: 112.48
               Mean episode length: 100.52
    Episode_Reward/reaching_object: 0.8773
     Episode_Reward/lifting_object: 22.2782
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0476
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 37.6667
--------------------------------------------------------------------------------
                   Total timesteps: 85327872
                    Iteration time: 2.36s
                      Time elapsed: 00:32:32
                               ETA: 01:01:13

################################################################################
                     [1m Learning iteration 868/2500 [0m                      

                       Computation: 38284 steps/s (collection: 2.449s, learning 0.119s)
             Mean action noise std: 2.68
          Mean value_function loss: 174.4948
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 74.1011
                       Mean reward: 131.00
               Mean episode length: 112.39
    Episode_Reward/reaching_object: 0.8768
     Episode_Reward/lifting_object: 23.2796
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 35.3750
--------------------------------------------------------------------------------
                   Total timesteps: 85426176
                    Iteration time: 2.57s
                      Time elapsed: 00:32:35
                               ETA: 01:01:11

################################################################################
                     [1m Learning iteration 869/2500 [0m                      

                       Computation: 40229 steps/s (collection: 2.314s, learning 0.130s)
             Mean action noise std: 2.68
          Mean value_function loss: 179.8553
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 74.1045
                       Mean reward: 108.54
               Mean episode length: 99.87
    Episode_Reward/reaching_object: 0.8513
     Episode_Reward/lifting_object: 22.3854
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 38.7083
--------------------------------------------------------------------------------
                   Total timesteps: 85524480
                    Iteration time: 2.44s
                      Time elapsed: 00:32:37
                               ETA: 01:01:09

################################################################################
                     [1m Learning iteration 870/2500 [0m                      

                       Computation: 39128 steps/s (collection: 2.365s, learning 0.147s)
             Mean action noise std: 2.68
          Mean value_function loss: 165.0076
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 74.1081
                       Mean reward: 124.72
               Mean episode length: 107.53
    Episode_Reward/reaching_object: 0.8799
     Episode_Reward/lifting_object: 23.4858
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0475
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 38.2500
--------------------------------------------------------------------------------
                   Total timesteps: 85622784
                    Iteration time: 2.51s
                      Time elapsed: 00:32:40
                               ETA: 01:01:08

################################################################################
                     [1m Learning iteration 871/2500 [0m                      

                       Computation: 41491 steps/s (collection: 2.255s, learning 0.115s)
             Mean action noise std: 2.68
          Mean value_function loss: 155.2936
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.1100
                       Mean reward: 97.41
               Mean episode length: 94.99
    Episode_Reward/reaching_object: 0.8587
     Episode_Reward/lifting_object: 22.2326
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 1.0833
     Episode_Termination/robot_out: 39.2500
--------------------------------------------------------------------------------
                   Total timesteps: 85721088
                    Iteration time: 2.37s
                      Time elapsed: 00:32:42
                               ETA: 01:01:05

################################################################################
                     [1m Learning iteration 872/2500 [0m                      

                       Computation: 38216 steps/s (collection: 2.418s, learning 0.155s)
             Mean action noise std: 2.68
          Mean value_function loss: 177.4229
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 74.1131
                       Mean reward: 123.86
               Mean episode length: 109.85
    Episode_Reward/reaching_object: 0.8397
     Episode_Reward/lifting_object: 22.1206
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0277
          Episode_Reward/joint_vel: -0.0450
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 37.3333
--------------------------------------------------------------------------------
                   Total timesteps: 85819392
                    Iteration time: 2.57s
                      Time elapsed: 00:32:44
                               ETA: 01:01:04

################################################################################
                     [1m Learning iteration 873/2500 [0m                      

                       Computation: 40521 steps/s (collection: 2.332s, learning 0.094s)
             Mean action noise std: 2.68
          Mean value_function loss: 158.5346
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 74.1162
                       Mean reward: 104.43
               Mean episode length: 96.29
    Episode_Reward/reaching_object: 0.8501
     Episode_Reward/lifting_object: 22.5941
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 37.6250
--------------------------------------------------------------------------------
                   Total timesteps: 85917696
                    Iteration time: 2.43s
                      Time elapsed: 00:32:47
                               ETA: 01:01:02

################################################################################
                     [1m Learning iteration 874/2500 [0m                      

                       Computation: 38293 steps/s (collection: 2.440s, learning 0.127s)
             Mean action noise std: 2.68
          Mean value_function loss: 166.0125
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 74.1204
                       Mean reward: 118.51
               Mean episode length: 103.18
    Episode_Reward/reaching_object: 0.8270
     Episode_Reward/lifting_object: 22.3592
      Episode_Reward/object_height: 0.0017
        Episode_Reward/action_rate: -0.0272
          Episode_Reward/joint_vel: -0.0442
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 38.4167
--------------------------------------------------------------------------------
                   Total timesteps: 86016000
                    Iteration time: 2.57s
                      Time elapsed: 00:32:49
                               ETA: 01:01:00

################################################################################
                     [1m Learning iteration 875/2500 [0m                      

                       Computation: 39920 steps/s (collection: 2.345s, learning 0.118s)
             Mean action noise std: 2.68
          Mean value_function loss: 172.3595
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 74.1243
                       Mean reward: 109.50
               Mean episode length: 97.47
    Episode_Reward/reaching_object: 0.8357
     Episode_Reward/lifting_object: 22.4864
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0273
          Episode_Reward/joint_vel: -0.0437
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 36.1667
--------------------------------------------------------------------------------
                   Total timesteps: 86114304
                    Iteration time: 2.46s
                      Time elapsed: 00:32:52
                               ETA: 01:00:58

################################################################################
                     [1m Learning iteration 876/2500 [0m                      

                       Computation: 40853 steps/s (collection: 2.280s, learning 0.127s)
             Mean action noise std: 2.68
          Mean value_function loss: 168.9030
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 74.1268
                       Mean reward: 114.55
               Mean episode length: 99.41
    Episode_Reward/reaching_object: 0.8539
     Episode_Reward/lifting_object: 23.5029
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 38.3750
--------------------------------------------------------------------------------
                   Total timesteps: 86212608
                    Iteration time: 2.41s
                      Time elapsed: 00:32:54
                               ETA: 01:00:56

################################################################################
                     [1m Learning iteration 877/2500 [0m                      

                       Computation: 41858 steps/s (collection: 2.258s, learning 0.090s)
             Mean action noise std: 2.68
          Mean value_function loss: 166.4595
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 74.1312
                       Mean reward: 122.51
               Mean episode length: 105.88
    Episode_Reward/reaching_object: 0.8564
     Episode_Reward/lifting_object: 23.2838
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0444
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 38.0833
--------------------------------------------------------------------------------
                   Total timesteps: 86310912
                    Iteration time: 2.35s
                      Time elapsed: 00:32:57
                               ETA: 01:00:54

################################################################################
                     [1m Learning iteration 878/2500 [0m                      

                       Computation: 38780 steps/s (collection: 2.356s, learning 0.179s)
             Mean action noise std: 2.68
          Mean value_function loss: 158.7146
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.1330
                       Mean reward: 131.61
               Mean episode length: 113.85
    Episode_Reward/reaching_object: 0.8490
     Episode_Reward/lifting_object: 22.7860
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0447
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 37.7500
--------------------------------------------------------------------------------
                   Total timesteps: 86409216
                    Iteration time: 2.53s
                      Time elapsed: 00:32:59
                               ETA: 01:00:53

################################################################################
                     [1m Learning iteration 879/2500 [0m                      

                       Computation: 39574 steps/s (collection: 2.354s, learning 0.130s)
             Mean action noise std: 2.68
          Mean value_function loss: 173.8963
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 74.1360
                       Mean reward: 120.30
               Mean episode length: 108.25
    Episode_Reward/reaching_object: 0.8721
     Episode_Reward/lifting_object: 23.6692
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 37.5417
--------------------------------------------------------------------------------
                   Total timesteps: 86507520
                    Iteration time: 2.48s
                      Time elapsed: 00:33:02
                               ETA: 01:00:51

################################################################################
                     [1m Learning iteration 880/2500 [0m                      

                       Computation: 40559 steps/s (collection: 2.297s, learning 0.127s)
             Mean action noise std: 2.68
          Mean value_function loss: 160.8738
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 74.1377
                       Mean reward: 126.35
               Mean episode length: 106.42
    Episode_Reward/reaching_object: 0.8634
     Episode_Reward/lifting_object: 23.8715
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0443
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.9167
     Episode_Termination/robot_out: 35.4583
--------------------------------------------------------------------------------
                   Total timesteps: 86605824
                    Iteration time: 2.42s
                      Time elapsed: 00:33:04
                               ETA: 01:00:49

################################################################################
                     [1m Learning iteration 881/2500 [0m                      

                       Computation: 41267 steps/s (collection: 2.253s, learning 0.129s)
             Mean action noise std: 2.68
          Mean value_function loss: 163.4824
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 74.1388
                       Mean reward: 131.14
               Mean episode length: 109.12
    Episode_Reward/reaching_object: 0.8524
     Episode_Reward/lifting_object: 23.9250
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0280
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 39.0000
--------------------------------------------------------------------------------
                   Total timesteps: 86704128
                    Iteration time: 2.38s
                      Time elapsed: 00:33:06
                               ETA: 01:00:47

################################################################################
                     [1m Learning iteration 882/2500 [0m                      

                       Computation: 41614 steps/s (collection: 2.242s, learning 0.121s)
             Mean action noise std: 2.68
          Mean value_function loss: 182.7159
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 74.1405
                       Mean reward: 117.61
               Mean episode length: 98.67
    Episode_Reward/reaching_object: 0.8639
     Episode_Reward/lifting_object: 23.7908
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0453
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 39.7917
--------------------------------------------------------------------------------
                   Total timesteps: 86802432
                    Iteration time: 2.36s
                      Time elapsed: 00:33:09
                               ETA: 01:00:45

################################################################################
                     [1m Learning iteration 883/2500 [0m                      

                       Computation: 41678 steps/s (collection: 2.243s, learning 0.116s)
             Mean action noise std: 2.68
          Mean value_function loss: 189.7567
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 74.1431
                       Mean reward: 128.17
               Mean episode length: 112.52
    Episode_Reward/reaching_object: 0.8709
     Episode_Reward/lifting_object: 24.1392
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 39.0833
--------------------------------------------------------------------------------
                   Total timesteps: 86900736
                    Iteration time: 2.36s
                      Time elapsed: 00:33:11
                               ETA: 01:00:43

################################################################################
                     [1m Learning iteration 884/2500 [0m                      

                       Computation: 41539 steps/s (collection: 2.242s, learning 0.125s)
             Mean action noise std: 2.68
          Mean value_function loss: 179.5277
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 74.1457
                       Mean reward: 121.80
               Mean episode length: 106.32
    Episode_Reward/reaching_object: 0.8492
     Episode_Reward/lifting_object: 23.5061
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0274
          Episode_Reward/joint_vel: -0.0440
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 37.9583
--------------------------------------------------------------------------------
                   Total timesteps: 86999040
                    Iteration time: 2.37s
                      Time elapsed: 00:33:14
                               ETA: 01:00:41

################################################################################
                     [1m Learning iteration 885/2500 [0m                      

                       Computation: 41326 steps/s (collection: 2.252s, learning 0.127s)
             Mean action noise std: 2.68
          Mean value_function loss: 154.0048
               Mean surrogate loss: -0.0053
                 Mean entropy loss: 74.1475
                       Mean reward: 121.38
               Mean episode length: 101.87
    Episode_Reward/reaching_object: 0.8720
     Episode_Reward/lifting_object: 24.6819
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0279
          Episode_Reward/joint_vel: -0.0460
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 37.0000
--------------------------------------------------------------------------------
                   Total timesteps: 87097344
                    Iteration time: 2.38s
                      Time elapsed: 00:33:16
                               ETA: 01:00:39

################################################################################
                     [1m Learning iteration 886/2500 [0m                      

                       Computation: 41597 steps/s (collection: 2.247s, learning 0.116s)
             Mean action noise std: 2.68
          Mean value_function loss: 170.6430
               Mean surrogate loss: -0.0052
                 Mean entropy loss: 74.1504
                       Mean reward: 122.54
               Mean episode length: 100.19
    Episode_Reward/reaching_object: 0.8681
     Episode_Reward/lifting_object: 24.8739
      Episode_Reward/object_height: 0.0018
        Episode_Reward/action_rate: -0.0275
          Episode_Reward/joint_vel: -0.0439
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 34.0417
--------------------------------------------------------------------------------
                   Total timesteps: 87195648
                    Iteration time: 2.36s
                      Time elapsed: 00:33:18
                               ETA: 01:00:37

################################################################################
                     [1m Learning iteration 887/2500 [0m                      

                       Computation: 41439 steps/s (collection: 2.252s, learning 0.120s)
             Mean action noise std: 2.68
          Mean value_function loss: 175.1275
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 74.1536
                       Mean reward: 130.64
               Mean episode length: 102.76
    Episode_Reward/reaching_object: 0.8976
     Episode_Reward/lifting_object: 24.9342
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0451
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 37.2500
--------------------------------------------------------------------------------
                   Total timesteps: 87293952
                    Iteration time: 2.37s
                      Time elapsed: 00:33:21
                               ETA: 01:00:35

################################################################################
                     [1m Learning iteration 888/2500 [0m                      

                       Computation: 41558 steps/s (collection: 2.245s, learning 0.120s)
             Mean action noise std: 2.68
          Mean value_function loss: 149.5857
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.1565
                       Mean reward: 135.08
               Mean episode length: 106.47
    Episode_Reward/reaching_object: 0.8982
     Episode_Reward/lifting_object: 25.3567
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 36.0833
--------------------------------------------------------------------------------
                   Total timesteps: 87392256
                    Iteration time: 2.37s
                      Time elapsed: 00:33:23
                               ETA: 01:00:33

################################################################################
                     [1m Learning iteration 889/2500 [0m                      

                       Computation: 40803 steps/s (collection: 2.288s, learning 0.122s)
             Mean action noise std: 2.68
          Mean value_function loss: 161.6531
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 74.1596
                       Mean reward: 138.86
               Mean episode length: 113.19
    Episode_Reward/reaching_object: 0.9523
     Episode_Reward/lifting_object: 26.5795
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0479
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 35.9167
--------------------------------------------------------------------------------
                   Total timesteps: 87490560
                    Iteration time: 2.41s
                      Time elapsed: 00:33:25
                               ETA: 01:00:31

################################################################################
                     [1m Learning iteration 890/2500 [0m                      

                       Computation: 40736 steps/s (collection: 2.300s, learning 0.114s)
             Mean action noise std: 2.68
          Mean value_function loss: 155.9060
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.1627
                       Mean reward: 127.96
               Mean episode length: 106.45
    Episode_Reward/reaching_object: 0.9433
     Episode_Reward/lifting_object: 26.9028
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0470
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 38.1250
--------------------------------------------------------------------------------
                   Total timesteps: 87588864
                    Iteration time: 2.41s
                      Time elapsed: 00:33:28
                               ETA: 01:00:29

################################################################################
                     [1m Learning iteration 891/2500 [0m                      

                       Computation: 41138 steps/s (collection: 2.270s, learning 0.119s)
             Mean action noise std: 2.68
          Mean value_function loss: 172.1336
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 74.1704
                       Mean reward: 148.35
               Mean episode length: 117.95
    Episode_Reward/reaching_object: 0.9092
     Episode_Reward/lifting_object: 25.6176
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0463
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 34.9167
--------------------------------------------------------------------------------
                   Total timesteps: 87687168
                    Iteration time: 2.39s
                      Time elapsed: 00:33:30
                               ETA: 01:00:27

################################################################################
                     [1m Learning iteration 892/2500 [0m                      

                       Computation: 41225 steps/s (collection: 2.260s, learning 0.124s)
             Mean action noise std: 2.68
          Mean value_function loss: 167.7424
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 74.1762
                       Mean reward: 139.85
               Mean episode length: 110.62
    Episode_Reward/reaching_object: 0.9267
     Episode_Reward/lifting_object: 26.0698
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0473
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 36.1667
--------------------------------------------------------------------------------
                   Total timesteps: 87785472
                    Iteration time: 2.38s
                      Time elapsed: 00:33:33
                               ETA: 01:00:25

################################################################################
                     [1m Learning iteration 893/2500 [0m                      

                       Computation: 41329 steps/s (collection: 2.257s, learning 0.122s)
             Mean action noise std: 2.68
          Mean value_function loss: 166.1998
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.1794
                       Mean reward: 135.71
               Mean episode length: 110.46
    Episode_Reward/reaching_object: 0.9508
     Episode_Reward/lifting_object: 27.0001
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0481
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 37.7083
--------------------------------------------------------------------------------
                   Total timesteps: 87883776
                    Iteration time: 2.38s
                      Time elapsed: 00:33:35
                               ETA: 01:00:23

################################################################################
                     [1m Learning iteration 894/2500 [0m                      

                       Computation: 41551 steps/s (collection: 2.244s, learning 0.122s)
             Mean action noise std: 2.68
          Mean value_function loss: 160.0056
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 74.1841
                       Mean reward: 139.67
               Mean episode length: 108.88
    Episode_Reward/reaching_object: 0.8976
     Episode_Reward/lifting_object: 26.0746
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0452
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 37.8333
--------------------------------------------------------------------------------
                   Total timesteps: 87982080
                    Iteration time: 2.37s
                      Time elapsed: 00:33:37
                               ETA: 01:00:20

################################################################################
                     [1m Learning iteration 895/2500 [0m                      

                       Computation: 41495 steps/s (collection: 2.267s, learning 0.102s)
             Mean action noise std: 2.68
          Mean value_function loss: 157.5845
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 74.1866
                       Mean reward: 131.43
               Mean episode length: 103.58
    Episode_Reward/reaching_object: 0.9037
     Episode_Reward/lifting_object: 26.3834
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0454
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 35.4167
--------------------------------------------------------------------------------
                   Total timesteps: 88080384
                    Iteration time: 2.37s
                      Time elapsed: 00:33:40
                               ETA: 01:00:18

################################################################################
                     [1m Learning iteration 896/2500 [0m                      

                       Computation: 40935 steps/s (collection: 2.278s, learning 0.123s)
             Mean action noise std: 2.68
          Mean value_function loss: 159.3227
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 74.1871
                       Mean reward: 139.97
               Mean episode length: 105.67
    Episode_Reward/reaching_object: 0.9259
     Episode_Reward/lifting_object: 27.1667
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0468
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 36.7083
--------------------------------------------------------------------------------
                   Total timesteps: 88178688
                    Iteration time: 2.40s
                      Time elapsed: 00:33:42
                               ETA: 01:00:16

################################################################################
                     [1m Learning iteration 897/2500 [0m                      

                       Computation: 41735 steps/s (collection: 2.235s, learning 0.120s)
             Mean action noise std: 2.68
          Mean value_function loss: 159.9965
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 74.1891
                       Mean reward: 141.72
               Mean episode length: 110.59
    Episode_Reward/reaching_object: 0.9039
     Episode_Reward/lifting_object: 26.4530
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 37.7500
--------------------------------------------------------------------------------
                   Total timesteps: 88276992
                    Iteration time: 2.36s
                      Time elapsed: 00:33:45
                               ETA: 01:00:14

################################################################################
                     [1m Learning iteration 898/2500 [0m                      

                       Computation: 40892 steps/s (collection: 2.291s, learning 0.113s)
             Mean action noise std: 2.68
          Mean value_function loss: 160.5897
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 74.1925
                       Mean reward: 131.38
               Mean episode length: 101.34
    Episode_Reward/reaching_object: 0.9156
     Episode_Reward/lifting_object: 26.8494
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 40.2500
--------------------------------------------------------------------------------
                   Total timesteps: 88375296
                    Iteration time: 2.40s
                      Time elapsed: 00:33:47
                               ETA: 01:00:12

################################################################################
                     [1m Learning iteration 899/2500 [0m                      

                       Computation: 41410 steps/s (collection: 2.253s, learning 0.121s)
             Mean action noise std: 2.68
          Mean value_function loss: 153.6182
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 74.1938
                       Mean reward: 136.80
               Mean episode length: 106.91
    Episode_Reward/reaching_object: 0.9173
     Episode_Reward/lifting_object: 27.6147
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0290
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 37.5833
--------------------------------------------------------------------------------
                   Total timesteps: 88473600
                    Iteration time: 2.37s
                      Time elapsed: 00:33:49
                               ETA: 01:00:10

################################################################################
                     [1m Learning iteration 900/2500 [0m                      

                       Computation: 41569 steps/s (collection: 2.246s, learning 0.119s)
             Mean action noise std: 2.68
          Mean value_function loss: 173.6284
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 74.1948
                       Mean reward: 125.67
               Mean episode length: 99.74
    Episode_Reward/reaching_object: 0.9026
     Episode_Reward/lifting_object: 27.2709
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0287
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 36.3750
--------------------------------------------------------------------------------
                   Total timesteps: 88571904
                    Iteration time: 2.36s
                      Time elapsed: 00:33:52
                               ETA: 01:00:08

################################################################################
                     [1m Learning iteration 901/2500 [0m                      

                       Computation: 41243 steps/s (collection: 2.289s, learning 0.094s)
             Mean action noise std: 2.68
          Mean value_function loss: 171.5820
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 74.1954
                       Mean reward: 135.88
               Mean episode length: 101.23
    Episode_Reward/reaching_object: 0.8808
     Episode_Reward/lifting_object: 25.8621
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0282
          Episode_Reward/joint_vel: -0.0449
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 36.4167
--------------------------------------------------------------------------------
                   Total timesteps: 88670208
                    Iteration time: 2.38s
                      Time elapsed: 00:33:54
                               ETA: 01:00:06

################################################################################
                     [1m Learning iteration 902/2500 [0m                      

                       Computation: 41680 steps/s (collection: 2.242s, learning 0.116s)
             Mean action noise std: 2.69
          Mean value_function loss: 169.3013
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.1976
                       Mean reward: 141.44
               Mean episode length: 107.52
    Episode_Reward/reaching_object: 0.8956
     Episode_Reward/lifting_object: 26.4832
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0289
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 38.1667
--------------------------------------------------------------------------------
                   Total timesteps: 88768512
                    Iteration time: 2.36s
                      Time elapsed: 00:33:56
                               ETA: 01:00:04

################################################################################
                     [1m Learning iteration 903/2500 [0m                      

                       Computation: 41938 steps/s (collection: 2.235s, learning 0.109s)
             Mean action noise std: 2.69
          Mean value_function loss: 176.7600
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 74.1994
                       Mean reward: 122.09
               Mean episode length: 102.62
    Episode_Reward/reaching_object: 0.8850
     Episode_Reward/lifting_object: 26.0449
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0471
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.6250
     Episode_Termination/robot_out: 38.2917
--------------------------------------------------------------------------------
                   Total timesteps: 88866816
                    Iteration time: 2.34s
                      Time elapsed: 00:33:59
                               ETA: 01:00:02

################################################################################
                     [1m Learning iteration 904/2500 [0m                      

                       Computation: 41197 steps/s (collection: 2.277s, learning 0.109s)
             Mean action noise std: 2.69
          Mean value_function loss: 182.7360
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 74.2015
                       Mean reward: 131.75
               Mean episode length: 100.28
    Episode_Reward/reaching_object: 0.8736
     Episode_Reward/lifting_object: 26.7692
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 37.0417
--------------------------------------------------------------------------------
                   Total timesteps: 88965120
                    Iteration time: 2.39s
                      Time elapsed: 00:34:01
                               ETA: 01:00:00

################################################################################
                     [1m Learning iteration 905/2500 [0m                      

                       Computation: 41526 steps/s (collection: 2.255s, learning 0.112s)
             Mean action noise std: 2.69
          Mean value_function loss: 189.1038
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 74.2049
                       Mean reward: 132.10
               Mean episode length: 104.62
    Episode_Reward/reaching_object: 0.8707
     Episode_Reward/lifting_object: 25.8421
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0457
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 40.4583
--------------------------------------------------------------------------------
                   Total timesteps: 89063424
                    Iteration time: 2.37s
                      Time elapsed: 00:34:04
                               ETA: 00:59:58

################################################################################
                     [1m Learning iteration 906/2500 [0m                      

                       Computation: 41472 steps/s (collection: 2.255s, learning 0.115s)
             Mean action noise std: 2.69
          Mean value_function loss: 188.2624
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.2078
                       Mean reward: 128.63
               Mean episode length: 100.37
    Episode_Reward/reaching_object: 0.8665
     Episode_Reward/lifting_object: 26.0395
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0458
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 39.7917
--------------------------------------------------------------------------------
                   Total timesteps: 89161728
                    Iteration time: 2.37s
                      Time elapsed: 00:34:06
                               ETA: 00:59:56

################################################################################
                     [1m Learning iteration 907/2500 [0m                      

                       Computation: 41407 steps/s (collection: 2.255s, learning 0.119s)
             Mean action noise std: 2.69
          Mean value_function loss: 193.3068
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 74.2119
                       Mean reward: 133.55
               Mean episode length: 105.24
    Episode_Reward/reaching_object: 0.8560
     Episode_Reward/lifting_object: 25.5620
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0281
          Episode_Reward/joint_vel: -0.0464
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 37.5000
--------------------------------------------------------------------------------
                   Total timesteps: 89260032
                    Iteration time: 2.37s
                      Time elapsed: 00:34:08
                               ETA: 00:59:54

################################################################################
                     [1m Learning iteration 908/2500 [0m                      

                       Computation: 41784 steps/s (collection: 2.237s, learning 0.116s)
             Mean action noise std: 2.69
          Mean value_function loss: 187.4567
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 74.2159
                       Mean reward: 146.61
               Mean episode length: 107.34
    Episode_Reward/reaching_object: 0.9136
     Episode_Reward/lifting_object: 27.4771
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 35.5417
--------------------------------------------------------------------------------
                   Total timesteps: 89358336
                    Iteration time: 2.35s
                      Time elapsed: 00:34:11
                               ETA: 00:59:52

################################################################################
                     [1m Learning iteration 909/2500 [0m                      

                       Computation: 41083 steps/s (collection: 2.273s, learning 0.120s)
             Mean action noise std: 2.69
          Mean value_function loss: 179.9983
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 74.2196
                       Mean reward: 139.35
               Mean episode length: 104.02
    Episode_Reward/reaching_object: 0.8706
     Episode_Reward/lifting_object: 26.3118
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0284
          Episode_Reward/joint_vel: -0.0467
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 39.3750
--------------------------------------------------------------------------------
                   Total timesteps: 89456640
                    Iteration time: 2.39s
                      Time elapsed: 00:34:13
                               ETA: 00:59:50

################################################################################
                     [1m Learning iteration 910/2500 [0m                      

                       Computation: 41250 steps/s (collection: 2.255s, learning 0.128s)
             Mean action noise std: 2.69
          Mean value_function loss: 229.8833
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 74.2222
                       Mean reward: 129.07
               Mean episode length: 104.33
    Episode_Reward/reaching_object: 0.8753
     Episode_Reward/lifting_object: 26.4986
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 39.2917
--------------------------------------------------------------------------------
                   Total timesteps: 89554944
                    Iteration time: 2.38s
                      Time elapsed: 00:34:15
                               ETA: 00:59:48

################################################################################
                     [1m Learning iteration 911/2500 [0m                      

                       Computation: 41587 steps/s (collection: 2.255s, learning 0.109s)
             Mean action noise std: 2.69
          Mean value_function loss: 190.1734
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 74.2260
                       Mean reward: 130.41
               Mean episode length: 97.22
    Episode_Reward/reaching_object: 0.9055
     Episode_Reward/lifting_object: 27.6507
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0295
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 37.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89653248
                    Iteration time: 2.36s
                      Time elapsed: 00:34:18
                               ETA: 00:59:46

################################################################################
                     [1m Learning iteration 912/2500 [0m                      

                       Computation: 41309 steps/s (collection: 2.258s, learning 0.122s)
             Mean action noise std: 2.69
          Mean value_function loss: 186.8284
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 74.2299
                       Mean reward: 143.96
               Mean episode length: 103.86
    Episode_Reward/reaching_object: 0.8570
     Episode_Reward/lifting_object: 25.9778
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0283
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 37.6667
--------------------------------------------------------------------------------
                   Total timesteps: 89751552
                    Iteration time: 2.38s
                      Time elapsed: 00:34:20
                               ETA: 00:59:44

################################################################################
                     [1m Learning iteration 913/2500 [0m                      

                       Computation: 41197 steps/s (collection: 2.260s, learning 0.126s)
             Mean action noise std: 2.69
          Mean value_function loss: 170.4225
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 74.2328
                       Mean reward: 127.08
               Mean episode length: 104.62
    Episode_Reward/reaching_object: 0.9059
     Episode_Reward/lifting_object: 27.9512
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 37.0417
--------------------------------------------------------------------------------
                   Total timesteps: 89849856
                    Iteration time: 2.39s
                      Time elapsed: 00:34:23
                               ETA: 00:59:42

################################################################################
                     [1m Learning iteration 914/2500 [0m                      

                       Computation: 41278 steps/s (collection: 2.261s, learning 0.120s)
             Mean action noise std: 2.69
          Mean value_function loss: 181.8892
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 74.2358
                       Mean reward: 128.89
               Mean episode length: 99.71
    Episode_Reward/reaching_object: 0.9102
     Episode_Reward/lifting_object: 27.8158
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0515
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 39.1250
--------------------------------------------------------------------------------
                   Total timesteps: 89948160
                    Iteration time: 2.38s
                      Time elapsed: 00:34:25
                               ETA: 00:59:40

################################################################################
                     [1m Learning iteration 915/2500 [0m                      

                       Computation: 41025 steps/s (collection: 2.287s, learning 0.109s)
             Mean action noise std: 2.69
          Mean value_function loss: 174.4312
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 74.2379
                       Mean reward: 140.81
               Mean episode length: 105.22
    Episode_Reward/reaching_object: 0.8952
     Episode_Reward/lifting_object: 27.4433
      Episode_Reward/object_height: 0.0019
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 34.7083
--------------------------------------------------------------------------------
                   Total timesteps: 90046464
                    Iteration time: 2.40s
                      Time elapsed: 00:34:27
                               ETA: 00:59:38

################################################################################
                     [1m Learning iteration 916/2500 [0m                      

                       Computation: 41515 steps/s (collection: 2.249s, learning 0.119s)
             Mean action noise std: 2.69
          Mean value_function loss: 176.3000
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 74.2413
                       Mean reward: 139.22
               Mean episode length: 100.14
    Episode_Reward/reaching_object: 0.9250
     Episode_Reward/lifting_object: 28.4757
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 37.9167
--------------------------------------------------------------------------------
                   Total timesteps: 90144768
                    Iteration time: 2.37s
                      Time elapsed: 00:34:30
                               ETA: 00:59:35

################################################################################
                     [1m Learning iteration 917/2500 [0m                      

                       Computation: 41816 steps/s (collection: 2.259s, learning 0.092s)
             Mean action noise std: 2.69
          Mean value_function loss: 180.4661
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 74.2453
                       Mean reward: 145.69
               Mean episode length: 107.24
    Episode_Reward/reaching_object: 0.9471
     Episode_Reward/lifting_object: 28.6618
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.7917
     Episode_Termination/robot_out: 38.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90243072
                    Iteration time: 2.35s
                      Time elapsed: 00:34:32
                               ETA: 00:59:33

################################################################################
                     [1m Learning iteration 918/2500 [0m                      

                       Computation: 41516 steps/s (collection: 2.246s, learning 0.122s)
             Mean action noise std: 2.69
          Mean value_function loss: 173.2093
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 74.2479
                       Mean reward: 153.99
               Mean episode length: 111.32
    Episode_Reward/reaching_object: 0.9344
     Episode_Reward/lifting_object: 28.8598
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 35.4583
--------------------------------------------------------------------------------
                   Total timesteps: 90341376
                    Iteration time: 2.37s
                      Time elapsed: 00:34:34
                               ETA: 00:59:31

################################################################################
                     [1m Learning iteration 919/2500 [0m                      

                       Computation: 40914 steps/s (collection: 2.278s, learning 0.125s)
             Mean action noise std: 2.69
          Mean value_function loss: 182.2332
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.2503
                       Mean reward: 137.29
               Mean episode length: 102.65
    Episode_Reward/reaching_object: 0.9435
     Episode_Reward/lifting_object: 28.8362
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0502
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 36.3750
--------------------------------------------------------------------------------
                   Total timesteps: 90439680
                    Iteration time: 2.40s
                      Time elapsed: 00:34:37
                               ETA: 00:59:29

################################################################################
                     [1m Learning iteration 920/2500 [0m                      

                       Computation: 40969 steps/s (collection: 2.286s, learning 0.113s)
             Mean action noise std: 2.69
          Mean value_function loss: 187.4999
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 74.2543
                       Mean reward: 161.05
               Mean episode length: 107.76
    Episode_Reward/reaching_object: 0.9535
     Episode_Reward/lifting_object: 29.0887
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 37.0000
--------------------------------------------------------------------------------
                   Total timesteps: 90537984
                    Iteration time: 2.40s
                      Time elapsed: 00:34:39
                               ETA: 00:59:27

################################################################################
                     [1m Learning iteration 921/2500 [0m                      

                       Computation: 41517 steps/s (collection: 2.247s, learning 0.121s)
             Mean action noise std: 2.69
          Mean value_function loss: 194.6570
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 74.2584
                       Mean reward: 162.03
               Mean episode length: 115.09
    Episode_Reward/reaching_object: 0.9421
     Episode_Reward/lifting_object: 28.7856
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0513
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 39.0417
--------------------------------------------------------------------------------
                   Total timesteps: 90636288
                    Iteration time: 2.37s
                      Time elapsed: 00:34:42
                               ETA: 00:59:25

################################################################################
                     [1m Learning iteration 922/2500 [0m                      

                       Computation: 41718 steps/s (collection: 2.238s, learning 0.119s)
             Mean action noise std: 2.69
          Mean value_function loss: 198.0778
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 74.2633
                       Mean reward: 143.53
               Mean episode length: 108.77
    Episode_Reward/reaching_object: 0.9325
     Episode_Reward/lifting_object: 28.7408
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 37.1667
--------------------------------------------------------------------------------
                   Total timesteps: 90734592
                    Iteration time: 2.36s
                      Time elapsed: 00:34:44
                               ETA: 00:59:23

################################################################################
                     [1m Learning iteration 923/2500 [0m                      

                       Computation: 41776 steps/s (collection: 2.233s, learning 0.121s)
             Mean action noise std: 2.69
          Mean value_function loss: 204.9338
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 74.2680
                       Mean reward: 146.77
               Mean episode length: 101.59
    Episode_Reward/reaching_object: 0.8903
     Episode_Reward/lifting_object: 28.0629
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0291
          Episode_Reward/joint_vel: -0.0486
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 35.8333
--------------------------------------------------------------------------------
                   Total timesteps: 90832896
                    Iteration time: 2.35s
                      Time elapsed: 00:34:46
                               ETA: 00:59:21

################################################################################
                     [1m Learning iteration 924/2500 [0m                      

                       Computation: 40688 steps/s (collection: 2.303s, learning 0.113s)
             Mean action noise std: 2.69
          Mean value_function loss: 185.4286
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 74.2708
                       Mean reward: 157.65
               Mean episode length: 115.46
    Episode_Reward/reaching_object: 0.9239
     Episode_Reward/lifting_object: 29.1717
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 37.1250
--------------------------------------------------------------------------------
                   Total timesteps: 90931200
                    Iteration time: 2.42s
                      Time elapsed: 00:34:49
                               ETA: 00:59:19

################################################################################
                     [1m Learning iteration 925/2500 [0m                      

                       Computation: 41693 steps/s (collection: 2.236s, learning 0.122s)
             Mean action noise std: 2.69
          Mean value_function loss: 178.5470
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.2743
                       Mean reward: 141.06
               Mean episode length: 103.15
    Episode_Reward/reaching_object: 0.9152
     Episode_Reward/lifting_object: 29.3573
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 36.5833
--------------------------------------------------------------------------------
                   Total timesteps: 91029504
                    Iteration time: 2.36s
                      Time elapsed: 00:34:51
                               ETA: 00:59:17

################################################################################
                     [1m Learning iteration 926/2500 [0m                      

                       Computation: 42279 steps/s (collection: 2.225s, learning 0.100s)
             Mean action noise std: 2.69
          Mean value_function loss: 204.1922
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 74.2790
                       Mean reward: 158.39
               Mean episode length: 110.18
    Episode_Reward/reaching_object: 0.9307
     Episode_Reward/lifting_object: 30.1961
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 36.9167
--------------------------------------------------------------------------------
                   Total timesteps: 91127808
                    Iteration time: 2.33s
                      Time elapsed: 00:34:53
                               ETA: 00:59:15

################################################################################
                     [1m Learning iteration 927/2500 [0m                      

                       Computation: 42413 steps/s (collection: 2.207s, learning 0.111s)
             Mean action noise std: 2.69
          Mean value_function loss: 206.0180
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 74.2832
                       Mean reward: 147.37
               Mean episode length: 107.84
    Episode_Reward/reaching_object: 0.9505
     Episode_Reward/lifting_object: 30.5538
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0525
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 35.7083
--------------------------------------------------------------------------------
                   Total timesteps: 91226112
                    Iteration time: 2.32s
                      Time elapsed: 00:34:56
                               ETA: 00:59:13

################################################################################
                     [1m Learning iteration 928/2500 [0m                      

                       Computation: 41115 steps/s (collection: 2.276s, learning 0.114s)
             Mean action noise std: 2.69
          Mean value_function loss: 189.3608
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 74.2861
                       Mean reward: 162.63
               Mean episode length: 113.24
    Episode_Reward/reaching_object: 0.9281
     Episode_Reward/lifting_object: 30.0234
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 41.0833
--------------------------------------------------------------------------------
                   Total timesteps: 91324416
                    Iteration time: 2.39s
                      Time elapsed: 00:34:58
                               ETA: 00:59:11

################################################################################
                     [1m Learning iteration 929/2500 [0m                      

                       Computation: 42288 steps/s (collection: 2.191s, learning 0.134s)
             Mean action noise std: 2.69
          Mean value_function loss: 188.5647
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.2877
                       Mean reward: 148.20
               Mean episode length: 104.15
    Episode_Reward/reaching_object: 0.8941
     Episode_Reward/lifting_object: 29.3207
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0497
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 36.2917
--------------------------------------------------------------------------------
                   Total timesteps: 91422720
                    Iteration time: 2.32s
                      Time elapsed: 00:35:00
                               ETA: 00:59:08

################################################################################
                     [1m Learning iteration 930/2500 [0m                      

                       Computation: 42619 steps/s (collection: 2.210s, learning 0.097s)
             Mean action noise std: 2.69
          Mean value_function loss: 210.0398
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 74.2903
                       Mean reward: 155.41
               Mean episode length: 108.09
    Episode_Reward/reaching_object: 0.8908
     Episode_Reward/lifting_object: 29.6935
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0294
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 37.2500
--------------------------------------------------------------------------------
                   Total timesteps: 91521024
                    Iteration time: 2.31s
                      Time elapsed: 00:35:03
                               ETA: 00:59:06

################################################################################
                     [1m Learning iteration 931/2500 [0m                      

                       Computation: 39850 steps/s (collection: 2.363s, learning 0.104s)
             Mean action noise std: 2.69
          Mean value_function loss: 190.4692
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 74.2924
                       Mean reward: 155.23
               Mean episode length: 109.43
    Episode_Reward/reaching_object: 0.8977
     Episode_Reward/lifting_object: 29.6381
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0488
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 36.9583
--------------------------------------------------------------------------------
                   Total timesteps: 91619328
                    Iteration time: 2.47s
                      Time elapsed: 00:35:05
                               ETA: 00:59:04

################################################################################
                     [1m Learning iteration 932/2500 [0m                      

                       Computation: 41451 steps/s (collection: 2.245s, learning 0.126s)
             Mean action noise std: 2.69
          Mean value_function loss: 222.0025
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 74.2945
                       Mean reward: 124.51
               Mean episode length: 94.00
    Episode_Reward/reaching_object: 0.9081
     Episode_Reward/lifting_object: 29.4052
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0498
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 36.8750
--------------------------------------------------------------------------------
                   Total timesteps: 91717632
                    Iteration time: 2.37s
                      Time elapsed: 00:35:08
                               ETA: 00:59:02

################################################################################
                     [1m Learning iteration 933/2500 [0m                      

                       Computation: 42787 steps/s (collection: 2.202s, learning 0.095s)
             Mean action noise std: 2.70
          Mean value_function loss: 203.3040
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 74.2983
                       Mean reward: 154.03
               Mean episode length: 109.96
    Episode_Reward/reaching_object: 0.9181
     Episode_Reward/lifting_object: 29.7316
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 39.6250
--------------------------------------------------------------------------------
                   Total timesteps: 91815936
                    Iteration time: 2.30s
                      Time elapsed: 00:35:10
                               ETA: 00:59:00

################################################################################
                     [1m Learning iteration 934/2500 [0m                      

                       Computation: 42758 steps/s (collection: 2.182s, learning 0.118s)
             Mean action noise std: 2.70
          Mean value_function loss: 204.7927
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 74.3034
                       Mean reward: 147.39
               Mean episode length: 105.91
    Episode_Reward/reaching_object: 0.9128
     Episode_Reward/lifting_object: 30.1632
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.8750
     Episode_Termination/robot_out: 38.5000
--------------------------------------------------------------------------------
                   Total timesteps: 91914240
                    Iteration time: 2.30s
                      Time elapsed: 00:35:12
                               ETA: 00:58:58

################################################################################
                     [1m Learning iteration 935/2500 [0m                      

                       Computation: 42521 steps/s (collection: 2.186s, learning 0.126s)
             Mean action noise std: 2.70
          Mean value_function loss: 183.9911
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 74.3081
                       Mean reward: 144.74
               Mean episode length: 101.35
    Episode_Reward/reaching_object: 0.8575
     Episode_Reward/lifting_object: 28.5372
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0286
          Episode_Reward/joint_vel: -0.0474
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 38.0833
--------------------------------------------------------------------------------
                   Total timesteps: 92012544
                    Iteration time: 2.31s
                      Time elapsed: 00:35:14
                               ETA: 00:58:56

################################################################################
                     [1m Learning iteration 936/2500 [0m                      

                       Computation: 42363 steps/s (collection: 2.207s, learning 0.113s)
             Mean action noise std: 2.70
          Mean value_function loss: 184.3539
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 74.3112
                       Mean reward: 147.34
               Mean episode length: 99.22
    Episode_Reward/reaching_object: 0.8676
     Episode_Reward/lifting_object: 29.4116
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0288
          Episode_Reward/joint_vel: -0.0469
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 36.5000
--------------------------------------------------------------------------------
                   Total timesteps: 92110848
                    Iteration time: 2.32s
                      Time elapsed: 00:35:17
                               ETA: 00:58:54

################################################################################
                     [1m Learning iteration 937/2500 [0m                      

                       Computation: 43376 steps/s (collection: 2.164s, learning 0.102s)
             Mean action noise std: 2.70
          Mean value_function loss: 187.7984
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 74.3139
                       Mean reward: 148.45
               Mean episode length: 100.13
    Episode_Reward/reaching_object: 0.9042
     Episode_Reward/lifting_object: 30.9069
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 35.9583
--------------------------------------------------------------------------------
                   Total timesteps: 92209152
                    Iteration time: 2.27s
                      Time elapsed: 00:35:19
                               ETA: 00:58:51

################################################################################
                     [1m Learning iteration 938/2500 [0m                      

                       Computation: 42723 steps/s (collection: 2.180s, learning 0.121s)
             Mean action noise std: 2.70
          Mean value_function loss: 181.9121
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.3154
                       Mean reward: 157.39
               Mean episode length: 108.59
    Episode_Reward/reaching_object: 0.9003
     Episode_Reward/lifting_object: 30.1322
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0298
          Episode_Reward/joint_vel: -0.0492
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 37.8333
--------------------------------------------------------------------------------
                   Total timesteps: 92307456
                    Iteration time: 2.30s
                      Time elapsed: 00:35:21
                               ETA: 00:58:49

################################################################################
                     [1m Learning iteration 939/2500 [0m                      

                       Computation: 43176 steps/s (collection: 2.173s, learning 0.104s)
             Mean action noise std: 2.70
          Mean value_function loss: 197.9770
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 74.3197
                       Mean reward: 146.28
               Mean episode length: 100.31
    Episode_Reward/reaching_object: 0.9163
     Episode_Reward/lifting_object: 30.6614
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0508
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 36.1250
--------------------------------------------------------------------------------
                   Total timesteps: 92405760
                    Iteration time: 2.28s
                      Time elapsed: 00:35:24
                               ETA: 00:58:47

################################################################################
                     [1m Learning iteration 940/2500 [0m                      

                       Computation: 43455 steps/s (collection: 2.152s, learning 0.111s)
             Mean action noise std: 2.70
          Mean value_function loss: 205.4259
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 74.3243
                       Mean reward: 160.69
               Mean episode length: 108.75
    Episode_Reward/reaching_object: 0.9353
     Episode_Reward/lifting_object: 31.2488
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 37.2500
--------------------------------------------------------------------------------
                   Total timesteps: 92504064
                    Iteration time: 2.26s
                      Time elapsed: 00:35:26
                               ETA: 00:58:45

################################################################################
                     [1m Learning iteration 941/2500 [0m                      

                       Computation: 43089 steps/s (collection: 2.169s, learning 0.112s)
             Mean action noise std: 2.70
          Mean value_function loss: 193.8148
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 74.3252
                       Mean reward: 163.22
               Mean episode length: 113.25
    Episode_Reward/reaching_object: 0.8970
     Episode_Reward/lifting_object: 30.1395
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0496
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 37.7500
--------------------------------------------------------------------------------
                   Total timesteps: 92602368
                    Iteration time: 2.28s
                      Time elapsed: 00:35:28
                               ETA: 00:58:42

################################################################################
                     [1m Learning iteration 942/2500 [0m                      

                       Computation: 42723 steps/s (collection: 2.183s, learning 0.118s)
             Mean action noise std: 2.70
          Mean value_function loss: 185.1755
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 74.3262
                       Mean reward: 147.42
               Mean episode length: 104.92
    Episode_Reward/reaching_object: 0.8958
     Episode_Reward/lifting_object: 30.2841
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0500
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 37.3333
--------------------------------------------------------------------------------
                   Total timesteps: 92700672
                    Iteration time: 2.30s
                      Time elapsed: 00:35:30
                               ETA: 00:58:40

################################################################################
                     [1m Learning iteration 943/2500 [0m                      

                       Computation: 43184 steps/s (collection: 2.159s, learning 0.118s)
             Mean action noise std: 2.70
          Mean value_function loss: 189.9997
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.3269
                       Mean reward: 152.84
               Mean episode length: 107.27
    Episode_Reward/reaching_object: 0.9196
     Episode_Reward/lifting_object: 30.5845
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0518
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 34.2917
--------------------------------------------------------------------------------
                   Total timesteps: 92798976
                    Iteration time: 2.28s
                      Time elapsed: 00:35:33
                               ETA: 00:58:38

################################################################################
                     [1m Learning iteration 944/2500 [0m                      

                       Computation: 41384 steps/s (collection: 2.257s, learning 0.119s)
             Mean action noise std: 2.70
          Mean value_function loss: 207.2604
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 74.3303
                       Mean reward: 165.35
               Mean episode length: 109.30
    Episode_Reward/reaching_object: 0.8990
     Episode_Reward/lifting_object: 31.0684
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0505
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 39.5833
--------------------------------------------------------------------------------
                   Total timesteps: 92897280
                    Iteration time: 2.38s
                      Time elapsed: 00:35:35
                               ETA: 00:58:36

################################################################################
                     [1m Learning iteration 945/2500 [0m                      

                       Computation: 40784 steps/s (collection: 2.296s, learning 0.115s)
             Mean action noise std: 2.70
          Mean value_function loss: 202.7517
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 74.3354
                       Mean reward: 149.17
               Mean episode length: 107.60
    Episode_Reward/reaching_object: 0.8879
     Episode_Reward/lifting_object: 30.0324
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0506
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 39.7083
--------------------------------------------------------------------------------
                   Total timesteps: 92995584
                    Iteration time: 2.41s
                      Time elapsed: 00:35:38
                               ETA: 00:58:34

################################################################################
                     [1m Learning iteration 946/2500 [0m                      

                       Computation: 41614 steps/s (collection: 2.245s, learning 0.117s)
             Mean action noise std: 2.70
          Mean value_function loss: 203.5527
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 74.3381
                       Mean reward: 142.28
               Mean episode length: 102.47
    Episode_Reward/reaching_object: 0.8722
     Episode_Reward/lifting_object: 30.0285
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0499
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 36.3750
--------------------------------------------------------------------------------
                   Total timesteps: 93093888
                    Iteration time: 2.36s
                      Time elapsed: 00:35:40
                               ETA: 00:58:32

################################################################################
                     [1m Learning iteration 947/2500 [0m                      

                       Computation: 41622 steps/s (collection: 2.251s, learning 0.111s)
             Mean action noise std: 2.70
          Mean value_function loss: 183.5215
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 74.3396
                       Mean reward: 158.70
               Mean episode length: 109.09
    Episode_Reward/reaching_object: 0.8641
     Episode_Reward/lifting_object: 29.9813
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0292
          Episode_Reward/joint_vel: -0.0487
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 37.3750
--------------------------------------------------------------------------------
                   Total timesteps: 93192192
                    Iteration time: 2.36s
                      Time elapsed: 00:35:42
                               ETA: 00:58:30

################################################################################
                     [1m Learning iteration 948/2500 [0m                      

                       Computation: 42055 steps/s (collection: 2.228s, learning 0.110s)
             Mean action noise std: 2.70
          Mean value_function loss: 204.4553
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 74.3427
                       Mean reward: 148.97
               Mean episode length: 100.70
    Episode_Reward/reaching_object: 0.8385
     Episode_Reward/lifting_object: 29.0980
      Episode_Reward/object_height: 0.0020
        Episode_Reward/action_rate: -0.0285
          Episode_Reward/joint_vel: -0.0482
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 37.5417
--------------------------------------------------------------------------------
                   Total timesteps: 93290496
                    Iteration time: 2.34s
                      Time elapsed: 00:35:45
                               ETA: 00:58:28

################################################################################
                     [1m Learning iteration 949/2500 [0m                      

                       Computation: 42162 steps/s (collection: 2.218s, learning 0.114s)
             Mean action noise std: 2.70
          Mean value_function loss: 200.9184
               Mean surrogate loss: -0.0047
                 Mean entropy loss: 74.3463
                       Mean reward: 149.80
               Mean episode length: 101.08
    Episode_Reward/reaching_object: 0.8902
     Episode_Reward/lifting_object: 31.1023
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 38.6250
--------------------------------------------------------------------------------
                   Total timesteps: 93388800
                    Iteration time: 2.33s
                      Time elapsed: 00:35:47
                               ETA: 00:58:25

################################################################################
                     [1m Learning iteration 950/2500 [0m                      

                       Computation: 40187 steps/s (collection: 2.350s, learning 0.096s)
             Mean action noise std: 2.70
          Mean value_function loss: 195.4605
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 74.3499
                       Mean reward: 168.56
               Mean episode length: 114.36
    Episode_Reward/reaching_object: 0.8892
     Episode_Reward/lifting_object: 30.4136
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 38.2917
--------------------------------------------------------------------------------
                   Total timesteps: 93487104
                    Iteration time: 2.45s
                      Time elapsed: 00:35:49
                               ETA: 00:58:23

################################################################################
                     [1m Learning iteration 951/2500 [0m                      

                       Computation: 41241 steps/s (collection: 2.276s, learning 0.108s)
             Mean action noise std: 2.70
          Mean value_function loss: 197.5890
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 74.3513
                       Mean reward: 164.34
               Mean episode length: 109.63
    Episode_Reward/reaching_object: 0.8955
     Episode_Reward/lifting_object: 30.9292
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.5833
     Episode_Termination/robot_out: 36.2917
--------------------------------------------------------------------------------
                   Total timesteps: 93585408
                    Iteration time: 2.38s
                      Time elapsed: 00:35:52
                               ETA: 00:58:21

################################################################################
                     [1m Learning iteration 952/2500 [0m                      

                       Computation: 41593 steps/s (collection: 2.251s, learning 0.112s)
             Mean action noise std: 2.70
          Mean value_function loss: 205.8424
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 74.3528
                       Mean reward: 155.73
               Mean episode length: 110.44
    Episode_Reward/reaching_object: 0.9028
     Episode_Reward/lifting_object: 31.2762
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0520
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 38.0417
--------------------------------------------------------------------------------
                   Total timesteps: 93683712
                    Iteration time: 2.36s
                      Time elapsed: 00:35:54
                               ETA: 00:58:19

################################################################################
                     [1m Learning iteration 953/2500 [0m                      

                       Computation: 43034 steps/s (collection: 2.173s, learning 0.112s)
             Mean action noise std: 2.70
          Mean value_function loss: 207.3892
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 74.3557
                       Mean reward: 149.19
               Mean episode length: 97.39
    Episode_Reward/reaching_object: 0.8775
     Episode_Reward/lifting_object: 31.1734
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0517
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 37.4583
--------------------------------------------------------------------------------
                   Total timesteps: 93782016
                    Iteration time: 2.28s
                      Time elapsed: 00:35:56
                               ETA: 00:58:17

################################################################################
                     [1m Learning iteration 954/2500 [0m                      

                       Computation: 43485 steps/s (collection: 2.146s, learning 0.115s)
             Mean action noise std: 2.70
          Mean value_function loss: 202.5645
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.3582
                       Mean reward: 166.60
               Mean episode length: 107.19
    Episode_Reward/reaching_object: 0.8919
     Episode_Reward/lifting_object: 31.5121
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 36.4583
--------------------------------------------------------------------------------
                   Total timesteps: 93880320
                    Iteration time: 2.26s
                      Time elapsed: 00:35:59
                               ETA: 00:58:15

################################################################################
                     [1m Learning iteration 955/2500 [0m                      

                       Computation: 43033 steps/s (collection: 2.183s, learning 0.101s)
             Mean action noise std: 2.70
          Mean value_function loss: 185.1952
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 74.3597
                       Mean reward: 159.26
               Mean episode length: 105.36
    Episode_Reward/reaching_object: 0.8920
     Episode_Reward/lifting_object: 30.9988
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 36.5417
--------------------------------------------------------------------------------
                   Total timesteps: 93978624
                    Iteration time: 2.28s
                      Time elapsed: 00:36:01
                               ETA: 00:58:13

################################################################################
                     [1m Learning iteration 956/2500 [0m                      

                       Computation: 43051 steps/s (collection: 2.176s, learning 0.108s)
             Mean action noise std: 2.70
          Mean value_function loss: 180.9712
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.3611
                       Mean reward: 177.10
               Mean episode length: 110.52
    Episode_Reward/reaching_object: 0.9271
     Episode_Reward/lifting_object: 33.1990
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 37.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94076928
                    Iteration time: 2.28s
                      Time elapsed: 00:36:03
                               ETA: 00:58:10

################################################################################
                     [1m Learning iteration 957/2500 [0m                      

                       Computation: 42575 steps/s (collection: 2.194s, learning 0.115s)
             Mean action noise std: 2.70
          Mean value_function loss: 196.3443
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 74.3645
                       Mean reward: 180.10
               Mean episode length: 117.20
    Episode_Reward/reaching_object: 0.9366
     Episode_Reward/lifting_object: 33.2013
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 36.3750
--------------------------------------------------------------------------------
                   Total timesteps: 94175232
                    Iteration time: 2.31s
                      Time elapsed: 00:36:06
                               ETA: 00:58:08

################################################################################
                     [1m Learning iteration 958/2500 [0m                      

                       Computation: 42813 steps/s (collection: 2.177s, learning 0.119s)
             Mean action noise std: 2.70
          Mean value_function loss: 191.5299
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 74.3690
                       Mean reward: 166.32
               Mean episode length: 110.92
    Episode_Reward/reaching_object: 0.9354
     Episode_Reward/lifting_object: 33.0532
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 37.2917
--------------------------------------------------------------------------------
                   Total timesteps: 94273536
                    Iteration time: 2.30s
                      Time elapsed: 00:36:08
                               ETA: 00:58:06

################################################################################
                     [1m Learning iteration 959/2500 [0m                      

                       Computation: 42250 steps/s (collection: 2.211s, learning 0.116s)
             Mean action noise std: 2.70
          Mean value_function loss: 189.2705
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 74.3751
                       Mean reward: 178.63
               Mean episode length: 111.42
    Episode_Reward/reaching_object: 0.9284
     Episode_Reward/lifting_object: 32.9216
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0529
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 38.1250
--------------------------------------------------------------------------------
                   Total timesteps: 94371840
                    Iteration time: 2.33s
                      Time elapsed: 00:36:10
                               ETA: 00:58:04

################################################################################
                     [1m Learning iteration 960/2500 [0m                      

                       Computation: 42993 steps/s (collection: 2.168s, learning 0.119s)
             Mean action noise std: 2.70
          Mean value_function loss: 196.4281
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.3792
                       Mean reward: 173.80
               Mean episode length: 111.25
    Episode_Reward/reaching_object: 0.9113
     Episode_Reward/lifting_object: 32.4816
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 36.8333
--------------------------------------------------------------------------------
                   Total timesteps: 94470144
                    Iteration time: 2.29s
                      Time elapsed: 00:36:12
                               ETA: 00:58:02

################################################################################
                     [1m Learning iteration 961/2500 [0m                      

                       Computation: 42607 steps/s (collection: 2.200s, learning 0.107s)
             Mean action noise std: 2.70
          Mean value_function loss: 186.7244
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 74.3819
                       Mean reward: 158.63
               Mean episode length: 103.52
    Episode_Reward/reaching_object: 0.9169
     Episode_Reward/lifting_object: 33.0150
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0305
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 38.0833
--------------------------------------------------------------------------------
                   Total timesteps: 94568448
                    Iteration time: 2.31s
                      Time elapsed: 00:36:15
                               ETA: 00:57:59

################################################################################
                     [1m Learning iteration 962/2500 [0m                      

                       Computation: 43254 steps/s (collection: 2.164s, learning 0.109s)
             Mean action noise std: 2.70
          Mean value_function loss: 205.0776
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 74.3862
                       Mean reward: 164.46
               Mean episode length: 104.12
    Episode_Reward/reaching_object: 0.9040
     Episode_Reward/lifting_object: 32.3461
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 38.3333
--------------------------------------------------------------------------------
                   Total timesteps: 94666752
                    Iteration time: 2.27s
                      Time elapsed: 00:36:17
                               ETA: 00:57:57

################################################################################
                     [1m Learning iteration 963/2500 [0m                      

                       Computation: 42922 steps/s (collection: 2.187s, learning 0.103s)
             Mean action noise std: 2.70
          Mean value_function loss: 200.5401
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 74.3923
                       Mean reward: 156.62
               Mean episode length: 99.69
    Episode_Reward/reaching_object: 0.8834
     Episode_Reward/lifting_object: 31.7188
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0489
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 38.0417
--------------------------------------------------------------------------------
                   Total timesteps: 94765056
                    Iteration time: 2.29s
                      Time elapsed: 00:36:19
                               ETA: 00:57:55

################################################################################
                     [1m Learning iteration 964/2500 [0m                      

                       Computation: 43158 steps/s (collection: 2.176s, learning 0.102s)
             Mean action noise std: 2.70
          Mean value_function loss: 217.6525
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 74.3964
                       Mean reward: 156.37
               Mean episode length: 106.93
    Episode_Reward/reaching_object: 0.9044
     Episode_Reward/lifting_object: 32.5501
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.6667
     Episode_Termination/robot_out: 37.8750
--------------------------------------------------------------------------------
                   Total timesteps: 94863360
                    Iteration time: 2.28s
                      Time elapsed: 00:36:22
                               ETA: 00:57:53

################################################################################
                     [1m Learning iteration 965/2500 [0m                      

                       Computation: 42944 steps/s (collection: 2.177s, learning 0.112s)
             Mean action noise std: 2.71
          Mean value_function loss: 198.1216
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 74.3971
                       Mean reward: 179.50
               Mean episode length: 110.15
    Episode_Reward/reaching_object: 0.9205
     Episode_Reward/lifting_object: 32.9430
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0523
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 37.3750
--------------------------------------------------------------------------------
                   Total timesteps: 94961664
                    Iteration time: 2.29s
                      Time elapsed: 00:36:24
                               ETA: 00:57:51

################################################################################
                     [1m Learning iteration 966/2500 [0m                      

                       Computation: 43497 steps/s (collection: 2.152s, learning 0.108s)
             Mean action noise std: 2.71
          Mean value_function loss: 196.0818
               Mean surrogate loss: -0.0042
                 Mean entropy loss: 74.3988
                       Mean reward: 179.40
               Mean episode length: 112.05
    Episode_Reward/reaching_object: 0.8972
     Episode_Reward/lifting_object: 32.6328
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.7083
     Episode_Termination/robot_out: 37.0833
--------------------------------------------------------------------------------
                   Total timesteps: 95059968
                    Iteration time: 2.26s
                      Time elapsed: 00:36:26
                               ETA: 00:57:48

################################################################################
                     [1m Learning iteration 967/2500 [0m                      

                       Computation: 42555 steps/s (collection: 2.202s, learning 0.108s)
             Mean action noise std: 2.71
          Mean value_function loss: 197.5759
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 74.4025
                       Mean reward: 182.08
               Mean episode length: 115.05
    Episode_Reward/reaching_object: 0.8927
     Episode_Reward/lifting_object: 32.1213
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0511
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 37.6667
--------------------------------------------------------------------------------
                   Total timesteps: 95158272
                    Iteration time: 2.31s
                      Time elapsed: 00:36:28
                               ETA: 00:57:46

################################################################################
                     [1m Learning iteration 968/2500 [0m                      

                       Computation: 43720 steps/s (collection: 2.158s, learning 0.090s)
             Mean action noise std: 2.71
          Mean value_function loss: 195.0913
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 74.4061
                       Mean reward: 167.57
               Mean episode length: 107.99
    Episode_Reward/reaching_object: 0.8703
     Episode_Reward/lifting_object: 32.1016
      Episode_Reward/object_height: 0.0021
        Episode_Reward/action_rate: -0.0296
          Episode_Reward/joint_vel: -0.0503
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 40.7083
--------------------------------------------------------------------------------
                   Total timesteps: 95256576
                    Iteration time: 2.25s
                      Time elapsed: 00:36:31
                               ETA: 00:57:44

################################################################################
                     [1m Learning iteration 969/2500 [0m                      

                       Computation: 43260 steps/s (collection: 2.158s, learning 0.114s)
             Mean action noise std: 2.71
          Mean value_function loss: 198.4805
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.4105
                       Mean reward: 154.81
               Mean episode length: 101.11
    Episode_Reward/reaching_object: 0.8788
     Episode_Reward/lifting_object: 32.3136
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0512
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 37.2500
--------------------------------------------------------------------------------
                   Total timesteps: 95354880
                    Iteration time: 2.27s
                      Time elapsed: 00:36:33
                               ETA: 00:57:42

################################################################################
                     [1m Learning iteration 970/2500 [0m                      

                       Computation: 42796 steps/s (collection: 2.186s, learning 0.111s)
             Mean action noise std: 2.71
          Mean value_function loss: 201.1510
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 74.4149
                       Mean reward: 154.87
               Mean episode length: 98.01
    Episode_Reward/reaching_object: 0.8811
     Episode_Reward/lifting_object: 31.9944
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0501
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 36.5000
--------------------------------------------------------------------------------
                   Total timesteps: 95453184
                    Iteration time: 2.30s
                      Time elapsed: 00:36:35
                               ETA: 00:57:39

################################################################################
                     [1m Learning iteration 971/2500 [0m                      

                       Computation: 42985 steps/s (collection: 2.173s, learning 0.114s)
             Mean action noise std: 2.71
          Mean value_function loss: 202.3687
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 74.4170
                       Mean reward: 176.51
               Mean episode length: 104.53
    Episode_Reward/reaching_object: 0.9045
     Episode_Reward/lifting_object: 34.1385
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0303
          Episode_Reward/joint_vel: -0.0514
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 38.4583
--------------------------------------------------------------------------------
                   Total timesteps: 95551488
                    Iteration time: 2.29s
                      Time elapsed: 00:36:38
                               ETA: 00:57:37

################################################################################
                     [1m Learning iteration 972/2500 [0m                      

                       Computation: 43345 steps/s (collection: 2.170s, learning 0.098s)
             Mean action noise std: 2.71
          Mean value_function loss: 202.2686
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 74.4200
                       Mean reward: 160.70
               Mean episode length: 102.37
    Episode_Reward/reaching_object: 0.8802
     Episode_Reward/lifting_object: 32.6444
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0510
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 39.2500
--------------------------------------------------------------------------------
                   Total timesteps: 95649792
                    Iteration time: 2.27s
                      Time elapsed: 00:36:40
                               ETA: 00:57:35

################################################################################
                     [1m Learning iteration 973/2500 [0m                      

                       Computation: 42731 steps/s (collection: 2.183s, learning 0.117s)
             Mean action noise std: 2.71
          Mean value_function loss: 209.3536
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 74.4231
                       Mean reward: 154.01
               Mean episode length: 98.76
    Episode_Reward/reaching_object: 0.8915
     Episode_Reward/lifting_object: 32.4636
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0299
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 37.4167
--------------------------------------------------------------------------------
                   Total timesteps: 95748096
                    Iteration time: 2.30s
                      Time elapsed: 00:36:42
                               ETA: 00:57:33

################################################################################
                     [1m Learning iteration 974/2500 [0m                      

                       Computation: 43448 steps/s (collection: 2.147s, learning 0.116s)
             Mean action noise std: 2.71
          Mean value_function loss: 218.6511
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.4256
                       Mean reward: 154.49
               Mean episode length: 101.04
    Episode_Reward/reaching_object: 0.9041
     Episode_Reward/lifting_object: 33.3333
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0509
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 37.5417
--------------------------------------------------------------------------------
                   Total timesteps: 95846400
                    Iteration time: 2.26s
                      Time elapsed: 00:36:44
                               ETA: 00:57:30

################################################################################
                     [1m Learning iteration 975/2500 [0m                      

                       Computation: 43323 steps/s (collection: 2.158s, learning 0.111s)
             Mean action noise std: 2.71
          Mean value_function loss: 225.6016
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 74.4306
                       Mean reward: 167.36
               Mean episode length: 104.36
    Episode_Reward/reaching_object: 0.9125
     Episode_Reward/lifting_object: 33.2162
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0527
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 39.0000
--------------------------------------------------------------------------------
                   Total timesteps: 95944704
                    Iteration time: 2.27s
                      Time elapsed: 00:36:47
                               ETA: 00:57:28

################################################################################
                     [1m Learning iteration 976/2500 [0m                      

                       Computation: 42664 steps/s (collection: 2.175s, learning 0.129s)
             Mean action noise std: 2.71
          Mean value_function loss: 212.1041
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.4337
                       Mean reward: 161.17
               Mean episode length: 105.49
    Episode_Reward/reaching_object: 0.8873
     Episode_Reward/lifting_object: 32.4849
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0297
          Episode_Reward/joint_vel: -0.0504
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 36.7917
--------------------------------------------------------------------------------
                   Total timesteps: 96043008
                    Iteration time: 2.30s
                      Time elapsed: 00:36:49
                               ETA: 00:57:26

################################################################################
                     [1m Learning iteration 977/2500 [0m                      

                       Computation: 43524 steps/s (collection: 2.154s, learning 0.105s)
             Mean action noise std: 2.71
          Mean value_function loss: 241.5102
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 74.4393
                       Mean reward: 179.16
               Mean episode length: 104.59
    Episode_Reward/reaching_object: 0.9131
     Episode_Reward/lifting_object: 33.6160
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 40.5000
--------------------------------------------------------------------------------
                   Total timesteps: 96141312
                    Iteration time: 2.26s
                      Time elapsed: 00:36:51
                               ETA: 00:57:24

################################################################################
                     [1m Learning iteration 978/2500 [0m                      

                       Computation: 42801 steps/s (collection: 2.187s, learning 0.109s)
             Mean action noise std: 2.71
          Mean value_function loss: 220.7383
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 74.4459
                       Mean reward: 168.01
               Mean episode length: 103.08
    Episode_Reward/reaching_object: 0.8994
     Episode_Reward/lifting_object: 33.2325
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 38.0417
--------------------------------------------------------------------------------
                   Total timesteps: 96239616
                    Iteration time: 2.30s
                      Time elapsed: 00:36:54
                               ETA: 00:57:21

################################################################################
                     [1m Learning iteration 979/2500 [0m                      

                       Computation: 43336 steps/s (collection: 2.158s, learning 0.111s)
             Mean action noise std: 2.71
          Mean value_function loss: 211.7703
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 74.4508
                       Mean reward: 176.76
               Mean episode length: 108.51
    Episode_Reward/reaching_object: 0.8883
     Episode_Reward/lifting_object: 32.8195
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0516
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.7500
     Episode_Termination/robot_out: 36.5833
--------------------------------------------------------------------------------
                   Total timesteps: 96337920
                    Iteration time: 2.27s
                      Time elapsed: 00:36:56
                               ETA: 00:57:19

################################################################################
                     [1m Learning iteration 980/2500 [0m                      

                       Computation: 43352 steps/s (collection: 2.158s, learning 0.110s)
             Mean action noise std: 2.71
          Mean value_function loss: 223.5494
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 74.4555
                       Mean reward: 168.97
               Mean episode length: 103.22
    Episode_Reward/reaching_object: 0.8873
     Episode_Reward/lifting_object: 33.0134
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0301
          Episode_Reward/joint_vel: -0.0519
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 36.4583
--------------------------------------------------------------------------------
                   Total timesteps: 96436224
                    Iteration time: 2.27s
                      Time elapsed: 00:36:58
                               ETA: 00:57:17

################################################################################
                     [1m Learning iteration 981/2500 [0m                      

                       Computation: 43502 steps/s (collection: 2.170s, learning 0.090s)
             Mean action noise std: 2.71
          Mean value_function loss: 225.0680
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 74.4592
                       Mean reward: 172.80
               Mean episode length: 108.30
    Episode_Reward/reaching_object: 0.9089
     Episode_Reward/lifting_object: 33.8541
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 37.3333
--------------------------------------------------------------------------------
                   Total timesteps: 96534528
                    Iteration time: 2.26s
                      Time elapsed: 00:37:00
                               ETA: 00:57:15

################################################################################
                     [1m Learning iteration 982/2500 [0m                      

                       Computation: 41213 steps/s (collection: 2.288s, learning 0.097s)
             Mean action noise std: 2.71
          Mean value_function loss: 205.9569
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 74.4631
                       Mean reward: 183.44
               Mean episode length: 107.63
    Episode_Reward/reaching_object: 0.9236
     Episode_Reward/lifting_object: 34.8485
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 38.7917
--------------------------------------------------------------------------------
                   Total timesteps: 96632832
                    Iteration time: 2.39s
                      Time elapsed: 00:37:03
                               ETA: 00:57:13

################################################################################
                     [1m Learning iteration 983/2500 [0m                      

                       Computation: 38065 steps/s (collection: 2.442s, learning 0.140s)
             Mean action noise std: 2.71
          Mean value_function loss: 205.0005
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 74.4654
                       Mean reward: 158.80
               Mean episode length: 98.02
    Episode_Reward/reaching_object: 0.9041
     Episode_Reward/lifting_object: 34.2547
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 35.1250
--------------------------------------------------------------------------------
                   Total timesteps: 96731136
                    Iteration time: 2.58s
                      Time elapsed: 00:37:05
                               ETA: 00:57:11

################################################################################
                     [1m Learning iteration 984/2500 [0m                      

                       Computation: 41495 steps/s (collection: 2.228s, learning 0.141s)
             Mean action noise std: 2.71
          Mean value_function loss: 219.0342
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 74.4680
                       Mean reward: 186.46
               Mean episode length: 115.11
    Episode_Reward/reaching_object: 0.8945
     Episode_Reward/lifting_object: 33.9394
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 39.7500
--------------------------------------------------------------------------------
                   Total timesteps: 96829440
                    Iteration time: 2.37s
                      Time elapsed: 00:37:08
                               ETA: 00:57:09

################################################################################
                     [1m Learning iteration 985/2500 [0m                      

                       Computation: 39467 steps/s (collection: 2.354s, learning 0.137s)
             Mean action noise std: 2.71
          Mean value_function loss: 214.4845
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 74.4714
                       Mean reward: 167.02
               Mean episode length: 101.23
    Episode_Reward/reaching_object: 0.9079
     Episode_Reward/lifting_object: 34.2762
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0543
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 37.7083
--------------------------------------------------------------------------------
                   Total timesteps: 96927744
                    Iteration time: 2.49s
                      Time elapsed: 00:37:10
                               ETA: 00:57:07

################################################################################
                     [1m Learning iteration 986/2500 [0m                      

                       Computation: 42834 steps/s (collection: 2.199s, learning 0.096s)
             Mean action noise std: 2.71
          Mean value_function loss: 233.5041
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 74.4759
                       Mean reward: 165.50
               Mean episode length: 100.51
    Episode_Reward/reaching_object: 0.8940
     Episode_Reward/lifting_object: 34.4554
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 38.6250
--------------------------------------------------------------------------------
                   Total timesteps: 97026048
                    Iteration time: 2.29s
                      Time elapsed: 00:37:12
                               ETA: 00:57:05

################################################################################
                     [1m Learning iteration 987/2500 [0m                      

                       Computation: 41181 steps/s (collection: 2.273s, learning 0.114s)
             Mean action noise std: 2.71
          Mean value_function loss: 222.3101
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 74.4792
                       Mean reward: 180.04
               Mean episode length: 109.02
    Episode_Reward/reaching_object: 0.8658
     Episode_Reward/lifting_object: 33.0776
      Episode_Reward/object_height: 0.0022
        Episode_Reward/action_rate: -0.0300
          Episode_Reward/joint_vel: -0.0524
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.5417
     Episode_Termination/robot_out: 38.2500
--------------------------------------------------------------------------------
                   Total timesteps: 97124352
                    Iteration time: 2.39s
                      Time elapsed: 00:37:15
                               ETA: 00:57:03

################################################################################
                     [1m Learning iteration 988/2500 [0m                      

                       Computation: 41685 steps/s (collection: 2.252s, learning 0.107s)
             Mean action noise std: 2.71
          Mean value_function loss: 227.9514
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 74.4829
                       Mean reward: 184.87
               Mean episode length: 108.81
    Episode_Reward/reaching_object: 0.9069
     Episode_Reward/lifting_object: 34.5594
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0549
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 38.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97222656
                    Iteration time: 2.36s
                      Time elapsed: 00:37:17
                               ETA: 00:57:00

################################################################################
                     [1m Learning iteration 989/2500 [0m                      

                       Computation: 40788 steps/s (collection: 2.320s, learning 0.091s)
             Mean action noise std: 2.71
          Mean value_function loss: 215.8443
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 74.4846
                       Mean reward: 162.49
               Mean episode length: 100.84
    Episode_Reward/reaching_object: 0.8854
     Episode_Reward/lifting_object: 34.1312
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0304
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 37.7917
--------------------------------------------------------------------------------
                   Total timesteps: 97320960
                    Iteration time: 2.41s
                      Time elapsed: 00:37:20
                               ETA: 00:56:58

################################################################################
                     [1m Learning iteration 990/2500 [0m                      

                       Computation: 40643 steps/s (collection: 2.311s, learning 0.108s)
             Mean action noise std: 2.71
          Mean value_function loss: 201.8175
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 74.4859
                       Mean reward: 195.70
               Mean episode length: 113.37
    Episode_Reward/reaching_object: 0.8967
     Episode_Reward/lifting_object: 34.5631
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 37.0000
--------------------------------------------------------------------------------
                   Total timesteps: 97419264
                    Iteration time: 2.42s
                      Time elapsed: 00:37:22
                               ETA: 00:56:56

################################################################################
                     [1m Learning iteration 991/2500 [0m                      

                       Computation: 40390 steps/s (collection: 2.344s, learning 0.090s)
             Mean action noise std: 2.71
          Mean value_function loss: 211.7625
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 74.4862
                       Mean reward: 179.67
               Mean episode length: 108.06
    Episode_Reward/reaching_object: 0.8994
     Episode_Reward/lifting_object: 34.6064
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 36.0417
--------------------------------------------------------------------------------
                   Total timesteps: 97517568
                    Iteration time: 2.43s
                      Time elapsed: 00:37:24
                               ETA: 00:56:54

################################################################################
                     [1m Learning iteration 992/2500 [0m                      

                       Computation: 42037 steps/s (collection: 2.225s, learning 0.114s)
             Mean action noise std: 2.71
          Mean value_function loss: 243.1977
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.4879
                       Mean reward: 183.51
               Mean episode length: 109.73
    Episode_Reward/reaching_object: 0.9131
     Episode_Reward/lifting_object: 34.8885
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 36.8333
--------------------------------------------------------------------------------
                   Total timesteps: 97615872
                    Iteration time: 2.34s
                      Time elapsed: 00:37:27
                               ETA: 00:56:52

################################################################################
                     [1m Learning iteration 993/2500 [0m                      

                       Computation: 38434 steps/s (collection: 2.402s, learning 0.156s)
             Mean action noise std: 2.71
          Mean value_function loss: 234.7412
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.4904
                       Mean reward: 180.35
               Mean episode length: 107.50
    Episode_Reward/reaching_object: 0.9141
     Episode_Reward/lifting_object: 35.0673
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 38.2083
--------------------------------------------------------------------------------
                   Total timesteps: 97714176
                    Iteration time: 2.56s
                      Time elapsed: 00:37:29
                               ETA: 00:56:50

################################################################################
                     [1m Learning iteration 994/2500 [0m                      

                       Computation: 39170 steps/s (collection: 2.357s, learning 0.153s)
             Mean action noise std: 2.71
          Mean value_function loss: 222.0000
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 74.4937
                       Mean reward: 203.50
               Mean episode length: 118.22
    Episode_Reward/reaching_object: 0.9369
     Episode_Reward/lifting_object: 36.1217
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 38.5417
--------------------------------------------------------------------------------
                   Total timesteps: 97812480
                    Iteration time: 2.51s
                      Time elapsed: 00:37:32
                               ETA: 00:56:49

################################################################################
                     [1m Learning iteration 995/2500 [0m                      

                       Computation: 37936 steps/s (collection: 2.449s, learning 0.143s)
             Mean action noise std: 2.72
          Mean value_function loss: 205.2065
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.4959
                       Mean reward: 175.26
               Mean episode length: 105.23
    Episode_Reward/reaching_object: 0.9186
     Episode_Reward/lifting_object: 35.5178
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 37.3750
--------------------------------------------------------------------------------
                   Total timesteps: 97910784
                    Iteration time: 2.59s
                      Time elapsed: 00:37:34
                               ETA: 00:56:47

################################################################################
                     [1m Learning iteration 996/2500 [0m                      

                       Computation: 38417 steps/s (collection: 2.399s, learning 0.160s)
             Mean action noise std: 2.72
          Mean value_function loss: 219.6810
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 74.5010
                       Mean reward: 187.50
               Mean episode length: 109.67
    Episode_Reward/reaching_object: 0.9066
     Episode_Reward/lifting_object: 35.7587
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 35.8333
--------------------------------------------------------------------------------
                   Total timesteps: 98009088
                    Iteration time: 2.56s
                      Time elapsed: 00:37:37
                               ETA: 00:56:45

################################################################################
                     [1m Learning iteration 997/2500 [0m                      

                       Computation: 37043 steps/s (collection: 2.463s, learning 0.191s)
             Mean action noise std: 2.72
          Mean value_function loss: 214.8371
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.5050
                       Mean reward: 175.82
               Mean episode length: 103.20
    Episode_Reward/reaching_object: 0.9092
     Episode_Reward/lifting_object: 35.4500
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0534
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 38.2500
--------------------------------------------------------------------------------
                   Total timesteps: 98107392
                    Iteration time: 2.65s
                      Time elapsed: 00:37:40
                               ETA: 00:56:43

################################################################################
                     [1m Learning iteration 998/2500 [0m                      

                       Computation: 39032 steps/s (collection: 2.377s, learning 0.142s)
             Mean action noise std: 2.72
          Mean value_function loss: 215.8445
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 74.5083
                       Mean reward: 179.23
               Mean episode length: 107.05
    Episode_Reward/reaching_object: 0.9133
     Episode_Reward/lifting_object: 35.0026
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 38.5833
--------------------------------------------------------------------------------
                   Total timesteps: 98205696
                    Iteration time: 2.52s
                      Time elapsed: 00:37:42
                               ETA: 00:56:41

################################################################################
                     [1m Learning iteration 999/2500 [0m                      

                       Computation: 38633 steps/s (collection: 2.444s, learning 0.101s)
             Mean action noise std: 2.72
          Mean value_function loss: 220.5476
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 74.5156
                       Mean reward: 177.83
               Mean episode length: 107.46
    Episode_Reward/reaching_object: 0.9566
     Episode_Reward/lifting_object: 37.0694
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 35.9583
--------------------------------------------------------------------------------
                   Total timesteps: 98304000
                    Iteration time: 2.54s
                      Time elapsed: 00:37:45
                               ETA: 00:56:40

################################################################################
                     [1m Learning iteration 1000/2500 [0m                     

                       Computation: 13563 steps/s (collection: 7.130s, learning 0.117s)
             Mean action noise std: 2.72
          Mean value_function loss: 242.6124
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 74.5204
                       Mean reward: 173.68
               Mean episode length: 102.11
    Episode_Reward/reaching_object: 0.9207
     Episode_Reward/lifting_object: 36.0401
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0314
          Episode_Reward/joint_vel: -0.0548
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 39.6667
--------------------------------------------------------------------------------
                   Total timesteps: 98402304
                    Iteration time: 7.25s
                      Time elapsed: 00:37:52
                               ETA: 00:56:45

################################################################################
                     [1m Learning iteration 1001/2500 [0m                     

                       Computation: 13560 steps/s (collection: 7.134s, learning 0.116s)
             Mean action noise std: 2.72
          Mean value_function loss: 217.9225
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 74.5232
                       Mean reward: 185.71
               Mean episode length: 109.40
    Episode_Reward/reaching_object: 0.9005
     Episode_Reward/lifting_object: 35.1787
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 35.9583
--------------------------------------------------------------------------------
                   Total timesteps: 98500608
                    Iteration time: 7.25s
                      Time elapsed: 00:37:59
                               ETA: 00:56:50

################################################################################
                     [1m Learning iteration 1002/2500 [0m                     

                       Computation: 13343 steps/s (collection: 7.232s, learning 0.135s)
             Mean action noise std: 2.72
          Mean value_function loss: 238.5704
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 74.5263
                       Mean reward: 180.41
               Mean episode length: 106.10
    Episode_Reward/reaching_object: 0.9190
     Episode_Reward/lifting_object: 35.8277
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 39.4167
--------------------------------------------------------------------------------
                   Total timesteps: 98598912
                    Iteration time: 7.37s
                      Time elapsed: 00:38:07
                               ETA: 00:56:55

################################################################################
                     [1m Learning iteration 1003/2500 [0m                     

                       Computation: 13805 steps/s (collection: 6.973s, learning 0.148s)
             Mean action noise std: 2.72
          Mean value_function loss: 231.9609
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 74.5302
                       Mean reward: 198.25
               Mean episode length: 114.82
    Episode_Reward/reaching_object: 0.9233
     Episode_Reward/lifting_object: 36.3393
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 36.6667
--------------------------------------------------------------------------------
                   Total timesteps: 98697216
                    Iteration time: 7.12s
                      Time elapsed: 00:38:14
                               ETA: 00:57:00

################################################################################
                     [1m Learning iteration 1004/2500 [0m                     

                       Computation: 13596 steps/s (collection: 7.104s, learning 0.126s)
             Mean action noise std: 2.72
          Mean value_function loss: 222.6366
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 74.5316
                       Mean reward: 177.96
               Mean episode length: 104.52
    Episode_Reward/reaching_object: 0.8864
     Episode_Reward/lifting_object: 34.6881
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0536
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 38.8333
--------------------------------------------------------------------------------
                   Total timesteps: 98795520
                    Iteration time: 7.23s
                      Time elapsed: 00:38:21
                               ETA: 00:57:05

################################################################################
                     [1m Learning iteration 1005/2500 [0m                     

                       Computation: 13529 steps/s (collection: 7.140s, learning 0.126s)
             Mean action noise std: 2.72
          Mean value_function loss: 225.0954
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 74.5348
                       Mean reward: 178.90
               Mean episode length: 105.86
    Episode_Reward/reaching_object: 0.9177
     Episode_Reward/lifting_object: 36.1497
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 38.5417
--------------------------------------------------------------------------------
                   Total timesteps: 98893824
                    Iteration time: 7.27s
                      Time elapsed: 00:38:28
                               ETA: 00:57:10

################################################################################
                     [1m Learning iteration 1006/2500 [0m                     

                       Computation: 13833 steps/s (collection: 6.985s, learning 0.121s)
             Mean action noise std: 2.72
          Mean value_function loss: 226.7177
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 74.5371
                       Mean reward: 188.53
               Mean episode length: 109.79
    Episode_Reward/reaching_object: 0.8866
     Episode_Reward/lifting_object: 34.7536
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0307
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 37.4583
--------------------------------------------------------------------------------
                   Total timesteps: 98992128
                    Iteration time: 7.11s
                      Time elapsed: 00:38:35
                               ETA: 00:57:15

################################################################################
                     [1m Learning iteration 1007/2500 [0m                     

                       Computation: 13709 steps/s (collection: 7.047s, learning 0.124s)
             Mean action noise std: 2.72
          Mean value_function loss: 229.5296
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 74.5391
                       Mean reward: 177.02
               Mean episode length: 105.35
    Episode_Reward/reaching_object: 0.9096
     Episode_Reward/lifting_object: 35.6312
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 40.3750
--------------------------------------------------------------------------------
                   Total timesteps: 99090432
                    Iteration time: 7.17s
                      Time elapsed: 00:38:42
                               ETA: 00:57:20

################################################################################
                     [1m Learning iteration 1008/2500 [0m                     

                       Computation: 15533 steps/s (collection: 6.234s, learning 0.095s)
             Mean action noise std: 2.72
          Mean value_function loss: 241.0144
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 74.5452
                       Mean reward: 190.35
               Mean episode length: 108.67
    Episode_Reward/reaching_object: 0.8868
     Episode_Reward/lifting_object: 34.9786
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 0.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 38.9167
--------------------------------------------------------------------------------
                   Total timesteps: 99188736
                    Iteration time: 6.33s
                      Time elapsed: 00:38:49
                               ETA: 00:57:24

################################################################################
                     [1m Learning iteration 1009/2500 [0m                     

                       Computation: 41751 steps/s (collection: 2.203s, learning 0.152s)
             Mean action noise std: 2.72
          Mean value_function loss: 224.5504
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 74.5526
                       Mean reward: 177.65
               Mean episode length: 101.54
    Episode_Reward/reaching_object: 0.9053
     Episode_Reward/lifting_object: 35.4063
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0552
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 38.3333
--------------------------------------------------------------------------------
                   Total timesteps: 99287040
                    Iteration time: 2.35s
                      Time elapsed: 00:38:51
                               ETA: 00:57:22

################################################################################
                     [1m Learning iteration 1010/2500 [0m                     

                       Computation: 43298 steps/s (collection: 2.182s, learning 0.089s)
             Mean action noise std: 2.72
          Mean value_function loss: 219.8790
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 74.5575
                       Mean reward: 180.48
               Mean episode length: 104.15
    Episode_Reward/reaching_object: 0.8695
     Episode_Reward/lifting_object: 34.3039
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0532
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 36.4167
--------------------------------------------------------------------------------
                   Total timesteps: 99385344
                    Iteration time: 2.27s
                      Time elapsed: 00:38:53
                               ETA: 00:57:19

################################################################################
                     [1m Learning iteration 1011/2500 [0m                     

                       Computation: 42448 steps/s (collection: 2.219s, learning 0.097s)
             Mean action noise std: 2.72
          Mean value_function loss: 237.9461
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.5594
                       Mean reward: 177.90
               Mean episode length: 101.85
    Episode_Reward/reaching_object: 0.8934
     Episode_Reward/lifting_object: 34.9255
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0539
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 40.3333
--------------------------------------------------------------------------------
                   Total timesteps: 99483648
                    Iteration time: 2.32s
                      Time elapsed: 00:38:56
                               ETA: 00:57:17

################################################################################
                     [1m Learning iteration 1012/2500 [0m                     

                       Computation: 42764 steps/s (collection: 2.186s, learning 0.113s)
             Mean action noise std: 2.72
          Mean value_function loss: 228.5212
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 74.5620
                       Mean reward: 171.90
               Mean episode length: 105.45
    Episode_Reward/reaching_object: 0.8960
     Episode_Reward/lifting_object: 34.6990
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0540
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 39.6250
--------------------------------------------------------------------------------
                   Total timesteps: 99581952
                    Iteration time: 2.30s
                      Time elapsed: 00:38:58
                               ETA: 00:57:15

################################################################################
                     [1m Learning iteration 1013/2500 [0m                     

                       Computation: 43189 steps/s (collection: 2.164s, learning 0.112s)
             Mean action noise std: 2.72
          Mean value_function loss: 220.0548
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 74.5654
                       Mean reward: 180.74
               Mean episode length: 105.68
    Episode_Reward/reaching_object: 0.9222
     Episode_Reward/lifting_object: 36.2270
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 37.2917
--------------------------------------------------------------------------------
                   Total timesteps: 99680256
                    Iteration time: 2.28s
                      Time elapsed: 00:39:00
                               ETA: 00:57:12

################################################################################
                     [1m Learning iteration 1014/2500 [0m                     

                       Computation: 42938 steps/s (collection: 2.200s, learning 0.089s)
             Mean action noise std: 2.72
          Mean value_function loss: 225.7518
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 74.5683
                       Mean reward: 181.81
               Mean episode length: 102.99
    Episode_Reward/reaching_object: 0.9088
     Episode_Reward/lifting_object: 35.8855
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0546
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 40.5000
--------------------------------------------------------------------------------
                   Total timesteps: 99778560
                    Iteration time: 2.29s
                      Time elapsed: 00:39:03
                               ETA: 00:57:10

################################################################################
                     [1m Learning iteration 1015/2500 [0m                     

                       Computation: 42871 steps/s (collection: 2.179s, learning 0.114s)
             Mean action noise std: 2.72
          Mean value_function loss: 219.1641
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 74.5709
                       Mean reward: 178.19
               Mean episode length: 100.00
    Episode_Reward/reaching_object: 0.8902
     Episode_Reward/lifting_object: 35.4236
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0308
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 38.1250
--------------------------------------------------------------------------------
                   Total timesteps: 99876864
                    Iteration time: 2.29s
                      Time elapsed: 00:39:05
                               ETA: 00:57:08

################################################################################
                     [1m Learning iteration 1016/2500 [0m                     

                       Computation: 42958 steps/s (collection: 2.181s, learning 0.107s)
             Mean action noise std: 2.72
          Mean value_function loss: 236.6755
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 74.5741
                       Mean reward: 184.61
               Mean episode length: 105.59
    Episode_Reward/reaching_object: 0.8976
     Episode_Reward/lifting_object: 35.7080
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0535
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 40.2083
--------------------------------------------------------------------------------
                   Total timesteps: 99975168
                    Iteration time: 2.29s
                      Time elapsed: 00:39:07
                               ETA: 00:57:05

################################################################################
                     [1m Learning iteration 1017/2500 [0m                     

                       Computation: 42804 steps/s (collection: 2.180s, learning 0.117s)
             Mean action noise std: 2.72
          Mean value_function loss: 225.6007
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.5788
                       Mean reward: 167.58
               Mean episode length: 97.44
    Episode_Reward/reaching_object: 0.8737
     Episode_Reward/lifting_object: 34.8301
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0530
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 38.8750
--------------------------------------------------------------------------------
                   Total timesteps: 100073472
                    Iteration time: 2.30s
                      Time elapsed: 00:39:09
                               ETA: 00:57:03

################################################################################
                     [1m Learning iteration 1018/2500 [0m                     

                       Computation: 42077 steps/s (collection: 2.233s, learning 0.104s)
             Mean action noise std: 2.72
          Mean value_function loss: 219.1836
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 74.5839
                       Mean reward: 173.38
               Mean episode length: 95.29
    Episode_Reward/reaching_object: 0.8703
     Episode_Reward/lifting_object: 35.1529
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0302
          Episode_Reward/joint_vel: -0.0521
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 37.0417
--------------------------------------------------------------------------------
                   Total timesteps: 100171776
                    Iteration time: 2.34s
                      Time elapsed: 00:39:12
                               ETA: 00:57:01

################################################################################
                     [1m Learning iteration 1019/2500 [0m                     

                       Computation: 43950 steps/s (collection: 2.139s, learning 0.098s)
             Mean action noise std: 2.72
          Mean value_function loss: 227.3817
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 74.5884
                       Mean reward: 191.94
               Mean episode length: 104.19
    Episode_Reward/reaching_object: 0.9028
     Episode_Reward/lifting_object: 36.2660
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0541
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 38.7917
--------------------------------------------------------------------------------
                   Total timesteps: 100270080
                    Iteration time: 2.24s
                      Time elapsed: 00:39:14
                               ETA: 00:56:58

################################################################################
                     [1m Learning iteration 1020/2500 [0m                     

                       Computation: 42107 steps/s (collection: 2.201s, learning 0.134s)
             Mean action noise std: 2.72
          Mean value_function loss: 234.5577
               Mean surrogate loss: -0.0049
                 Mean entropy loss: 74.5919
                       Mean reward: 196.08
               Mean episode length: 113.72
    Episode_Reward/reaching_object: 0.9016
     Episode_Reward/lifting_object: 36.1188
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0554
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 37.1667
--------------------------------------------------------------------------------
                   Total timesteps: 100368384
                    Iteration time: 2.33s
                      Time elapsed: 00:39:16
                               ETA: 00:56:56

################################################################################
                     [1m Learning iteration 1021/2500 [0m                     

                       Computation: 43348 steps/s (collection: 2.165s, learning 0.103s)
             Mean action noise std: 2.73
          Mean value_function loss: 220.9337
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 74.5972
                       Mean reward: 191.95
               Mean episode length: 108.72
    Episode_Reward/reaching_object: 0.9096
     Episode_Reward/lifting_object: 36.8291
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0562
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 36.7083
--------------------------------------------------------------------------------
                   Total timesteps: 100466688
                    Iteration time: 2.27s
                      Time elapsed: 00:39:19
                               ETA: 00:56:54

################################################################################
                     [1m Learning iteration 1022/2500 [0m                     

                       Computation: 42135 steps/s (collection: 2.228s, learning 0.106s)
             Mean action noise std: 2.73
          Mean value_function loss: 225.8018
               Mean surrogate loss: 0.0078
                 Mean entropy loss: 74.6024
                       Mean reward: 187.17
               Mean episode length: 107.77
    Episode_Reward/reaching_object: 0.9171
     Episode_Reward/lifting_object: 36.9943
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 36.5417
--------------------------------------------------------------------------------
                   Total timesteps: 100564992
                    Iteration time: 2.33s
                      Time elapsed: 00:39:21
                               ETA: 00:56:51

################################################################################
                     [1m Learning iteration 1023/2500 [0m                     

                       Computation: 42735 steps/s (collection: 2.199s, learning 0.101s)
             Mean action noise std: 2.73
          Mean value_function loss: 225.8995
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 74.6040
                       Mean reward: 183.37
               Mean episode length: 106.34
    Episode_Reward/reaching_object: 0.9338
     Episode_Reward/lifting_object: 37.1214
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 37.0000
--------------------------------------------------------------------------------
                   Total timesteps: 100663296
                    Iteration time: 2.30s
                      Time elapsed: 00:39:23
                               ETA: 00:56:49

################################################################################
                     [1m Learning iteration 1024/2500 [0m                     

                       Computation: 42968 steps/s (collection: 2.199s, learning 0.089s)
             Mean action noise std: 2.73
          Mean value_function loss: 253.0735
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 74.6061
                       Mean reward: 200.00
               Mean episode length: 116.97
    Episode_Reward/reaching_object: 0.9427
     Episode_Reward/lifting_object: 37.7064
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 38.8333
--------------------------------------------------------------------------------
                   Total timesteps: 100761600
                    Iteration time: 2.29s
                      Time elapsed: 00:39:26
                               ETA: 00:56:47

################################################################################
                     [1m Learning iteration 1025/2500 [0m                     

                       Computation: 43140 steps/s (collection: 2.177s, learning 0.102s)
             Mean action noise std: 2.73
          Mean value_function loss: 234.4760
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 74.6106
                       Mean reward: 201.73
               Mean episode length: 110.89
    Episode_Reward/reaching_object: 0.9148
     Episode_Reward/lifting_object: 37.0365
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 39.7917
--------------------------------------------------------------------------------
                   Total timesteps: 100859904
                    Iteration time: 2.28s
                      Time elapsed: 00:39:28
                               ETA: 00:56:44

################################################################################
                     [1m Learning iteration 1026/2500 [0m                     

                       Computation: 43708 steps/s (collection: 2.151s, learning 0.098s)
             Mean action noise std: 2.73
          Mean value_function loss: 247.2308
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 74.6161
                       Mean reward: 175.93
               Mean episode length: 100.73
    Episode_Reward/reaching_object: 0.9128
     Episode_Reward/lifting_object: 36.6207
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0321
          Episode_Reward/joint_vel: -0.0564
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 40.5000
--------------------------------------------------------------------------------
                   Total timesteps: 100958208
                    Iteration time: 2.25s
                      Time elapsed: 00:39:30
                               ETA: 00:56:42

################################################################################
                     [1m Learning iteration 1027/2500 [0m                     

                       Computation: 42812 steps/s (collection: 2.202s, learning 0.095s)
             Mean action noise std: 2.73
          Mean value_function loss: 234.3896
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 74.6183
                       Mean reward: 191.86
               Mean episode length: 107.67
    Episode_Reward/reaching_object: 0.9124
     Episode_Reward/lifting_object: 37.2872
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 38.3750
--------------------------------------------------------------------------------
                   Total timesteps: 101056512
                    Iteration time: 2.30s
                      Time elapsed: 00:39:32
                               ETA: 00:56:40

################################################################################
                     [1m Learning iteration 1028/2500 [0m                     

                       Computation: 42710 steps/s (collection: 2.201s, learning 0.101s)
             Mean action noise std: 2.73
          Mean value_function loss: 245.5356
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 74.6230
                       Mean reward: 188.16
               Mean episode length: 107.57
    Episode_Reward/reaching_object: 0.8789
     Episode_Reward/lifting_object: 35.6096
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0309
          Episode_Reward/joint_vel: -0.0537
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 37.7917
--------------------------------------------------------------------------------
                   Total timesteps: 101154816
                    Iteration time: 2.30s
                      Time elapsed: 00:39:35
                               ETA: 00:56:37

################################################################################
                     [1m Learning iteration 1029/2500 [0m                     

                       Computation: 42832 steps/s (collection: 2.197s, learning 0.098s)
             Mean action noise std: 2.73
          Mean value_function loss: 235.4347
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.6273
                       Mean reward: 188.46
               Mean episode length: 108.01
    Episode_Reward/reaching_object: 0.9250
     Episode_Reward/lifting_object: 37.4796
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0556
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 38.3750
--------------------------------------------------------------------------------
                   Total timesteps: 101253120
                    Iteration time: 2.30s
                      Time elapsed: 00:39:37
                               ETA: 00:56:35

################################################################################
                     [1m Learning iteration 1030/2500 [0m                     

                       Computation: 42953 steps/s (collection: 2.194s, learning 0.095s)
             Mean action noise std: 2.73
          Mean value_function loss: 249.7904
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 74.6314
                       Mean reward: 166.13
               Mean episode length: 94.64
    Episode_Reward/reaching_object: 0.8651
     Episode_Reward/lifting_object: 35.3065
      Episode_Reward/object_height: 0.0023
        Episode_Reward/action_rate: -0.0306
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 40.2083
--------------------------------------------------------------------------------
                   Total timesteps: 101351424
                    Iteration time: 2.29s
                      Time elapsed: 00:39:39
                               ETA: 00:56:33

################################################################################
                     [1m Learning iteration 1031/2500 [0m                     

                       Computation: 43005 steps/s (collection: 2.191s, learning 0.095s)
             Mean action noise std: 2.73
          Mean value_function loss: 222.2329
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 74.6357
                       Mean reward: 195.78
               Mean episode length: 105.32
    Episode_Reward/reaching_object: 0.9018
     Episode_Reward/lifting_object: 36.8188
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0319
          Episode_Reward/joint_vel: -0.0560
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 38.1250
--------------------------------------------------------------------------------
                   Total timesteps: 101449728
                    Iteration time: 2.29s
                      Time elapsed: 00:39:42
                               ETA: 00:56:30

################################################################################
                     [1m Learning iteration 1032/2500 [0m                     

                       Computation: 43003 steps/s (collection: 2.189s, learning 0.097s)
             Mean action noise std: 2.73
          Mean value_function loss: 241.7801
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 74.6394
                       Mean reward: 185.08
               Mean episode length: 101.60
    Episode_Reward/reaching_object: 0.8862
     Episode_Reward/lifting_object: 36.4288
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0311
          Episode_Reward/joint_vel: -0.0538
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 39.5417
--------------------------------------------------------------------------------
                   Total timesteps: 101548032
                    Iteration time: 2.29s
                      Time elapsed: 00:39:44
                               ETA: 00:56:28

################################################################################
                     [1m Learning iteration 1033/2500 [0m                     

                       Computation: 42931 steps/s (collection: 2.204s, learning 0.086s)
             Mean action noise std: 2.73
          Mean value_function loss: 245.1266
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 74.6424
                       Mean reward: 202.90
               Mean episode length: 108.26
    Episode_Reward/reaching_object: 0.8784
     Episode_Reward/lifting_object: 35.9066
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0310
          Episode_Reward/joint_vel: -0.0544
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 38.6667
--------------------------------------------------------------------------------
                   Total timesteps: 101646336
                    Iteration time: 2.29s
                      Time elapsed: 00:39:46
                               ETA: 00:56:26

################################################################################
                     [1m Learning iteration 1034/2500 [0m                     

                       Computation: 42382 steps/s (collection: 2.178s, learning 0.141s)
             Mean action noise std: 2.73
          Mean value_function loss: 245.0176
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.6454
                       Mean reward: 190.80
               Mean episode length: 104.63
    Episode_Reward/reaching_object: 0.8912
     Episode_Reward/lifting_object: 37.2393
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0547
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 37.9167
--------------------------------------------------------------------------------
                   Total timesteps: 101744640
                    Iteration time: 2.32s
                      Time elapsed: 00:39:48
                               ETA: 00:56:23

################################################################################
                     [1m Learning iteration 1035/2500 [0m                     

                       Computation: 43352 steps/s (collection: 2.165s, learning 0.102s)
             Mean action noise std: 2.73
          Mean value_function loss: 245.9960
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 74.6512
                       Mean reward: 180.31
               Mean episode length: 100.27
    Episode_Reward/reaching_object: 0.8966
     Episode_Reward/lifting_object: 36.5710
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 37.6250
--------------------------------------------------------------------------------
                   Total timesteps: 101842944
                    Iteration time: 2.27s
                      Time elapsed: 00:39:51
                               ETA: 00:56:21

################################################################################
                     [1m Learning iteration 1036/2500 [0m                     

                       Computation: 42543 steps/s (collection: 2.192s, learning 0.119s)
             Mean action noise std: 2.73
          Mean value_function loss: 256.6406
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 74.6553
                       Mean reward: 191.85
               Mean episode length: 107.32
    Episode_Reward/reaching_object: 0.9093
     Episode_Reward/lifting_object: 37.5258
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0323
          Episode_Reward/joint_vel: -0.2256
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 39.0417
--------------------------------------------------------------------------------
                   Total timesteps: 101941248
                    Iteration time: 2.31s
                      Time elapsed: 00:39:53
                               ETA: 00:56:19

################################################################################
                     [1m Learning iteration 1037/2500 [0m                     

                       Computation: 43598 steps/s (collection: 2.165s, learning 0.090s)
             Mean action noise std: 2.73
          Mean value_function loss: 235.3209
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 74.6575
                       Mean reward: 189.96
               Mean episode length: 102.72
    Episode_Reward/reaching_object: 0.8991
     Episode_Reward/lifting_object: 37.2590
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0557
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 38.1250
--------------------------------------------------------------------------------
                   Total timesteps: 102039552
                    Iteration time: 2.25s
                      Time elapsed: 00:39:55
                               ETA: 00:56:16

################################################################################
                     [1m Learning iteration 1038/2500 [0m                     

                       Computation: 43125 steps/s (collection: 2.187s, learning 0.092s)
             Mean action noise std: 2.73
          Mean value_function loss: 242.2439
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 74.6600
                       Mean reward: 189.62
               Mean episode length: 105.49
    Episode_Reward/reaching_object: 0.9175
     Episode_Reward/lifting_object: 38.3813
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0569
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 38.1667
--------------------------------------------------------------------------------
                   Total timesteps: 102137856
                    Iteration time: 2.28s
                      Time elapsed: 00:39:58
                               ETA: 00:56:14

################################################################################
                     [1m Learning iteration 1039/2500 [0m                     

                       Computation: 43459 steps/s (collection: 2.167s, learning 0.095s)
             Mean action noise std: 2.73
          Mean value_function loss: 242.7555
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 74.6645
                       Mean reward: 186.13
               Mean episode length: 101.41
    Episode_Reward/reaching_object: 0.9067
     Episode_Reward/lifting_object: 37.5907
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 40.2083
--------------------------------------------------------------------------------
                   Total timesteps: 102236160
                    Iteration time: 2.26s
                      Time elapsed: 00:40:00
                               ETA: 00:56:12

################################################################################
                     [1m Learning iteration 1040/2500 [0m                     

                       Computation: 43357 steps/s (collection: 2.156s, learning 0.112s)
             Mean action noise std: 2.73
          Mean value_function loss: 239.5808
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 74.6677
                       Mean reward: 197.44
               Mean episode length: 109.36
    Episode_Reward/reaching_object: 0.8837
     Episode_Reward/lifting_object: 37.2270
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0555
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 38.7917
--------------------------------------------------------------------------------
                   Total timesteps: 102334464
                    Iteration time: 2.27s
                      Time elapsed: 00:40:02
                               ETA: 00:56:09

################################################################################
                     [1m Learning iteration 1041/2500 [0m                     

                       Computation: 42070 steps/s (collection: 2.181s, learning 0.155s)
             Mean action noise std: 2.73
          Mean value_function loss: 239.8248
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 74.6724
                       Mean reward: 188.87
               Mean episode length: 104.27
    Episode_Reward/reaching_object: 0.8860
     Episode_Reward/lifting_object: 37.3145
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0563
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 37.5000
--------------------------------------------------------------------------------
                   Total timesteps: 102432768
                    Iteration time: 2.34s
                      Time elapsed: 00:40:04
                               ETA: 00:56:07

################################################################################
                     [1m Learning iteration 1042/2500 [0m                     

                       Computation: 42800 steps/s (collection: 2.194s, learning 0.103s)
             Mean action noise std: 2.73
          Mean value_function loss: 242.1113
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 74.6792
                       Mean reward: 179.44
               Mean episode length: 97.16
    Episode_Reward/reaching_object: 0.8768
     Episode_Reward/lifting_object: 37.0305
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0313
          Episode_Reward/joint_vel: -0.0551
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 38.1250
--------------------------------------------------------------------------------
                   Total timesteps: 102531072
                    Iteration time: 2.30s
                      Time elapsed: 00:40:07
                               ETA: 00:56:05

################################################################################
                     [1m Learning iteration 1043/2500 [0m                     

                       Computation: 42944 steps/s (collection: 2.178s, learning 0.111s)
             Mean action noise std: 2.73
          Mean value_function loss: 255.7121
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 74.6854
                       Mean reward: 199.69
               Mean episode length: 109.49
    Episode_Reward/reaching_object: 0.9118
     Episode_Reward/lifting_object: 38.7518
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0324
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 41.0000
--------------------------------------------------------------------------------
                   Total timesteps: 102629376
                    Iteration time: 2.29s
                      Time elapsed: 00:40:09
                               ETA: 00:56:02

################################################################################
                     [1m Learning iteration 1044/2500 [0m                     

                       Computation: 42997 steps/s (collection: 2.187s, learning 0.100s)
             Mean action noise std: 2.73
          Mean value_function loss: 243.9461
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 74.6915
                       Mean reward: 189.44
               Mean episode length: 100.38
    Episode_Reward/reaching_object: 0.8893
     Episode_Reward/lifting_object: 37.8130
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0317
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 38.4167
--------------------------------------------------------------------------------
                   Total timesteps: 102727680
                    Iteration time: 2.29s
                      Time elapsed: 00:40:11
                               ETA: 00:56:00

################################################################################
                     [1m Learning iteration 1045/2500 [0m                     

                       Computation: 42959 steps/s (collection: 2.182s, learning 0.106s)
             Mean action noise std: 2.73
          Mean value_function loss: 253.4310
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 74.6933
                       Mean reward: 197.24
               Mean episode length: 106.31
    Episode_Reward/reaching_object: 0.8794
     Episode_Reward/lifting_object: 37.7012
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0312
          Episode_Reward/joint_vel: -0.0559
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 39.7917
--------------------------------------------------------------------------------
                   Total timesteps: 102825984
                    Iteration time: 2.29s
                      Time elapsed: 00:40:14
                               ETA: 00:55:58

################################################################################
                     [1m Learning iteration 1046/2500 [0m                     

                       Computation: 43734 steps/s (collection: 2.159s, learning 0.089s)
             Mean action noise std: 2.74
          Mean value_function loss: 268.9615
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 74.6968
                       Mean reward: 187.00
               Mean episode length: 103.04
    Episode_Reward/reaching_object: 0.8909
     Episode_Reward/lifting_object: 37.2803
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0316
          Episode_Reward/joint_vel: -0.0567
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 37.9167
--------------------------------------------------------------------------------
                   Total timesteps: 102924288
                    Iteration time: 2.25s
                      Time elapsed: 00:40:16
                               ETA: 00:55:55

################################################################################
                     [1m Learning iteration 1047/2500 [0m                     

                       Computation: 42734 steps/s (collection: 2.210s, learning 0.090s)
             Mean action noise std: 2.74
          Mean value_function loss: 251.1805
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 74.7023
                       Mean reward: 186.78
               Mean episode length: 100.91
    Episode_Reward/reaching_object: 0.8856
     Episode_Reward/lifting_object: 37.3708
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 0.1250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 38.2500
--------------------------------------------------------------------------------
                   Total timesteps: 103022592
                    Iteration time: 2.30s
                      Time elapsed: 00:40:18
                               ETA: 00:55:53

################################################################################
                     [1m Learning iteration 1048/2500 [0m                     

                       Computation: 42375 steps/s (collection: 2.192s, learning 0.128s)
             Mean action noise std: 2.74
          Mean value_function loss: 253.9520
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 74.7070
                       Mean reward: 188.34
               Mean episode length: 103.32
    Episode_Reward/reaching_object: 0.8964
     Episode_Reward/lifting_object: 38.2446
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0318
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 36.0417
--------------------------------------------------------------------------------
                   Total timesteps: 103120896
                    Iteration time: 2.32s
                      Time elapsed: 00:40:20
                               ETA: 00:55:51

################################################################################
                     [1m Learning iteration 1049/2500 [0m                     

                       Computation: 43035 steps/s (collection: 2.190s, learning 0.095s)
             Mean action noise std: 2.74
          Mean value_function loss: 258.0843
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 74.7126
                       Mean reward: 198.68
               Mean episode length: 108.99
    Episode_Reward/reaching_object: 0.9045
     Episode_Reward/lifting_object: 38.3104
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0320
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 40.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103219200
                    Iteration time: 2.28s
                      Time elapsed: 00:40:23
                               ETA: 00:55:48

################################################################################
                     [1m Learning iteration 1050/2500 [0m                     

                       Computation: 43396 steps/s (collection: 2.170s, learning 0.095s)
             Mean action noise std: 2.74
          Mean value_function loss: 258.9604
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 74.7189
                       Mean reward: 187.39
               Mean episode length: 107.47
    Episode_Reward/reaching_object: 0.9242
     Episode_Reward/lifting_object: 38.7690
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0328
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 37.5000
--------------------------------------------------------------------------------
                   Total timesteps: 103317504
                    Iteration time: 2.27s
                      Time elapsed: 00:40:25
                               ETA: 00:55:46

################################################################################
                     [1m Learning iteration 1051/2500 [0m                     

                       Computation: 42319 steps/s (collection: 2.210s, learning 0.113s)
             Mean action noise std: 2.74
          Mean value_function loss: 246.9780
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 74.7231
                       Mean reward: 185.05
               Mean episode length: 99.04
    Episode_Reward/reaching_object: 0.9040
     Episode_Reward/lifting_object: 38.2371
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 0.1667
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 39.2500
--------------------------------------------------------------------------------
                   Total timesteps: 103415808
                    Iteration time: 2.32s
                      Time elapsed: 00:40:27
                               ETA: 00:55:44

################################################################################
                     [1m Learning iteration 1052/2500 [0m                     

                       Computation: 42554 steps/s (collection: 2.210s, learning 0.100s)
             Mean action noise std: 2.74
          Mean value_function loss: 262.5889
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 74.7268
                       Mean reward: 203.24
               Mean episode length: 106.88
    Episode_Reward/reaching_object: 0.9286
     Episode_Reward/lifting_object: 39.6544
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0330
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 37.6250
--------------------------------------------------------------------------------
                   Total timesteps: 103514112
                    Iteration time: 2.31s
                      Time elapsed: 00:40:30
                               ETA: 00:55:41

################################################################################
                     [1m Learning iteration 1053/2500 [0m                     

                       Computation: 43254 steps/s (collection: 2.176s, learning 0.097s)
             Mean action noise std: 2.74
          Mean value_function loss: 261.2383
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 74.7323
                       Mean reward: 200.87
               Mean episode length: 108.56
    Episode_Reward/reaching_object: 0.9161
     Episode_Reward/lifting_object: 38.8626
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0326
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 38.0833
--------------------------------------------------------------------------------
                   Total timesteps: 103612416
                    Iteration time: 2.27s
                      Time elapsed: 00:40:32
                               ETA: 00:55:39

################################################################################
                     [1m Learning iteration 1054/2500 [0m                     

                       Computation: 41288 steps/s (collection: 2.258s, learning 0.123s)
             Mean action noise std: 2.74
          Mean value_function loss: 269.9703
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 74.7376
                       Mean reward: 200.20
               Mean episode length: 104.42
    Episode_Reward/reaching_object: 0.8786
     Episode_Reward/lifting_object: 37.4569
      Episode_Reward/object_height: 0.0024
        Episode_Reward/action_rate: -0.0315
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 38.0000
--------------------------------------------------------------------------------
                   Total timesteps: 103710720
                    Iteration time: 2.38s
                      Time elapsed: 00:40:34
                               ETA: 00:55:37

################################################################################
                     [1m Learning iteration 1055/2500 [0m                     

                       Computation: 37568 steps/s (collection: 2.484s, learning 0.133s)
             Mean action noise std: 2.74
          Mean value_function loss: 253.4343
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 74.7426
                       Mean reward: 208.23
               Mean episode length: 106.52
    Episode_Reward/reaching_object: 0.8999
     Episode_Reward/lifting_object: 38.6033
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0322
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 38.1250
--------------------------------------------------------------------------------
                   Total timesteps: 103809024
                    Iteration time: 2.62s
                      Time elapsed: 00:40:37
                               ETA: 00:55:35

################################################################################
                     [1m Learning iteration 1056/2500 [0m                     

                       Computation: 38460 steps/s (collection: 2.399s, learning 0.157s)
             Mean action noise std: 2.74
          Mean value_function loss: 261.1355
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 74.7460
                       Mean reward: 201.88
               Mean episode length: 106.36
    Episode_Reward/reaching_object: 0.9144
     Episode_Reward/lifting_object: 39.3133
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0325
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 38.1667
--------------------------------------------------------------------------------
                   Total timesteps: 103907328
                    Iteration time: 2.56s
                      Time elapsed: 00:40:39
                               ETA: 00:55:33

################################################################################
                     [1m Learning iteration 1057/2500 [0m                     

                       Computation: 41269 steps/s (collection: 2.269s, learning 0.113s)
             Mean action noise std: 2.74
          Mean value_function loss: 248.2580
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 74.7532
                       Mean reward: 194.71
               Mean episode length: 104.82
    Episode_Reward/reaching_object: 0.9342
     Episode_Reward/lifting_object: 39.5005
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0331
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.4167
     Episode_Termination/robot_out: 36.1250
--------------------------------------------------------------------------------
                   Total timesteps: 104005632
                    Iteration time: 2.38s
                      Time elapsed: 00:40:42
                               ETA: 00:55:31

################################################################################
                     [1m Learning iteration 1058/2500 [0m                     

                       Computation: 42782 steps/s (collection: 2.190s, learning 0.108s)
             Mean action noise std: 2.74
          Mean value_function loss: 238.6252
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 74.7586
                       Mean reward: 208.00
               Mean episode length: 110.86
    Episode_Reward/reaching_object: 0.9422
     Episode_Reward/lifting_object: 40.3708
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 35.0417
--------------------------------------------------------------------------------
                   Total timesteps: 104103936
                    Iteration time: 2.30s
                      Time elapsed: 00:40:44
                               ETA: 00:55:28

################################################################################
                     [1m Learning iteration 1059/2500 [0m                     

                       Computation: 43093 steps/s (collection: 2.194s, learning 0.088s)
             Mean action noise std: 2.74
          Mean value_function loss: 258.2534
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 74.7613
                       Mean reward: 196.68
               Mean episode length: 105.15
    Episode_Reward/reaching_object: 0.9222
     Episode_Reward/lifting_object: 39.4273
      Episode_Reward/object_height: 0.0025
        Episode_Reward/action_rate: -0.0327
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 0.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 38.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104202240
                    Iteration time: 2.28s
                      Time elapsed: 00:40:46
                               ETA: 00:55:26

################################################################################
                     [1m Learning iteration 1060/2500 [0m                     

                       Computation: 42397 steps/s (collection: 2.221s, learning 0.098s)
             Mean action noise std: 2.74
          Mean value_function loss: 266.4841
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 74.7659
                       Mean reward: 205.02
               Mean episode length: 110.52
    Episode_Reward/reaching_object: 0.9454
     Episode_Reward/lifting_object: 40.4841
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0627
      Episode_Termination/time_out: 0.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 36.2917
--------------------------------------------------------------------------------
                   Total timesteps: 104300544
                    Iteration time: 2.32s
                      Time elapsed: 00:40:49
                               ETA: 00:55:24

################################################################################
                     [1m Learning iteration 1061/2500 [0m                     

                       Computation: 43326 steps/s (collection: 2.168s, learning 0.101s)
             Mean action noise std: 2.74
          Mean value_function loss: 266.7367
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 74.7712
                       Mean reward: 208.99
               Mean episode length: 110.01
    Episode_Reward/reaching_object: 0.9458
     Episode_Reward/lifting_object: 40.4400
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 37.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104398848
                    Iteration time: 2.27s
                      Time elapsed: 00:40:51
                               ETA: 00:55:21

################################################################################
                     [1m Learning iteration 1062/2500 [0m                     

                       Computation: 43489 steps/s (collection: 2.168s, learning 0.093s)
             Mean action noise std: 2.74
          Mean value_function loss: 241.2340
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 74.7759
                       Mean reward: 213.98
               Mean episode length: 110.69
    Episode_Reward/reaching_object: 0.9656
     Episode_Reward/lifting_object: 41.2207
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0342
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 36.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104497152
                    Iteration time: 2.26s
                      Time elapsed: 00:40:53
                               ETA: 00:55:19

################################################################################
                     [1m Learning iteration 1063/2500 [0m                     

                       Computation: 42421 steps/s (collection: 2.219s, learning 0.099s)
             Mean action noise std: 2.74
          Mean value_function loss: 241.6623
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 74.7786
                       Mean reward: 214.88
               Mean episode length: 112.69
    Episode_Reward/reaching_object: 0.9652
     Episode_Reward/lifting_object: 41.5639
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0341
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 35.0000
--------------------------------------------------------------------------------
                   Total timesteps: 104595456
                    Iteration time: 2.32s
                      Time elapsed: 00:40:56
                               ETA: 00:55:17

################################################################################
                     [1m Learning iteration 1064/2500 [0m                     

                       Computation: 42843 steps/s (collection: 2.191s, learning 0.104s)
             Mean action noise std: 2.74
          Mean value_function loss: 267.0232
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 74.7832
                       Mean reward: 212.96
               Mean episode length: 113.22
    Episode_Reward/reaching_object: 0.9555
     Episode_Reward/lifting_object: 41.0402
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0337
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 35.2500
--------------------------------------------------------------------------------
                   Total timesteps: 104693760
                    Iteration time: 2.29s
                      Time elapsed: 00:40:58
                               ETA: 00:55:14

################################################################################
                     [1m Learning iteration 1065/2500 [0m                     

                       Computation: 42329 steps/s (collection: 2.211s, learning 0.112s)
             Mean action noise std: 2.74
          Mean value_function loss: 268.7881
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 74.7871
                       Mean reward: 232.69
               Mean episode length: 120.28
    Episode_Reward/reaching_object: 0.9773
     Episode_Reward/lifting_object: 41.9703
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0345
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 0.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 34.1667
--------------------------------------------------------------------------------
                   Total timesteps: 104792064
                    Iteration time: 2.32s
                      Time elapsed: 00:41:00
                               ETA: 00:55:12

################################################################################
                     [1m Learning iteration 1066/2500 [0m                     

                       Computation: 41413 steps/s (collection: 2.251s, learning 0.123s)
             Mean action noise std: 2.74
          Mean value_function loss: 264.1244
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 74.7934
                       Mean reward: 205.09
               Mean episode length: 108.91
    Episode_Reward/reaching_object: 0.9429
     Episode_Reward/lifting_object: 39.9899
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0335
          Episode_Reward/joint_vel: -0.0631
      Episode_Termination/time_out: 0.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 35.2917
--------------------------------------------------------------------------------
                   Total timesteps: 104890368
                    Iteration time: 2.37s
                      Time elapsed: 00:41:03
                               ETA: 00:55:10

################################################################################
                     [1m Learning iteration 1067/2500 [0m                     

                       Computation: 42323 steps/s (collection: 2.222s, learning 0.101s)
             Mean action noise std: 2.75
          Mean value_function loss: 247.3070
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 74.7984
                       Mean reward: 227.06
               Mean episode length: 117.55
    Episode_Reward/reaching_object: 1.0143
     Episode_Reward/lifting_object: 43.5727
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 0.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 35.5000
--------------------------------------------------------------------------------
                   Total timesteps: 104988672
                    Iteration time: 2.32s
                      Time elapsed: 00:41:05
                               ETA: 00:55:07

################################################################################
                     [1m Learning iteration 1068/2500 [0m                     

                       Computation: 42565 steps/s (collection: 2.196s, learning 0.114s)
             Mean action noise std: 2.75
          Mean value_function loss: 266.2005
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 74.8031
                       Mean reward: 199.43
               Mean episode length: 103.53
    Episode_Reward/reaching_object: 0.9971
     Episode_Reward/lifting_object: 42.7927
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0352
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 0.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 34.7083
--------------------------------------------------------------------------------
                   Total timesteps: 105086976
                    Iteration time: 2.31s
                      Time elapsed: 00:41:07
                               ETA: 00:55:05

################################################################################
                     [1m Learning iteration 1069/2500 [0m                     

                       Computation: 42587 steps/s (collection: 2.206s, learning 0.103s)
             Mean action noise std: 2.75
          Mean value_function loss: 243.7097
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 74.8098
                       Mean reward: 216.02
               Mean episode length: 123.87
    Episode_Reward/reaching_object: 1.0076
     Episode_Reward/lifting_object: 42.6333
      Episode_Reward/object_height: 0.0027
        Episode_Reward/action_rate: -0.0355
          Episode_Reward/joint_vel: -0.0667
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.5000
     Episode_Termination/robot_out: 33.0000
--------------------------------------------------------------------------------
                   Total timesteps: 105185280
                    Iteration time: 2.31s
                      Time elapsed: 00:41:10
                               ETA: 00:55:03

################################################################################
                     [1m Learning iteration 1070/2500 [0m                     

                       Computation: 42346 steps/s (collection: 2.171s, learning 0.151s)
             Mean action noise std: 2.75
          Mean value_function loss: 249.4809
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 74.8151
                       Mean reward: 215.24
               Mean episode length: 115.63
    Episode_Reward/reaching_object: 0.9829
     Episode_Reward/lifting_object: 41.6614
      Episode_Reward/object_height: 0.0026
        Episode_Reward/action_rate: -0.0348
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 35.2083
--------------------------------------------------------------------------------
                   Total timesteps: 105283584
                    Iteration time: 2.32s
                      Time elapsed: 00:41:12
                               ETA: 00:55:01

################################################################################
                     [1m Learning iteration 1071/2500 [0m                     

                       Computation: 42095 steps/s (collection: 2.234s, learning 0.101s)
             Mean action noise std: 2.75
          Mean value_function loss: 257.1416
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 74.8212
                       Mean reward: 233.28
               Mean episode length: 120.71
    Episode_Reward/reaching_object: 1.0205
     Episode_Reward/lifting_object: 44.0198
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0359
          Episode_Reward/joint_vel: -0.0687
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 33.2500
--------------------------------------------------------------------------------
                   Total timesteps: 105381888
                    Iteration time: 2.34s
                      Time elapsed: 00:41:14
                               ETA: 00:54:58

################################################################################
                     [1m Learning iteration 1072/2500 [0m                     

                       Computation: 43078 steps/s (collection: 2.182s, learning 0.100s)
             Mean action noise std: 2.75
          Mean value_function loss: 243.5295
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 74.8249
                       Mean reward: 225.97
               Mean episode length: 113.60
    Episode_Reward/reaching_object: 1.0155
     Episode_Reward/lifting_object: 44.4107
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0356
          Episode_Reward/joint_vel: -0.0672
      Episode_Termination/time_out: 0.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 33.4583
--------------------------------------------------------------------------------
                   Total timesteps: 105480192
                    Iteration time: 2.28s
                      Time elapsed: 00:41:16
                               ETA: 00:54:56

################################################################################
                     [1m Learning iteration 1073/2500 [0m                     

                       Computation: 42340 steps/s (collection: 2.217s, learning 0.105s)
             Mean action noise std: 2.75
          Mean value_function loss: 246.5520
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.8278
                       Mean reward: 228.26
               Mean episode length: 116.75
    Episode_Reward/reaching_object: 1.0327
     Episode_Reward/lifting_object: 44.5369
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0691
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 34.3333
--------------------------------------------------------------------------------
                   Total timesteps: 105578496
                    Iteration time: 2.32s
                      Time elapsed: 00:41:19
                               ETA: 00:54:54

################################################################################
                     [1m Learning iteration 1074/2500 [0m                     

                       Computation: 41981 steps/s (collection: 2.256s, learning 0.086s)
             Mean action noise std: 2.75
          Mean value_function loss: 269.1908
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 74.8314
                       Mean reward: 212.47
               Mean episode length: 113.72
    Episode_Reward/reaching_object: 1.0128
     Episode_Reward/lifting_object: 43.8902
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0357
          Episode_Reward/joint_vel: -0.0678
      Episode_Termination/time_out: 0.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 34.3333
--------------------------------------------------------------------------------
                   Total timesteps: 105676800
                    Iteration time: 2.34s
                      Time elapsed: 00:41:21
                               ETA: 00:54:51

################################################################################
                     [1m Learning iteration 1075/2500 [0m                     

                       Computation: 41664 steps/s (collection: 2.243s, learning 0.116s)
             Mean action noise std: 2.75
          Mean value_function loss: 241.1535
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 74.8377
                       Mean reward: 228.65
               Mean episode length: 121.15
    Episode_Reward/reaching_object: 1.0294
     Episode_Reward/lifting_object: 44.0680
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0697
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 32.5417
--------------------------------------------------------------------------------
                   Total timesteps: 105775104
                    Iteration time: 2.36s
                      Time elapsed: 00:41:23
                               ETA: 00:54:49

################################################################################
                     [1m Learning iteration 1076/2500 [0m                     

                       Computation: 42652 steps/s (collection: 2.190s, learning 0.115s)
             Mean action noise std: 2.75
          Mean value_function loss: 242.7440
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 74.8422
                       Mean reward: 244.69
               Mean episode length: 122.95
    Episode_Reward/reaching_object: 1.0275
     Episode_Reward/lifting_object: 44.7936
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0363
          Episode_Reward/joint_vel: -0.0708
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 33.3333
--------------------------------------------------------------------------------
                   Total timesteps: 105873408
                    Iteration time: 2.30s
                      Time elapsed: 00:41:26
                               ETA: 00:54:47

################################################################################
                     [1m Learning iteration 1077/2500 [0m                     

                       Computation: 41771 steps/s (collection: 2.231s, learning 0.123s)
             Mean action noise std: 2.75
          Mean value_function loss: 249.6126
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 74.8471
                       Mean reward: 217.87
               Mean episode length: 115.25
    Episode_Reward/reaching_object: 1.0133
     Episode_Reward/lifting_object: 43.7053
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0360
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 32.5833
--------------------------------------------------------------------------------
                   Total timesteps: 105971712
                    Iteration time: 2.35s
                      Time elapsed: 00:41:28
                               ETA: 00:54:45

################################################################################
                     [1m Learning iteration 1078/2500 [0m                     

                       Computation: 42786 steps/s (collection: 2.197s, learning 0.100s)
             Mean action noise std: 2.75
          Mean value_function loss: 261.4724
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 74.8557
                       Mean reward: 249.66
               Mean episode length: 123.92
    Episode_Reward/reaching_object: 1.0507
     Episode_Reward/lifting_object: 45.6853
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0709
      Episode_Termination/time_out: 0.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 33.6667
--------------------------------------------------------------------------------
                   Total timesteps: 106070016
                    Iteration time: 2.30s
                      Time elapsed: 00:41:30
                               ETA: 00:54:42

################################################################################
                     [1m Learning iteration 1079/2500 [0m                     

                       Computation: 42951 steps/s (collection: 2.198s, learning 0.091s)
             Mean action noise std: 2.75
          Mean value_function loss: 261.9823
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 74.8608
                       Mean reward: 229.22
               Mean episode length: 121.32
    Episode_Reward/reaching_object: 1.0633
     Episode_Reward/lifting_object: 45.8352
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0730
      Episode_Termination/time_out: 0.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 33.3750
--------------------------------------------------------------------------------
                   Total timesteps: 106168320
                    Iteration time: 2.29s
                      Time elapsed: 00:41:33
                               ETA: 00:54:40

################################################################################
                     [1m Learning iteration 1080/2500 [0m                     

                       Computation: 42483 steps/s (collection: 2.209s, learning 0.105s)
             Mean action noise std: 2.75
          Mean value_function loss: 254.9360
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 74.8678
                       Mean reward: 224.12
               Mean episode length: 112.78
    Episode_Reward/reaching_object: 1.0544
     Episode_Reward/lifting_object: 45.0473
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0728
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 32.2917
--------------------------------------------------------------------------------
                   Total timesteps: 106266624
                    Iteration time: 2.31s
                      Time elapsed: 00:41:35
                               ETA: 00:54:38

################################################################################
                     [1m Learning iteration 1081/2500 [0m                     

                       Computation: 42522 steps/s (collection: 2.212s, learning 0.100s)
             Mean action noise std: 2.75
          Mean value_function loss: 244.0483
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 74.8716
                       Mean reward: 239.77
               Mean episode length: 122.90
    Episode_Reward/reaching_object: 1.0882
     Episode_Reward/lifting_object: 46.9049
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0753
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 30.5000
--------------------------------------------------------------------------------
                   Total timesteps: 106364928
                    Iteration time: 2.31s
                      Time elapsed: 00:41:37
                               ETA: 00:54:35

################################################################################
                     [1m Learning iteration 1082/2500 [0m                     

                       Computation: 42209 steps/s (collection: 2.222s, learning 0.107s)
             Mean action noise std: 2.75
          Mean value_function loss: 277.3392
               Mean surrogate loss: 0.0072
                 Mean entropy loss: 74.8729
                       Mean reward: 244.61
               Mean episode length: 130.46
    Episode_Reward/reaching_object: 1.0381
     Episode_Reward/lifting_object: 44.6609
      Episode_Reward/object_height: 0.0028
        Episode_Reward/action_rate: -0.0369
          Episode_Reward/joint_vel: -0.0718
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 33.3333
--------------------------------------------------------------------------------
                   Total timesteps: 106463232
                    Iteration time: 2.33s
                      Time elapsed: 00:41:40
                               ETA: 00:54:33

################################################################################
                     [1m Learning iteration 1083/2500 [0m                     

                       Computation: 42096 steps/s (collection: 2.211s, learning 0.124s)
             Mean action noise std: 2.75
          Mean value_function loss: 255.8648
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 74.8737
                       Mean reward: 226.94
               Mean episode length: 119.29
    Episode_Reward/reaching_object: 1.0570
     Episode_Reward/lifting_object: 45.4473
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0374
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 33.7083
--------------------------------------------------------------------------------
                   Total timesteps: 106561536
                    Iteration time: 2.34s
                      Time elapsed: 00:41:42
                               ETA: 00:54:31

################################################################################
                     [1m Learning iteration 1084/2500 [0m                     

                       Computation: 42674 steps/s (collection: 2.184s, learning 0.120s)
             Mean action noise std: 2.75
          Mean value_function loss: 250.7640
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 74.8756
                       Mean reward: 238.91
               Mean episode length: 123.43
    Episode_Reward/reaching_object: 1.0514
     Episode_Reward/lifting_object: 45.5678
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 31.0000
--------------------------------------------------------------------------------
                   Total timesteps: 106659840
                    Iteration time: 2.30s
                      Time elapsed: 00:41:44
                               ETA: 00:54:28

################################################################################
                     [1m Learning iteration 1085/2500 [0m                     

                       Computation: 42531 steps/s (collection: 2.209s, learning 0.102s)
             Mean action noise std: 2.75
          Mean value_function loss: 242.5173
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 74.8785
                       Mean reward: 241.09
               Mean episode length: 121.25
    Episode_Reward/reaching_object: 1.0867
     Episode_Reward/lifting_object: 47.1597
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0750
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 32.7500
--------------------------------------------------------------------------------
                   Total timesteps: 106758144
                    Iteration time: 2.31s
                      Time elapsed: 00:41:47
                               ETA: 00:54:26

################################################################################
                     [1m Learning iteration 1086/2500 [0m                     

                       Computation: 41713 steps/s (collection: 2.229s, learning 0.128s)
             Mean action noise std: 2.75
          Mean value_function loss: 222.0986
               Mean surrogate loss: -0.0046
                 Mean entropy loss: 74.8827
                       Mean reward: 244.39
               Mean episode length: 125.62
    Episode_Reward/reaching_object: 1.0764
     Episode_Reward/lifting_object: 46.9992
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0377
          Episode_Reward/joint_vel: -0.0731
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.2083
--------------------------------------------------------------------------------
                   Total timesteps: 106856448
                    Iteration time: 2.36s
                      Time elapsed: 00:41:49
                               ETA: 00:54:24

################################################################################
                     [1m Learning iteration 1087/2500 [0m                     

                       Computation: 42718 steps/s (collection: 2.208s, learning 0.093s)
             Mean action noise std: 2.75
          Mean value_function loss: 262.9096
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 74.8869
                       Mean reward: 221.21
               Mean episode length: 111.54
    Episode_Reward/reaching_object: 1.0530
     Episode_Reward/lifting_object: 45.9315
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0371
          Episode_Reward/joint_vel: -0.0733
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 32.7500
--------------------------------------------------------------------------------
                   Total timesteps: 106954752
                    Iteration time: 2.30s
                      Time elapsed: 00:41:51
                               ETA: 00:54:22

################################################################################
                     [1m Learning iteration 1088/2500 [0m                     

                       Computation: 44113 steps/s (collection: 2.141s, learning 0.087s)
             Mean action noise std: 2.75
          Mean value_function loss: 249.0458
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 74.8899
                       Mean reward: 259.89
               Mean episode length: 132.61
    Episode_Reward/reaching_object: 1.0804
     Episode_Reward/lifting_object: 46.9342
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 32.8333
--------------------------------------------------------------------------------
                   Total timesteps: 107053056
                    Iteration time: 2.23s
                      Time elapsed: 00:41:54
                               ETA: 00:54:19

################################################################################
                     [1m Learning iteration 1089/2500 [0m                     

                       Computation: 42419 steps/s (collection: 2.196s, learning 0.121s)
             Mean action noise std: 2.75
          Mean value_function loss: 271.8543
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 74.8932
                       Mean reward: 261.84
               Mean episode length: 136.79
    Episode_Reward/reaching_object: 1.1138
     Episode_Reward/lifting_object: 48.3951
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0775
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 32.0417
--------------------------------------------------------------------------------
                   Total timesteps: 107151360
                    Iteration time: 2.32s
                      Time elapsed: 00:41:56
                               ETA: 00:54:17

################################################################################
                     [1m Learning iteration 1090/2500 [0m                     

                       Computation: 41755 steps/s (collection: 2.213s, learning 0.141s)
             Mean action noise std: 2.75
          Mean value_function loss: 249.6941
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 74.8952
                       Mean reward: 238.00
               Mean episode length: 124.81
    Episode_Reward/reaching_object: 1.1235
     Episode_Reward/lifting_object: 48.4310
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0762
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.9583
--------------------------------------------------------------------------------
                   Total timesteps: 107249664
                    Iteration time: 2.35s
                      Time elapsed: 00:41:58
                               ETA: 00:54:15

################################################################################
                     [1m Learning iteration 1091/2500 [0m                     

                       Computation: 43278 steps/s (collection: 2.180s, learning 0.091s)
             Mean action noise std: 2.76
          Mean value_function loss: 246.9927
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 74.8987
                       Mean reward: 224.05
               Mean episode length: 118.48
    Episode_Reward/reaching_object: 1.1132
     Episode_Reward/lifting_object: 48.2536
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0393
          Episode_Reward/joint_vel: -0.0763
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.3333
--------------------------------------------------------------------------------
                   Total timesteps: 107347968
                    Iteration time: 2.27s
                      Time elapsed: 00:42:00
                               ETA: 00:54:12

################################################################################
                     [1m Learning iteration 1092/2500 [0m                     

                       Computation: 43397 steps/s (collection: 2.178s, learning 0.087s)
             Mean action noise std: 2.76
          Mean value_function loss: 246.8846
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 74.9039
                       Mean reward: 235.34
               Mean episode length: 122.18
    Episode_Reward/reaching_object: 1.0860
     Episode_Reward/lifting_object: 47.6506
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0751
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.3750
     Episode_Termination/robot_out: 31.5833
--------------------------------------------------------------------------------
                   Total timesteps: 107446272
                    Iteration time: 2.27s
                      Time elapsed: 00:42:03
                               ETA: 00:54:10

################################################################################
                     [1m Learning iteration 1093/2500 [0m                     

                       Computation: 41931 steps/s (collection: 2.203s, learning 0.141s)
             Mean action noise std: 2.76
          Mean value_function loss: 260.4015
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 74.9100
                       Mean reward: 256.08
               Mean episode length: 128.73
    Episode_Reward/reaching_object: 1.1078
     Episode_Reward/lifting_object: 48.7832
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 32.0417
--------------------------------------------------------------------------------
                   Total timesteps: 107544576
                    Iteration time: 2.34s
                      Time elapsed: 00:42:05
                               ETA: 00:54:08

################################################################################
                     [1m Learning iteration 1094/2500 [0m                     

                       Computation: 42837 steps/s (collection: 2.176s, learning 0.119s)
             Mean action noise std: 2.76
          Mean value_function loss: 269.2594
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 74.9143
                       Mean reward: 253.48
               Mean episode length: 129.50
    Episode_Reward/reaching_object: 1.0705
     Episode_Reward/lifting_object: 47.3600
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0381
          Episode_Reward/joint_vel: -0.0739
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 31.2083
--------------------------------------------------------------------------------
                   Total timesteps: 107642880
                    Iteration time: 2.29s
                      Time elapsed: 00:42:07
                               ETA: 00:54:05

################################################################################
                     [1m Learning iteration 1095/2500 [0m                     

                       Computation: 41611 steps/s (collection: 2.250s, learning 0.113s)
             Mean action noise std: 2.76
          Mean value_function loss: 255.3473
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 74.9163
                       Mean reward: 231.33
               Mean episode length: 121.19
    Episode_Reward/reaching_object: 1.0800
     Episode_Reward/lifting_object: 47.4507
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0385
          Episode_Reward/joint_vel: -0.0749
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 32.1250
--------------------------------------------------------------------------------
                   Total timesteps: 107741184
                    Iteration time: 2.36s
                      Time elapsed: 00:42:10
                               ETA: 00:54:03

################################################################################
                     [1m Learning iteration 1096/2500 [0m                     

                       Computation: 42989 steps/s (collection: 2.198s, learning 0.089s)
             Mean action noise std: 2.76
          Mean value_function loss: 246.4971
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 74.9204
                       Mean reward: 229.66
               Mean episode length: 119.30
    Episode_Reward/reaching_object: 1.0678
     Episode_Reward/lifting_object: 47.8762
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 32.6250
--------------------------------------------------------------------------------
                   Total timesteps: 107839488
                    Iteration time: 2.29s
                      Time elapsed: 00:42:12
                               ETA: 00:54:01

################################################################################
                     [1m Learning iteration 1097/2500 [0m                     

                       Computation: 42762 steps/s (collection: 2.199s, learning 0.100s)
             Mean action noise std: 2.76
          Mean value_function loss: 265.7987
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 74.9268
                       Mean reward: 248.63
               Mean episode length: 133.69
    Episode_Reward/reaching_object: 1.0688
     Episode_Reward/lifting_object: 47.4666
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0758
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 33.7083
--------------------------------------------------------------------------------
                   Total timesteps: 107937792
                    Iteration time: 2.30s
                      Time elapsed: 00:42:14
                               ETA: 00:53:58

################################################################################
                     [1m Learning iteration 1098/2500 [0m                     

                       Computation: 43213 steps/s (collection: 2.176s, learning 0.099s)
             Mean action noise std: 2.76
          Mean value_function loss: 254.7987
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 74.9311
                       Mean reward: 237.18
               Mean episode length: 121.63
    Episode_Reward/reaching_object: 1.0771
     Episode_Reward/lifting_object: 47.8735
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0387
          Episode_Reward/joint_vel: -0.0747
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.2500
--------------------------------------------------------------------------------
                   Total timesteps: 108036096
                    Iteration time: 2.27s
                      Time elapsed: 00:42:17
                               ETA: 00:53:56

################################################################################
                     [1m Learning iteration 1099/2500 [0m                     

                       Computation: 42957 steps/s (collection: 2.194s, learning 0.095s)
             Mean action noise std: 2.76
          Mean value_function loss: 257.2082
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 74.9340
                       Mean reward: 248.71
               Mean episode length: 127.45
    Episode_Reward/reaching_object: 1.0933
     Episode_Reward/lifting_object: 49.0044
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0770
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.9167
--------------------------------------------------------------------------------
                   Total timesteps: 108134400
                    Iteration time: 2.29s
                      Time elapsed: 00:42:19
                               ETA: 00:53:54

################################################################################
                     [1m Learning iteration 1100/2500 [0m                     

                       Computation: 42357 steps/s (collection: 2.210s, learning 0.111s)
             Mean action noise std: 2.76
          Mean value_function loss: 259.2190
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 74.9375
                       Mean reward: 242.05
               Mean episode length: 123.35
    Episode_Reward/reaching_object: 1.0351
     Episode_Reward/lifting_object: 46.3547
      Episode_Reward/object_height: 0.0029
        Episode_Reward/action_rate: -0.0372
          Episode_Reward/joint_vel: -0.0721
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 31.4167
--------------------------------------------------------------------------------
                   Total timesteps: 108232704
                    Iteration time: 2.32s
                      Time elapsed: 00:42:21
                               ETA: 00:53:51

################################################################################
                     [1m Learning iteration 1101/2500 [0m                     

                       Computation: 41646 steps/s (collection: 2.256s, learning 0.105s)
             Mean action noise std: 2.76
          Mean value_function loss: 248.6346
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 74.9409
                       Mean reward: 247.56
               Mean episode length: 123.48
    Episode_Reward/reaching_object: 1.0661
     Episode_Reward/lifting_object: 47.8939
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0383
          Episode_Reward/joint_vel: -0.0736
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 32.7500
--------------------------------------------------------------------------------
                   Total timesteps: 108331008
                    Iteration time: 2.36s
                      Time elapsed: 00:42:24
                               ETA: 00:53:49

################################################################################
                     [1m Learning iteration 1102/2500 [0m                     

                       Computation: 42979 steps/s (collection: 2.187s, learning 0.100s)
             Mean action noise std: 2.76
          Mean value_function loss: 254.3149
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 74.9442
                       Mean reward: 242.13
               Mean episode length: 123.20
    Episode_Reward/reaching_object: 1.0723
     Episode_Reward/lifting_object: 48.5194
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0746
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 32.1667
--------------------------------------------------------------------------------
                   Total timesteps: 108429312
                    Iteration time: 2.29s
                      Time elapsed: 00:42:26
                               ETA: 00:53:47

################################################################################
                     [1m Learning iteration 1103/2500 [0m                     

                       Computation: 41451 steps/s (collection: 2.188s, learning 0.183s)
             Mean action noise std: 2.76
          Mean value_function loss: 268.5519
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 74.9495
                       Mean reward: 241.62
               Mean episode length: 122.40
    Episode_Reward/reaching_object: 1.0702
     Episode_Reward/lifting_object: 48.5072
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0386
          Episode_Reward/joint_vel: -0.0745
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 30.7500
--------------------------------------------------------------------------------
                   Total timesteps: 108527616
                    Iteration time: 2.37s
                      Time elapsed: 00:42:28
                               ETA: 00:53:45

################################################################################
                     [1m Learning iteration 1104/2500 [0m                     

                       Computation: 43434 steps/s (collection: 2.173s, learning 0.091s)
             Mean action noise std: 2.76
          Mean value_function loss: 265.9020
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 74.9563
                       Mean reward: 253.62
               Mean episode length: 127.44
    Episode_Reward/reaching_object: 1.0552
     Episode_Reward/lifting_object: 47.6813
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0744
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 31.2917
--------------------------------------------------------------------------------
                   Total timesteps: 108625920
                    Iteration time: 2.26s
                      Time elapsed: 00:42:30
                               ETA: 00:53:42

################################################################################
                     [1m Learning iteration 1105/2500 [0m                     

                       Computation: 43780 steps/s (collection: 2.152s, learning 0.093s)
             Mean action noise std: 2.76
          Mean value_function loss: 264.6690
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 74.9583
                       Mean reward: 218.86
               Mean episode length: 111.06
    Episode_Reward/reaching_object: 1.0151
     Episode_Reward/lifting_object: 46.6099
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0368
          Episode_Reward/joint_vel: -0.0696
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 32.2083
--------------------------------------------------------------------------------
                   Total timesteps: 108724224
                    Iteration time: 2.25s
                      Time elapsed: 00:42:33
                               ETA: 00:53:40

################################################################################
                     [1m Learning iteration 1106/2500 [0m                     

                       Computation: 43545 steps/s (collection: 2.162s, learning 0.096s)
             Mean action noise std: 2.76
          Mean value_function loss: 247.5995
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 74.9619
                       Mean reward: 262.62
               Mean episode length: 132.45
    Episode_Reward/reaching_object: 1.0937
     Episode_Reward/lifting_object: 50.3033
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0392
          Episode_Reward/joint_vel: -0.0753
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.7917
--------------------------------------------------------------------------------
                   Total timesteps: 108822528
                    Iteration time: 2.26s
                      Time elapsed: 00:42:35
                               ETA: 00:53:38

################################################################################
                     [1m Learning iteration 1107/2500 [0m                     

                       Computation: 42957 steps/s (collection: 2.190s, learning 0.099s)
             Mean action noise std: 2.76
          Mean value_function loss: 252.1716
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 74.9665
                       Mean reward: 232.62
               Mean episode length: 118.05
    Episode_Reward/reaching_object: 1.0401
     Episode_Reward/lifting_object: 47.1908
      Episode_Reward/object_height: 0.0030
        Episode_Reward/action_rate: -0.0376
          Episode_Reward/joint_vel: -0.0719
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 30.6667
--------------------------------------------------------------------------------
                   Total timesteps: 108920832
                    Iteration time: 2.29s
                      Time elapsed: 00:42:37
                               ETA: 00:53:35

################################################################################
                     [1m Learning iteration 1108/2500 [0m                     

                       Computation: 42915 steps/s (collection: 2.197s, learning 0.094s)
             Mean action noise std: 2.76
          Mean value_function loss: 278.6005
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 74.9703
                       Mean reward: 253.67
               Mean episode length: 125.64
    Episode_Reward/reaching_object: 1.1299
     Episode_Reward/lifting_object: 51.9976
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0777
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.7917
--------------------------------------------------------------------------------
                   Total timesteps: 109019136
                    Iteration time: 2.29s
                      Time elapsed: 00:42:40
                               ETA: 00:53:33

################################################################################
                     [1m Learning iteration 1109/2500 [0m                     

                       Computation: 42723 steps/s (collection: 2.203s, learning 0.098s)
             Mean action noise std: 2.76
          Mean value_function loss: 261.4817
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 74.9753
                       Mean reward: 256.22
               Mean episode length: 125.72
    Episode_Reward/reaching_object: 1.0490
     Episode_Reward/lifting_object: 47.9623
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0382
          Episode_Reward/joint_vel: -0.0732
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.5000
--------------------------------------------------------------------------------
                   Total timesteps: 109117440
                    Iteration time: 2.30s
                      Time elapsed: 00:42:42
                               ETA: 00:53:31

################################################################################
                     [1m Learning iteration 1110/2500 [0m                     

                       Computation: 42229 steps/s (collection: 2.238s, learning 0.090s)
             Mean action noise std: 2.76
          Mean value_function loss: 281.7749
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 74.9806
                       Mean reward: 269.32
               Mean episode length: 137.85
    Episode_Reward/reaching_object: 1.1031
     Episode_Reward/lifting_object: 50.5798
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0398
          Episode_Reward/joint_vel: -0.0770
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.7917
--------------------------------------------------------------------------------
                   Total timesteps: 109215744
                    Iteration time: 2.33s
                      Time elapsed: 00:42:44
                               ETA: 00:53:28

################################################################################
                     [1m Learning iteration 1111/2500 [0m                     

                       Computation: 42741 steps/s (collection: 2.192s, learning 0.108s)
             Mean action noise std: 2.76
          Mean value_function loss: 255.1326
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 74.9842
                       Mean reward: 265.27
               Mean episode length: 130.80
    Episode_Reward/reaching_object: 1.1200
     Episode_Reward/lifting_object: 51.2017
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.3333
--------------------------------------------------------------------------------
                   Total timesteps: 109314048
                    Iteration time: 2.30s
                      Time elapsed: 00:42:46
                               ETA: 00:53:26

################################################################################
                     [1m Learning iteration 1112/2500 [0m                     

                       Computation: 42598 steps/s (collection: 2.217s, learning 0.091s)
             Mean action noise std: 2.76
          Mean value_function loss: 255.1244
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 74.9865
                       Mean reward: 265.88
               Mean episode length: 131.57
    Episode_Reward/reaching_object: 1.0566
     Episode_Reward/lifting_object: 48.6733
      Episode_Reward/object_height: 0.0031
        Episode_Reward/action_rate: -0.0384
          Episode_Reward/joint_vel: -0.0740
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.1250
--------------------------------------------------------------------------------
                   Total timesteps: 109412352
                    Iteration time: 2.31s
                      Time elapsed: 00:42:49
                               ETA: 00:53:24

################################################################################
                     [1m Learning iteration 1113/2500 [0m                     

                       Computation: 42720 steps/s (collection: 2.205s, learning 0.096s)
             Mean action noise std: 2.76
          Mean value_function loss: 255.1394
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 74.9893
                       Mean reward: 246.57
               Mean episode length: 124.58
    Episode_Reward/reaching_object: 1.0949
     Episode_Reward/lifting_object: 50.3123
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0760
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 31.0417
--------------------------------------------------------------------------------
                   Total timesteps: 109510656
                    Iteration time: 2.30s
                      Time elapsed: 00:42:51
                               ETA: 00:53:21

################################################################################
                     [1m Learning iteration 1114/2500 [0m                     

                       Computation: 43548 steps/s (collection: 2.169s, learning 0.089s)
             Mean action noise std: 2.77
          Mean value_function loss: 250.9355
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 74.9956
                       Mean reward: 255.09
               Mean episode length: 127.66
    Episode_Reward/reaching_object: 1.0872
     Episode_Reward/lifting_object: 50.0578
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0394
          Episode_Reward/joint_vel: -0.0760
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 30.0000
--------------------------------------------------------------------------------
                   Total timesteps: 109608960
                    Iteration time: 2.26s
                      Time elapsed: 00:42:53
                               ETA: 00:53:19

################################################################################
                     [1m Learning iteration 1115/2500 [0m                     

                       Computation: 41926 steps/s (collection: 2.245s, learning 0.100s)
             Mean action noise std: 2.77
          Mean value_function loss: 264.2121
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 75.0043
                       Mean reward: 258.68
               Mean episode length: 127.49
    Episode_Reward/reaching_object: 1.1140
     Episode_Reward/lifting_object: 51.7434
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0403
          Episode_Reward/joint_vel: -0.0777
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.6667
--------------------------------------------------------------------------------
                   Total timesteps: 109707264
                    Iteration time: 2.34s
                      Time elapsed: 00:42:56
                               ETA: 00:53:17

################################################################################
                     [1m Learning iteration 1116/2500 [0m                     

                       Computation: 43046 steps/s (collection: 2.188s, learning 0.096s)
             Mean action noise std: 2.77
          Mean value_function loss: 260.4066
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 75.0080
                       Mean reward: 244.73
               Mean episode length: 123.45
    Episode_Reward/reaching_object: 1.1037
     Episode_Reward/lifting_object: 50.9554
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0401
          Episode_Reward/joint_vel: -0.0779
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.4167
--------------------------------------------------------------------------------
                   Total timesteps: 109805568
                    Iteration time: 2.28s
                      Time elapsed: 00:42:58
                               ETA: 00:53:14

################################################################################
                     [1m Learning iteration 1117/2500 [0m                     

                       Computation: 42889 steps/s (collection: 2.199s, learning 0.094s)
             Mean action noise std: 2.77
          Mean value_function loss: 268.5688
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 75.0118
                       Mean reward: 257.09
               Mean episode length: 130.03
    Episode_Reward/reaching_object: 1.0935
     Episode_Reward/lifting_object: 50.3784
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0397
          Episode_Reward/joint_vel: -0.0755
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.8333
--------------------------------------------------------------------------------
                   Total timesteps: 109903872
                    Iteration time: 2.29s
                      Time elapsed: 00:43:00
                               ETA: 00:53:12

################################################################################
                     [1m Learning iteration 1118/2500 [0m                     

                       Computation: 43119 steps/s (collection: 2.183s, learning 0.097s)
             Mean action noise std: 2.77
          Mean value_function loss: 261.1282
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 75.0193
                       Mean reward: 251.03
               Mean episode length: 126.80
    Episode_Reward/reaching_object: 1.1116
     Episode_Reward/lifting_object: 51.3194
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0786
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 29.5417
--------------------------------------------------------------------------------
                   Total timesteps: 110002176
                    Iteration time: 2.28s
                      Time elapsed: 00:43:03
                               ETA: 00:53:10

################################################################################
                     [1m Learning iteration 1119/2500 [0m                     

                       Computation: 43286 steps/s (collection: 2.175s, learning 0.096s)
             Mean action noise std: 2.77
          Mean value_function loss: 258.7763
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 75.0263
                       Mean reward: 264.72
               Mean episode length: 129.13
    Episode_Reward/reaching_object: 1.1195
     Episode_Reward/lifting_object: 52.0904
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0789
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.5833
--------------------------------------------------------------------------------
                   Total timesteps: 110100480
                    Iteration time: 2.27s
                      Time elapsed: 00:43:05
                               ETA: 00:53:07

################################################################################
                     [1m Learning iteration 1120/2500 [0m                     

                       Computation: 43324 steps/s (collection: 2.180s, learning 0.089s)
             Mean action noise std: 2.77
          Mean value_function loss: 260.7108
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 75.0321
                       Mean reward: 254.17
               Mean episode length: 128.77
    Episode_Reward/reaching_object: 1.1222
     Episode_Reward/lifting_object: 51.8876
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0792
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 31.3750
--------------------------------------------------------------------------------
                   Total timesteps: 110198784
                    Iteration time: 2.27s
                      Time elapsed: 00:43:07
                               ETA: 00:53:05

################################################################################
                     [1m Learning iteration 1121/2500 [0m                     

                       Computation: 42496 steps/s (collection: 2.191s, learning 0.122s)
             Mean action noise std: 2.77
          Mean value_function loss: 268.2137
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 75.0394
                       Mean reward: 266.34
               Mean episode length: 127.59
    Episode_Reward/reaching_object: 1.1173
     Episode_Reward/lifting_object: 51.7527
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0783
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.7500
--------------------------------------------------------------------------------
                   Total timesteps: 110297088
                    Iteration time: 2.31s
                      Time elapsed: 00:43:09
                               ETA: 00:53:03

################################################################################
                     [1m Learning iteration 1122/2500 [0m                     

                       Computation: 43366 steps/s (collection: 2.170s, learning 0.097s)
             Mean action noise std: 2.77
          Mean value_function loss: 265.2133
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 75.0436
                       Mean reward: 263.78
               Mean episode length: 132.68
    Episode_Reward/reaching_object: 1.1070
     Episode_Reward/lifting_object: 51.1793
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0404
          Episode_Reward/joint_vel: -0.0797
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.7083
--------------------------------------------------------------------------------
                   Total timesteps: 110395392
                    Iteration time: 2.27s
                      Time elapsed: 00:43:12
                               ETA: 00:53:00

################################################################################
                     [1m Learning iteration 1123/2500 [0m                     

                       Computation: 43127 steps/s (collection: 2.164s, learning 0.116s)
             Mean action noise std: 2.77
          Mean value_function loss: 258.8611
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 75.0483
                       Mean reward: 272.92
               Mean episode length: 133.44
    Episode_Reward/reaching_object: 1.1227
     Episode_Reward/lifting_object: 51.9523
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0791
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110493696
                    Iteration time: 2.28s
                      Time elapsed: 00:43:14
                               ETA: 00:52:58

################################################################################
                     [1m Learning iteration 1124/2500 [0m                     

                       Computation: 43383 steps/s (collection: 2.162s, learning 0.104s)
             Mean action noise std: 2.77
          Mean value_function loss: 282.3524
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 75.0505
                       Mean reward: 279.14
               Mean episode length: 137.74
    Episode_Reward/reaching_object: 1.1430
     Episode_Reward/lifting_object: 53.2509
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.8750
--------------------------------------------------------------------------------
                   Total timesteps: 110592000
                    Iteration time: 2.27s
                      Time elapsed: 00:43:16
                               ETA: 00:52:56

################################################################################
                     [1m Learning iteration 1125/2500 [0m                     

                       Computation: 42846 steps/s (collection: 2.169s, learning 0.126s)
             Mean action noise std: 2.77
          Mean value_function loss: 260.2899
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 75.0532
                       Mean reward: 260.03
               Mean episode length: 129.47
    Episode_Reward/reaching_object: 1.1406
     Episode_Reward/lifting_object: 52.5227
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0805
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 28.0833
--------------------------------------------------------------------------------
                   Total timesteps: 110690304
                    Iteration time: 2.29s
                      Time elapsed: 00:43:19
                               ETA: 00:52:53

################################################################################
                     [1m Learning iteration 1126/2500 [0m                     

                       Computation: 42172 steps/s (collection: 2.227s, learning 0.104s)
             Mean action noise std: 2.77
          Mean value_function loss: 251.5615
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 75.0563
                       Mean reward: 262.60
               Mean episode length: 128.72
    Episode_Reward/reaching_object: 1.1532
     Episode_Reward/lifting_object: 53.2145
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0837
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.5417
--------------------------------------------------------------------------------
                   Total timesteps: 110788608
                    Iteration time: 2.33s
                      Time elapsed: 00:43:21
                               ETA: 00:52:51

################################################################################
                     [1m Learning iteration 1127/2500 [0m                     

                       Computation: 40677 steps/s (collection: 2.297s, learning 0.119s)
             Mean action noise std: 2.77
          Mean value_function loss: 279.9836
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 75.0627
                       Mean reward: 247.85
               Mean episode length: 125.39
    Episode_Reward/reaching_object: 1.1217
     Episode_Reward/lifting_object: 52.1313
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0407
          Episode_Reward/joint_vel: -0.0805
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.2917
--------------------------------------------------------------------------------
                   Total timesteps: 110886912
                    Iteration time: 2.42s
                      Time elapsed: 00:43:23
                               ETA: 00:52:49

################################################################################
                     [1m Learning iteration 1128/2500 [0m                     

                       Computation: 41686 steps/s (collection: 2.255s, learning 0.103s)
             Mean action noise std: 2.77
          Mean value_function loss: 259.7214
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 75.0720
                       Mean reward: 273.37
               Mean episode length: 132.83
    Episode_Reward/reaching_object: 1.1623
     Episode_Reward/lifting_object: 53.2364
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0421
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.6667
--------------------------------------------------------------------------------
                   Total timesteps: 110985216
                    Iteration time: 2.36s
                      Time elapsed: 00:43:26
                               ETA: 00:52:47

################################################################################
                     [1m Learning iteration 1129/2500 [0m                     

                       Computation: 41346 steps/s (collection: 2.273s, learning 0.105s)
             Mean action noise std: 2.77
          Mean value_function loss: 246.3361
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 75.0785
                       Mean reward: 269.23
               Mean episode length: 135.49
    Episode_Reward/reaching_object: 1.1512
     Episode_Reward/lifting_object: 53.3217
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0827
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.4583
--------------------------------------------------------------------------------
                   Total timesteps: 111083520
                    Iteration time: 2.38s
                      Time elapsed: 00:43:28
                               ETA: 00:52:44

################################################################################
                     [1m Learning iteration 1130/2500 [0m                     

                       Computation: 41787 steps/s (collection: 2.264s, learning 0.088s)
             Mean action noise std: 2.77
          Mean value_function loss: 276.7392
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 75.0830
                       Mean reward: 269.03
               Mean episode length: 129.43
    Episode_Reward/reaching_object: 1.1548
     Episode_Reward/lifting_object: 53.4487
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.1667
--------------------------------------------------------------------------------
                   Total timesteps: 111181824
                    Iteration time: 2.35s
                      Time elapsed: 00:43:30
                               ETA: 00:52:42

################################################################################
                     [1m Learning iteration 1131/2500 [0m                     

                       Computation: 41746 steps/s (collection: 2.244s, learning 0.111s)
             Mean action noise std: 2.77
          Mean value_function loss: 271.1107
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 75.0893
                       Mean reward: 267.14
               Mean episode length: 129.85
    Episode_Reward/reaching_object: 1.1483
     Episode_Reward/lifting_object: 53.1820
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 26.6250
--------------------------------------------------------------------------------
                   Total timesteps: 111280128
                    Iteration time: 2.35s
                      Time elapsed: 00:43:33
                               ETA: 00:52:40

################################################################################
                     [1m Learning iteration 1132/2500 [0m                     

                       Computation: 41483 steps/s (collection: 2.279s, learning 0.091s)
             Mean action noise std: 2.77
          Mean value_function loss: 281.4591
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 75.0947
                       Mean reward: 245.44
               Mean episode length: 122.96
    Episode_Reward/reaching_object: 1.1708
     Episode_Reward/lifting_object: 53.4320
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0859
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.4583
     Episode_Termination/robot_out: 30.9583
--------------------------------------------------------------------------------
                   Total timesteps: 111378432
                    Iteration time: 2.37s
                      Time elapsed: 00:43:35
                               ETA: 00:52:38

################################################################################
                     [1m Learning iteration 1133/2500 [0m                     

                       Computation: 41240 steps/s (collection: 2.288s, learning 0.096s)
             Mean action noise std: 2.78
          Mean value_function loss: 276.7502
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 75.0979
                       Mean reward: 237.88
               Mean episode length: 126.16
    Episode_Reward/reaching_object: 1.1853
     Episode_Reward/lifting_object: 53.8538
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0430
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.6250
--------------------------------------------------------------------------------
                   Total timesteps: 111476736
                    Iteration time: 2.38s
                      Time elapsed: 00:43:37
                               ETA: 00:52:35

################################################################################
                     [1m Learning iteration 1134/2500 [0m                     

                       Computation: 42870 steps/s (collection: 2.201s, learning 0.093s)
             Mean action noise std: 2.78
          Mean value_function loss: 260.3744
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 75.1052
                       Mean reward: 255.44
               Mean episode length: 129.09
    Episode_Reward/reaching_object: 1.1465
     Episode_Reward/lifting_object: 52.9835
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0417
          Episode_Reward/joint_vel: -0.0840
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.5833
--------------------------------------------------------------------------------
                   Total timesteps: 111575040
                    Iteration time: 2.29s
                      Time elapsed: 00:43:40
                               ETA: 00:52:33

################################################################################
                     [1m Learning iteration 1135/2500 [0m                     

                       Computation: 41849 steps/s (collection: 2.244s, learning 0.105s)
             Mean action noise std: 2.78
          Mean value_function loss: 292.2343
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 75.1125
                       Mean reward: 274.05
               Mean episode length: 132.35
    Episode_Reward/reaching_object: 1.1404
     Episode_Reward/lifting_object: 52.1516
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.5417
--------------------------------------------------------------------------------
                   Total timesteps: 111673344
                    Iteration time: 2.35s
                      Time elapsed: 00:43:42
                               ETA: 00:52:31

################################################################################
                     [1m Learning iteration 1136/2500 [0m                     

                       Computation: 40205 steps/s (collection: 2.323s, learning 0.122s)
             Mean action noise std: 2.78
          Mean value_function loss: 294.8026
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 75.1175
                       Mean reward: 262.67
               Mean episode length: 129.99
    Episode_Reward/reaching_object: 1.1528
     Episode_Reward/lifting_object: 53.1082
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0847
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 111771648
                    Iteration time: 2.45s
                      Time elapsed: 00:43:45
                               ETA: 00:52:29

################################################################################
                     [1m Learning iteration 1137/2500 [0m                     

                       Computation: 42218 steps/s (collection: 2.238s, learning 0.091s)
             Mean action noise std: 2.78
          Mean value_function loss: 286.0170
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 75.1238
                       Mean reward: 269.01
               Mean episode length: 131.28
    Episode_Reward/reaching_object: 1.1579
     Episode_Reward/lifting_object: 53.4861
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0420
          Episode_Reward/joint_vel: -0.0850
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 31.5000
--------------------------------------------------------------------------------
                   Total timesteps: 111869952
                    Iteration time: 2.33s
                      Time elapsed: 00:43:47
                               ETA: 00:52:26

################################################################################
                     [1m Learning iteration 1138/2500 [0m                     

                       Computation: 42274 steps/s (collection: 2.223s, learning 0.102s)
             Mean action noise std: 2.78
          Mean value_function loss: 287.3460
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 75.1322
                       Mean reward: 264.82
               Mean episode length: 129.42
    Episode_Reward/reaching_object: 1.1091
     Episode_Reward/lifting_object: 51.4611
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0405
          Episode_Reward/joint_vel: -0.0810
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.1667
--------------------------------------------------------------------------------
                   Total timesteps: 111968256
                    Iteration time: 2.33s
                      Time elapsed: 00:43:49
                               ETA: 00:52:24

################################################################################
                     [1m Learning iteration 1139/2500 [0m                     

                       Computation: 42911 steps/s (collection: 2.201s, learning 0.090s)
             Mean action noise std: 2.78
          Mean value_function loss: 292.5696
               Mean surrogate loss: 0.0065
                 Mean entropy loss: 75.1362
                       Mean reward: 266.30
               Mean episode length: 130.79
    Episode_Reward/reaching_object: 1.1217
     Episode_Reward/lifting_object: 52.3784
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0409
          Episode_Reward/joint_vel: -0.0817
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 29.8750
--------------------------------------------------------------------------------
                   Total timesteps: 112066560
                    Iteration time: 2.29s
                      Time elapsed: 00:43:52
                               ETA: 00:52:22

################################################################################
                     [1m Learning iteration 1140/2500 [0m                     

                       Computation: 42764 steps/s (collection: 2.195s, learning 0.104s)
             Mean action noise std: 2.78
          Mean value_function loss: 268.9712
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 75.1374
                       Mean reward: 244.27
               Mean episode length: 120.52
    Episode_Reward/reaching_object: 1.1268
     Episode_Reward/lifting_object: 52.9538
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0823
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.4167
--------------------------------------------------------------------------------
                   Total timesteps: 112164864
                    Iteration time: 2.30s
                      Time elapsed: 00:43:54
                               ETA: 00:52:19

################################################################################
                     [1m Learning iteration 1141/2500 [0m                     

                       Computation: 42592 steps/s (collection: 2.213s, learning 0.095s)
             Mean action noise std: 2.78
          Mean value_function loss: 300.0392
               Mean surrogate loss: 0.0017
                 Mean entropy loss: 75.1405
                       Mean reward: 250.23
               Mean episode length: 123.64
    Episode_Reward/reaching_object: 1.0876
     Episode_Reward/lifting_object: 50.3745
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0400
          Episode_Reward/joint_vel: -0.0801
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 112263168
                    Iteration time: 2.31s
                      Time elapsed: 00:43:56
                               ETA: 00:52:17

################################################################################
                     [1m Learning iteration 1142/2500 [0m                     

                       Computation: 42865 steps/s (collection: 2.181s, learning 0.113s)
             Mean action noise std: 2.78
          Mean value_function loss: 269.7236
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 75.1441
                       Mean reward: 271.57
               Mean episode length: 133.24
    Episode_Reward/reaching_object: 1.0931
     Episode_Reward/lifting_object: 50.8007
      Episode_Reward/object_height: 0.0032
        Episode_Reward/action_rate: -0.0402
          Episode_Reward/joint_vel: -0.0816
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.7500
--------------------------------------------------------------------------------
                   Total timesteps: 112361472
                    Iteration time: 2.29s
                      Time elapsed: 00:43:58
                               ETA: 00:52:15

################################################################################
                     [1m Learning iteration 1143/2500 [0m                     

                       Computation: 42161 steps/s (collection: 2.214s, learning 0.118s)
             Mean action noise std: 2.78
          Mean value_function loss: 298.2141
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 75.1464
                       Mean reward: 252.91
               Mean episode length: 127.74
    Episode_Reward/reaching_object: 1.1171
     Episode_Reward/lifting_object: 51.8762
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0412
          Episode_Reward/joint_vel: -0.0838
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 32.1667
--------------------------------------------------------------------------------
                   Total timesteps: 112459776
                    Iteration time: 2.33s
                      Time elapsed: 00:44:01
                               ETA: 00:52:12

################################################################################
                     [1m Learning iteration 1144/2500 [0m                     

                       Computation: 42134 steps/s (collection: 2.238s, learning 0.095s)
             Mean action noise std: 2.78
          Mean value_function loss: 274.9552
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 75.1472
                       Mean reward: 249.64
               Mean episode length: 121.37
    Episode_Reward/reaching_object: 1.1302
     Episode_Reward/lifting_object: 52.7565
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0415
          Episode_Reward/joint_vel: -0.0848
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.2083
--------------------------------------------------------------------------------
                   Total timesteps: 112558080
                    Iteration time: 2.33s
                      Time elapsed: 00:44:03
                               ETA: 00:52:10

################################################################################
                     [1m Learning iteration 1145/2500 [0m                     

                       Computation: 42128 steps/s (collection: 2.219s, learning 0.115s)
             Mean action noise std: 2.78
          Mean value_function loss: 275.1915
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 75.1484
                       Mean reward: 274.37
               Mean episode length: 129.57
    Episode_Reward/reaching_object: 1.1195
     Episode_Reward/lifting_object: 52.3206
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0410
          Episode_Reward/joint_vel: -0.0820
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 112656384
                    Iteration time: 2.33s
                      Time elapsed: 00:44:05
                               ETA: 00:52:08

################################################################################
                     [1m Learning iteration 1146/2500 [0m                     

                       Computation: 42285 steps/s (collection: 2.212s, learning 0.113s)
             Mean action noise std: 2.78
          Mean value_function loss: 278.3131
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 75.1526
                       Mean reward: 245.44
               Mean episode length: 121.09
    Episode_Reward/reaching_object: 1.1346
     Episode_Reward/lifting_object: 53.0158
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0416
          Episode_Reward/joint_vel: -0.0849
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 31.7500
--------------------------------------------------------------------------------
                   Total timesteps: 112754688
                    Iteration time: 2.32s
                      Time elapsed: 00:44:08
                               ETA: 00:52:06

################################################################################
                     [1m Learning iteration 1147/2500 [0m                     

                       Computation: 42314 steps/s (collection: 2.213s, learning 0.110s)
             Mean action noise std: 2.78
          Mean value_function loss: 296.0023
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 75.1607
                       Mean reward: 278.39
               Mean episode length: 131.39
    Episode_Reward/reaching_object: 1.1254
     Episode_Reward/lifting_object: 53.0930
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0414
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.3333
--------------------------------------------------------------------------------
                   Total timesteps: 112852992
                    Iteration time: 2.32s
                      Time elapsed: 00:44:10
                               ETA: 00:52:03

################################################################################
                     [1m Learning iteration 1148/2500 [0m                     

                       Computation: 41350 steps/s (collection: 2.258s, learning 0.120s)
             Mean action noise std: 2.78
          Mean value_function loss: 268.7176
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 75.1675
                       Mean reward: 262.56
               Mean episode length: 124.80
    Episode_Reward/reaching_object: 1.1148
     Episode_Reward/lifting_object: 52.8770
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0411
          Episode_Reward/joint_vel: -0.0832
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.3333
--------------------------------------------------------------------------------
                   Total timesteps: 112951296
                    Iteration time: 2.38s
                      Time elapsed: 00:44:12
                               ETA: 00:52:01

################################################################################
                     [1m Learning iteration 1149/2500 [0m                     

                       Computation: 42485 steps/s (collection: 2.202s, learning 0.112s)
             Mean action noise std: 2.78
          Mean value_function loss: 281.7217
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 75.1723
                       Mean reward: 283.38
               Mean episode length: 137.11
    Episode_Reward/reaching_object: 1.1447
     Episode_Reward/lifting_object: 53.8079
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0422
          Episode_Reward/joint_vel: -0.0858
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.5000
--------------------------------------------------------------------------------
                   Total timesteps: 113049600
                    Iteration time: 2.31s
                      Time elapsed: 00:44:15
                               ETA: 00:51:59

################################################################################
                     [1m Learning iteration 1150/2500 [0m                     

                       Computation: 42428 steps/s (collection: 2.205s, learning 0.112s)
             Mean action noise std: 2.78
          Mean value_function loss: 280.4709
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 75.1769
                       Mean reward: 269.68
               Mean episode length: 133.00
    Episode_Reward/reaching_object: 1.1013
     Episode_Reward/lifting_object: 51.7317
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0406
          Episode_Reward/joint_vel: -0.0814
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113147904
                    Iteration time: 2.32s
                      Time elapsed: 00:44:17
                               ETA: 00:51:57

################################################################################
                     [1m Learning iteration 1151/2500 [0m                     

                       Computation: 42570 steps/s (collection: 2.218s, learning 0.091s)
             Mean action noise std: 2.78
          Mean value_function loss: 274.3366
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 75.1803
                       Mean reward: 253.09
               Mean episode length: 126.60
    Episode_Reward/reaching_object: 1.1194
     Episode_Reward/lifting_object: 52.4478
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0839
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 113246208
                    Iteration time: 2.31s
                      Time elapsed: 00:44:19
                               ETA: 00:51:54

################################################################################
                     [1m Learning iteration 1152/2500 [0m                     

                       Computation: 42273 steps/s (collection: 2.216s, learning 0.109s)
             Mean action noise std: 2.78
          Mean value_function loss: 274.5470
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 75.1855
                       Mean reward: 287.90
               Mean episode length: 134.42
    Episode_Reward/reaching_object: 1.1360
     Episode_Reward/lifting_object: 53.9390
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0846
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113344512
                    Iteration time: 2.33s
                      Time elapsed: 00:44:22
                               ETA: 00:51:52

################################################################################
                     [1m Learning iteration 1153/2500 [0m                     

                       Computation: 42182 steps/s (collection: 2.204s, learning 0.127s)
             Mean action noise std: 2.78
          Mean value_function loss: 269.8517
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 75.1925
                       Mean reward: 271.81
               Mean episode length: 131.12
    Episode_Reward/reaching_object: 1.1394
     Episode_Reward/lifting_object: 53.5967
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0419
          Episode_Reward/joint_vel: -0.0856
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.5000
--------------------------------------------------------------------------------
                   Total timesteps: 113442816
                    Iteration time: 2.33s
                      Time elapsed: 00:44:24
                               ETA: 00:51:50

################################################################################
                     [1m Learning iteration 1154/2500 [0m                     

                       Computation: 42836 steps/s (collection: 2.201s, learning 0.094s)
             Mean action noise std: 2.79
          Mean value_function loss: 274.7121
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 75.1987
                       Mean reward: 273.79
               Mean episode length: 131.13
    Episode_Reward/reaching_object: 1.1243
     Episode_Reward/lifting_object: 53.0145
      Episode_Reward/object_height: 0.0033
        Episode_Reward/action_rate: -0.0413
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.5833
--------------------------------------------------------------------------------
                   Total timesteps: 113541120
                    Iteration time: 2.29s
                      Time elapsed: 00:44:26
                               ETA: 00:51:47

################################################################################
                     [1m Learning iteration 1155/2500 [0m                     

                       Computation: 42339 steps/s (collection: 2.210s, learning 0.112s)
             Mean action noise std: 2.79
          Mean value_function loss: 271.3374
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 75.2035
                       Mean reward: 266.08
               Mean episode length: 130.70
    Episode_Reward/reaching_object: 1.1440
     Episode_Reward/lifting_object: 53.8392
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0869
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.4583
--------------------------------------------------------------------------------
                   Total timesteps: 113639424
                    Iteration time: 2.32s
                      Time elapsed: 00:44:29
                               ETA: 00:51:45

################################################################################
                     [1m Learning iteration 1156/2500 [0m                     

                       Computation: 43155 steps/s (collection: 2.176s, learning 0.102s)
             Mean action noise std: 2.79
          Mean value_function loss: 276.4261
               Mean surrogate loss: 0.0082
                 Mean entropy loss: 75.2063
                       Mean reward: 276.98
               Mean episode length: 137.38
    Episode_Reward/reaching_object: 1.1831
     Episode_Reward/lifting_object: 56.0140
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0880
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.0833
--------------------------------------------------------------------------------
                   Total timesteps: 113737728
                    Iteration time: 2.28s
                      Time elapsed: 00:44:31
                               ETA: 00:51:43

################################################################################
                     [1m Learning iteration 1157/2500 [0m                     

                       Computation: 41920 steps/s (collection: 2.233s, learning 0.112s)
             Mean action noise std: 2.79
          Mean value_function loss: 275.2630
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 75.2085
                       Mean reward: 277.24
               Mean episode length: 133.91
    Episode_Reward/reaching_object: 1.1694
     Episode_Reward/lifting_object: 55.0506
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0874
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.3333
--------------------------------------------------------------------------------
                   Total timesteps: 113836032
                    Iteration time: 2.35s
                      Time elapsed: 00:44:33
                               ETA: 00:51:40

################################################################################
                     [1m Learning iteration 1158/2500 [0m                     

                       Computation: 41682 steps/s (collection: 2.250s, learning 0.109s)
             Mean action noise std: 2.79
          Mean value_function loss: 301.9030
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 75.2137
                       Mean reward: 285.23
               Mean episode length: 140.62
    Episode_Reward/reaching_object: 1.1967
     Episode_Reward/lifting_object: 55.6303
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.7083
--------------------------------------------------------------------------------
                   Total timesteps: 113934336
                    Iteration time: 2.36s
                      Time elapsed: 00:44:36
                               ETA: 00:51:38

################################################################################
                     [1m Learning iteration 1159/2500 [0m                     

                       Computation: 40637 steps/s (collection: 2.262s, learning 0.158s)
             Mean action noise std: 2.79
          Mean value_function loss: 273.5274
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 75.2221
                       Mean reward: 261.77
               Mean episode length: 128.23
    Episode_Reward/reaching_object: 1.2057
     Episode_Reward/lifting_object: 55.5520
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.6667
--------------------------------------------------------------------------------
                   Total timesteps: 114032640
                    Iteration time: 2.42s
                      Time elapsed: 00:44:38
                               ETA: 00:51:36

################################################################################
                     [1m Learning iteration 1160/2500 [0m                     

                       Computation: 42894 steps/s (collection: 2.190s, learning 0.101s)
             Mean action noise std: 2.79
          Mean value_function loss: 309.2799
               Mean surrogate loss: 0.0063
                 Mean entropy loss: 75.2291
                       Mean reward: 277.07
               Mean episode length: 134.47
    Episode_Reward/reaching_object: 1.1942
     Episode_Reward/lifting_object: 54.8386
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0906
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.5000
--------------------------------------------------------------------------------
                   Total timesteps: 114130944
                    Iteration time: 2.29s
                      Time elapsed: 00:44:40
                               ETA: 00:51:34

################################################################################
                     [1m Learning iteration 1161/2500 [0m                     

                       Computation: 42638 steps/s (collection: 2.216s, learning 0.090s)
             Mean action noise std: 2.79
          Mean value_function loss: 284.3181
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 75.2318
                       Mean reward: 269.74
               Mean episode length: 132.74
    Episode_Reward/reaching_object: 1.2063
     Episode_Reward/lifting_object: 54.9706
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0930
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 27.4583
--------------------------------------------------------------------------------
                   Total timesteps: 114229248
                    Iteration time: 2.31s
                      Time elapsed: 00:44:43
                               ETA: 00:51:31

################################################################################
                     [1m Learning iteration 1162/2500 [0m                     

                       Computation: 42833 steps/s (collection: 2.188s, learning 0.107s)
             Mean action noise std: 2.79
          Mean value_function loss: 276.1165
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 75.2360
                       Mean reward: 297.11
               Mean episode length: 141.06
    Episode_Reward/reaching_object: 1.2116
     Episode_Reward/lifting_object: 55.5378
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0917
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.3750
--------------------------------------------------------------------------------
                   Total timesteps: 114327552
                    Iteration time: 2.30s
                      Time elapsed: 00:44:45
                               ETA: 00:51:29

################################################################################
                     [1m Learning iteration 1163/2500 [0m                     

                       Computation: 42483 steps/s (collection: 2.203s, learning 0.111s)
             Mean action noise std: 2.79
          Mean value_function loss: 271.3762
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 75.2400
                       Mean reward: 280.14
               Mean episode length: 136.96
    Episode_Reward/reaching_object: 1.2416
     Episode_Reward/lifting_object: 56.7026
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.0417
--------------------------------------------------------------------------------
                   Total timesteps: 114425856
                    Iteration time: 2.31s
                      Time elapsed: 00:44:47
                               ETA: 00:51:27

################################################################################
                     [1m Learning iteration 1164/2500 [0m                     

                       Computation: 42220 steps/s (collection: 2.200s, learning 0.128s)
             Mean action noise std: 2.79
          Mean value_function loss: 271.8698
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 75.2481
                       Mean reward: 277.25
               Mean episode length: 132.23
    Episode_Reward/reaching_object: 1.2010
     Episode_Reward/lifting_object: 55.0625
      Episode_Reward/object_height: 0.0034
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 26.6250
--------------------------------------------------------------------------------
                   Total timesteps: 114524160
                    Iteration time: 2.33s
                      Time elapsed: 00:44:50
                               ETA: 00:51:24

################################################################################
                     [1m Learning iteration 1165/2500 [0m                     

                       Computation: 41243 steps/s (collection: 2.266s, learning 0.118s)
             Mean action noise std: 2.79
          Mean value_function loss: 272.6871
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 75.2571
                       Mean reward: 305.81
               Mean episode length: 143.50
    Episode_Reward/reaching_object: 1.2475
     Episode_Reward/lifting_object: 57.7049
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.9583
--------------------------------------------------------------------------------
                   Total timesteps: 114622464
                    Iteration time: 2.38s
                      Time elapsed: 00:44:52
                               ETA: 00:51:22

################################################################################
                     [1m Learning iteration 1166/2500 [0m                     

                       Computation: 41395 steps/s (collection: 2.258s, learning 0.117s)
             Mean action noise std: 2.79
          Mean value_function loss: 279.8164
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 75.2635
                       Mean reward: 264.37
               Mean episode length: 126.87
    Episode_Reward/reaching_object: 1.2545
     Episode_Reward/lifting_object: 57.7843
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114720768
                    Iteration time: 2.37s
                      Time elapsed: 00:44:54
                               ETA: 00:51:20

################################################################################
                     [1m Learning iteration 1167/2500 [0m                     

                       Computation: 41049 steps/s (collection: 2.277s, learning 0.118s)
             Mean action noise std: 2.79
          Mean value_function loss: 272.7660
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 75.2678
                       Mean reward: 315.98
               Mean episode length: 148.75
    Episode_Reward/reaching_object: 1.2677
     Episode_Reward/lifting_object: 58.8102
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0961
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.5417
--------------------------------------------------------------------------------
                   Total timesteps: 114819072
                    Iteration time: 2.39s
                      Time elapsed: 00:44:57
                               ETA: 00:51:18

################################################################################
                     [1m Learning iteration 1168/2500 [0m                     

                       Computation: 42180 steps/s (collection: 2.229s, learning 0.102s)
             Mean action noise std: 2.79
          Mean value_function loss: 290.1691
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 75.2761
                       Mean reward: 297.16
               Mean episode length: 139.55
    Episode_Reward/reaching_object: 1.2563
     Episode_Reward/lifting_object: 58.5089
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 28.2083
--------------------------------------------------------------------------------
                   Total timesteps: 114917376
                    Iteration time: 2.33s
                      Time elapsed: 00:44:59
                               ETA: 00:51:15

################################################################################
                     [1m Learning iteration 1169/2500 [0m                     

                       Computation: 42282 steps/s (collection: 2.215s, learning 0.110s)
             Mean action noise std: 2.79
          Mean value_function loss: 278.2105
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 75.2854
                       Mean reward: 302.96
               Mean episode length: 144.00
    Episode_Reward/reaching_object: 1.2704
     Episode_Reward/lifting_object: 58.7555
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 26.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115015680
                    Iteration time: 2.32s
                      Time elapsed: 00:45:01
                               ETA: 00:51:13

################################################################################
                     [1m Learning iteration 1170/2500 [0m                     

                       Computation: 42535 steps/s (collection: 2.220s, learning 0.091s)
             Mean action noise std: 2.79
          Mean value_function loss: 279.6287
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 75.2890
                       Mean reward: 298.80
               Mean episode length: 141.37
    Episode_Reward/reaching_object: 1.2482
     Episode_Reward/lifting_object: 57.5456
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.6667
--------------------------------------------------------------------------------
                   Total timesteps: 115113984
                    Iteration time: 2.31s
                      Time elapsed: 00:45:04
                               ETA: 00:51:11

################################################################################
                     [1m Learning iteration 1171/2500 [0m                     

                       Computation: 42237 steps/s (collection: 2.214s, learning 0.114s)
             Mean action noise std: 2.80
          Mean value_function loss: 313.3242
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 75.2960
                       Mean reward: 288.67
               Mean episode length: 139.20
    Episode_Reward/reaching_object: 1.2388
     Episode_Reward/lifting_object: 57.2180
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.2500
--------------------------------------------------------------------------------
                   Total timesteps: 115212288
                    Iteration time: 2.33s
                      Time elapsed: 00:45:06
                               ETA: 00:51:09

################################################################################
                     [1m Learning iteration 1172/2500 [0m                     

                       Computation: 41679 steps/s (collection: 2.216s, learning 0.143s)
             Mean action noise std: 2.80
          Mean value_function loss: 287.3118
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 75.2995
                       Mean reward: 305.46
               Mean episode length: 143.07
    Episode_Reward/reaching_object: 1.2151
     Episode_Reward/lifting_object: 56.4769
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0929
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.7917
--------------------------------------------------------------------------------
                   Total timesteps: 115310592
                    Iteration time: 2.36s
                      Time elapsed: 00:45:08
                               ETA: 00:51:06

################################################################################
                     [1m Learning iteration 1173/2500 [0m                     

                       Computation: 41881 steps/s (collection: 2.240s, learning 0.107s)
             Mean action noise std: 2.80
          Mean value_function loss: 274.2452
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 75.3042
                       Mean reward: 294.64
               Mean episode length: 139.82
    Episode_Reward/reaching_object: 1.2480
     Episode_Reward/lifting_object: 57.6290
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0954
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 25.9167
--------------------------------------------------------------------------------
                   Total timesteps: 115408896
                    Iteration time: 2.35s
                      Time elapsed: 00:45:11
                               ETA: 00:51:04

################################################################################
                     [1m Learning iteration 1174/2500 [0m                     

                       Computation: 41213 steps/s (collection: 2.257s, learning 0.128s)
             Mean action noise std: 2.80
          Mean value_function loss: 266.2350
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 75.3086
                       Mean reward: 300.98
               Mean episode length: 141.85
    Episode_Reward/reaching_object: 1.2580
     Episode_Reward/lifting_object: 58.3935
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0964
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 24.5000
--------------------------------------------------------------------------------
                   Total timesteps: 115507200
                    Iteration time: 2.39s
                      Time elapsed: 00:45:13
                               ETA: 00:51:02

################################################################################
                     [1m Learning iteration 1175/2500 [0m                     

                       Computation: 41477 steps/s (collection: 2.251s, learning 0.119s)
             Mean action noise std: 2.80
          Mean value_function loss: 254.4782
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 75.3150
                       Mean reward: 325.37
               Mean episode length: 153.89
    Episode_Reward/reaching_object: 1.3081
     Episode_Reward/lifting_object: 60.1113
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0472
          Episode_Reward/joint_vel: -0.1007
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 24.0000
--------------------------------------------------------------------------------
                   Total timesteps: 115605504
                    Iteration time: 2.37s
                      Time elapsed: 00:45:15
                               ETA: 00:51:00

################################################################################
                     [1m Learning iteration 1176/2500 [0m                     

                       Computation: 42055 steps/s (collection: 2.225s, learning 0.112s)
             Mean action noise std: 2.80
          Mean value_function loss: 291.8278
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 75.3225
                       Mean reward: 291.68
               Mean episode length: 138.60
    Episode_Reward/reaching_object: 1.2533
     Episode_Reward/lifting_object: 57.8452
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0984
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.7917
--------------------------------------------------------------------------------
                   Total timesteps: 115703808
                    Iteration time: 2.34s
                      Time elapsed: 00:45:18
                               ETA: 00:50:57

################################################################################
                     [1m Learning iteration 1177/2500 [0m                     

                       Computation: 42139 steps/s (collection: 2.214s, learning 0.119s)
             Mean action noise std: 2.80
          Mean value_function loss: 281.2582
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 75.3299
                       Mean reward: 287.01
               Mean episode length: 139.64
    Episode_Reward/reaching_object: 1.3287
     Episode_Reward/lifting_object: 61.4404
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0481
          Episode_Reward/joint_vel: -0.1035
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 24.6250
--------------------------------------------------------------------------------
                   Total timesteps: 115802112
                    Iteration time: 2.33s
                      Time elapsed: 00:45:20
                               ETA: 00:50:55

################################################################################
                     [1m Learning iteration 1178/2500 [0m                     

                       Computation: 41303 steps/s (collection: 2.264s, learning 0.116s)
             Mean action noise std: 2.80
          Mean value_function loss: 268.4147
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 75.3373
                       Mean reward: 322.33
               Mean episode length: 155.20
    Episode_Reward/reaching_object: 1.3253
     Episode_Reward/lifting_object: 60.8553
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.1027
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.5000
--------------------------------------------------------------------------------
                   Total timesteps: 115900416
                    Iteration time: 2.38s
                      Time elapsed: 00:45:23
                               ETA: 00:50:53

################################################################################
                     [1m Learning iteration 1179/2500 [0m                     

                       Computation: 41389 steps/s (collection: 2.254s, learning 0.121s)
             Mean action noise std: 2.80
          Mean value_function loss: 265.5236
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 75.3456
                       Mean reward: 303.00
               Mean episode length: 143.87
    Episode_Reward/reaching_object: 1.3141
     Episode_Reward/lifting_object: 60.4102
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.1026
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 26.2917
--------------------------------------------------------------------------------
                   Total timesteps: 115998720
                    Iteration time: 2.38s
                      Time elapsed: 00:45:25
                               ETA: 00:50:51

################################################################################
                     [1m Learning iteration 1180/2500 [0m                     

                       Computation: 41933 steps/s (collection: 2.217s, learning 0.127s)
             Mean action noise std: 2.80
          Mean value_function loss: 262.7927
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 75.3486
                       Mean reward: 306.93
               Mean episode length: 146.42
    Episode_Reward/reaching_object: 1.3251
     Episode_Reward/lifting_object: 61.3434
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.1021
      Episode_Termination/time_out: 2.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.9583
--------------------------------------------------------------------------------
                   Total timesteps: 116097024
                    Iteration time: 2.34s
                      Time elapsed: 00:45:27
                               ETA: 00:50:48

################################################################################
                     [1m Learning iteration 1181/2500 [0m                     

                       Computation: 41765 steps/s (collection: 2.255s, learning 0.099s)
             Mean action noise std: 2.80
          Mean value_function loss: 275.4267
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 75.3507
                       Mean reward: 284.85
               Mean episode length: 138.76
    Episode_Reward/reaching_object: 1.2840
     Episode_Reward/lifting_object: 59.0307
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.1000
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 25.9167
--------------------------------------------------------------------------------
                   Total timesteps: 116195328
                    Iteration time: 2.35s
                      Time elapsed: 00:45:30
                               ETA: 00:50:46

################################################################################
                     [1m Learning iteration 1182/2500 [0m                     

                       Computation: 41825 steps/s (collection: 2.237s, learning 0.114s)
             Mean action noise std: 2.80
          Mean value_function loss: 275.3734
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 75.3561
                       Mean reward: 297.24
               Mean episode length: 145.28
    Episode_Reward/reaching_object: 1.3505
     Episode_Reward/lifting_object: 61.6176
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.1057
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 24.2083
--------------------------------------------------------------------------------
                   Total timesteps: 116293632
                    Iteration time: 2.35s
                      Time elapsed: 00:45:32
                               ETA: 00:50:44

################################################################################
                     [1m Learning iteration 1183/2500 [0m                     

                       Computation: 40911 steps/s (collection: 2.266s, learning 0.137s)
             Mean action noise std: 2.80
          Mean value_function loss: 287.5138
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 75.3633
                       Mean reward: 326.46
               Mean episode length: 157.61
    Episode_Reward/reaching_object: 1.3286
     Episode_Reward/lifting_object: 60.4877
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.1053
      Episode_Termination/time_out: 3.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.8333
--------------------------------------------------------------------------------
                   Total timesteps: 116391936
                    Iteration time: 2.40s
                      Time elapsed: 00:45:34
                               ETA: 00:50:42

################################################################################
                     [1m Learning iteration 1184/2500 [0m                     

                       Computation: 41974 steps/s (collection: 2.240s, learning 0.102s)
             Mean action noise std: 2.80
          Mean value_function loss: 275.2403
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 75.3725
                       Mean reward: 286.34
               Mean episode length: 138.35
    Episode_Reward/reaching_object: 1.3088
     Episode_Reward/lifting_object: 60.4732
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.1021
      Episode_Termination/time_out: 3.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 25.2917
--------------------------------------------------------------------------------
                   Total timesteps: 116490240
                    Iteration time: 2.34s
                      Time elapsed: 00:45:37
                               ETA: 00:50:39

################################################################################
                     [1m Learning iteration 1185/2500 [0m                     

                       Computation: 42419 steps/s (collection: 2.229s, learning 0.088s)
             Mean action noise std: 2.80
          Mean value_function loss: 252.0822
               Mean surrogate loss: 0.0073
                 Mean entropy loss: 75.3810
                       Mean reward: 301.81
               Mean episode length: 146.87
    Episode_Reward/reaching_object: 1.3433
     Episode_Reward/lifting_object: 61.8057
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0486
          Episode_Reward/joint_vel: -0.1052
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 22.8333
--------------------------------------------------------------------------------
                   Total timesteps: 116588544
                    Iteration time: 2.32s
                      Time elapsed: 00:45:39
                               ETA: 00:50:37

################################################################################
                     [1m Learning iteration 1186/2500 [0m                     

                       Computation: 42147 steps/s (collection: 2.217s, learning 0.115s)
             Mean action noise std: 2.80
          Mean value_function loss: 291.8914
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 75.3844
                       Mean reward: 291.99
               Mean episode length: 138.81
    Episode_Reward/reaching_object: 1.2929
     Episode_Reward/lifting_object: 59.9638
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0977
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 25.5417
--------------------------------------------------------------------------------
                   Total timesteps: 116686848
                    Iteration time: 2.33s
                      Time elapsed: 00:45:41
                               ETA: 00:50:35

################################################################################
                     [1m Learning iteration 1187/2500 [0m                     

                       Computation: 42015 steps/s (collection: 2.200s, learning 0.140s)
             Mean action noise std: 2.81
          Mean value_function loss: 290.1090
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 75.3894
                       Mean reward: 318.93
               Mean episode length: 152.28
    Episode_Reward/reaching_object: 1.3130
     Episode_Reward/lifting_object: 60.1884
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.1007
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.4167
--------------------------------------------------------------------------------
                   Total timesteps: 116785152
                    Iteration time: 2.34s
                      Time elapsed: 00:45:44
                               ETA: 00:50:32

################################################################################
                     [1m Learning iteration 1188/2500 [0m                     

                       Computation: 42146 steps/s (collection: 2.238s, learning 0.095s)
             Mean action noise std: 2.81
          Mean value_function loss: 271.3939
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 75.3976
                       Mean reward: 313.67
               Mean episode length: 146.06
    Episode_Reward/reaching_object: 1.3056
     Episode_Reward/lifting_object: 61.1422
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.1001
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 26.5833
--------------------------------------------------------------------------------
                   Total timesteps: 116883456
                    Iteration time: 2.33s
                      Time elapsed: 00:45:46
                               ETA: 00:50:30

################################################################################
                     [1m Learning iteration 1189/2500 [0m                     

                       Computation: 41433 steps/s (collection: 2.267s, learning 0.106s)
             Mean action noise std: 2.81
          Mean value_function loss: 269.0019
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 75.4059
                       Mean reward: 326.34
               Mean episode length: 152.80
    Episode_Reward/reaching_object: 1.3202
     Episode_Reward/lifting_object: 61.7864
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.1010
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 24.2083
--------------------------------------------------------------------------------
                   Total timesteps: 116981760
                    Iteration time: 2.37s
                      Time elapsed: 00:45:48
                               ETA: 00:50:28

################################################################################
                     [1m Learning iteration 1190/2500 [0m                     

                       Computation: 41667 steps/s (collection: 2.243s, learning 0.117s)
             Mean action noise std: 2.81
          Mean value_function loss: 279.0109
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 75.4136
                       Mean reward: 336.92
               Mean episode length: 156.68
    Episode_Reward/reaching_object: 1.3282
     Episode_Reward/lifting_object: 62.0435
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.1015
      Episode_Termination/time_out: 2.9583
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 25.5417
--------------------------------------------------------------------------------
                   Total timesteps: 117080064
                    Iteration time: 2.36s
                      Time elapsed: 00:45:51
                               ETA: 00:50:26

################################################################################
                     [1m Learning iteration 1191/2500 [0m                     

                       Computation: 41379 steps/s (collection: 2.271s, learning 0.104s)
             Mean action noise std: 2.81
          Mean value_function loss: 308.8363
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 75.4208
                       Mean reward: 300.97
               Mean episode length: 144.96
    Episode_Reward/reaching_object: 1.2838
     Episode_Reward/lifting_object: 59.9561
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0977
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.8333
--------------------------------------------------------------------------------
                   Total timesteps: 117178368
                    Iteration time: 2.38s
                      Time elapsed: 00:45:53
                               ETA: 00:50:23

################################################################################
                     [1m Learning iteration 1192/2500 [0m                     

                       Computation: 41894 steps/s (collection: 2.237s, learning 0.110s)
             Mean action noise std: 2.81
          Mean value_function loss: 282.8082
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 75.4261
                       Mean reward: 293.45
               Mean episode length: 138.15
    Episode_Reward/reaching_object: 1.2669
     Episode_Reward/lifting_object: 58.5951
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0976
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 26.5833
--------------------------------------------------------------------------------
                   Total timesteps: 117276672
                    Iteration time: 2.35s
                      Time elapsed: 00:45:55
                               ETA: 00:50:21

################################################################################
                     [1m Learning iteration 1193/2500 [0m                     

                       Computation: 42872 steps/s (collection: 2.199s, learning 0.094s)
             Mean action noise std: 2.81
          Mean value_function loss: 302.0441
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 75.4335
                       Mean reward: 325.93
               Mean episode length: 152.33
    Episode_Reward/reaching_object: 1.2866
     Episode_Reward/lifting_object: 59.6114
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0989
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.1667
--------------------------------------------------------------------------------
                   Total timesteps: 117374976
                    Iteration time: 2.29s
                      Time elapsed: 00:45:58
                               ETA: 00:50:19

################################################################################
                     [1m Learning iteration 1194/2500 [0m                     

                       Computation: 42348 steps/s (collection: 2.207s, learning 0.114s)
             Mean action noise std: 2.81
          Mean value_function loss: 316.8223
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 75.4477
                       Mean reward: 280.61
               Mean episode length: 133.40
    Episode_Reward/reaching_object: 1.2536
     Episode_Reward/lifting_object: 58.7389
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0957
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 25.7917
--------------------------------------------------------------------------------
                   Total timesteps: 117473280
                    Iteration time: 2.32s
                      Time elapsed: 00:46:00
                               ETA: 00:50:17

################################################################################
                     [1m Learning iteration 1195/2500 [0m                     

                       Computation: 41832 steps/s (collection: 2.227s, learning 0.123s)
             Mean action noise std: 2.81
          Mean value_function loss: 275.7191
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 75.4557
                       Mean reward: 304.62
               Mean episode length: 146.47
    Episode_Reward/reaching_object: 1.2656
     Episode_Reward/lifting_object: 58.6668
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0994
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 25.9167
--------------------------------------------------------------------------------
                   Total timesteps: 117571584
                    Iteration time: 2.35s
                      Time elapsed: 00:46:02
                               ETA: 00:50:14

################################################################################
                     [1m Learning iteration 1196/2500 [0m                     

                       Computation: 41666 steps/s (collection: 2.238s, learning 0.121s)
             Mean action noise std: 2.81
          Mean value_function loss: 265.7291
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 75.4595
                       Mean reward: 333.90
               Mean episode length: 153.24
    Episode_Reward/reaching_object: 1.2917
     Episode_Reward/lifting_object: 60.8262
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.1014
      Episode_Termination/time_out: 2.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 24.7917
--------------------------------------------------------------------------------
                   Total timesteps: 117669888
                    Iteration time: 2.36s
                      Time elapsed: 00:46:05
                               ETA: 00:50:12

################################################################################
                     [1m Learning iteration 1197/2500 [0m                     

                       Computation: 42691 steps/s (collection: 2.196s, learning 0.107s)
             Mean action noise std: 2.81
          Mean value_function loss: 282.7540
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 75.4650
                       Mean reward: 303.05
               Mean episode length: 140.25
    Episode_Reward/reaching_object: 1.3234
     Episode_Reward/lifting_object: 62.4566
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0478
          Episode_Reward/joint_vel: -0.1015
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 24.3333
--------------------------------------------------------------------------------
                   Total timesteps: 117768192
                    Iteration time: 2.30s
                      Time elapsed: 00:46:07
                               ETA: 00:50:10

################################################################################
                     [1m Learning iteration 1198/2500 [0m                     

                       Computation: 41645 steps/s (collection: 2.266s, learning 0.095s)
             Mean action noise std: 2.81
          Mean value_function loss: 274.9794
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 75.4686
                       Mean reward: 332.46
               Mean episode length: 153.06
    Episode_Reward/reaching_object: 1.2924
     Episode_Reward/lifting_object: 60.5454
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0998
      Episode_Termination/time_out: 2.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.3333
--------------------------------------------------------------------------------
                   Total timesteps: 117866496
                    Iteration time: 2.36s
                      Time elapsed: 00:46:09
                               ETA: 00:50:07

################################################################################
                     [1m Learning iteration 1199/2500 [0m                     

                       Computation: 41147 steps/s (collection: 2.260s, learning 0.129s)
             Mean action noise std: 2.81
          Mean value_function loss: 286.4187
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 75.4754
                       Mean reward: 323.13
               Mean episode length: 149.59
    Episode_Reward/reaching_object: 1.2853
     Episode_Reward/lifting_object: 60.9580
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0992
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 117964800
                    Iteration time: 2.39s
                      Time elapsed: 00:46:12
                               ETA: 00:50:05

################################################################################
                     [1m Learning iteration 1200/2500 [0m                     

                       Computation: 41686 steps/s (collection: 2.259s, learning 0.099s)
             Mean action noise std: 2.81
          Mean value_function loss: 271.6421
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 75.4831
                       Mean reward: 313.92
               Mean episode length: 148.32
    Episode_Reward/reaching_object: 1.2855
     Episode_Reward/lifting_object: 60.7363
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.1007
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 24.8750
--------------------------------------------------------------------------------
                   Total timesteps: 118063104
                    Iteration time: 2.36s
                      Time elapsed: 00:46:14
                               ETA: 00:50:03

################################################################################
                     [1m Learning iteration 1201/2500 [0m                     

                       Computation: 41593 steps/s (collection: 2.256s, learning 0.108s)
             Mean action noise std: 2.82
          Mean value_function loss: 284.8711
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 75.4910
                       Mean reward: 307.18
               Mean episode length: 146.34
    Episode_Reward/reaching_object: 1.3398
     Episode_Reward/lifting_object: 62.4944
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.1052
      Episode_Termination/time_out: 3.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 23.9583
--------------------------------------------------------------------------------
                   Total timesteps: 118161408
                    Iteration time: 2.36s
                      Time elapsed: 00:46:17
                               ETA: 00:50:01

################################################################################
                     [1m Learning iteration 1202/2500 [0m                     

                       Computation: 41928 steps/s (collection: 2.240s, learning 0.105s)
             Mean action noise std: 2.82
          Mean value_function loss: 302.5747
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 75.5001
                       Mean reward: 322.88
               Mean episode length: 151.62
    Episode_Reward/reaching_object: 1.3137
     Episode_Reward/lifting_object: 61.8233
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.1025
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 26.1250
--------------------------------------------------------------------------------
                   Total timesteps: 118259712
                    Iteration time: 2.34s
                      Time elapsed: 00:46:19
                               ETA: 00:49:58

################################################################################
                     [1m Learning iteration 1203/2500 [0m                     

                       Computation: 42561 steps/s (collection: 2.192s, learning 0.118s)
             Mean action noise std: 2.82
          Mean value_function loss: 306.7720
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 75.5140
                       Mean reward: 310.00
               Mean episode length: 144.95
    Episode_Reward/reaching_object: 1.2833
     Episode_Reward/lifting_object: 60.6821
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0468
          Episode_Reward/joint_vel: -0.1004
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 25.5417
--------------------------------------------------------------------------------
                   Total timesteps: 118358016
                    Iteration time: 2.31s
                      Time elapsed: 00:46:21
                               ETA: 00:49:56

################################################################################
                     [1m Learning iteration 1204/2500 [0m                     

                       Computation: 42296 steps/s (collection: 2.227s, learning 0.097s)
             Mean action noise std: 2.82
          Mean value_function loss: 286.2189
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 75.5227
                       Mean reward: 303.96
               Mean episode length: 141.14
    Episode_Reward/reaching_object: 1.2737
     Episode_Reward/lifting_object: 59.7725
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.1006
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 25.5417
--------------------------------------------------------------------------------
                   Total timesteps: 118456320
                    Iteration time: 2.32s
                      Time elapsed: 00:46:24
                               ETA: 00:49:54

################################################################################
                     [1m Learning iteration 1205/2500 [0m                     

                       Computation: 41889 steps/s (collection: 2.221s, learning 0.126s)
             Mean action noise std: 2.82
          Mean value_function loss: 296.6171
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 75.5349
                       Mean reward: 327.45
               Mean episode length: 155.58
    Episode_Reward/reaching_object: 1.3219
     Episode_Reward/lifting_object: 61.6248
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.1044
      Episode_Termination/time_out: 3.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 118554624
                    Iteration time: 2.35s
                      Time elapsed: 00:46:26
                               ETA: 00:49:52

################################################################################
                     [1m Learning iteration 1206/2500 [0m                     

                       Computation: 41897 steps/s (collection: 2.238s, learning 0.108s)
             Mean action noise std: 2.82
          Mean value_function loss: 286.0664
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 75.5463
                       Mean reward: 304.31
               Mean episode length: 141.08
    Episode_Reward/reaching_object: 1.2855
     Episode_Reward/lifting_object: 60.3606
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.1000
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.0417
--------------------------------------------------------------------------------
                   Total timesteps: 118652928
                    Iteration time: 2.35s
                      Time elapsed: 00:46:28
                               ETA: 00:49:49

################################################################################
                     [1m Learning iteration 1207/2500 [0m                     

                       Computation: 41835 steps/s (collection: 2.231s, learning 0.119s)
             Mean action noise std: 2.82
          Mean value_function loss: 297.4125
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 75.5512
                       Mean reward: 294.58
               Mean episode length: 134.49
    Episode_Reward/reaching_object: 1.2448
     Episode_Reward/lifting_object: 59.1130
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0975
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.5000
--------------------------------------------------------------------------------
                   Total timesteps: 118751232
                    Iteration time: 2.35s
                      Time elapsed: 00:46:31
                               ETA: 00:49:47

################################################################################
                     [1m Learning iteration 1208/2500 [0m                     

                       Computation: 42375 steps/s (collection: 2.233s, learning 0.087s)
             Mean action noise std: 2.82
          Mean value_function loss: 306.5644
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 75.5610
                       Mean reward: 296.31
               Mean episode length: 139.02
    Episode_Reward/reaching_object: 1.2641
     Episode_Reward/lifting_object: 60.0765
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0977
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.1667
--------------------------------------------------------------------------------
                   Total timesteps: 118849536
                    Iteration time: 2.32s
                      Time elapsed: 00:46:33
                               ETA: 00:49:45

################################################################################
                     [1m Learning iteration 1209/2500 [0m                     

                       Computation: 42474 steps/s (collection: 2.223s, learning 0.092s)
             Mean action noise std: 2.82
          Mean value_function loss: 279.4943
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 75.5761
                       Mean reward: 323.50
               Mean episode length: 146.23
    Episode_Reward/reaching_object: 1.2721
     Episode_Reward/lifting_object: 60.4672
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0982
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 118947840
                    Iteration time: 2.31s
                      Time elapsed: 00:46:35
                               ETA: 00:49:42

################################################################################
                     [1m Learning iteration 1210/2500 [0m                     

                       Computation: 43082 steps/s (collection: 2.191s, learning 0.091s)
             Mean action noise std: 2.83
          Mean value_function loss: 308.0163
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 75.5877
                       Mean reward: 322.90
               Mean episode length: 144.79
    Episode_Reward/reaching_object: 1.2689
     Episode_Reward/lifting_object: 60.5083
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0966
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.4583
--------------------------------------------------------------------------------
                   Total timesteps: 119046144
                    Iteration time: 2.28s
                      Time elapsed: 00:46:38
                               ETA: 00:49:40

################################################################################
                     [1m Learning iteration 1211/2500 [0m                     

                       Computation: 42174 steps/s (collection: 2.230s, learning 0.101s)
             Mean action noise std: 2.83
          Mean value_function loss: 325.4970
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 75.5898
                       Mean reward: 286.89
               Mean episode length: 134.46
    Episode_Reward/reaching_object: 1.2245
     Episode_Reward/lifting_object: 58.0811
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0952
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.0833
--------------------------------------------------------------------------------
                   Total timesteps: 119144448
                    Iteration time: 2.33s
                      Time elapsed: 00:46:40
                               ETA: 00:49:38

################################################################################
                     [1m Learning iteration 1212/2500 [0m                     

                       Computation: 42024 steps/s (collection: 2.242s, learning 0.097s)
             Mean action noise std: 2.83
          Mean value_function loss: 301.3989
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 75.5928
                       Mean reward: 293.68
               Mean episode length: 136.50
    Episode_Reward/reaching_object: 1.1808
     Episode_Reward/lifting_object: 56.7576
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0911
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.1667
--------------------------------------------------------------------------------
                   Total timesteps: 119242752
                    Iteration time: 2.34s
                      Time elapsed: 00:46:42
                               ETA: 00:49:35

################################################################################
                     [1m Learning iteration 1213/2500 [0m                     

                       Computation: 42609 steps/s (collection: 2.198s, learning 0.109s)
             Mean action noise std: 2.83
          Mean value_function loss: 297.3808
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 75.5964
                       Mean reward: 311.40
               Mean episode length: 138.61
    Episode_Reward/reaching_object: 1.2370
     Episode_Reward/lifting_object: 59.1185
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0453
          Episode_Reward/joint_vel: -0.0957
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.9583
--------------------------------------------------------------------------------
                   Total timesteps: 119341056
                    Iteration time: 2.31s
                      Time elapsed: 00:46:44
                               ETA: 00:49:33

################################################################################
                     [1m Learning iteration 1214/2500 [0m                     

                       Computation: 40887 steps/s (collection: 2.252s, learning 0.152s)
             Mean action noise std: 2.83
          Mean value_function loss: 308.4704
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 75.5993
                       Mean reward: 289.99
               Mean episode length: 134.63
    Episode_Reward/reaching_object: 1.1798
     Episode_Reward/lifting_object: 56.0336
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0435
          Episode_Reward/joint_vel: -0.0922
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.3333
--------------------------------------------------------------------------------
                   Total timesteps: 119439360
                    Iteration time: 2.40s
                      Time elapsed: 00:46:47
                               ETA: 00:49:31

################################################################################
                     [1m Learning iteration 1215/2500 [0m                     

                       Computation: 41239 steps/s (collection: 2.264s, learning 0.120s)
             Mean action noise std: 2.83
          Mean value_function loss: 315.0449
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 75.6057
                       Mean reward: 312.62
               Mean episode length: 143.61
    Episode_Reward/reaching_object: 1.2160
     Episode_Reward/lifting_object: 58.0854
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0946
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.4583
--------------------------------------------------------------------------------
                   Total timesteps: 119537664
                    Iteration time: 2.38s
                      Time elapsed: 00:46:49
                               ETA: 00:49:29

################################################################################
                     [1m Learning iteration 1216/2500 [0m                     

                       Computation: 41007 steps/s (collection: 2.297s, learning 0.100s)
             Mean action noise std: 2.83
          Mean value_function loss: 327.2572
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 75.6185
                       Mean reward: 308.67
               Mean episode length: 141.15
    Episode_Reward/reaching_object: 1.2215
     Episode_Reward/lifting_object: 59.0053
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0940
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.1667
--------------------------------------------------------------------------------
                   Total timesteps: 119635968
                    Iteration time: 2.40s
                      Time elapsed: 00:46:52
                               ETA: 00:49:26

################################################################################
                     [1m Learning iteration 1217/2500 [0m                     

                       Computation: 41280 steps/s (collection: 2.269s, learning 0.113s)
             Mean action noise std: 2.83
          Mean value_function loss: 309.6689
               Mean surrogate loss: 0.0036
                 Mean entropy loss: 75.6314
                       Mean reward: 289.33
               Mean episode length: 136.66
    Episode_Reward/reaching_object: 1.1900
     Episode_Reward/lifting_object: 57.2789
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0930
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 119734272
                    Iteration time: 2.38s
                      Time elapsed: 00:46:54
                               ETA: 00:49:24

################################################################################
                     [1m Learning iteration 1218/2500 [0m                     

                       Computation: 42039 steps/s (collection: 2.245s, learning 0.094s)
             Mean action noise std: 2.83
          Mean value_function loss: 322.1874
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 75.6368
                       Mean reward: 293.43
               Mean episode length: 132.51
    Episode_Reward/reaching_object: 1.2021
     Episode_Reward/lifting_object: 58.6009
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.5417
--------------------------------------------------------------------------------
                   Total timesteps: 119832576
                    Iteration time: 2.34s
                      Time elapsed: 00:46:56
                               ETA: 00:49:22

################################################################################
                     [1m Learning iteration 1219/2500 [0m                     

                       Computation: 41651 steps/s (collection: 2.231s, learning 0.130s)
             Mean action noise std: 2.83
          Mean value_function loss: 305.0727
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 75.6472
                       Mean reward: 285.18
               Mean episode length: 128.54
    Episode_Reward/reaching_object: 1.1450
     Episode_Reward/lifting_object: 55.4468
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0426
          Episode_Reward/joint_vel: -0.0891
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.8333
--------------------------------------------------------------------------------
                   Total timesteps: 119930880
                    Iteration time: 2.36s
                      Time elapsed: 00:46:59
                               ETA: 00:49:20

################################################################################
                     [1m Learning iteration 1220/2500 [0m                     

                       Computation: 41098 steps/s (collection: 2.266s, learning 0.126s)
             Mean action noise std: 2.83
          Mean value_function loss: 305.7952
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 75.6610
                       Mean reward: 295.27
               Mean episode length: 132.75
    Episode_Reward/reaching_object: 1.1622
     Episode_Reward/lifting_object: 56.5320
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0904
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.4583
--------------------------------------------------------------------------------
                   Total timesteps: 120029184
                    Iteration time: 2.39s
                      Time elapsed: 00:47:01
                               ETA: 00:49:17

################################################################################
                     [1m Learning iteration 1221/2500 [0m                     

                       Computation: 41538 steps/s (collection: 2.263s, learning 0.104s)
             Mean action noise std: 2.83
          Mean value_function loss: 311.8258
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 75.6700
                       Mean reward: 294.14
               Mean episode length: 132.64
    Episode_Reward/reaching_object: 1.1652
     Episode_Reward/lifting_object: 57.4509
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0431
          Episode_Reward/joint_vel: -0.0893
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.5000
--------------------------------------------------------------------------------
                   Total timesteps: 120127488
                    Iteration time: 2.37s
                      Time elapsed: 00:47:04
                               ETA: 00:49:15

################################################################################
                     [1m Learning iteration 1222/2500 [0m                     

                       Computation: 41591 steps/s (collection: 2.246s, learning 0.118s)
             Mean action noise std: 2.83
          Mean value_function loss: 341.0485
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 75.6814
                       Mean reward: 311.08
               Mean episode length: 137.57
    Episode_Reward/reaching_object: 1.1667
     Episode_Reward/lifting_object: 57.0314
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0898
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.5833
--------------------------------------------------------------------------------
                   Total timesteps: 120225792
                    Iteration time: 2.36s
                      Time elapsed: 00:47:06
                               ETA: 00:49:13

################################################################################
                     [1m Learning iteration 1223/2500 [0m                     

                       Computation: 42174 steps/s (collection: 2.215s, learning 0.116s)
             Mean action noise std: 2.84
          Mean value_function loss: 294.0465
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 75.6925
                       Mean reward: 295.73
               Mean episode length: 135.91
    Episode_Reward/reaching_object: 1.1879
     Episode_Reward/lifting_object: 58.1968
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0441
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.6667
--------------------------------------------------------------------------------
                   Total timesteps: 120324096
                    Iteration time: 2.33s
                      Time elapsed: 00:47:08
                               ETA: 00:49:11

################################################################################
                     [1m Learning iteration 1224/2500 [0m                     

                       Computation: 41286 steps/s (collection: 2.263s, learning 0.118s)
             Mean action noise std: 2.84
          Mean value_function loss: 307.9545
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 75.7012
                       Mean reward: 302.77
               Mean episode length: 134.84
    Episode_Reward/reaching_object: 1.1850
     Episode_Reward/lifting_object: 57.9227
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0439
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.0000
--------------------------------------------------------------------------------
                   Total timesteps: 120422400
                    Iteration time: 2.38s
                      Time elapsed: 00:47:11
                               ETA: 00:49:08

################################################################################
                     [1m Learning iteration 1225/2500 [0m                     

                       Computation: 41891 steps/s (collection: 2.233s, learning 0.114s)
             Mean action noise std: 2.84
          Mean value_function loss: 341.1060
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 75.7089
                       Mean reward: 288.67
               Mean episode length: 131.31
    Episode_Reward/reaching_object: 1.1365
     Episode_Reward/lifting_object: 55.7532
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0424
          Episode_Reward/joint_vel: -0.0882
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.7500
--------------------------------------------------------------------------------
                   Total timesteps: 120520704
                    Iteration time: 2.35s
                      Time elapsed: 00:47:13
                               ETA: 00:49:06

################################################################################
                     [1m Learning iteration 1226/2500 [0m                     

                       Computation: 41765 steps/s (collection: 2.237s, learning 0.117s)
             Mean action noise std: 2.84
          Mean value_function loss: 315.8698
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 75.7193
                       Mean reward: 256.17
               Mean episode length: 116.86
    Episode_Reward/reaching_object: 1.1318
     Episode_Reward/lifting_object: 55.0249
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0423
          Episode_Reward/joint_vel: -0.0875
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 120619008
                    Iteration time: 2.35s
                      Time elapsed: 00:47:15
                               ETA: 00:49:04

################################################################################
                     [1m Learning iteration 1227/2500 [0m                     

                       Computation: 41719 steps/s (collection: 2.241s, learning 0.115s)
             Mean action noise std: 2.84
          Mean value_function loss: 316.1027
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 75.7283
                       Mean reward: 282.92
               Mean episode length: 128.85
    Episode_Reward/reaching_object: 1.1394
     Episode_Reward/lifting_object: 56.0444
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0425
          Episode_Reward/joint_vel: -0.0883
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.5417
--------------------------------------------------------------------------------
                   Total timesteps: 120717312
                    Iteration time: 2.36s
                      Time elapsed: 00:47:18
                               ETA: 00:49:02

################################################################################
                     [1m Learning iteration 1228/2500 [0m                     

                       Computation: 41859 steps/s (collection: 2.233s, learning 0.115s)
             Mean action noise std: 2.84
          Mean value_function loss: 335.4738
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 75.7337
                       Mean reward: 292.13
               Mean episode length: 132.53
    Episode_Reward/reaching_object: 1.1635
     Episode_Reward/lifting_object: 57.2318
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.0833
--------------------------------------------------------------------------------
                   Total timesteps: 120815616
                    Iteration time: 2.35s
                      Time elapsed: 00:47:20
                               ETA: 00:48:59

################################################################################
                     [1m Learning iteration 1229/2500 [0m                     

                       Computation: 41847 steps/s (collection: 2.260s, learning 0.089s)
             Mean action noise std: 2.84
          Mean value_function loss: 342.0447
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 75.7408
                       Mean reward: 286.59
               Mean episode length: 131.33
    Episode_Reward/reaching_object: 1.1553
     Episode_Reward/lifting_object: 56.1154
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0432
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.4167
--------------------------------------------------------------------------------
                   Total timesteps: 120913920
                    Iteration time: 2.35s
                      Time elapsed: 00:47:22
                               ETA: 00:48:57

################################################################################
                     [1m Learning iteration 1230/2500 [0m                     

                       Computation: 41728 steps/s (collection: 2.242s, learning 0.114s)
             Mean action noise std: 2.84
          Mean value_function loss: 302.3266
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 75.7482
                       Mean reward: 283.02
               Mean episode length: 127.31
    Episode_Reward/reaching_object: 1.1811
     Episode_Reward/lifting_object: 57.5375
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.8333
--------------------------------------------------------------------------------
                   Total timesteps: 121012224
                    Iteration time: 2.36s
                      Time elapsed: 00:47:25
                               ETA: 00:48:55

################################################################################
                     [1m Learning iteration 1231/2500 [0m                     

                       Computation: 41562 steps/s (collection: 2.259s, learning 0.107s)
             Mean action noise std: 2.84
          Mean value_function loss: 316.5455
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 75.7565
                       Mean reward: 303.01
               Mean episode length: 136.70
    Episode_Reward/reaching_object: 1.2169
     Episode_Reward/lifting_object: 59.7759
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0951
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.5417
--------------------------------------------------------------------------------
                   Total timesteps: 121110528
                    Iteration time: 2.37s
                      Time elapsed: 00:47:27
                               ETA: 00:48:53

################################################################################
                     [1m Learning iteration 1232/2500 [0m                     

                       Computation: 41740 steps/s (collection: 2.231s, learning 0.124s)
             Mean action noise std: 2.84
          Mean value_function loss: 287.0791
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 75.7667
                       Mean reward: 284.52
               Mean episode length: 133.42
    Episode_Reward/reaching_object: 1.1870
     Episode_Reward/lifting_object: 57.9109
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0444
          Episode_Reward/joint_vel: -0.0925
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.2917
--------------------------------------------------------------------------------
                   Total timesteps: 121208832
                    Iteration time: 2.36s
                      Time elapsed: 00:47:29
                               ETA: 00:48:50

################################################################################
                     [1m Learning iteration 1233/2500 [0m                     

                       Computation: 41887 steps/s (collection: 2.232s, learning 0.115s)
             Mean action noise std: 2.84
          Mean value_function loss: 299.5080
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 75.7757
                       Mean reward: 290.72
               Mean episode length: 128.65
    Episode_Reward/reaching_object: 1.2360
     Episode_Reward/lifting_object: 61.1602
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0960
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.1667
--------------------------------------------------------------------------------
                   Total timesteps: 121307136
                    Iteration time: 2.35s
                      Time elapsed: 00:47:32
                               ETA: 00:48:48

################################################################################
                     [1m Learning iteration 1234/2500 [0m                     

                       Computation: 41891 steps/s (collection: 2.215s, learning 0.132s)
             Mean action noise std: 2.84
          Mean value_function loss: 309.0638
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 75.7815
                       Mean reward: 315.96
               Mean episode length: 141.01
    Episode_Reward/reaching_object: 1.2505
     Episode_Reward/lifting_object: 61.5621
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0966
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.5000
--------------------------------------------------------------------------------
                   Total timesteps: 121405440
                    Iteration time: 2.35s
                      Time elapsed: 00:47:34
                               ETA: 00:48:46

################################################################################
                     [1m Learning iteration 1235/2500 [0m                     

                       Computation: 41118 steps/s (collection: 2.275s, learning 0.116s)
             Mean action noise std: 2.85
          Mean value_function loss: 303.8908
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 75.7901
                       Mean reward: 320.50
               Mean episode length: 144.36
    Episode_Reward/reaching_object: 1.2093
     Episode_Reward/lifting_object: 59.1786
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0963
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.8333
--------------------------------------------------------------------------------
                   Total timesteps: 121503744
                    Iteration time: 2.39s
                      Time elapsed: 00:47:36
                               ETA: 00:48:44

################################################################################
                     [1m Learning iteration 1236/2500 [0m                     

                       Computation: 41718 steps/s (collection: 2.240s, learning 0.117s)
             Mean action noise std: 2.85
          Mean value_function loss: 308.3487
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 75.7983
                       Mean reward: 324.98
               Mean episode length: 145.99
    Episode_Reward/reaching_object: 1.2452
     Episode_Reward/lifting_object: 61.3825
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0981
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121602048
                    Iteration time: 2.36s
                      Time elapsed: 00:47:39
                               ETA: 00:48:41

################################################################################
                     [1m Learning iteration 1237/2500 [0m                     

                       Computation: 41564 steps/s (collection: 2.240s, learning 0.125s)
             Mean action noise std: 2.85
          Mean value_function loss: 325.2735
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 75.8037
                       Mean reward: 305.87
               Mean episode length: 137.47
    Episode_Reward/reaching_object: 1.2136
     Episode_Reward/lifting_object: 59.3150
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.6250
--------------------------------------------------------------------------------
                   Total timesteps: 121700352
                    Iteration time: 2.37s
                      Time elapsed: 00:47:41
                               ETA: 00:48:39

################################################################################
                     [1m Learning iteration 1238/2500 [0m                     

                       Computation: 41673 steps/s (collection: 2.266s, learning 0.093s)
             Mean action noise std: 2.85
          Mean value_function loss: 307.4441
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 75.8089
                       Mean reward: 300.27
               Mean episode length: 135.47
    Episode_Reward/reaching_object: 1.2230
     Episode_Reward/lifting_object: 60.5411
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0961
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.0000
--------------------------------------------------------------------------------
                   Total timesteps: 121798656
                    Iteration time: 2.36s
                      Time elapsed: 00:47:44
                               ETA: 00:48:37

################################################################################
                     [1m Learning iteration 1239/2500 [0m                     

                       Computation: 40409 steps/s (collection: 2.326s, learning 0.107s)
             Mean action noise std: 2.85
          Mean value_function loss: 323.8027
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 75.8147
                       Mean reward: 296.15
               Mean episode length: 133.50
    Episode_Reward/reaching_object: 1.2044
     Episode_Reward/lifting_object: 59.4373
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0451
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 29.8333
--------------------------------------------------------------------------------
                   Total timesteps: 121896960
                    Iteration time: 2.43s
                      Time elapsed: 00:47:46
                               ETA: 00:48:35

################################################################################
                     [1m Learning iteration 1240/2500 [0m                     

                       Computation: 41480 steps/s (collection: 2.243s, learning 0.127s)
             Mean action noise std: 2.85
          Mean value_function loss: 320.7692
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 75.8285
                       Mean reward: 289.25
               Mean episode length: 129.49
    Episode_Reward/reaching_object: 1.2212
     Episode_Reward/lifting_object: 59.9402
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0458
          Episode_Reward/joint_vel: -0.0962
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 121995264
                    Iteration time: 2.37s
                      Time elapsed: 00:47:48
                               ETA: 00:48:32

################################################################################
                     [1m Learning iteration 1241/2500 [0m                     

                       Computation: 41392 steps/s (collection: 2.277s, learning 0.098s)
             Mean action noise std: 2.85
          Mean value_function loss: 301.5821
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 75.8390
                       Mean reward: 290.63
               Mean episode length: 133.20
    Episode_Reward/reaching_object: 1.1630
     Episode_Reward/lifting_object: 58.1707
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0898
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 29.2500
--------------------------------------------------------------------------------
                   Total timesteps: 122093568
                    Iteration time: 2.37s
                      Time elapsed: 00:47:51
                               ETA: 00:48:30

################################################################################
                     [1m Learning iteration 1242/2500 [0m                     

                       Computation: 40981 steps/s (collection: 2.280s, learning 0.119s)
             Mean action noise std: 2.85
          Mean value_function loss: 317.1311
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 75.8513
                       Mean reward: 304.07
               Mean episode length: 135.49
    Episode_Reward/reaching_object: 1.1797
     Episode_Reward/lifting_object: 58.3516
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.3333
--------------------------------------------------------------------------------
                   Total timesteps: 122191872
                    Iteration time: 2.40s
                      Time elapsed: 00:47:53
                               ETA: 00:48:28

################################################################################
                     [1m Learning iteration 1243/2500 [0m                     

                       Computation: 41494 steps/s (collection: 2.266s, learning 0.104s)
             Mean action noise std: 2.85
          Mean value_function loss: 315.0909
               Mean surrogate loss: 0.0049
                 Mean entropy loss: 75.8607
                       Mean reward: 279.00
               Mean episode length: 127.39
    Episode_Reward/reaching_object: 1.1655
     Episode_Reward/lifting_object: 57.7663
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.0000
--------------------------------------------------------------------------------
                   Total timesteps: 122290176
                    Iteration time: 2.37s
                      Time elapsed: 00:47:56
                               ETA: 00:48:26

################################################################################
                     [1m Learning iteration 1244/2500 [0m                     

                       Computation: 42069 steps/s (collection: 2.239s, learning 0.098s)
             Mean action noise std: 2.85
          Mean value_function loss: 319.5438
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 75.8649
                       Mean reward: 288.60
               Mean episode length: 127.11
    Episode_Reward/reaching_object: 1.1523
     Episode_Reward/lifting_object: 56.8611
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0434
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 29.3333
--------------------------------------------------------------------------------
                   Total timesteps: 122388480
                    Iteration time: 2.34s
                      Time elapsed: 00:47:58
                               ETA: 00:48:23

################################################################################
                     [1m Learning iteration 1245/2500 [0m                     

                       Computation: 40880 steps/s (collection: 2.276s, learning 0.129s)
             Mean action noise std: 2.85
          Mean value_function loss: 336.7823
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 75.8742
                       Mean reward: 283.13
               Mean episode length: 127.83
    Episode_Reward/reaching_object: 1.1385
     Episode_Reward/lifting_object: 56.0796
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0428
          Episode_Reward/joint_vel: -0.0894
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 28.8750
--------------------------------------------------------------------------------
                   Total timesteps: 122486784
                    Iteration time: 2.40s
                      Time elapsed: 00:48:00
                               ETA: 00:48:21

################################################################################
                     [1m Learning iteration 1246/2500 [0m                     

                       Computation: 41553 steps/s (collection: 2.252s, learning 0.114s)
             Mean action noise std: 2.86
          Mean value_function loss: 331.1880
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 75.8888
                       Mean reward: 304.52
               Mean episode length: 133.64
    Episode_Reward/reaching_object: 1.1872
     Episode_Reward/lifting_object: 58.6197
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0928
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 30.7917
--------------------------------------------------------------------------------
                   Total timesteps: 122585088
                    Iteration time: 2.37s
                      Time elapsed: 00:48:03
                               ETA: 00:48:19

################################################################################
                     [1m Learning iteration 1247/2500 [0m                     

                       Computation: 41583 steps/s (collection: 2.255s, learning 0.109s)
             Mean action noise std: 2.86
          Mean value_function loss: 340.4040
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 75.9030
                       Mean reward: 290.63
               Mean episode length: 130.71
    Episode_Reward/reaching_object: 1.1522
     Episode_Reward/lifting_object: 56.9139
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0433
          Episode_Reward/joint_vel: -0.0901
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.7500
--------------------------------------------------------------------------------
                   Total timesteps: 122683392
                    Iteration time: 2.36s
                      Time elapsed: 00:48:05
                               ETA: 00:48:17

################################################################################
                     [1m Learning iteration 1248/2500 [0m                     

                       Computation: 40895 steps/s (collection: 2.278s, learning 0.126s)
             Mean action noise std: 2.86
          Mean value_function loss: 317.1321
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 75.9094
                       Mean reward: 307.32
               Mean episode length: 136.03
    Episode_Reward/reaching_object: 1.1668
     Episode_Reward/lifting_object: 58.1808
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0914
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.2917
     Episode_Termination/robot_out: 27.9167
--------------------------------------------------------------------------------
                   Total timesteps: 122781696
                    Iteration time: 2.40s
                      Time elapsed: 00:48:07
                               ETA: 00:48:14

################################################################################
                     [1m Learning iteration 1249/2500 [0m                     

                       Computation: 41532 steps/s (collection: 2.247s, learning 0.120s)
             Mean action noise std: 2.86
          Mean value_function loss: 312.3791
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 75.9185
                       Mean reward: 278.65
               Mean episode length: 129.08
    Episode_Reward/reaching_object: 1.1636
     Episode_Reward/lifting_object: 56.9643
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.2917
--------------------------------------------------------------------------------
                   Total timesteps: 122880000
                    Iteration time: 2.37s
                      Time elapsed: 00:48:10
                               ETA: 00:48:12

################################################################################
                     [1m Learning iteration 1250/2500 [0m                     

                       Computation: 42099 steps/s (collection: 2.240s, learning 0.096s)
             Mean action noise std: 2.86
          Mean value_function loss: 326.2919
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 75.9300
                       Mean reward: 316.05
               Mean episode length: 143.39
    Episode_Reward/reaching_object: 1.2023
     Episode_Reward/lifting_object: 59.1069
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.1250
--------------------------------------------------------------------------------
                   Total timesteps: 122978304
                    Iteration time: 2.34s
                      Time elapsed: 00:48:12
                               ETA: 00:48:10

################################################################################
                     [1m Learning iteration 1251/2500 [0m                     

                       Computation: 40408 steps/s (collection: 2.310s, learning 0.123s)
             Mean action noise std: 2.86
          Mean value_function loss: 329.8613
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 75.9429
                       Mean reward: 307.06
               Mean episode length: 133.30
    Episode_Reward/reaching_object: 1.2055
     Episode_Reward/lifting_object: 59.3316
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 29.2500
--------------------------------------------------------------------------------
                   Total timesteps: 123076608
                    Iteration time: 2.43s
                      Time elapsed: 00:48:15
                               ETA: 00:48:08

################################################################################
                     [1m Learning iteration 1252/2500 [0m                     

                       Computation: 41181 steps/s (collection: 2.270s, learning 0.118s)
             Mean action noise std: 2.86
          Mean value_function loss: 316.5078
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 75.9537
                       Mean reward: 310.22
               Mean episode length: 137.75
    Episode_Reward/reaching_object: 1.2186
     Episode_Reward/lifting_object: 60.6758
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.1737
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 29.0833
--------------------------------------------------------------------------------
                   Total timesteps: 123174912
                    Iteration time: 2.39s
                      Time elapsed: 00:48:17
                               ETA: 00:48:05

################################################################################
                     [1m Learning iteration 1253/2500 [0m                     

                       Computation: 41298 steps/s (collection: 2.282s, learning 0.098s)
             Mean action noise std: 2.86
          Mean value_function loss: 319.2497
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 75.9618
                       Mean reward: 287.15
               Mean episode length: 131.07
    Episode_Reward/reaching_object: 1.1527
     Episode_Reward/lifting_object: 56.8386
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0926
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.8750
--------------------------------------------------------------------------------
                   Total timesteps: 123273216
                    Iteration time: 2.38s
                      Time elapsed: 00:48:19
                               ETA: 00:48:03

################################################################################
                     [1m Learning iteration 1254/2500 [0m                     

                       Computation: 41130 steps/s (collection: 2.283s, learning 0.107s)
             Mean action noise std: 2.86
          Mean value_function loss: 299.2203
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 75.9753
                       Mean reward: 288.86
               Mean episode length: 128.36
    Episode_Reward/reaching_object: 1.1897
     Episode_Reward/lifting_object: 59.0380
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 27.6250
--------------------------------------------------------------------------------
                   Total timesteps: 123371520
                    Iteration time: 2.39s
                      Time elapsed: 00:48:22
                               ETA: 00:48:01

################################################################################
                     [1m Learning iteration 1255/2500 [0m                     

                       Computation: 41659 steps/s (collection: 2.243s, learning 0.117s)
             Mean action noise std: 2.87
          Mean value_function loss: 316.8547
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 75.9872
                       Mean reward: 294.70
               Mean episode length: 132.18
    Episode_Reward/reaching_object: 1.2013
     Episode_Reward/lifting_object: 59.1008
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0950
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.8750
--------------------------------------------------------------------------------
                   Total timesteps: 123469824
                    Iteration time: 2.36s
                      Time elapsed: 00:48:24
                               ETA: 00:47:59

################################################################################
                     [1m Learning iteration 1256/2500 [0m                     

                       Computation: 41516 steps/s (collection: 2.259s, learning 0.109s)
             Mean action noise std: 2.87
          Mean value_function loss: 348.6156
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 75.9953
                       Mean reward: 310.15
               Mean episode length: 137.85
    Episode_Reward/reaching_object: 1.2140
     Episode_Reward/lifting_object: 59.8650
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0964
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 31.2083
--------------------------------------------------------------------------------
                   Total timesteps: 123568128
                    Iteration time: 2.37s
                      Time elapsed: 00:48:26
                               ETA: 00:47:56

################################################################################
                     [1m Learning iteration 1257/2500 [0m                     

                       Computation: 41243 steps/s (collection: 2.264s, learning 0.119s)
             Mean action noise std: 2.87
          Mean value_function loss: 324.5419
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 76.0035
                       Mean reward: 300.38
               Mean episode length: 130.95
    Episode_Reward/reaching_object: 1.1979
     Episode_Reward/lifting_object: 59.5225
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 29.2083
--------------------------------------------------------------------------------
                   Total timesteps: 123666432
                    Iteration time: 2.38s
                      Time elapsed: 00:48:29
                               ETA: 00:47:54

################################################################################
                     [1m Learning iteration 1258/2500 [0m                     

                       Computation: 41479 steps/s (collection: 2.248s, learning 0.122s)
             Mean action noise std: 2.87
          Mean value_function loss: 306.6125
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 76.0125
                       Mean reward: 295.34
               Mean episode length: 130.56
    Episode_Reward/reaching_object: 1.2141
     Episode_Reward/lifting_object: 59.5653
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0979
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.6250
--------------------------------------------------------------------------------
                   Total timesteps: 123764736
                    Iteration time: 2.37s
                      Time elapsed: 00:48:31
                               ETA: 00:47:52

################################################################################
                     [1m Learning iteration 1259/2500 [0m                     

                       Computation: 40754 steps/s (collection: 2.279s, learning 0.134s)
             Mean action noise std: 2.87
          Mean value_function loss: 308.4199
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 76.0228
                       Mean reward: 291.25
               Mean episode length: 128.55
    Episode_Reward/reaching_object: 1.1791
     Episode_Reward/lifting_object: 58.6602
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0445
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.5833
--------------------------------------------------------------------------------
                   Total timesteps: 123863040
                    Iteration time: 2.41s
                      Time elapsed: 00:48:34
                               ETA: 00:47:50

################################################################################
                     [1m Learning iteration 1260/2500 [0m                     

                       Computation: 41417 steps/s (collection: 2.270s, learning 0.104s)
             Mean action noise std: 2.87
          Mean value_function loss: 334.7967
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 76.0312
                       Mean reward: 294.10
               Mean episode length: 132.84
    Episode_Reward/reaching_object: 1.1926
     Episode_Reward/lifting_object: 58.9045
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.4167
--------------------------------------------------------------------------------
                   Total timesteps: 123961344
                    Iteration time: 2.37s
                      Time elapsed: 00:48:36
                               ETA: 00:47:47

################################################################################
                     [1m Learning iteration 1261/2500 [0m                     

                       Computation: 41347 steps/s (collection: 2.258s, learning 0.120s)
             Mean action noise std: 2.87
          Mean value_function loss: 304.5038
               Mean surrogate loss: 0.0057
                 Mean entropy loss: 76.0392
                       Mean reward: 300.62
               Mean episode length: 133.89
    Episode_Reward/reaching_object: 1.2221
     Episode_Reward/lifting_object: 60.1078
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0990
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.8333
--------------------------------------------------------------------------------
                   Total timesteps: 124059648
                    Iteration time: 2.38s
                      Time elapsed: 00:48:38
                               ETA: 00:47:45

################################################################################
                     [1m Learning iteration 1262/2500 [0m                     

                       Computation: 41408 steps/s (collection: 2.262s, learning 0.112s)
             Mean action noise std: 2.87
          Mean value_function loss: 325.2455
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 76.0438
                       Mean reward: 301.35
               Mean episode length: 135.30
    Episode_Reward/reaching_object: 1.1887
     Episode_Reward/lifting_object: 58.8039
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0957
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 124157952
                    Iteration time: 2.37s
                      Time elapsed: 00:48:41
                               ETA: 00:47:43

################################################################################
                     [1m Learning iteration 1263/2500 [0m                     

                       Computation: 41495 steps/s (collection: 2.255s, learning 0.114s)
             Mean action noise std: 2.87
          Mean value_function loss: 342.5847
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 76.0509
                       Mean reward: 304.68
               Mean episode length: 134.34
    Episode_Reward/reaching_object: 1.1837
     Episode_Reward/lifting_object: 58.9603
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0949
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 31.2500
--------------------------------------------------------------------------------
                   Total timesteps: 124256256
                    Iteration time: 2.37s
                      Time elapsed: 00:48:43
                               ETA: 00:47:41

################################################################################
                     [1m Learning iteration 1264/2500 [0m                     

                       Computation: 41933 steps/s (collection: 2.230s, learning 0.114s)
             Mean action noise std: 2.87
          Mean value_function loss: 315.1252
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 76.0561
                       Mean reward: 305.81
               Mean episode length: 133.68
    Episode_Reward/reaching_object: 1.2045
     Episode_Reward/lifting_object: 60.1830
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0968
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.3750
--------------------------------------------------------------------------------
                   Total timesteps: 124354560
                    Iteration time: 2.34s
                      Time elapsed: 00:48:45
                               ETA: 00:47:38

################################################################################
                     [1m Learning iteration 1265/2500 [0m                     

                       Computation: 42084 steps/s (collection: 2.242s, learning 0.094s)
             Mean action noise std: 2.87
          Mean value_function loss: 328.1383
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 76.0668
                       Mean reward: 316.29
               Mean episode length: 137.06
    Episode_Reward/reaching_object: 1.1790
     Episode_Reward/lifting_object: 58.6101
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0448
          Episode_Reward/joint_vel: -0.0952
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 30.4583
--------------------------------------------------------------------------------
                   Total timesteps: 124452864
                    Iteration time: 2.34s
                      Time elapsed: 00:48:48
                               ETA: 00:47:36

################################################################################
                     [1m Learning iteration 1266/2500 [0m                     

                       Computation: 41538 steps/s (collection: 2.268s, learning 0.099s)
             Mean action noise std: 2.88
          Mean value_function loss: 349.3853
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 76.0796
                       Mean reward: 287.94
               Mean episode length: 128.18
    Episode_Reward/reaching_object: 1.1732
     Episode_Reward/lifting_object: 58.9534
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 32.4583
--------------------------------------------------------------------------------
                   Total timesteps: 124551168
                    Iteration time: 2.37s
                      Time elapsed: 00:48:50
                               ETA: 00:47:34

################################################################################
                     [1m Learning iteration 1267/2500 [0m                     

                       Computation: 40381 steps/s (collection: 2.314s, learning 0.120s)
             Mean action noise std: 2.88
          Mean value_function loss: 335.5410
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 76.0897
                       Mean reward: 281.77
               Mean episode length: 122.27
    Episode_Reward/reaching_object: 1.1568
     Episode_Reward/lifting_object: 57.9367
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.6250
--------------------------------------------------------------------------------
                   Total timesteps: 124649472
                    Iteration time: 2.43s
                      Time elapsed: 00:48:53
                               ETA: 00:47:32

################################################################################
                     [1m Learning iteration 1268/2500 [0m                     

                       Computation: 40889 steps/s (collection: 2.285s, learning 0.119s)
             Mean action noise std: 2.88
          Mean value_function loss: 327.9715
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 76.1005
                       Mean reward: 301.37
               Mean episode length: 131.20
    Episode_Reward/reaching_object: 1.1454
     Episode_Reward/lifting_object: 57.2744
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0917
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.3333
--------------------------------------------------------------------------------
                   Total timesteps: 124747776
                    Iteration time: 2.40s
                      Time elapsed: 00:48:55
                               ETA: 00:47:29

################################################################################
                     [1m Learning iteration 1269/2500 [0m                     

                       Computation: 41363 steps/s (collection: 2.262s, learning 0.114s)
             Mean action noise std: 2.88
          Mean value_function loss: 335.4226
               Mean surrogate loss: 0.0015
                 Mean entropy loss: 76.1090
                       Mean reward: 301.63
               Mean episode length: 132.71
    Episode_Reward/reaching_object: 1.1451
     Episode_Reward/lifting_object: 57.3035
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0436
          Episode_Reward/joint_vel: -0.0918
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.6250
--------------------------------------------------------------------------------
                   Total timesteps: 124846080
                    Iteration time: 2.38s
                      Time elapsed: 00:48:57
                               ETA: 00:47:27

################################################################################
                     [1m Learning iteration 1270/2500 [0m                     

                       Computation: 41473 steps/s (collection: 2.268s, learning 0.103s)
             Mean action noise std: 2.88
          Mean value_function loss: 356.3898
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 76.1206
                       Mean reward: 287.40
               Mean episode length: 127.50
    Episode_Reward/reaching_object: 1.1480
     Episode_Reward/lifting_object: 57.3260
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0438
          Episode_Reward/joint_vel: -0.0912
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 30.3333
--------------------------------------------------------------------------------
                   Total timesteps: 124944384
                    Iteration time: 2.37s
                      Time elapsed: 00:49:00
                               ETA: 00:47:25

################################################################################
                     [1m Learning iteration 1271/2500 [0m                     

                       Computation: 41187 steps/s (collection: 2.255s, learning 0.132s)
             Mean action noise std: 2.88
          Mean value_function loss: 331.4853
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 76.1356
                       Mean reward: 292.88
               Mean episode length: 128.51
    Episode_Reward/reaching_object: 1.1507
     Episode_Reward/lifting_object: 57.5808
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0440
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.5000
--------------------------------------------------------------------------------
                   Total timesteps: 125042688
                    Iteration time: 2.39s
                      Time elapsed: 00:49:02
                               ETA: 00:47:23

################################################################################
                     [1m Learning iteration 1272/2500 [0m                     

                       Computation: 41314 steps/s (collection: 2.250s, learning 0.129s)
             Mean action noise std: 2.88
          Mean value_function loss: 358.0106
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 76.1463
                       Mean reward: 310.37
               Mean episode length: 135.68
    Episode_Reward/reaching_object: 1.1668
     Episode_Reward/lifting_object: 58.2011
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0446
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.6667
--------------------------------------------------------------------------------
                   Total timesteps: 125140992
                    Iteration time: 2.38s
                      Time elapsed: 00:49:04
                               ETA: 00:47:20

################################################################################
                     [1m Learning iteration 1273/2500 [0m                     

                       Computation: 41045 steps/s (collection: 2.274s, learning 0.121s)
             Mean action noise std: 2.88
          Mean value_function loss: 320.5328
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 76.1609
                       Mean reward: 303.40
               Mean episode length: 131.56
    Episode_Reward/reaching_object: 1.1554
     Episode_Reward/lifting_object: 57.2453
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0442
          Episode_Reward/joint_vel: -0.0926
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.2500
--------------------------------------------------------------------------------
                   Total timesteps: 125239296
                    Iteration time: 2.39s
                      Time elapsed: 00:49:07
                               ETA: 00:47:18

################################################################################
                     [1m Learning iteration 1274/2500 [0m                     

                       Computation: 41304 steps/s (collection: 2.253s, learning 0.127s)
             Mean action noise std: 2.89
          Mean value_function loss: 361.6588
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 76.1787
                       Mean reward: 281.51
               Mean episode length: 125.73
    Episode_Reward/reaching_object: 1.1179
     Episode_Reward/lifting_object: 55.3047
      Episode_Reward/object_height: 0.0035
        Episode_Reward/action_rate: -0.0429
          Episode_Reward/joint_vel: -0.0899
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 31.0000
--------------------------------------------------------------------------------
                   Total timesteps: 125337600
                    Iteration time: 2.38s
                      Time elapsed: 00:49:09
                               ETA: 00:47:16

################################################################################
                     [1m Learning iteration 1275/2500 [0m                     

                       Computation: 41704 steps/s (collection: 2.258s, learning 0.100s)
             Mean action noise std: 2.89
          Mean value_function loss: 310.5862
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 76.1902
                       Mean reward: 309.57
               Mean episode length: 134.57
    Episode_Reward/reaching_object: 1.1603
     Episode_Reward/lifting_object: 57.8911
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.3750
--------------------------------------------------------------------------------
                   Total timesteps: 125435904
                    Iteration time: 2.36s
                      Time elapsed: 00:49:12
                               ETA: 00:47:14

################################################################################
                     [1m Learning iteration 1276/2500 [0m                     

                       Computation: 41470 steps/s (collection: 2.258s, learning 0.112s)
             Mean action noise std: 2.89
          Mean value_function loss: 337.8486
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 76.1969
                       Mean reward: 307.84
               Mean episode length: 139.16
    Episode_Reward/reaching_object: 1.1749
     Episode_Reward/lifting_object: 58.0086
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0449
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 30.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125534208
                    Iteration time: 2.37s
                      Time elapsed: 00:49:14
                               ETA: 00:47:11

################################################################################
                     [1m Learning iteration 1277/2500 [0m                     

                       Computation: 41367 steps/s (collection: 2.258s, learning 0.119s)
             Mean action noise std: 2.89
          Mean value_function loss: 322.0936
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 76.2019
                       Mean reward: 287.19
               Mean episode length: 124.85
    Episode_Reward/reaching_object: 1.1572
     Episode_Reward/lifting_object: 57.2328
      Episode_Reward/object_height: 0.0036
        Episode_Reward/action_rate: -0.0443
          Episode_Reward/joint_vel: -0.0936
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.6667
--------------------------------------------------------------------------------
                   Total timesteps: 125632512
                    Iteration time: 2.38s
                      Time elapsed: 00:49:16
                               ETA: 00:47:09

################################################################################
                     [1m Learning iteration 1278/2500 [0m                     

                       Computation: 41127 steps/s (collection: 2.282s, learning 0.109s)
             Mean action noise std: 2.89
          Mean value_function loss: 335.8118
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 76.2114
                       Mean reward: 285.23
               Mean episode length: 129.73
    Episode_Reward/reaching_object: 1.2086
     Episode_Reward/lifting_object: 60.0930
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0976
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.9167
--------------------------------------------------------------------------------
                   Total timesteps: 125730816
                    Iteration time: 2.39s
                      Time elapsed: 00:49:19
                               ETA: 00:47:07

################################################################################
                     [1m Learning iteration 1279/2500 [0m                     

                       Computation: 41612 steps/s (collection: 2.270s, learning 0.093s)
             Mean action noise std: 2.89
          Mean value_function loss: 329.6957
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 76.2269
                       Mean reward: 308.84
               Mean episode length: 138.49
    Episode_Reward/reaching_object: 1.2035
     Episode_Reward/lifting_object: 59.8564
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0460
          Episode_Reward/joint_vel: -0.0966
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.1250
--------------------------------------------------------------------------------
                   Total timesteps: 125829120
                    Iteration time: 2.36s
                      Time elapsed: 00:49:21
                               ETA: 00:47:05

################################################################################
                     [1m Learning iteration 1280/2500 [0m                     

                       Computation: 41791 steps/s (collection: 2.259s, learning 0.094s)
             Mean action noise std: 2.89
          Mean value_function loss: 313.0178
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 76.2419
                       Mean reward: 302.30
               Mean episode length: 134.46
    Episode_Reward/reaching_object: 1.1818
     Episode_Reward/lifting_object: 58.0083
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0961
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.7083
--------------------------------------------------------------------------------
                   Total timesteps: 125927424
                    Iteration time: 2.35s
                      Time elapsed: 00:49:23
                               ETA: 00:47:02

################################################################################
                     [1m Learning iteration 1281/2500 [0m                     

                       Computation: 41191 steps/s (collection: 2.291s, learning 0.095s)
             Mean action noise std: 2.89
          Mean value_function loss: 324.8659
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 76.2513
                       Mean reward: 309.09
               Mean episode length: 139.22
    Episode_Reward/reaching_object: 1.2123
     Episode_Reward/lifting_object: 60.0139
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0960
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.2083
--------------------------------------------------------------------------------
                   Total timesteps: 126025728
                    Iteration time: 2.39s
                      Time elapsed: 00:49:26
                               ETA: 00:47:00

################################################################################
                     [1m Learning iteration 1282/2500 [0m                     

                       Computation: 41042 steps/s (collection: 2.300s, learning 0.096s)
             Mean action noise std: 2.89
          Mean value_function loss: 315.6036
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 76.2625
                       Mean reward: 305.93
               Mean episode length: 135.52
    Episode_Reward/reaching_object: 1.2238
     Episode_Reward/lifting_object: 61.2912
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.3333
--------------------------------------------------------------------------------
                   Total timesteps: 126124032
                    Iteration time: 2.40s
                      Time elapsed: 00:49:28
                               ETA: 00:46:58

################################################################################
                     [1m Learning iteration 1283/2500 [0m                     

                       Computation: 40913 steps/s (collection: 2.302s, learning 0.101s)
             Mean action noise std: 2.90
          Mean value_function loss: 330.5212
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 76.2721
                       Mean reward: 322.85
               Mean episode length: 142.41
    Episode_Reward/reaching_object: 1.2044
     Episode_Reward/lifting_object: 60.2523
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0956
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 29.3333
--------------------------------------------------------------------------------
                   Total timesteps: 126222336
                    Iteration time: 2.40s
                      Time elapsed: 00:49:31
                               ETA: 00:46:56

################################################################################
                     [1m Learning iteration 1284/2500 [0m                     

                       Computation: 41121 steps/s (collection: 2.300s, learning 0.091s)
             Mean action noise std: 2.90
          Mean value_function loss: 330.1918
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 76.2845
                       Mean reward: 312.78
               Mean episode length: 141.66
    Episode_Reward/reaching_object: 1.1919
     Episode_Reward/lifting_object: 59.2454
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0455
          Episode_Reward/joint_vel: -0.0942
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.8333
--------------------------------------------------------------------------------
                   Total timesteps: 126320640
                    Iteration time: 2.39s
                      Time elapsed: 00:49:33
                               ETA: 00:46:53

################################################################################
                     [1m Learning iteration 1285/2500 [0m                     

                       Computation: 39675 steps/s (collection: 2.384s, learning 0.094s)
             Mean action noise std: 2.90
          Mean value_function loss: 317.2200
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 76.2962
                       Mean reward: 300.82
               Mean episode length: 131.32
    Episode_Reward/reaching_object: 1.2033
     Episode_Reward/lifting_object: 59.9835
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0955
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.1667
--------------------------------------------------------------------------------
                   Total timesteps: 126418944
                    Iteration time: 2.48s
                      Time elapsed: 00:49:36
                               ETA: 00:46:51

################################################################################
                     [1m Learning iteration 1286/2500 [0m                     

                       Computation: 41026 steps/s (collection: 2.283s, learning 0.114s)
             Mean action noise std: 2.90
          Mean value_function loss: 320.2224
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 76.3100
                       Mean reward: 325.02
               Mean episode length: 143.88
    Episode_Reward/reaching_object: 1.2102
     Episode_Reward/lifting_object: 60.3226
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0956
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.1667
--------------------------------------------------------------------------------
                   Total timesteps: 126517248
                    Iteration time: 2.40s
                      Time elapsed: 00:49:38
                               ETA: 00:46:49

################################################################################
                     [1m Learning iteration 1287/2500 [0m                     

                       Computation: 41183 steps/s (collection: 2.289s, learning 0.098s)
             Mean action noise std: 2.90
          Mean value_function loss: 311.3498
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 76.3196
                       Mean reward: 284.67
               Mean episode length: 127.01
    Episode_Reward/reaching_object: 1.2276
     Episode_Reward/lifting_object: 61.3587
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0471
          Episode_Reward/joint_vel: -0.0976
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.5417
--------------------------------------------------------------------------------
                   Total timesteps: 126615552
                    Iteration time: 2.39s
                      Time elapsed: 00:49:40
                               ETA: 00:46:47

################################################################################
                     [1m Learning iteration 1288/2500 [0m                     

                       Computation: 40820 steps/s (collection: 2.310s, learning 0.098s)
             Mean action noise std: 2.90
          Mean value_function loss: 304.4752
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 76.3305
                       Mean reward: 308.46
               Mean episode length: 133.13
    Episode_Reward/reaching_object: 1.1975
     Episode_Reward/lifting_object: 59.7991
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0461
          Episode_Reward/joint_vel: -0.0950
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.2500
--------------------------------------------------------------------------------
                   Total timesteps: 126713856
                    Iteration time: 2.41s
                      Time elapsed: 00:49:43
                               ETA: 00:46:44

################################################################################
                     [1m Learning iteration 1289/2500 [0m                     

                       Computation: 41163 steps/s (collection: 2.286s, learning 0.103s)
             Mean action noise std: 2.90
          Mean value_function loss: 310.3215
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 76.3377
                       Mean reward: 306.18
               Mean episode length: 134.54
    Episode_Reward/reaching_object: 1.1891
     Episode_Reward/lifting_object: 59.9538
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0456
          Episode_Reward/joint_vel: -0.0923
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126812160
                    Iteration time: 2.39s
                      Time elapsed: 00:49:45
                               ETA: 00:46:42

################################################################################
                     [1m Learning iteration 1290/2500 [0m                     

                       Computation: 40618 steps/s (collection: 2.326s, learning 0.094s)
             Mean action noise std: 2.90
          Mean value_function loss: 334.3476
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 76.3461
                       Mean reward: 296.21
               Mean episode length: 131.53
    Episode_Reward/reaching_object: 1.1700
     Episode_Reward/lifting_object: 58.5448
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0913
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 31.1250
--------------------------------------------------------------------------------
                   Total timesteps: 126910464
                    Iteration time: 2.42s
                      Time elapsed: 00:49:48
                               ETA: 00:46:40

################################################################################
                     [1m Learning iteration 1291/2500 [0m                     

                       Computation: 41358 steps/s (collection: 2.251s, learning 0.126s)
             Mean action noise std: 2.91
          Mean value_function loss: 333.9456
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 76.3645
                       Mean reward: 336.43
               Mean episode length: 143.79
    Episode_Reward/reaching_object: 1.2249
     Episode_Reward/lifting_object: 61.6264
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0956
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 127008768
                    Iteration time: 2.38s
                      Time elapsed: 00:49:50
                               ETA: 00:46:38

################################################################################
                     [1m Learning iteration 1292/2500 [0m                     

                       Computation: 41646 steps/s (collection: 2.248s, learning 0.112s)
             Mean action noise std: 2.91
          Mean value_function loss: 324.9848
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 76.3781
                       Mean reward: 306.27
               Mean episode length: 134.65
    Episode_Reward/reaching_object: 1.2154
     Episode_Reward/lifting_object: 60.6387
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0469
          Episode_Reward/joint_vel: -0.0960
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 27.9583
--------------------------------------------------------------------------------
                   Total timesteps: 127107072
                    Iteration time: 2.36s
                      Time elapsed: 00:49:52
                               ETA: 00:46:36

################################################################################
                     [1m Learning iteration 1293/2500 [0m                     

                       Computation: 41734 steps/s (collection: 2.251s, learning 0.104s)
             Mean action noise std: 2.91
          Mean value_function loss: 305.4525
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 76.3853
                       Mean reward: 315.29
               Mean episode length: 138.98
    Episode_Reward/reaching_object: 1.2222
     Episode_Reward/lifting_object: 61.1185
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0957
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.0417
--------------------------------------------------------------------------------
                   Total timesteps: 127205376
                    Iteration time: 2.36s
                      Time elapsed: 00:49:55
                               ETA: 00:46:33

################################################################################
                     [1m Learning iteration 1294/2500 [0m                     

                       Computation: 41202 steps/s (collection: 2.262s, learning 0.124s)
             Mean action noise std: 2.91
          Mean value_function loss: 334.1741
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 76.3931
                       Mean reward: 291.86
               Mean episode length: 126.98
    Episode_Reward/reaching_object: 1.1679
     Episode_Reward/lifting_object: 58.8627
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0450
          Episode_Reward/joint_vel: -0.0916
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 31.0000
--------------------------------------------------------------------------------
                   Total timesteps: 127303680
                    Iteration time: 2.39s
                      Time elapsed: 00:49:57
                               ETA: 00:46:31

################################################################################
                     [1m Learning iteration 1295/2500 [0m                     

                       Computation: 41202 steps/s (collection: 2.258s, learning 0.128s)
             Mean action noise std: 2.91
          Mean value_function loss: 345.0299
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 76.4002
                       Mean reward: 315.86
               Mean episode length: 138.23
    Episode_Reward/reaching_object: 1.2037
     Episode_Reward/lifting_object: 60.5839
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0465
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127401984
                    Iteration time: 2.39s
                      Time elapsed: 00:49:59
                               ETA: 00:46:29

################################################################################
                     [1m Learning iteration 1296/2500 [0m                     

                       Computation: 41858 steps/s (collection: 2.255s, learning 0.094s)
             Mean action noise std: 2.91
          Mean value_function loss: 329.9779
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 76.4143
                       Mean reward: 301.94
               Mean episode length: 136.38
    Episode_Reward/reaching_object: 1.1849
     Episode_Reward/lifting_object: 59.0183
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0937
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.2917
--------------------------------------------------------------------------------
                   Total timesteps: 127500288
                    Iteration time: 2.35s
                      Time elapsed: 00:50:02
                               ETA: 00:46:26

################################################################################
                     [1m Learning iteration 1297/2500 [0m                     

                       Computation: 41198 steps/s (collection: 2.279s, learning 0.107s)
             Mean action noise std: 2.91
          Mean value_function loss: 320.3066
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 76.4284
                       Mean reward: 293.99
               Mean episode length: 126.70
    Episode_Reward/reaching_object: 1.1592
     Episode_Reward/lifting_object: 58.4876
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0447
          Episode_Reward/joint_vel: -0.0903
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.2083
--------------------------------------------------------------------------------
                   Total timesteps: 127598592
                    Iteration time: 2.39s
                      Time elapsed: 00:50:04
                               ETA: 00:46:24

################################################################################
                     [1m Learning iteration 1298/2500 [0m                     

                       Computation: 41547 steps/s (collection: 2.255s, learning 0.111s)
             Mean action noise std: 2.91
          Mean value_function loss: 325.4255
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 76.4455
                       Mean reward: 307.72
               Mean episode length: 135.25
    Episode_Reward/reaching_object: 1.2036
     Episode_Reward/lifting_object: 60.5370
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.7917
--------------------------------------------------------------------------------
                   Total timesteps: 127696896
                    Iteration time: 2.37s
                      Time elapsed: 00:50:06
                               ETA: 00:46:22

################################################################################
                     [1m Learning iteration 1299/2500 [0m                     

                       Computation: 41284 steps/s (collection: 2.275s, learning 0.106s)
             Mean action noise std: 2.92
          Mean value_function loss: 336.9848
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 76.4585
                       Mean reward: 305.38
               Mean episode length: 131.28
    Episode_Reward/reaching_object: 1.1855
     Episode_Reward/lifting_object: 59.9760
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0918
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 31.6250
--------------------------------------------------------------------------------
                   Total timesteps: 127795200
                    Iteration time: 2.38s
                      Time elapsed: 00:50:09
                               ETA: 00:46:20

################################################################################
                     [1m Learning iteration 1300/2500 [0m                     

                       Computation: 41468 steps/s (collection: 2.262s, learning 0.108s)
             Mean action noise std: 2.92
          Mean value_function loss: 317.2256
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 76.4662
                       Mean reward: 320.43
               Mean episode length: 140.38
    Episode_Reward/reaching_object: 1.2060
     Episode_Reward/lifting_object: 60.8018
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.5417
--------------------------------------------------------------------------------
                   Total timesteps: 127893504
                    Iteration time: 2.37s
                      Time elapsed: 00:50:11
                               ETA: 00:46:17

################################################################################
                     [1m Learning iteration 1301/2500 [0m                     

                       Computation: 41212 steps/s (collection: 2.277s, learning 0.109s)
             Mean action noise std: 2.92
          Mean value_function loss: 317.6458
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 76.4774
                       Mean reward: 303.19
               Mean episode length: 131.88
    Episode_Reward/reaching_object: 1.1869
     Episode_Reward/lifting_object: 59.6206
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0949
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.0417
--------------------------------------------------------------------------------
                   Total timesteps: 127991808
                    Iteration time: 2.39s
                      Time elapsed: 00:50:14
                               ETA: 00:46:15

################################################################################
                     [1m Learning iteration 1302/2500 [0m                     

                       Computation: 41425 steps/s (collection: 2.247s, learning 0.126s)
             Mean action noise std: 2.92
          Mean value_function loss: 341.0727
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 76.4927
                       Mean reward: 303.03
               Mean episode length: 134.83
    Episode_Reward/reaching_object: 1.2383
     Episode_Reward/lifting_object: 62.1508
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0476
          Episode_Reward/joint_vel: -0.0986
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.2083
--------------------------------------------------------------------------------
                   Total timesteps: 128090112
                    Iteration time: 2.37s
                      Time elapsed: 00:50:16
                               ETA: 00:46:13

################################################################################
                     [1m Learning iteration 1303/2500 [0m                     

                       Computation: 41549 steps/s (collection: 2.256s, learning 0.110s)
             Mean action noise std: 2.92
          Mean value_function loss: 348.9469
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 76.4989
                       Mean reward: 288.29
               Mean episode length: 129.64
    Episode_Reward/reaching_object: 1.1739
     Episode_Reward/lifting_object: 59.1607
      Episode_Reward/object_height: 0.0037
        Episode_Reward/action_rate: -0.0452
          Episode_Reward/joint_vel: -0.0929
      Episode_Termination/time_out: 0.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.9167
--------------------------------------------------------------------------------
                   Total timesteps: 128188416
                    Iteration time: 2.37s
                      Time elapsed: 00:50:18
                               ETA: 00:46:11

################################################################################
                     [1m Learning iteration 1304/2500 [0m                     

                       Computation: 41598 steps/s (collection: 2.244s, learning 0.120s)
             Mean action noise std: 2.92
          Mean value_function loss: 341.0046
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 76.5070
                       Mean reward: 311.43
               Mean episode length: 136.06
    Episode_Reward/reaching_object: 1.1785
     Episode_Reward/lifting_object: 59.7038
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0454
          Episode_Reward/joint_vel: -0.0933
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.3333
--------------------------------------------------------------------------------
                   Total timesteps: 128286720
                    Iteration time: 2.36s
                      Time elapsed: 00:50:21
                               ETA: 00:46:08

################################################################################
                     [1m Learning iteration 1305/2500 [0m                     

                       Computation: 41358 steps/s (collection: 2.244s, learning 0.133s)
             Mean action noise std: 2.92
          Mean value_function loss: 323.7040
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 76.5168
                       Mean reward: 290.96
               Mean episode length: 129.24
    Episode_Reward/reaching_object: 1.1923
     Episode_Reward/lifting_object: 59.9762
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0462
          Episode_Reward/joint_vel: -0.0948
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.7500
--------------------------------------------------------------------------------
                   Total timesteps: 128385024
                    Iteration time: 2.38s
                      Time elapsed: 00:50:23
                               ETA: 00:46:06

################################################################################
                     [1m Learning iteration 1306/2500 [0m                     

                       Computation: 42457 steps/s (collection: 2.223s, learning 0.093s)
             Mean action noise std: 2.92
          Mean value_function loss: 336.5652
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 76.5273
                       Mean reward: 308.31
               Mean episode length: 133.56
    Episode_Reward/reaching_object: 1.1857
     Episode_Reward/lifting_object: 60.2305
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0459
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.9167
--------------------------------------------------------------------------------
                   Total timesteps: 128483328
                    Iteration time: 2.32s
                      Time elapsed: 00:50:25
                               ETA: 00:46:04

################################################################################
                     [1m Learning iteration 1307/2500 [0m                     

                       Computation: 41774 steps/s (collection: 2.236s, learning 0.117s)
             Mean action noise std: 2.93
          Mean value_function loss: 314.8004
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 76.5473
                       Mean reward: 324.45
               Mean episode length: 138.58
    Episode_Reward/reaching_object: 1.2023
     Episode_Reward/lifting_object: 61.3618
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0953
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.6250
--------------------------------------------------------------------------------
                   Total timesteps: 128581632
                    Iteration time: 2.35s
                      Time elapsed: 00:50:28
                               ETA: 00:46:02

################################################################################
                     [1m Learning iteration 1308/2500 [0m                     

                       Computation: 42133 steps/s (collection: 2.211s, learning 0.123s)
             Mean action noise std: 2.93
          Mean value_function loss: 328.1662
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 76.5610
                       Mean reward: 299.01
               Mean episode length: 133.96
    Episode_Reward/reaching_object: 1.1705
     Episode_Reward/lifting_object: 59.4171
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0457
          Episode_Reward/joint_vel: -0.0939
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.8750
--------------------------------------------------------------------------------
                   Total timesteps: 128679936
                    Iteration time: 2.33s
                      Time elapsed: 00:50:30
                               ETA: 00:45:59

################################################################################
                     [1m Learning iteration 1309/2500 [0m                     

                       Computation: 42010 steps/s (collection: 2.227s, learning 0.113s)
             Mean action noise std: 2.93
          Mean value_function loss: 329.3982
               Mean surrogate loss: 0.0014
                 Mean entropy loss: 76.5693
                       Mean reward: 295.41
               Mean episode length: 130.24
    Episode_Reward/reaching_object: 1.1992
     Episode_Reward/lifting_object: 60.5832
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0467
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 29.8750
--------------------------------------------------------------------------------
                   Total timesteps: 128778240
                    Iteration time: 2.34s
                      Time elapsed: 00:50:32
                               ETA: 00:45:57

################################################################################
                     [1m Learning iteration 1310/2500 [0m                     

                       Computation: 42296 steps/s (collection: 2.210s, learning 0.114s)
             Mean action noise std: 2.93
          Mean value_function loss: 349.8062
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 76.5743
                       Mean reward: 314.20
               Mean episode length: 135.22
    Episode_Reward/reaching_object: 1.2176
     Episode_Reward/lifting_object: 61.9539
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0975
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.0833
--------------------------------------------------------------------------------
                   Total timesteps: 128876544
                    Iteration time: 2.32s
                      Time elapsed: 00:50:35
                               ETA: 00:45:55

################################################################################
                     [1m Learning iteration 1311/2500 [0m                     

                       Computation: 42156 steps/s (collection: 2.219s, learning 0.113s)
             Mean action noise std: 2.93
          Mean value_function loss: 350.3384
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 76.5833
                       Mean reward: 303.38
               Mean episode length: 129.71
    Episode_Reward/reaching_object: 1.1878
     Episode_Reward/lifting_object: 60.4021
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0463
          Episode_Reward/joint_vel: -0.0954
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.1250
--------------------------------------------------------------------------------
                   Total timesteps: 128974848
                    Iteration time: 2.33s
                      Time elapsed: 00:50:37
                               ETA: 00:45:52

################################################################################
                     [1m Learning iteration 1312/2500 [0m                     

                       Computation: 41825 steps/s (collection: 2.238s, learning 0.112s)
             Mean action noise std: 2.93
          Mean value_function loss: 324.2355
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 76.5970
                       Mean reward: 327.84
               Mean episode length: 139.20
    Episode_Reward/reaching_object: 1.2178
     Episode_Reward/lifting_object: 61.9674
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.4583
--------------------------------------------------------------------------------
                   Total timesteps: 129073152
                    Iteration time: 2.35s
                      Time elapsed: 00:50:39
                               ETA: 00:45:50

################################################################################
                     [1m Learning iteration 1313/2500 [0m                     

                       Computation: 42638 steps/s (collection: 2.198s, learning 0.108s)
             Mean action noise std: 2.93
          Mean value_function loss: 335.3031
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 76.6130
                       Mean reward: 303.19
               Mean episode length: 133.30
    Episode_Reward/reaching_object: 1.1911
     Episode_Reward/lifting_object: 59.8740
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0466
          Episode_Reward/joint_vel: -0.0963
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 129171456
                    Iteration time: 2.31s
                      Time elapsed: 00:50:42
                               ETA: 00:45:48

################################################################################
                     [1m Learning iteration 1314/2500 [0m                     

                       Computation: 42367 steps/s (collection: 2.197s, learning 0.124s)
             Mean action noise std: 2.93
          Mean value_function loss: 317.2695
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 76.6245
                       Mean reward: 327.93
               Mean episode length: 140.96
    Episode_Reward/reaching_object: 1.2098
     Episode_Reward/lifting_object: 61.0336
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0474
          Episode_Reward/joint_vel: -0.0970
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.5833
--------------------------------------------------------------------------------
                   Total timesteps: 129269760
                    Iteration time: 2.32s
                      Time elapsed: 00:50:44
                               ETA: 00:45:45

################################################################################
                     [1m Learning iteration 1315/2500 [0m                     

                       Computation: 43251 steps/s (collection: 2.170s, learning 0.103s)
             Mean action noise std: 2.93
          Mean value_function loss: 304.0895
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 76.6357
                       Mean reward: 317.21
               Mean episode length: 134.63
    Episode_Reward/reaching_object: 1.1984
     Episode_Reward/lifting_object: 61.3204
      Episode_Reward/object_height: 0.0038
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.2917
--------------------------------------------------------------------------------
                   Total timesteps: 129368064
                    Iteration time: 2.27s
                      Time elapsed: 00:50:46
                               ETA: 00:45:43

################################################################################
                     [1m Learning iteration 1316/2500 [0m                     

                       Computation: 43065 steps/s (collection: 2.172s, learning 0.111s)
             Mean action noise std: 2.94
          Mean value_function loss: 311.5172
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 76.6480
                       Mean reward: 306.57
               Mean episode length: 130.74
    Episode_Reward/reaching_object: 1.2082
     Episode_Reward/lifting_object: 62.2926
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0470
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 1.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.4583
--------------------------------------------------------------------------------
                   Total timesteps: 129466368
                    Iteration time: 2.28s
                      Time elapsed: 00:50:49
                               ETA: 00:45:41

################################################################################
                     [1m Learning iteration 1317/2500 [0m                     

                       Computation: 43201 steps/s (collection: 2.179s, learning 0.097s)
             Mean action noise std: 2.94
          Mean value_function loss: 316.4818
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 76.6565
                       Mean reward: 312.19
               Mean episode length: 135.94
    Episode_Reward/reaching_object: 1.2164
     Episode_Reward/lifting_object: 62.1520
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0477
          Episode_Reward/joint_vel: -0.0978
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.7917
--------------------------------------------------------------------------------
                   Total timesteps: 129564672
                    Iteration time: 2.28s
                      Time elapsed: 00:50:51
                               ETA: 00:45:38

################################################################################
                     [1m Learning iteration 1318/2500 [0m                     

                       Computation: 42572 steps/s (collection: 2.190s, learning 0.119s)
             Mean action noise std: 2.94
          Mean value_function loss: 322.3075
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 76.6603
                       Mean reward: 326.13
               Mean episode length: 141.07
    Episode_Reward/reaching_object: 1.2214
     Episode_Reward/lifting_object: 62.5934
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0480
          Episode_Reward/joint_vel: -0.0985
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.1667
--------------------------------------------------------------------------------
                   Total timesteps: 129662976
                    Iteration time: 2.31s
                      Time elapsed: 00:50:53
                               ETA: 00:45:36

################################################################################
                     [1m Learning iteration 1319/2500 [0m                     

                       Computation: 42768 steps/s (collection: 2.185s, learning 0.113s)
             Mean action noise std: 2.94
          Mean value_function loss: 343.9581
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 76.6721
                       Mean reward: 337.81
               Mean episode length: 144.06
    Episode_Reward/reaching_object: 1.2652
     Episode_Reward/lifting_object: 65.7437
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.1011
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.0833
--------------------------------------------------------------------------------
                   Total timesteps: 129761280
                    Iteration time: 2.30s
                      Time elapsed: 00:50:56
                               ETA: 00:45:34

################################################################################
                     [1m Learning iteration 1320/2500 [0m                     

                       Computation: 43313 steps/s (collection: 2.157s, learning 0.113s)
             Mean action noise std: 2.94
          Mean value_function loss: 330.7297
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 76.6864
                       Mean reward: 325.06
               Mean episode length: 139.52
    Episode_Reward/reaching_object: 1.2049
     Episode_Reward/lifting_object: 61.7079
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0473
          Episode_Reward/joint_vel: -0.0966
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.2500
--------------------------------------------------------------------------------
                   Total timesteps: 129859584
                    Iteration time: 2.27s
                      Time elapsed: 00:50:58
                               ETA: 00:45:31

################################################################################
                     [1m Learning iteration 1321/2500 [0m                     

                       Computation: 40259 steps/s (collection: 2.304s, learning 0.138s)
             Mean action noise std: 2.94
          Mean value_function loss: 338.3086
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 76.6930
                       Mean reward: 322.09
               Mean episode length: 138.83
    Episode_Reward/reaching_object: 1.2253
     Episode_Reward/lifting_object: 62.7691
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.1001
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 129957888
                    Iteration time: 2.44s
                      Time elapsed: 00:51:00
                               ETA: 00:45:29

################################################################################
                     [1m Learning iteration 1322/2500 [0m                     

                       Computation: 40482 steps/s (collection: 2.336s, learning 0.092s)
             Mean action noise std: 2.94
          Mean value_function loss: 309.1676
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 76.7022
                       Mean reward: 323.25
               Mean episode length: 137.09
    Episode_Reward/reaching_object: 1.1830
     Episode_Reward/lifting_object: 61.3166
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0464
          Episode_Reward/joint_vel: -0.0938
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.7083
--------------------------------------------------------------------------------
                   Total timesteps: 130056192
                    Iteration time: 2.43s
                      Time elapsed: 00:51:03
                               ETA: 00:45:27

################################################################################
                     [1m Learning iteration 1323/2500 [0m                     

                       Computation: 42034 steps/s (collection: 2.218s, learning 0.121s)
             Mean action noise std: 2.94
          Mean value_function loss: 318.5787
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 76.7101
                       Mean reward: 312.48
               Mean episode length: 134.75
    Episode_Reward/reaching_object: 1.2074
     Episode_Reward/lifting_object: 62.2595
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0475
          Episode_Reward/joint_vel: -0.0973
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.7083
--------------------------------------------------------------------------------
                   Total timesteps: 130154496
                    Iteration time: 2.34s
                      Time elapsed: 00:51:05
                               ETA: 00:45:25

################################################################################
                     [1m Learning iteration 1324/2500 [0m                     

                       Computation: 41956 steps/s (collection: 2.241s, learning 0.102s)
             Mean action noise std: 2.94
          Mean value_function loss: 317.0682
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 76.7142
                       Mean reward: 317.15
               Mean episode length: 136.39
    Episode_Reward/reaching_object: 1.2314
     Episode_Reward/lifting_object: 63.1480
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0485
          Episode_Reward/joint_vel: -0.1006
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.0833
--------------------------------------------------------------------------------
                   Total timesteps: 130252800
                    Iteration time: 2.34s
                      Time elapsed: 00:51:07
                               ETA: 00:45:22

################################################################################
                     [1m Learning iteration 1325/2500 [0m                     

                       Computation: 42777 steps/s (collection: 2.190s, learning 0.108s)
             Mean action noise std: 2.94
          Mean value_function loss: 313.4204
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 76.7228
                       Mean reward: 331.30
               Mean episode length: 138.64
    Episode_Reward/reaching_object: 1.2306
     Episode_Reward/lifting_object: 64.0631
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0483
          Episode_Reward/joint_vel: -0.0998
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.6250
--------------------------------------------------------------------------------
                   Total timesteps: 130351104
                    Iteration time: 2.30s
                      Time elapsed: 00:51:10
                               ETA: 00:45:20

################################################################################
                     [1m Learning iteration 1326/2500 [0m                     

                       Computation: 43353 steps/s (collection: 2.176s, learning 0.092s)
             Mean action noise std: 2.95
          Mean value_function loss: 318.9752
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 76.7422
                       Mean reward: 334.21
               Mean episode length: 143.47
    Episode_Reward/reaching_object: 1.2494
     Episode_Reward/lifting_object: 64.4981
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.1033
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 130449408
                    Iteration time: 2.27s
                      Time elapsed: 00:51:12
                               ETA: 00:45:18

################################################################################
                     [1m Learning iteration 1327/2500 [0m                     

                       Computation: 42505 steps/s (collection: 2.197s, learning 0.116s)
             Mean action noise std: 2.95
          Mean value_function loss: 313.1496
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 76.7531
                       Mean reward: 372.00
               Mean episode length: 155.48
    Episode_Reward/reaching_object: 1.2999
     Episode_Reward/lifting_object: 66.6958
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0507
          Episode_Reward/joint_vel: -0.1046
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.4583
--------------------------------------------------------------------------------
                   Total timesteps: 130547712
                    Iteration time: 2.31s
                      Time elapsed: 00:51:14
                               ETA: 00:45:15

################################################################################
                     [1m Learning iteration 1328/2500 [0m                     

                       Computation: 42750 steps/s (collection: 2.207s, learning 0.092s)
             Mean action noise std: 2.95
          Mean value_function loss: 332.1204
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 76.7612
                       Mean reward: 322.49
               Mean episode length: 140.08
    Episode_Reward/reaching_object: 1.2809
     Episode_Reward/lifting_object: 65.6064
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.1048
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.5000
--------------------------------------------------------------------------------
                   Total timesteps: 130646016
                    Iteration time: 2.30s
                      Time elapsed: 00:51:17
                               ETA: 00:45:13

################################################################################
                     [1m Learning iteration 1329/2500 [0m                     

                       Computation: 42492 steps/s (collection: 2.196s, learning 0.117s)
             Mean action noise std: 2.95
          Mean value_function loss: 328.3067
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 76.7763
                       Mean reward: 324.24
               Mean episode length: 139.37
    Episode_Reward/reaching_object: 1.2850
     Episode_Reward/lifting_object: 65.8836
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.1047
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.4583
--------------------------------------------------------------------------------
                   Total timesteps: 130744320
                    Iteration time: 2.31s
                      Time elapsed: 00:51:19
                               ETA: 00:45:11

################################################################################
                     [1m Learning iteration 1330/2500 [0m                     

                       Computation: 42441 steps/s (collection: 2.205s, learning 0.112s)
             Mean action noise std: 2.95
          Mean value_function loss: 318.5585
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 76.7916
                       Mean reward: 338.46
               Mean episode length: 144.69
    Episode_Reward/reaching_object: 1.2691
     Episode_Reward/lifting_object: 64.7922
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.1057
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.3750
--------------------------------------------------------------------------------
                   Total timesteps: 130842624
                    Iteration time: 2.32s
                      Time elapsed: 00:51:21
                               ETA: 00:45:08

################################################################################
                     [1m Learning iteration 1331/2500 [0m                     

                       Computation: 42174 steps/s (collection: 2.221s, learning 0.110s)
             Mean action noise std: 2.95
          Mean value_function loss: 328.6979
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 76.8027
                       Mean reward: 342.45
               Mean episode length: 147.19
    Episode_Reward/reaching_object: 1.2697
     Episode_Reward/lifting_object: 64.9807
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.2083
--------------------------------------------------------------------------------
                   Total timesteps: 130940928
                    Iteration time: 2.33s
                      Time elapsed: 00:51:23
                               ETA: 00:45:06

################################################################################
                     [1m Learning iteration 1332/2500 [0m                     

                       Computation: 41541 steps/s (collection: 2.257s, learning 0.110s)
             Mean action noise std: 2.95
          Mean value_function loss: 358.9673
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 76.8129
                       Mean reward: 312.08
               Mean episode length: 134.76
    Episode_Reward/reaching_object: 1.2417
     Episode_Reward/lifting_object: 63.4349
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.1021
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.7500
--------------------------------------------------------------------------------
                   Total timesteps: 131039232
                    Iteration time: 2.37s
                      Time elapsed: 00:51:26
                               ETA: 00:45:04

################################################################################
                     [1m Learning iteration 1333/2500 [0m                     

                       Computation: 18003 steps/s (collection: 5.329s, learning 0.131s)
             Mean action noise std: 2.96
          Mean value_function loss: 335.2318
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 76.8271
                       Mean reward: 331.10
               Mean episode length: 142.62
    Episode_Reward/reaching_object: 1.2379
     Episode_Reward/lifting_object: 62.9005
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.1030
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.6250
--------------------------------------------------------------------------------
                   Total timesteps: 131137536
                    Iteration time: 5.46s
                      Time elapsed: 00:51:31
                               ETA: 00:45:04

################################################################################
                     [1m Learning iteration 1334/2500 [0m                     

                       Computation: 13460 steps/s (collection: 7.173s, learning 0.130s)
             Mean action noise std: 2.96
          Mean value_function loss: 321.7281
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 76.8359
                       Mean reward: 331.43
               Mean episode length: 141.05
    Episode_Reward/reaching_object: 1.2725
     Episode_Reward/lifting_object: 65.0524
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0501
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.4583
--------------------------------------------------------------------------------
                   Total timesteps: 131235840
                    Iteration time: 7.30s
                      Time elapsed: 00:51:39
                               ETA: 00:45:06

################################################################################
                     [1m Learning iteration 1335/2500 [0m                     

                       Computation: 13434 steps/s (collection: 7.181s, learning 0.137s)
             Mean action noise std: 2.96
          Mean value_function loss: 328.7258
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 76.8413
                       Mean reward: 328.84
               Mean episode length: 141.34
    Episode_Reward/reaching_object: 1.2847
     Episode_Reward/lifting_object: 65.9045
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131334144
                    Iteration time: 7.32s
                      Time elapsed: 00:51:46
                               ETA: 00:45:08

################################################################################
                     [1m Learning iteration 1336/2500 [0m                     

                       Computation: 13746 steps/s (collection: 7.035s, learning 0.116s)
             Mean action noise std: 2.96
          Mean value_function loss: 348.3990
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 76.8527
                       Mean reward: 334.36
               Mean episode length: 140.08
    Episode_Reward/reaching_object: 1.2600
     Episode_Reward/lifting_object: 64.0834
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.1048
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.5417
--------------------------------------------------------------------------------
                   Total timesteps: 131432448
                    Iteration time: 7.15s
                      Time elapsed: 00:51:53
                               ETA: 00:45:10

################################################################################
                     [1m Learning iteration 1337/2500 [0m                     

                       Computation: 13766 steps/s (collection: 7.010s, learning 0.131s)
             Mean action noise std: 2.96
          Mean value_function loss: 357.7392
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 76.8648
                       Mean reward: 327.11
               Mean episode length: 136.36
    Episode_Reward/reaching_object: 1.2886
     Episode_Reward/lifting_object: 65.5353
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.1066
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 30.5000
--------------------------------------------------------------------------------
                   Total timesteps: 131530752
                    Iteration time: 7.14s
                      Time elapsed: 00:52:00
                               ETA: 00:45:12

################################################################################
                     [1m Learning iteration 1338/2500 [0m                     

                       Computation: 13445 steps/s (collection: 7.183s, learning 0.128s)
             Mean action noise std: 2.96
          Mean value_function loss: 359.6805
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 76.8818
                       Mean reward: 305.02
               Mean episode length: 131.88
    Episode_Reward/reaching_object: 1.2177
     Episode_Reward/lifting_object: 61.4162
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.1009
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 30.1667
--------------------------------------------------------------------------------
                   Total timesteps: 131629056
                    Iteration time: 7.31s
                      Time elapsed: 00:52:08
                               ETA: 00:45:14

################################################################################
                     [1m Learning iteration 1339/2500 [0m                     

                       Computation: 13339 steps/s (collection: 7.233s, learning 0.136s)
             Mean action noise std: 2.96
          Mean value_function loss: 341.1951
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 76.8959
                       Mean reward: 339.74
               Mean episode length: 146.03
    Episode_Reward/reaching_object: 1.2513
     Episode_Reward/lifting_object: 63.9746
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0493
          Episode_Reward/joint_vel: -0.1030
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.2083
--------------------------------------------------------------------------------
                   Total timesteps: 131727360
                    Iteration time: 7.37s
                      Time elapsed: 00:52:15
                               ETA: 00:45:16

################################################################################
                     [1m Learning iteration 1340/2500 [0m                     

                       Computation: 13453 steps/s (collection: 7.186s, learning 0.121s)
             Mean action noise std: 2.96
          Mean value_function loss: 333.4798
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 76.9043
                       Mean reward: 321.20
               Mean episode length: 138.20
    Episode_Reward/reaching_object: 1.2189
     Episode_Reward/lifting_object: 62.0090
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0482
          Episode_Reward/joint_vel: -0.1020
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.9167
--------------------------------------------------------------------------------
                   Total timesteps: 131825664
                    Iteration time: 7.31s
                      Time elapsed: 00:52:22
                               ETA: 00:45:18

################################################################################
                     [1m Learning iteration 1341/2500 [0m                     

                       Computation: 12689 steps/s (collection: 7.627s, learning 0.120s)
             Mean action noise std: 2.97
          Mean value_function loss: 337.9064
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 76.9214
                       Mean reward: 313.88
               Mean episode length: 136.07
    Episode_Reward/reaching_object: 1.2329
     Episode_Reward/lifting_object: 62.3953
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.1015
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.8333
--------------------------------------------------------------------------------
                   Total timesteps: 131923968
                    Iteration time: 7.75s
                      Time elapsed: 00:52:30
                               ETA: 00:45:20

################################################################################
                     [1m Learning iteration 1342/2500 [0m                     

                       Computation: 42233 steps/s (collection: 2.217s, learning 0.111s)
             Mean action noise std: 2.97
          Mean value_function loss: 335.3963
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 76.9421
                       Mean reward: 316.74
               Mean episode length: 136.05
    Episode_Reward/reaching_object: 1.2092
     Episode_Reward/lifting_object: 61.9350
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0479
          Episode_Reward/joint_vel: -0.1001
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.8750
--------------------------------------------------------------------------------
                   Total timesteps: 132022272
                    Iteration time: 2.33s
                      Time elapsed: 00:52:32
                               ETA: 00:45:18

################################################################################
                     [1m Learning iteration 1343/2500 [0m                     

                       Computation: 42820 steps/s (collection: 2.189s, learning 0.107s)
             Mean action noise std: 2.97
          Mean value_function loss: 346.1844
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 76.9595
                       Mean reward: 302.99
               Mean episode length: 130.10
    Episode_Reward/reaching_object: 1.2345
     Episode_Reward/lifting_object: 63.0772
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0489
          Episode_Reward/joint_vel: -0.1028
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.0833
--------------------------------------------------------------------------------
                   Total timesteps: 132120576
                    Iteration time: 2.30s
                      Time elapsed: 00:52:35
                               ETA: 00:45:16

################################################################################
                     [1m Learning iteration 1344/2500 [0m                     

                       Computation: 43541 steps/s (collection: 2.159s, learning 0.099s)
             Mean action noise std: 2.97
          Mean value_function loss: 341.9314
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 76.9742
                       Mean reward: 325.64
               Mean episode length: 136.02
    Episode_Reward/reaching_object: 1.2456
     Episode_Reward/lifting_object: 63.8162
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.1031
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 25.5833
--------------------------------------------------------------------------------
                   Total timesteps: 132218880
                    Iteration time: 2.26s
                      Time elapsed: 00:52:37
                               ETA: 00:45:13

################################################################################
                     [1m Learning iteration 1345/2500 [0m                     

                       Computation: 40094 steps/s (collection: 2.338s, learning 0.114s)
             Mean action noise std: 2.97
          Mean value_function loss: 333.7097
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 76.9850
                       Mean reward: 330.20
               Mean episode length: 142.78
    Episode_Reward/reaching_object: 1.2570
     Episode_Reward/lifting_object: 64.2043
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.1057
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.9167
--------------------------------------------------------------------------------
                   Total timesteps: 132317184
                    Iteration time: 2.45s
                      Time elapsed: 00:52:39
                               ETA: 00:45:11

################################################################################
                     [1m Learning iteration 1346/2500 [0m                     

                       Computation: 42959 steps/s (collection: 2.188s, learning 0.101s)
             Mean action noise std: 2.97
          Mean value_function loss: 328.3666
               Mean surrogate loss: -0.0043
                 Mean entropy loss: 76.9979
                       Mean reward: 326.18
               Mean episode length: 139.03
    Episode_Reward/reaching_object: 1.2611
     Episode_Reward/lifting_object: 64.2905
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0498
          Episode_Reward/joint_vel: -0.1045
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 26.5417
--------------------------------------------------------------------------------
                   Total timesteps: 132415488
                    Iteration time: 2.29s
                      Time elapsed: 00:52:42
                               ETA: 00:45:08

################################################################################
                     [1m Learning iteration 1347/2500 [0m                     

                       Computation: 42245 steps/s (collection: 2.211s, learning 0.116s)
             Mean action noise std: 2.97
          Mean value_function loss: 346.8261
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 77.0115
                       Mean reward: 354.09
               Mean episode length: 147.03
    Episode_Reward/reaching_object: 1.2735
     Episode_Reward/lifting_object: 65.2302
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.6250
--------------------------------------------------------------------------------
                   Total timesteps: 132513792
                    Iteration time: 2.33s
                      Time elapsed: 00:52:44
                               ETA: 00:45:06

################################################################################
                     [1m Learning iteration 1348/2500 [0m                     

                       Computation: 41515 steps/s (collection: 2.251s, learning 0.117s)
             Mean action noise std: 2.98
          Mean value_function loss: 353.3515
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 77.0194
                       Mean reward: 342.98
               Mean episode length: 146.49
    Episode_Reward/reaching_object: 1.2928
     Episode_Reward/lifting_object: 66.7583
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.1068
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.5000
--------------------------------------------------------------------------------
                   Total timesteps: 132612096
                    Iteration time: 2.37s
                      Time elapsed: 00:52:46
                               ETA: 00:45:04

################################################################################
                     [1m Learning iteration 1349/2500 [0m                     

                       Computation: 43067 steps/s (collection: 2.189s, learning 0.094s)
             Mean action noise std: 2.98
          Mean value_function loss: 333.1536
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 77.0341
                       Mean reward: 340.66
               Mean episode length: 144.89
    Episode_Reward/reaching_object: 1.2575
     Episode_Reward/lifting_object: 64.2738
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0499
          Episode_Reward/joint_vel: -0.1051
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 28.0417
--------------------------------------------------------------------------------
                   Total timesteps: 132710400
                    Iteration time: 2.28s
                      Time elapsed: 00:52:49
                               ETA: 00:45:01

################################################################################
                     [1m Learning iteration 1350/2500 [0m                     

                       Computation: 42411 steps/s (collection: 2.203s, learning 0.115s)
             Mean action noise std: 2.98
          Mean value_function loss: 335.0649
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 77.0465
                       Mean reward: 336.07
               Mean episode length: 143.16
    Episode_Reward/reaching_object: 1.2567
     Episode_Reward/lifting_object: 65.0497
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.3750
--------------------------------------------------------------------------------
                   Total timesteps: 132808704
                    Iteration time: 2.32s
                      Time elapsed: 00:52:51
                               ETA: 00:44:59

################################################################################
                     [1m Learning iteration 1351/2500 [0m                     

                       Computation: 42331 steps/s (collection: 2.225s, learning 0.097s)
             Mean action noise std: 2.98
          Mean value_function loss: 340.7114
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 77.0581
                       Mean reward: 328.71
               Mean episode length: 138.40
    Episode_Reward/reaching_object: 1.2222
     Episode_Reward/lifting_object: 62.8619
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.1017
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.7500
--------------------------------------------------------------------------------
                   Total timesteps: 132907008
                    Iteration time: 2.32s
                      Time elapsed: 00:52:53
                               ETA: 00:44:57

################################################################################
                     [1m Learning iteration 1352/2500 [0m                     

                       Computation: 42678 steps/s (collection: 2.198s, learning 0.106s)
             Mean action noise std: 2.98
          Mean value_function loss: 364.6558
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 77.0718
                       Mean reward: 319.13
               Mean episode length: 136.46
    Episode_Reward/reaching_object: 1.2438
     Episode_Reward/lifting_object: 63.7757
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0495
          Episode_Reward/joint_vel: -0.1041
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.6667
--------------------------------------------------------------------------------
                   Total timesteps: 133005312
                    Iteration time: 2.30s
                      Time elapsed: 00:52:55
                               ETA: 00:44:54

################################################################################
                     [1m Learning iteration 1353/2500 [0m                     

                       Computation: 42476 steps/s (collection: 2.213s, learning 0.102s)
             Mean action noise std: 2.98
          Mean value_function loss: 328.3067
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 77.0850
                       Mean reward: 326.05
               Mean episode length: 141.29
    Episode_Reward/reaching_object: 1.2170
     Episode_Reward/lifting_object: 62.5857
      Episode_Reward/object_height: 0.0039
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.1026
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.7917
--------------------------------------------------------------------------------
                   Total timesteps: 133103616
                    Iteration time: 2.31s
                      Time elapsed: 00:52:58
                               ETA: 00:44:52

################################################################################
                     [1m Learning iteration 1354/2500 [0m                     

                       Computation: 41647 steps/s (collection: 2.205s, learning 0.156s)
             Mean action noise std: 2.98
          Mean value_function loss: 322.6514
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 77.0946
                       Mean reward: 337.82
               Mean episode length: 143.17
    Episode_Reward/reaching_object: 1.2441
     Episode_Reward/lifting_object: 64.4700
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.1017
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.4167
--------------------------------------------------------------------------------
                   Total timesteps: 133201920
                    Iteration time: 2.36s
                      Time elapsed: 00:53:00
                               ETA: 00:44:50

################################################################################
                     [1m Learning iteration 1355/2500 [0m                     

                       Computation: 41901 steps/s (collection: 2.250s, learning 0.096s)
             Mean action noise std: 2.98
          Mean value_function loss: 342.1475
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 77.1064
                       Mean reward: 335.42
               Mean episode length: 136.50
    Episode_Reward/reaching_object: 1.2846
     Episode_Reward/lifting_object: 66.6832
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.1064
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.8750
--------------------------------------------------------------------------------
                   Total timesteps: 133300224
                    Iteration time: 2.35s
                      Time elapsed: 00:53:03
                               ETA: 00:44:47

################################################################################
                     [1m Learning iteration 1356/2500 [0m                     

                       Computation: 42089 steps/s (collection: 2.233s, learning 0.102s)
             Mean action noise std: 2.99
          Mean value_function loss: 334.3446
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 77.1201
                       Mean reward: 324.53
               Mean episode length: 135.55
    Episode_Reward/reaching_object: 1.2684
     Episode_Reward/lifting_object: 65.3452
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.1051
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133398528
                    Iteration time: 2.34s
                      Time elapsed: 00:53:05
                               ETA: 00:44:45

################################################################################
                     [1m Learning iteration 1357/2500 [0m                     

                       Computation: 42047 steps/s (collection: 2.216s, learning 0.122s)
             Mean action noise std: 2.99
          Mean value_function loss: 329.5289
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 77.1346
                       Mean reward: 342.09
               Mean episode length: 142.01
    Episode_Reward/reaching_object: 1.2473
     Episode_Reward/lifting_object: 64.1999
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.1034
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133496832
                    Iteration time: 2.34s
                      Time elapsed: 00:53:07
                               ETA: 00:44:43

################################################################################
                     [1m Learning iteration 1358/2500 [0m                     

                       Computation: 42676 steps/s (collection: 2.201s, learning 0.103s)
             Mean action noise std: 2.99
          Mean value_function loss: 337.5033
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 77.1478
                       Mean reward: 325.68
               Mean episode length: 134.16
    Episode_Reward/reaching_object: 1.2689
     Episode_Reward/lifting_object: 65.6576
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.0833
--------------------------------------------------------------------------------
                   Total timesteps: 133595136
                    Iteration time: 2.30s
                      Time elapsed: 00:53:09
                               ETA: 00:44:40

################################################################################
                     [1m Learning iteration 1359/2500 [0m                     

                       Computation: 42805 steps/s (collection: 2.199s, learning 0.098s)
             Mean action noise std: 2.99
          Mean value_function loss: 339.1901
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 77.1565
                       Mean reward: 342.34
               Mean episode length: 144.02
    Episode_Reward/reaching_object: 1.2876
     Episode_Reward/lifting_object: 66.7624
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.1058
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.5833
--------------------------------------------------------------------------------
                   Total timesteps: 133693440
                    Iteration time: 2.30s
                      Time elapsed: 00:53:12
                               ETA: 00:44:38

################################################################################
                     [1m Learning iteration 1360/2500 [0m                     

                       Computation: 42982 steps/s (collection: 2.180s, learning 0.107s)
             Mean action noise std: 2.99
          Mean value_function loss: 326.0170
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 77.1657
                       Mean reward: 339.64
               Mean episode length: 141.79
    Episode_Reward/reaching_object: 1.3377
     Episode_Reward/lifting_object: 69.0604
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.1090
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.1250
--------------------------------------------------------------------------------
                   Total timesteps: 133791744
                    Iteration time: 2.29s
                      Time elapsed: 00:53:14
                               ETA: 00:44:35

################################################################################
                     [1m Learning iteration 1361/2500 [0m                     

                       Computation: 42054 steps/s (collection: 2.228s, learning 0.110s)
             Mean action noise std: 2.99
          Mean value_function loss: 351.7412
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 77.1744
                       Mean reward: 350.33
               Mean episode length: 145.51
    Episode_Reward/reaching_object: 1.2940
     Episode_Reward/lifting_object: 67.4584
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 27.5417
--------------------------------------------------------------------------------
                   Total timesteps: 133890048
                    Iteration time: 2.34s
                      Time elapsed: 00:53:16
                               ETA: 00:44:33

################################################################################
                     [1m Learning iteration 1362/2500 [0m                     

                       Computation: 41426 steps/s (collection: 2.255s, learning 0.118s)
             Mean action noise std: 2.99
          Mean value_function loss: 349.8476
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 77.1900
                       Mean reward: 322.08
               Mean episode length: 133.76
    Episode_Reward/reaching_object: 1.2741
     Episode_Reward/lifting_object: 66.1854
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.1034
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.1667
--------------------------------------------------------------------------------
                   Total timesteps: 133988352
                    Iteration time: 2.37s
                      Time elapsed: 00:53:19
                               ETA: 00:44:31

################################################################################
                     [1m Learning iteration 1363/2500 [0m                     

                       Computation: 40597 steps/s (collection: 2.279s, learning 0.142s)
             Mean action noise std: 3.00
          Mean value_function loss: 348.9170
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 77.2049
                       Mean reward: 350.48
               Mean episode length: 147.93
    Episode_Reward/reaching_object: 1.3025
     Episode_Reward/lifting_object: 67.4495
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.5417
--------------------------------------------------------------------------------
                   Total timesteps: 134086656
                    Iteration time: 2.42s
                      Time elapsed: 00:53:21
                               ETA: 00:44:28

################################################################################
                     [1m Learning iteration 1364/2500 [0m                     

                       Computation: 42142 steps/s (collection: 2.239s, learning 0.094s)
             Mean action noise std: 3.00
          Mean value_function loss: 308.8347
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 77.2184
                       Mean reward: 356.21
               Mean episode length: 152.09
    Episode_Reward/reaching_object: 1.3081
     Episode_Reward/lifting_object: 67.9374
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.1073
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.7500
--------------------------------------------------------------------------------
                   Total timesteps: 134184960
                    Iteration time: 2.33s
                      Time elapsed: 00:53:24
                               ETA: 00:44:26

################################################################################
                     [1m Learning iteration 1365/2500 [0m                     

                       Computation: 43144 steps/s (collection: 2.181s, learning 0.097s)
             Mean action noise std: 3.00
          Mean value_function loss: 308.3511
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 77.2374
                       Mean reward: 360.37
               Mean episode length: 149.96
    Episode_Reward/reaching_object: 1.3101
     Episode_Reward/lifting_object: 67.8214
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.1067
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 25.5000
--------------------------------------------------------------------------------
                   Total timesteps: 134283264
                    Iteration time: 2.28s
                      Time elapsed: 00:53:26
                               ETA: 00:44:24

################################################################################
                     [1m Learning iteration 1366/2500 [0m                     

                       Computation: 42984 steps/s (collection: 2.191s, learning 0.096s)
             Mean action noise std: 3.00
          Mean value_function loss: 330.8466
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 77.2495
                       Mean reward: 330.74
               Mean episode length: 138.41
    Episode_Reward/reaching_object: 1.2846
     Episode_Reward/lifting_object: 66.8370
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.1053
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 25.5417
--------------------------------------------------------------------------------
                   Total timesteps: 134381568
                    Iteration time: 2.29s
                      Time elapsed: 00:53:28
                               ETA: 00:44:21

################################################################################
                     [1m Learning iteration 1367/2500 [0m                     

                       Computation: 41569 steps/s (collection: 2.224s, learning 0.141s)
             Mean action noise std: 3.00
          Mean value_function loss: 334.8451
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 77.2660
                       Mean reward: 345.11
               Mean episode length: 146.49
    Episode_Reward/reaching_object: 1.3063
     Episode_Reward/lifting_object: 67.1408
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.1072
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.5000
--------------------------------------------------------------------------------
                   Total timesteps: 134479872
                    Iteration time: 2.36s
                      Time elapsed: 00:53:30
                               ETA: 00:44:19

################################################################################
                     [1m Learning iteration 1368/2500 [0m                     

                       Computation: 42863 steps/s (collection: 2.203s, learning 0.091s)
             Mean action noise std: 3.00
          Mean value_function loss: 289.5673
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 77.2773
                       Mean reward: 362.57
               Mean episode length: 150.48
    Episode_Reward/reaching_object: 1.3549
     Episode_Reward/lifting_object: 70.2494
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.1104
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 23.3750
--------------------------------------------------------------------------------
                   Total timesteps: 134578176
                    Iteration time: 2.29s
                      Time elapsed: 00:53:33
                               ETA: 00:44:16

################################################################################
                     [1m Learning iteration 1369/2500 [0m                     

                       Computation: 41394 steps/s (collection: 2.274s, learning 0.101s)
             Mean action noise std: 3.00
          Mean value_function loss: 326.9390
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 77.2855
                       Mean reward: 335.56
               Mean episode length: 141.47
    Episode_Reward/reaching_object: 1.3369
     Episode_Reward/lifting_object: 69.4961
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.1100
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.5833
--------------------------------------------------------------------------------
                   Total timesteps: 134676480
                    Iteration time: 2.37s
                      Time elapsed: 00:53:35
                               ETA: 00:44:14

################################################################################
                     [1m Learning iteration 1370/2500 [0m                     

                       Computation: 41085 steps/s (collection: 2.266s, learning 0.127s)
             Mean action noise std: 3.00
          Mean value_function loss: 331.3742
               Mean surrogate loss: 0.0059
                 Mean entropy loss: 77.2916
                       Mean reward: 351.97
               Mean episode length: 148.23
    Episode_Reward/reaching_object: 1.3491
     Episode_Reward/lifting_object: 70.2303
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.1102
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.7917
--------------------------------------------------------------------------------
                   Total timesteps: 134774784
                    Iteration time: 2.39s
                      Time elapsed: 00:53:38
                               ETA: 00:44:12

################################################################################
                     [1m Learning iteration 1371/2500 [0m                     

                       Computation: 42453 steps/s (collection: 2.203s, learning 0.112s)
             Mean action noise std: 3.01
          Mean value_function loss: 326.2011
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 77.2954
                       Mean reward: 352.93
               Mean episode length: 148.26
    Episode_Reward/reaching_object: 1.3829
     Episode_Reward/lifting_object: 72.1748
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0548
          Episode_Reward/joint_vel: -0.1137
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.5417
--------------------------------------------------------------------------------
                   Total timesteps: 134873088
                    Iteration time: 2.32s
                      Time elapsed: 00:53:40
                               ETA: 00:44:09

################################################################################
                     [1m Learning iteration 1372/2500 [0m                     

                       Computation: 42778 steps/s (collection: 2.207s, learning 0.091s)
             Mean action noise std: 3.01
          Mean value_function loss: 338.7053
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 77.3078
                       Mean reward: 352.76
               Mean episode length: 148.38
    Episode_Reward/reaching_object: 1.3418
     Episode_Reward/lifting_object: 69.8762
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.1084
      Episode_Termination/time_out: 2.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.6667
--------------------------------------------------------------------------------
                   Total timesteps: 134971392
                    Iteration time: 2.30s
                      Time elapsed: 00:53:42
                               ETA: 00:44:07

################################################################################
                     [1m Learning iteration 1373/2500 [0m                     

                       Computation: 41360 steps/s (collection: 2.274s, learning 0.103s)
             Mean action noise std: 3.01
          Mean value_function loss: 318.0825
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 77.3246
                       Mean reward: 365.67
               Mean episode length: 152.45
    Episode_Reward/reaching_object: 1.3485
     Episode_Reward/lifting_object: 70.7485
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.1101
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 23.7500
--------------------------------------------------------------------------------
                   Total timesteps: 135069696
                    Iteration time: 2.38s
                      Time elapsed: 00:53:45
                               ETA: 00:44:05

################################################################################
                     [1m Learning iteration 1374/2500 [0m                     

                       Computation: 41840 steps/s (collection: 2.218s, learning 0.132s)
             Mean action noise std: 3.01
          Mean value_function loss: 331.1500
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 77.3383
                       Mean reward: 359.39
               Mean episode length: 148.73
    Episode_Reward/reaching_object: 1.3253
     Episode_Reward/lifting_object: 69.4103
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.1064
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.5000
--------------------------------------------------------------------------------
                   Total timesteps: 135168000
                    Iteration time: 2.35s
                      Time elapsed: 00:53:47
                               ETA: 00:44:02

################################################################################
                     [1m Learning iteration 1375/2500 [0m                     

                       Computation: 42621 steps/s (collection: 2.217s, learning 0.090s)
             Mean action noise std: 3.01
          Mean value_function loss: 321.3504
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 77.3445
                       Mean reward: 381.44
               Mean episode length: 157.29
    Episode_Reward/reaching_object: 1.3598
     Episode_Reward/lifting_object: 71.3712
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.1105
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.8333
--------------------------------------------------------------------------------
                   Total timesteps: 135266304
                    Iteration time: 2.31s
                      Time elapsed: 00:53:49
                               ETA: 00:44:00

################################################################################
                     [1m Learning iteration 1376/2500 [0m                     

                       Computation: 41351 steps/s (collection: 2.229s, learning 0.149s)
             Mean action noise std: 3.01
          Mean value_function loss: 330.4555
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 77.3585
                       Mean reward: 340.64
               Mean episode length: 143.07
    Episode_Reward/reaching_object: 1.3688
     Episode_Reward/lifting_object: 71.4270
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.1103
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.1250
--------------------------------------------------------------------------------
                   Total timesteps: 135364608
                    Iteration time: 2.38s
                      Time elapsed: 00:53:52
                               ETA: 00:43:58

################################################################################
                     [1m Learning iteration 1377/2500 [0m                     

                       Computation: 41878 steps/s (collection: 2.227s, learning 0.120s)
             Mean action noise std: 3.01
          Mean value_function loss: 341.3230
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 77.3774
                       Mean reward: 339.08
               Mean episode length: 140.35
    Episode_Reward/reaching_object: 1.3281
     Episode_Reward/lifting_object: 69.4590
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.1078
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.5417
--------------------------------------------------------------------------------
                   Total timesteps: 135462912
                    Iteration time: 2.35s
                      Time elapsed: 00:53:54
                               ETA: 00:43:55

################################################################################
                     [1m Learning iteration 1378/2500 [0m                     

                       Computation: 42400 steps/s (collection: 2.210s, learning 0.108s)
             Mean action noise std: 3.02
          Mean value_function loss: 321.8809
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 77.3903
                       Mean reward: 357.05
               Mean episode length: 149.31
    Episode_Reward/reaching_object: 1.3557
     Episode_Reward/lifting_object: 70.3745
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.1118
      Episode_Termination/time_out: 2.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 135561216
                    Iteration time: 2.32s
                      Time elapsed: 00:53:56
                               ETA: 00:43:53

################################################################################
                     [1m Learning iteration 1379/2500 [0m                     

                       Computation: 41058 steps/s (collection: 2.285s, learning 0.109s)
             Mean action noise std: 3.02
          Mean value_function loss: 349.2781
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 77.4020
                       Mean reward: 363.20
               Mean episode length: 150.50
    Episode_Reward/reaching_object: 1.3283
     Episode_Reward/lifting_object: 68.4252
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.1078
      Episode_Termination/time_out: 2.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.5833
--------------------------------------------------------------------------------
                   Total timesteps: 135659520
                    Iteration time: 2.39s
                      Time elapsed: 00:53:59
                               ETA: 00:43:51

################################################################################
                     [1m Learning iteration 1380/2500 [0m                     

                       Computation: 42051 steps/s (collection: 2.228s, learning 0.110s)
             Mean action noise std: 3.02
          Mean value_function loss: 337.3705
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 77.4109
                       Mean reward: 341.71
               Mean episode length: 141.34
    Episode_Reward/reaching_object: 1.3203
     Episode_Reward/lifting_object: 69.0220
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.1071
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 135757824
                    Iteration time: 2.34s
                      Time elapsed: 00:54:01
                               ETA: 00:43:48

################################################################################
                     [1m Learning iteration 1381/2500 [0m                     

                       Computation: 41099 steps/s (collection: 2.294s, learning 0.098s)
             Mean action noise std: 3.02
          Mean value_function loss: 362.7129
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 77.4259
                       Mean reward: 355.23
               Mean episode length: 150.09
    Episode_Reward/reaching_object: 1.3120
     Episode_Reward/lifting_object: 67.4476
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.1073
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.5833
--------------------------------------------------------------------------------
                   Total timesteps: 135856128
                    Iteration time: 2.39s
                      Time elapsed: 00:54:03
                               ETA: 00:43:46

################################################################################
                     [1m Learning iteration 1382/2500 [0m                     

                       Computation: 42216 steps/s (collection: 2.230s, learning 0.098s)
             Mean action noise std: 3.02
          Mean value_function loss: 326.7497
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 77.4385
                       Mean reward: 340.19
               Mean episode length: 144.33
    Episode_Reward/reaching_object: 1.3540
     Episode_Reward/lifting_object: 70.4930
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.1101
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.8333
--------------------------------------------------------------------------------
                   Total timesteps: 135954432
                    Iteration time: 2.33s
                      Time elapsed: 00:54:06
                               ETA: 00:43:44

################################################################################
                     [1m Learning iteration 1383/2500 [0m                     

                       Computation: 42469 steps/s (collection: 2.226s, learning 0.089s)
             Mean action noise std: 3.02
          Mean value_function loss: 358.3302
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 77.4472
                       Mean reward: 312.80
               Mean episode length: 134.30
    Episode_Reward/reaching_object: 1.3079
     Episode_Reward/lifting_object: 67.3085
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.1063
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.5833
--------------------------------------------------------------------------------
                   Total timesteps: 136052736
                    Iteration time: 2.31s
                      Time elapsed: 00:54:08
                               ETA: 00:43:41

################################################################################
                     [1m Learning iteration 1384/2500 [0m                     

                       Computation: 42283 steps/s (collection: 2.227s, learning 0.098s)
             Mean action noise std: 3.02
          Mean value_function loss: 349.0941
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 77.4561
                       Mean reward: 351.66
               Mean episode length: 145.38
    Episode_Reward/reaching_object: 1.3057
     Episode_Reward/lifting_object: 68.2742
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.1051
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 136151040
                    Iteration time: 2.32s
                      Time elapsed: 00:54:10
                               ETA: 00:43:39

################################################################################
                     [1m Learning iteration 1385/2500 [0m                     

                       Computation: 42159 steps/s (collection: 2.206s, learning 0.126s)
             Mean action noise std: 3.02
          Mean value_function loss: 334.0200
               Mean surrogate loss: -0.0048
                 Mean entropy loss: 77.4614
                       Mean reward: 334.04
               Mean episode length: 139.32
    Episode_Reward/reaching_object: 1.2815
     Episode_Reward/lifting_object: 66.6594
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.1040
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136249344
                    Iteration time: 2.33s
                      Time elapsed: 00:54:13
                               ETA: 00:43:37

################################################################################
                     [1m Learning iteration 1386/2500 [0m                     

                       Computation: 42631 steps/s (collection: 2.218s, learning 0.088s)
             Mean action noise std: 3.02
          Mean value_function loss: 364.6319
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 77.4703
                       Mean reward: 332.79
               Mean episode length: 139.34
    Episode_Reward/reaching_object: 1.2720
     Episode_Reward/lifting_object: 66.1806
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0509
          Episode_Reward/joint_vel: -0.1033
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.9583
--------------------------------------------------------------------------------
                   Total timesteps: 136347648
                    Iteration time: 2.31s
                      Time elapsed: 00:54:15
                               ETA: 00:43:34

################################################################################
                     [1m Learning iteration 1387/2500 [0m                     

                       Computation: 42499 steps/s (collection: 2.202s, learning 0.111s)
             Mean action noise std: 3.03
          Mean value_function loss: 350.3713
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 77.4813
                       Mean reward: 329.38
               Mean episode length: 137.26
    Episode_Reward/reaching_object: 1.2595
     Episode_Reward/lifting_object: 65.8842
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.1020
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 26.5000
--------------------------------------------------------------------------------
                   Total timesteps: 136445952
                    Iteration time: 2.31s
                      Time elapsed: 00:54:17
                               ETA: 00:43:32

################################################################################
                     [1m Learning iteration 1388/2500 [0m                     

                       Computation: 42417 steps/s (collection: 2.204s, learning 0.114s)
             Mean action noise std: 3.03
          Mean value_function loss: 342.7879
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 77.4871
                       Mean reward: 359.49
               Mean episode length: 148.67
    Episode_Reward/reaching_object: 1.2870
     Episode_Reward/lifting_object: 67.6911
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.1026
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 136544256
                    Iteration time: 2.32s
                      Time elapsed: 00:54:20
                               ETA: 00:43:29

################################################################################
                     [1m Learning iteration 1389/2500 [0m                     

                       Computation: 42611 steps/s (collection: 2.203s, learning 0.104s)
             Mean action noise std: 3.03
          Mean value_function loss: 351.6725
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 77.4964
                       Mean reward: 348.88
               Mean episode length: 143.18
    Episode_Reward/reaching_object: 1.3103
     Episode_Reward/lifting_object: 68.8086
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.1056
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.2083
--------------------------------------------------------------------------------
                   Total timesteps: 136642560
                    Iteration time: 2.31s
                      Time elapsed: 00:54:22
                               ETA: 00:43:27

################################################################################
                     [1m Learning iteration 1390/2500 [0m                     

                       Computation: 42561 steps/s (collection: 2.203s, learning 0.107s)
             Mean action noise std: 3.03
          Mean value_function loss: 348.3231
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 77.5088
                       Mean reward: 371.12
               Mean episode length: 151.43
    Episode_Reward/reaching_object: 1.2985
     Episode_Reward/lifting_object: 68.1246
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.1049
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.8333
--------------------------------------------------------------------------------
                   Total timesteps: 136740864
                    Iteration time: 2.31s
                      Time elapsed: 00:54:24
                               ETA: 00:43:25

################################################################################
                     [1m Learning iteration 1391/2500 [0m                     

                       Computation: 42512 steps/s (collection: 2.219s, learning 0.094s)
             Mean action noise std: 3.03
          Mean value_function loss: 348.1137
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 77.5255
                       Mean reward: 336.45
               Mean episode length: 137.99
    Episode_Reward/reaching_object: 1.2583
     Episode_Reward/lifting_object: 66.4894
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.1012
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.5000
--------------------------------------------------------------------------------
                   Total timesteps: 136839168
                    Iteration time: 2.31s
                      Time elapsed: 00:54:26
                               ETA: 00:43:22

################################################################################
                     [1m Learning iteration 1392/2500 [0m                     

                       Computation: 42302 steps/s (collection: 2.212s, learning 0.112s)
             Mean action noise std: 3.03
          Mean value_function loss: 356.2412
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 77.5408
                       Mean reward: 343.21
               Mean episode length: 138.41
    Episode_Reward/reaching_object: 1.2949
     Episode_Reward/lifting_object: 68.4111
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0521
          Episode_Reward/joint_vel: -0.1023
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.1667
--------------------------------------------------------------------------------
                   Total timesteps: 136937472
                    Iteration time: 2.32s
                      Time elapsed: 00:54:29
                               ETA: 00:43:20

################################################################################
                     [1m Learning iteration 1393/2500 [0m                     

                       Computation: 41822 steps/s (collection: 2.230s, learning 0.121s)
             Mean action noise std: 3.03
          Mean value_function loss: 357.3116
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 77.5533
                       Mean reward: 354.76
               Mean episode length: 144.91
    Episode_Reward/reaching_object: 1.2373
     Episode_Reward/lifting_object: 65.7631
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0981
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 28.0000
--------------------------------------------------------------------------------
                   Total timesteps: 137035776
                    Iteration time: 2.35s
                      Time elapsed: 00:54:31
                               ETA: 00:43:18

################################################################################
                     [1m Learning iteration 1394/2500 [0m                     

                       Computation: 42919 steps/s (collection: 2.190s, learning 0.101s)
             Mean action noise std: 3.04
          Mean value_function loss: 345.8152
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 77.5669
                       Mean reward: 349.04
               Mean episode length: 143.66
    Episode_Reward/reaching_object: 1.2582
     Episode_Reward/lifting_object: 67.3030
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0510
          Episode_Reward/joint_vel: -0.0991
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 27.9167
--------------------------------------------------------------------------------
                   Total timesteps: 137134080
                    Iteration time: 2.29s
                      Time elapsed: 00:54:33
                               ETA: 00:43:15

################################################################################
                     [1m Learning iteration 1395/2500 [0m                     

                       Computation: 41135 steps/s (collection: 2.263s, learning 0.127s)
             Mean action noise std: 3.04
          Mean value_function loss: 333.3083
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 77.5809
                       Mean reward: 349.46
               Mean episode length: 140.27
    Episode_Reward/reaching_object: 1.2815
     Episode_Reward/lifting_object: 68.5950
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.1006
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 27.0417
--------------------------------------------------------------------------------
                   Total timesteps: 137232384
                    Iteration time: 2.39s
                      Time elapsed: 00:54:36
                               ETA: 00:43:13

################################################################################
                     [1m Learning iteration 1396/2500 [0m                     

                       Computation: 42755 steps/s (collection: 2.188s, learning 0.112s)
             Mean action noise std: 3.04
          Mean value_function loss: 371.3719
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 77.5861
                       Mean reward: 344.89
               Mean episode length: 141.28
    Episode_Reward/reaching_object: 1.2565
     Episode_Reward/lifting_object: 66.9987
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.1000
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.9167
--------------------------------------------------------------------------------
                   Total timesteps: 137330688
                    Iteration time: 2.30s
                      Time elapsed: 00:54:38
                               ETA: 00:43:11

################################################################################
                     [1m Learning iteration 1397/2500 [0m                     

                       Computation: 41985 steps/s (collection: 2.227s, learning 0.115s)
             Mean action noise std: 3.04
          Mean value_function loss: 342.4801
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 77.5911
                       Mean reward: 341.27
               Mean episode length: 138.41
    Episode_Reward/reaching_object: 1.2313
     Episode_Reward/lifting_object: 66.0132
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0500
          Episode_Reward/joint_vel: -0.0965
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 27.3333
--------------------------------------------------------------------------------
                   Total timesteps: 137428992
                    Iteration time: 2.34s
                      Time elapsed: 00:54:40
                               ETA: 00:43:08

################################################################################
                     [1m Learning iteration 1398/2500 [0m                     

                       Computation: 42761 steps/s (collection: 2.196s, learning 0.103s)
             Mean action noise std: 3.04
          Mean value_function loss: 405.9775
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 77.6049
                       Mean reward: 294.98
               Mean episode length: 125.04
    Episode_Reward/reaching_object: 1.2066
     Episode_Reward/lifting_object: 64.6980
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0945
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 32.7083
--------------------------------------------------------------------------------
                   Total timesteps: 137527296
                    Iteration time: 2.30s
                      Time elapsed: 00:54:43
                               ETA: 00:43:06

################################################################################
                     [1m Learning iteration 1399/2500 [0m                     

                       Computation: 41685 steps/s (collection: 2.252s, learning 0.106s)
             Mean action noise std: 3.04
          Mean value_function loss: 370.2350
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 77.6160
                       Mean reward: 342.08
               Mean episode length: 139.37
    Episode_Reward/reaching_object: 1.2227
     Episode_Reward/lifting_object: 66.1884
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0497
          Episode_Reward/joint_vel: -0.0957
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 29.6667
--------------------------------------------------------------------------------
                   Total timesteps: 137625600
                    Iteration time: 2.36s
                      Time elapsed: 00:54:45
                               ETA: 00:43:03

################################################################################
                     [1m Learning iteration 1400/2500 [0m                     

                       Computation: 41473 steps/s (collection: 2.276s, learning 0.094s)
             Mean action noise std: 3.04
          Mean value_function loss: 372.8150
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 77.6249
                       Mean reward: 315.22
               Mean episode length: 131.64
    Episode_Reward/reaching_object: 1.1920
     Episode_Reward/lifting_object: 63.7875
      Episode_Reward/object_height: 0.0040
        Episode_Reward/action_rate: -0.0487
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 0.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.8333
--------------------------------------------------------------------------------
                   Total timesteps: 137723904
                    Iteration time: 2.37s
                      Time elapsed: 00:54:48
                               ETA: 00:43:01

################################################################################
                     [1m Learning iteration 1401/2500 [0m                     

                       Computation: 42037 steps/s (collection: 2.234s, learning 0.104s)
             Mean action noise std: 3.04
          Mean value_function loss: 356.0966
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 77.6339
                       Mean reward: 318.72
               Mean episode length: 131.50
    Episode_Reward/reaching_object: 1.1845
     Episode_Reward/lifting_object: 63.4750
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0484
          Episode_Reward/joint_vel: -0.0921
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.8750
--------------------------------------------------------------------------------
                   Total timesteps: 137822208
                    Iteration time: 2.34s
                      Time elapsed: 00:54:50
                               ETA: 00:42:59

################################################################################
                     [1m Learning iteration 1402/2500 [0m                     

                       Computation: 41383 steps/s (collection: 2.267s, learning 0.108s)
             Mean action noise std: 3.04
          Mean value_function loss: 369.0666
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 77.6443
                       Mean reward: 314.60
               Mean episode length: 125.64
    Episode_Reward/reaching_object: 1.1983
     Episode_Reward/lifting_object: 64.4228
      Episode_Reward/object_height: 0.0041
        Episode_Reward/action_rate: -0.0490
          Episode_Reward/joint_vel: -0.0940
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.1667
--------------------------------------------------------------------------------
                   Total timesteps: 137920512
                    Iteration time: 2.38s
                      Time elapsed: 00:54:52
                               ETA: 00:42:56

################################################################################
                     [1m Learning iteration 1403/2500 [0m                     

                       Computation: 41630 steps/s (collection: 2.250s, learning 0.111s)
             Mean action noise std: 3.05
          Mean value_function loss: 380.8051
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 77.6549
                       Mean reward: 340.62
               Mean episode length: 136.51
    Episode_Reward/reaching_object: 1.2057
     Episode_Reward/lifting_object: 64.8984
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0494
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 31.2500
--------------------------------------------------------------------------------
                   Total timesteps: 138018816
                    Iteration time: 2.36s
                      Time elapsed: 00:54:55
                               ETA: 00:42:54

################################################################################
                     [1m Learning iteration 1404/2500 [0m                     

                       Computation: 40835 steps/s (collection: 2.257s, learning 0.150s)
             Mean action noise std: 3.05
          Mean value_function loss: 388.0617
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 77.6641
                       Mean reward: 341.34
               Mean episode length: 137.84
    Episode_Reward/reaching_object: 1.2341
     Episode_Reward/lifting_object: 66.7477
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0505
          Episode_Reward/joint_vel: -0.0969
      Episode_Termination/time_out: 0.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 28.5000
--------------------------------------------------------------------------------
                   Total timesteps: 138117120
                    Iteration time: 2.41s
                      Time elapsed: 00:54:57
                               ETA: 00:42:52

################################################################################
                     [1m Learning iteration 1405/2500 [0m                     

                       Computation: 41887 steps/s (collection: 2.243s, learning 0.104s)
             Mean action noise std: 3.05
          Mean value_function loss: 349.8956
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 77.6814
                       Mean reward: 350.27
               Mean episode length: 141.09
    Episode_Reward/reaching_object: 1.2004
     Episode_Reward/lifting_object: 64.6133
      Episode_Reward/object_height: 0.0042
        Episode_Reward/action_rate: -0.0492
          Episode_Reward/joint_vel: -0.0931
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138215424
                    Iteration time: 2.35s
                      Time elapsed: 00:54:59
                               ETA: 00:42:49

################################################################################
                     [1m Learning iteration 1406/2500 [0m                     

                       Computation: 42971 steps/s (collection: 2.199s, learning 0.089s)
             Mean action noise std: 3.05
          Mean value_function loss: 337.5839
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 77.6937
                       Mean reward: 357.86
               Mean episode length: 143.72
    Episode_Reward/reaching_object: 1.2346
     Episode_Reward/lifting_object: 67.1564
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0506
          Episode_Reward/joint_vel: -0.0970
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.0000
--------------------------------------------------------------------------------
                   Total timesteps: 138313728
                    Iteration time: 2.29s
                      Time elapsed: 00:55:02
                               ETA: 00:42:47

################################################################################
                     [1m Learning iteration 1407/2500 [0m                     

                       Computation: 42898 steps/s (collection: 2.188s, learning 0.103s)
             Mean action noise std: 3.05
          Mean value_function loss: 348.4196
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 77.7027
                       Mean reward: 360.31
               Mean episode length: 146.24
    Episode_Reward/reaching_object: 1.2557
     Episode_Reward/lifting_object: 67.7688
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0993
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 138412032
                    Iteration time: 2.29s
                      Time elapsed: 00:55:04
                               ETA: 00:42:45

################################################################################
                     [1m Learning iteration 1408/2500 [0m                     

                       Computation: 41709 steps/s (collection: 2.234s, learning 0.123s)
             Mean action noise std: 3.05
          Mean value_function loss: 396.8372
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 77.7120
                       Mean reward: 335.20
               Mean episode length: 138.27
    Episode_Reward/reaching_object: 1.2315
     Episode_Reward/lifting_object: 66.3435
      Episode_Reward/object_height: 0.0043
        Episode_Reward/action_rate: -0.0508
          Episode_Reward/joint_vel: -0.0972
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 138510336
                    Iteration time: 2.36s
                      Time elapsed: 00:55:06
                               ETA: 00:42:42

################################################################################
                     [1m Learning iteration 1409/2500 [0m                     

                       Computation: 41032 steps/s (collection: 2.276s, learning 0.120s)
             Mean action noise std: 3.05
          Mean value_function loss: 346.8375
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 77.7155
                       Mean reward: 344.77
               Mean episode length: 140.71
    Episode_Reward/reaching_object: 1.2567
     Episode_Reward/lifting_object: 67.6066
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0516
          Episode_Reward/joint_vel: -0.0987
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 25.4167
--------------------------------------------------------------------------------
                   Total timesteps: 138608640
                    Iteration time: 2.40s
                      Time elapsed: 00:55:09
                               ETA: 00:42:40

################################################################################
                     [1m Learning iteration 1410/2500 [0m                     

                       Computation: 42886 steps/s (collection: 2.205s, learning 0.088s)
             Mean action noise std: 3.05
          Mean value_function loss: 374.7595
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 77.7166
                       Mean reward: 307.11
               Mean episode length: 126.72
    Episode_Reward/reaching_object: 1.2560
     Episode_Reward/lifting_object: 68.1572
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0993
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.3333
--------------------------------------------------------------------------------
                   Total timesteps: 138706944
                    Iteration time: 2.29s
                      Time elapsed: 00:55:11
                               ETA: 00:42:38

################################################################################
                     [1m Learning iteration 1411/2500 [0m                     

                       Computation: 41929 steps/s (collection: 2.241s, learning 0.104s)
             Mean action noise std: 3.05
          Mean value_function loss: 343.7975
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 77.7183
                       Mean reward: 377.83
               Mean episode length: 153.38
    Episode_Reward/reaching_object: 1.3158
     Episode_Reward/lifting_object: 71.0041
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0540
          Episode_Reward/joint_vel: -0.1038
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.0833
--------------------------------------------------------------------------------
                   Total timesteps: 138805248
                    Iteration time: 2.34s
                      Time elapsed: 00:55:13
                               ETA: 00:42:35

################################################################################
                     [1m Learning iteration 1412/2500 [0m                     

                       Computation: 42249 steps/s (collection: 2.217s, learning 0.109s)
             Mean action noise std: 3.05
          Mean value_function loss: 333.4584
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 77.7219
                       Mean reward: 352.75
               Mean episode length: 142.38
    Episode_Reward/reaching_object: 1.2908
     Episode_Reward/lifting_object: 69.9430
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.1030
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 25.6250
--------------------------------------------------------------------------------
                   Total timesteps: 138903552
                    Iteration time: 2.33s
                      Time elapsed: 00:55:16
                               ETA: 00:42:33

################################################################################
                     [1m Learning iteration 1413/2500 [0m                     

                       Computation: 42066 steps/s (collection: 2.219s, learning 0.118s)
             Mean action noise std: 3.05
          Mean value_function loss: 346.4428
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 77.7287
                       Mean reward: 360.37
               Mean episode length: 142.98
    Episode_Reward/reaching_object: 1.2612
     Episode_Reward/lifting_object: 68.7627
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0980
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.2917
--------------------------------------------------------------------------------
                   Total timesteps: 139001856
                    Iteration time: 2.34s
                      Time elapsed: 00:55:18
                               ETA: 00:42:31

################################################################################
                     [1m Learning iteration 1414/2500 [0m                     

                       Computation: 40609 steps/s (collection: 2.311s, learning 0.110s)
             Mean action noise std: 3.05
          Mean value_function loss: 368.1558
               Mean surrogate loss: 0.0030
                 Mean entropy loss: 77.7337
                       Mean reward: 370.69
               Mean episode length: 147.93
    Episode_Reward/reaching_object: 1.2913
     Episode_Reward/lifting_object: 69.8161
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.1021
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 27.6250
--------------------------------------------------------------------------------
                   Total timesteps: 139100160
                    Iteration time: 2.42s
                      Time elapsed: 00:55:20
                               ETA: 00:42:28

################################################################################
                     [1m Learning iteration 1415/2500 [0m                     

                       Computation: 41278 steps/s (collection: 2.269s, learning 0.112s)
             Mean action noise std: 3.06
          Mean value_function loss: 353.6044
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 77.7390
                       Mean reward: 354.09
               Mean episode length: 143.49
    Episode_Reward/reaching_object: 1.2934
     Episode_Reward/lifting_object: 70.5281
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.1011
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.1250
--------------------------------------------------------------------------------
                   Total timesteps: 139198464
                    Iteration time: 2.38s
                      Time elapsed: 00:55:23
                               ETA: 00:42:26

################################################################################
                     [1m Learning iteration 1416/2500 [0m                     

                       Computation: 42225 steps/s (collection: 2.227s, learning 0.101s)
             Mean action noise std: 3.06
          Mean value_function loss: 346.2878
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 77.7538
                       Mean reward: 356.28
               Mean episode length: 141.92
    Episode_Reward/reaching_object: 1.2867
     Episode_Reward/lifting_object: 70.6075
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.1005
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 25.9167
--------------------------------------------------------------------------------
                   Total timesteps: 139296768
                    Iteration time: 2.33s
                      Time elapsed: 00:55:25
                               ETA: 00:42:24

################################################################################
                     [1m Learning iteration 1417/2500 [0m                     

                       Computation: 40425 steps/s (collection: 2.320s, learning 0.112s)
             Mean action noise std: 3.06
          Mean value_function loss: 352.3827
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 77.7674
                       Mean reward: 352.41
               Mean episode length: 139.96
    Episode_Reward/reaching_object: 1.2841
     Episode_Reward/lifting_object: 70.1317
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.1013
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.4583
--------------------------------------------------------------------------------
                   Total timesteps: 139395072
                    Iteration time: 2.43s
                      Time elapsed: 00:55:28
                               ETA: 00:42:21

################################################################################
                     [1m Learning iteration 1418/2500 [0m                     

                       Computation: 42412 steps/s (collection: 2.228s, learning 0.090s)
             Mean action noise std: 3.06
          Mean value_function loss: 351.5919
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 77.7761
                       Mean reward: 378.46
               Mean episode length: 154.29
    Episode_Reward/reaching_object: 1.2848
     Episode_Reward/lifting_object: 69.7395
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.1022
      Episode_Termination/time_out: 2.4583
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 26.7500
--------------------------------------------------------------------------------
                   Total timesteps: 139493376
                    Iteration time: 2.32s
                      Time elapsed: 00:55:30
                               ETA: 00:42:19

################################################################################
                     [1m Learning iteration 1419/2500 [0m                     

                       Computation: 41718 steps/s (collection: 2.257s, learning 0.099s)
             Mean action noise std: 3.06
          Mean value_function loss: 360.8217
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 77.7849
                       Mean reward: 344.62
               Mean episode length: 139.32
    Episode_Reward/reaching_object: 1.2666
     Episode_Reward/lifting_object: 69.6514
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.1001
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.4167
--------------------------------------------------------------------------------
                   Total timesteps: 139591680
                    Iteration time: 2.36s
                      Time elapsed: 00:55:32
                               ETA: 00:42:17

################################################################################
                     [1m Learning iteration 1420/2500 [0m                     

                       Computation: 41113 steps/s (collection: 2.284s, learning 0.107s)
             Mean action noise std: 3.06
          Mean value_function loss: 360.4517
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 77.7959
                       Mean reward: 331.37
               Mean episode length: 134.61
    Episode_Reward/reaching_object: 1.2670
     Episode_Reward/lifting_object: 69.5883
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.0999
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 139689984
                    Iteration time: 2.39s
                      Time elapsed: 00:55:35
                               ETA: 00:42:14

################################################################################
                     [1m Learning iteration 1421/2500 [0m                     

                       Computation: 41786 steps/s (collection: 2.245s, learning 0.107s)
             Mean action noise std: 3.06
          Mean value_function loss: 370.5827
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 77.8054
                       Mean reward: 343.12
               Mean episode length: 141.17
    Episode_Reward/reaching_object: 1.2576
     Episode_Reward/lifting_object: 68.4710
      Episode_Reward/object_height: 0.0044
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0985
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 26.6250
--------------------------------------------------------------------------------
                   Total timesteps: 139788288
                    Iteration time: 2.35s
                      Time elapsed: 00:55:37
                               ETA: 00:42:12

################################################################################
                     [1m Learning iteration 1422/2500 [0m                     

                       Computation: 41844 steps/s (collection: 2.260s, learning 0.089s)
             Mean action noise std: 3.06
          Mean value_function loss: 339.4849
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 77.8124
                       Mean reward: 372.86
               Mean episode length: 146.18
    Episode_Reward/reaching_object: 1.3037
     Episode_Reward/lifting_object: 71.8192
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.1028
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 24.0833
--------------------------------------------------------------------------------
                   Total timesteps: 139886592
                    Iteration time: 2.35s
                      Time elapsed: 00:55:39
                               ETA: 00:42:10

################################################################################
                     [1m Learning iteration 1423/2500 [0m                     

                       Computation: 42321 steps/s (collection: 2.207s, learning 0.116s)
             Mean action noise std: 3.07
          Mean value_function loss: 354.8238
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 77.8237
                       Mean reward: 384.11
               Mean episode length: 154.19
    Episode_Reward/reaching_object: 1.3017
     Episode_Reward/lifting_object: 71.1912
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.1044
      Episode_Termination/time_out: 2.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 25.4167
--------------------------------------------------------------------------------
                   Total timesteps: 139984896
                    Iteration time: 2.32s
                      Time elapsed: 00:55:42
                               ETA: 00:42:07

################################################################################
                     [1m Learning iteration 1424/2500 [0m                     

                       Computation: 40538 steps/s (collection: 2.295s, learning 0.130s)
             Mean action noise std: 3.07
          Mean value_function loss: 349.3211
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 77.8411
                       Mean reward: 366.90
               Mean episode length: 147.98
    Episode_Reward/reaching_object: 1.2996
     Episode_Reward/lifting_object: 71.3870
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.1032
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 24.5000
--------------------------------------------------------------------------------
                   Total timesteps: 140083200
                    Iteration time: 2.42s
                      Time elapsed: 00:55:44
                               ETA: 00:42:05

################################################################################
                     [1m Learning iteration 1425/2500 [0m                     

                       Computation: 39856 steps/s (collection: 2.364s, learning 0.103s)
             Mean action noise std: 3.07
          Mean value_function loss: 381.8926
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 77.8630
                       Mean reward: 376.79
               Mean episode length: 149.20
    Episode_Reward/reaching_object: 1.3224
     Episode_Reward/lifting_object: 72.7206
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.1053
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.2083
--------------------------------------------------------------------------------
                   Total timesteps: 140181504
                    Iteration time: 2.47s
                      Time elapsed: 00:55:47
                               ETA: 00:42:03

################################################################################
                     [1m Learning iteration 1426/2500 [0m                     

                       Computation: 41494 steps/s (collection: 2.275s, learning 0.095s)
             Mean action noise std: 3.07
          Mean value_function loss: 367.5859
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 77.8814
                       Mean reward: 379.55
               Mean episode length: 149.47
    Episode_Reward/reaching_object: 1.2897
     Episode_Reward/lifting_object: 70.9118
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.1020
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.2083
--------------------------------------------------------------------------------
                   Total timesteps: 140279808
                    Iteration time: 2.37s
                      Time elapsed: 00:55:49
                               ETA: 00:42:00

################################################################################
                     [1m Learning iteration 1427/2500 [0m                     

                       Computation: 40530 steps/s (collection: 2.246s, learning 0.180s)
             Mean action noise std: 3.07
          Mean value_function loss: 353.3012
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 77.9003
                       Mean reward: 370.05
               Mean episode length: 147.82
    Episode_Reward/reaching_object: 1.3418
     Episode_Reward/lifting_object: 74.0651
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.1047
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.0417
--------------------------------------------------------------------------------
                   Total timesteps: 140378112
                    Iteration time: 2.43s
                      Time elapsed: 00:55:51
                               ETA: 00:41:58

################################################################################
                     [1m Learning iteration 1428/2500 [0m                     

                       Computation: 42497 steps/s (collection: 2.210s, learning 0.104s)
             Mean action noise std: 3.08
          Mean value_function loss: 356.7784
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 77.9197
                       Mean reward: 397.23
               Mean episode length: 156.00
    Episode_Reward/reaching_object: 1.3413
     Episode_Reward/lifting_object: 73.9888
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.1052
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 140476416
                    Iteration time: 2.31s
                      Time elapsed: 00:55:54
                               ETA: 00:41:56

################################################################################
                     [1m Learning iteration 1429/2500 [0m                     

                       Computation: 42004 steps/s (collection: 2.230s, learning 0.110s)
             Mean action noise std: 3.08
          Mean value_function loss: 370.0782
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 77.9270
                       Mean reward: 354.98
               Mean episode length: 142.42
    Episode_Reward/reaching_object: 1.2687
     Episode_Reward/lifting_object: 70.2854
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.1000
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.3750
--------------------------------------------------------------------------------
                   Total timesteps: 140574720
                    Iteration time: 2.34s
                      Time elapsed: 00:55:56
                               ETA: 00:41:53

################################################################################
                     [1m Learning iteration 1430/2500 [0m                     

                       Computation: 41389 steps/s (collection: 2.248s, learning 0.128s)
             Mean action noise std: 3.08
          Mean value_function loss: 357.5111
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 77.9337
                       Mean reward: 350.58
               Mean episode length: 137.89
    Episode_Reward/reaching_object: 1.2658
     Episode_Reward/lifting_object: 70.3945
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0989
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 27.3333
--------------------------------------------------------------------------------
                   Total timesteps: 140673024
                    Iteration time: 2.38s
                      Time elapsed: 00:55:58
                               ETA: 00:41:51

################################################################################
                     [1m Learning iteration 1431/2500 [0m                     

                       Computation: 42056 steps/s (collection: 2.221s, learning 0.117s)
             Mean action noise std: 3.08
          Mean value_function loss: 356.7603
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 77.9448
                       Mean reward: 358.10
               Mean episode length: 139.97
    Episode_Reward/reaching_object: 1.2768
     Episode_Reward/lifting_object: 70.5652
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0998
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 24.9167
--------------------------------------------------------------------------------
                   Total timesteps: 140771328
                    Iteration time: 2.34s
                      Time elapsed: 00:56:01
                               ETA: 00:41:49

################################################################################
                     [1m Learning iteration 1432/2500 [0m                     

                       Computation: 42170 steps/s (collection: 2.234s, learning 0.097s)
             Mean action noise std: 3.08
          Mean value_function loss: 356.0409
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 77.9615
                       Mean reward: 361.86
               Mean episode length: 143.42
    Episode_Reward/reaching_object: 1.2878
     Episode_Reward/lifting_object: 71.6821
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0995
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 25.8750
--------------------------------------------------------------------------------
                   Total timesteps: 140869632
                    Iteration time: 2.33s
                      Time elapsed: 00:56:03
                               ETA: 00:41:46

################################################################################
                     [1m Learning iteration 1433/2500 [0m                     

                       Computation: 42167 steps/s (collection: 2.190s, learning 0.142s)
             Mean action noise std: 3.08
          Mean value_function loss: 391.9944
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 77.9769
                       Mean reward: 348.75
               Mean episode length: 142.13
    Episode_Reward/reaching_object: 1.2745
     Episode_Reward/lifting_object: 70.8710
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0528
          Episode_Reward/joint_vel: -0.1001
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.5833
--------------------------------------------------------------------------------
                   Total timesteps: 140967936
                    Iteration time: 2.33s
                      Time elapsed: 00:56:05
                               ETA: 00:41:44

################################################################################
                     [1m Learning iteration 1434/2500 [0m                     

                       Computation: 41279 steps/s (collection: 2.269s, learning 0.113s)
             Mean action noise std: 3.08
          Mean value_function loss: 391.3973
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 77.9929
                       Mean reward: 383.00
               Mean episode length: 151.01
    Episode_Reward/reaching_object: 1.2897
     Episode_Reward/lifting_object: 71.1194
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0536
          Episode_Reward/joint_vel: -0.1012
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.8750
--------------------------------------------------------------------------------
                   Total timesteps: 141066240
                    Iteration time: 2.38s
                      Time elapsed: 00:56:08
                               ETA: 00:41:42

################################################################################
                     [1m Learning iteration 1435/2500 [0m                     

                       Computation: 41978 steps/s (collection: 2.228s, learning 0.114s)
             Mean action noise std: 3.09
          Mean value_function loss: 360.1533
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 78.0050
                       Mean reward: 352.30
               Mean episode length: 139.69
    Episode_Reward/reaching_object: 1.2537
     Episode_Reward/lifting_object: 70.1449
      Episode_Reward/object_height: 0.0045
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.0982
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 25.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141164544
                    Iteration time: 2.34s
                      Time elapsed: 00:56:10
                               ETA: 00:41:39

################################################################################
                     [1m Learning iteration 1436/2500 [0m                     

                       Computation: 41931 steps/s (collection: 2.257s, learning 0.087s)
             Mean action noise std: 3.09
          Mean value_function loss: 389.4667
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 78.0108
                       Mean reward: 360.47
               Mean episode length: 144.24
    Episode_Reward/reaching_object: 1.2938
     Episode_Reward/lifting_object: 71.6119
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.1012
      Episode_Termination/time_out: 2.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.5000
--------------------------------------------------------------------------------
                   Total timesteps: 141262848
                    Iteration time: 2.34s
                      Time elapsed: 00:56:12
                               ETA: 00:41:37

################################################################################
                     [1m Learning iteration 1437/2500 [0m                     

                       Computation: 42080 steps/s (collection: 2.226s, learning 0.110s)
             Mean action noise std: 3.09
          Mean value_function loss: 343.2204
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 78.0216
                       Mean reward: 357.05
               Mean episode length: 142.51
    Episode_Reward/reaching_object: 1.2946
     Episode_Reward/lifting_object: 72.2349
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0537
          Episode_Reward/joint_vel: -0.1000
      Episode_Termination/time_out: 1.9583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 24.8750
--------------------------------------------------------------------------------
                   Total timesteps: 141361152
                    Iteration time: 2.34s
                      Time elapsed: 00:56:15
                               ETA: 00:41:35

################################################################################
                     [1m Learning iteration 1438/2500 [0m                     

                       Computation: 42156 steps/s (collection: 2.226s, learning 0.106s)
             Mean action noise std: 3.09
          Mean value_function loss: 376.9293
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 78.0309
                       Mean reward: 351.21
               Mean episode length: 137.38
    Episode_Reward/reaching_object: 1.2752
     Episode_Reward/lifting_object: 71.5806
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.1005
      Episode_Termination/time_out: 1.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.1250
--------------------------------------------------------------------------------
                   Total timesteps: 141459456
                    Iteration time: 2.33s
                      Time elapsed: 00:56:17
                               ETA: 00:41:32

################################################################################
                     [1m Learning iteration 1439/2500 [0m                     

                       Computation: 42320 steps/s (collection: 2.214s, learning 0.109s)
             Mean action noise std: 3.09
          Mean value_function loss: 363.8330
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 78.0362
                       Mean reward: 344.55
               Mean episode length: 137.51
    Episode_Reward/reaching_object: 1.2956
     Episode_Reward/lifting_object: 72.5502
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.1018
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.5833
--------------------------------------------------------------------------------
                   Total timesteps: 141557760
                    Iteration time: 2.32s
                      Time elapsed: 00:56:19
                               ETA: 00:41:30

################################################################################
                     [1m Learning iteration 1440/2500 [0m                     

                       Computation: 41795 steps/s (collection: 2.243s, learning 0.109s)
             Mean action noise std: 3.09
          Mean value_function loss: 397.5201
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 78.0495
                       Mean reward: 375.95
               Mean episode length: 145.78
    Episode_Reward/reaching_object: 1.2820
     Episode_Reward/lifting_object: 72.0699
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.1013
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 27.0417
--------------------------------------------------------------------------------
                   Total timesteps: 141656064
                    Iteration time: 2.35s
                      Time elapsed: 00:56:22
                               ETA: 00:41:27

################################################################################
                     [1m Learning iteration 1441/2500 [0m                     

                       Computation: 42197 steps/s (collection: 2.244s, learning 0.086s)
             Mean action noise std: 3.09
          Mean value_function loss: 386.9169
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 78.0619
                       Mean reward: 330.74
               Mean episode length: 132.24
    Episode_Reward/reaching_object: 1.2703
     Episode_Reward/lifting_object: 70.6984
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0999
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.8333
--------------------------------------------------------------------------------
                   Total timesteps: 141754368
                    Iteration time: 2.33s
                      Time elapsed: 00:56:24
                               ETA: 00:41:25

################################################################################
                     [1m Learning iteration 1442/2500 [0m                     

                       Computation: 41507 steps/s (collection: 2.227s, learning 0.142s)
             Mean action noise std: 3.09
          Mean value_function loss: 347.7290
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 78.0768
                       Mean reward: 409.62
               Mean episode length: 157.78
    Episode_Reward/reaching_object: 1.3263
     Episode_Reward/lifting_object: 74.4766
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0552
          Episode_Reward/joint_vel: -0.1035
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.8333
--------------------------------------------------------------------------------
                   Total timesteps: 141852672
                    Iteration time: 2.37s
                      Time elapsed: 00:56:26
                               ETA: 00:41:23

################################################################################
                     [1m Learning iteration 1443/2500 [0m                     

                       Computation: 42078 steps/s (collection: 2.241s, learning 0.095s)
             Mean action noise std: 3.10
          Mean value_function loss: 369.6357
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 78.1019
                       Mean reward: 399.86
               Mean episode length: 155.13
    Episode_Reward/reaching_object: 1.2788
     Episode_Reward/lifting_object: 71.9662
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0991
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.3333
     Episode_Termination/robot_out: 27.2083
--------------------------------------------------------------------------------
                   Total timesteps: 141950976
                    Iteration time: 2.34s
                      Time elapsed: 00:56:29
                               ETA: 00:41:20

################################################################################
                     [1m Learning iteration 1444/2500 [0m                     

                       Computation: 42613 steps/s (collection: 2.219s, learning 0.088s)
             Mean action noise std: 3.10
          Mean value_function loss: 347.6838
               Mean surrogate loss: 0.0022
                 Mean entropy loss: 78.1144
                       Mean reward: 373.62
               Mean episode length: 142.97
    Episode_Reward/reaching_object: 1.2784
     Episode_Reward/lifting_object: 71.9451
      Episode_Reward/object_height: 0.0046
        Episode_Reward/action_rate: -0.0533
          Episode_Reward/joint_vel: -0.0992
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 25.0417
--------------------------------------------------------------------------------
                   Total timesteps: 142049280
                    Iteration time: 2.31s
                      Time elapsed: 00:56:31
                               ETA: 00:41:18

################################################################################
                     [1m Learning iteration 1445/2500 [0m                     

                       Computation: 42143 steps/s (collection: 2.195s, learning 0.137s)
             Mean action noise std: 3.10
          Mean value_function loss: 366.8031
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 78.1167
                       Mean reward: 370.18
               Mean episode length: 144.60
    Episode_Reward/reaching_object: 1.2868
     Episode_Reward/lifting_object: 71.9590
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0538
          Episode_Reward/joint_vel: -0.1000
      Episode_Termination/time_out: 1.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.4583
--------------------------------------------------------------------------------
                   Total timesteps: 142147584
                    Iteration time: 2.33s
                      Time elapsed: 00:56:33
                               ETA: 00:41:16

################################################################################
                     [1m Learning iteration 1446/2500 [0m                     

                       Computation: 41905 steps/s (collection: 2.245s, learning 0.101s)
             Mean action noise std: 3.10
          Mean value_function loss: 364.3338
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 78.1225
                       Mean reward: 372.57
               Mean episode length: 143.99
    Episode_Reward/reaching_object: 1.2854
     Episode_Reward/lifting_object: 73.2425
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0539
          Episode_Reward/joint_vel: -0.1003
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.5417
--------------------------------------------------------------------------------
                   Total timesteps: 142245888
                    Iteration time: 2.35s
                      Time elapsed: 00:56:36
                               ETA: 00:41:13

################################################################################
                     [1m Learning iteration 1447/2500 [0m                     

                       Computation: 43189 steps/s (collection: 2.186s, learning 0.090s)
             Mean action noise std: 3.10
          Mean value_function loss: 393.1777
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 78.1344
                       Mean reward: 370.58
               Mean episode length: 143.66
    Episode_Reward/reaching_object: 1.3174
     Episode_Reward/lifting_object: 74.8932
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0550
          Episode_Reward/joint_vel: -0.1011
      Episode_Termination/time_out: 2.2917
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.4583
--------------------------------------------------------------------------------
                   Total timesteps: 142344192
                    Iteration time: 2.28s
                      Time elapsed: 00:56:38
                               ETA: 00:41:11

################################################################################
                     [1m Learning iteration 1448/2500 [0m                     

                       Computation: 42478 steps/s (collection: 2.215s, learning 0.099s)
             Mean action noise std: 3.10
          Mean value_function loss: 387.2496
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 78.1462
                       Mean reward: 384.80
               Mean episode length: 147.99
    Episode_Reward/reaching_object: 1.2679
     Episode_Reward/lifting_object: 72.0616
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0973
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.8750
--------------------------------------------------------------------------------
                   Total timesteps: 142442496
                    Iteration time: 2.31s
                      Time elapsed: 00:56:40
                               ETA: 00:41:09

################################################################################
                     [1m Learning iteration 1449/2500 [0m                     

                       Computation: 42367 steps/s (collection: 2.202s, learning 0.118s)
             Mean action noise std: 3.10
          Mean value_function loss: 376.9516
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 78.1552
                       Mean reward: 369.93
               Mean episode length: 143.97
    Episode_Reward/reaching_object: 1.2982
     Episode_Reward/lifting_object: 74.3900
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0997
      Episode_Termination/time_out: 2.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.5833
--------------------------------------------------------------------------------
                   Total timesteps: 142540800
                    Iteration time: 2.32s
                      Time elapsed: 00:56:43
                               ETA: 00:41:06

################################################################################
                     [1m Learning iteration 1450/2500 [0m                     

                       Computation: 41319 steps/s (collection: 2.250s, learning 0.129s)
             Mean action noise std: 3.10
          Mean value_function loss: 357.0719
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 78.1653
                       Mean reward: 368.40
               Mean episode length: 141.27
    Episode_Reward/reaching_object: 1.2567
     Episode_Reward/lifting_object: 72.2684
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0959
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.6667
--------------------------------------------------------------------------------
                   Total timesteps: 142639104
                    Iteration time: 2.38s
                      Time elapsed: 00:56:45
                               ETA: 00:41:04

################################################################################
                     [1m Learning iteration 1451/2500 [0m                     

                       Computation: 41322 steps/s (collection: 2.280s, learning 0.099s)
             Mean action noise std: 3.11
          Mean value_function loss: 401.6665
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 78.1751
                       Mean reward: 362.15
               Mean episode length: 140.81
    Episode_Reward/reaching_object: 1.2953
     Episode_Reward/lifting_object: 74.3191
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0994
      Episode_Termination/time_out: 2.1667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 27.2917
--------------------------------------------------------------------------------
                   Total timesteps: 142737408
                    Iteration time: 2.38s
                      Time elapsed: 00:56:47
                               ETA: 00:41:02

################################################################################
                     [1m Learning iteration 1452/2500 [0m                     

                       Computation: 42263 steps/s (collection: 2.218s, learning 0.108s)
             Mean action noise std: 3.11
          Mean value_function loss: 395.5355
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 78.1869
                       Mean reward: 357.41
               Mean episode length: 135.74
    Episode_Reward/reaching_object: 1.2549
     Episode_Reward/lifting_object: 72.1335
      Episode_Reward/object_height: 0.0047
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0963
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 27.5833
--------------------------------------------------------------------------------
                   Total timesteps: 142835712
                    Iteration time: 2.33s
                      Time elapsed: 00:56:50
                               ETA: 00:40:59

################################################################################
                     [1m Learning iteration 1453/2500 [0m                     

                       Computation: 41231 steps/s (collection: 2.241s, learning 0.144s)
             Mean action noise std: 3.11
          Mean value_function loss: 385.2884
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 78.2025
                       Mean reward: 370.34
               Mean episode length: 141.71
    Episode_Reward/reaching_object: 1.2909
     Episode_Reward/lifting_object: 74.9007
      Episode_Reward/object_height: 0.0049
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0971
      Episode_Termination/time_out: 2.2500
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 26.6250
--------------------------------------------------------------------------------
                   Total timesteps: 142934016
                    Iteration time: 2.38s
                      Time elapsed: 00:56:52
                               ETA: 00:40:57

################################################################################
                     [1m Learning iteration 1454/2500 [0m                     

                       Computation: 43217 steps/s (collection: 2.181s, learning 0.094s)
             Mean action noise std: 3.11
          Mean value_function loss: 373.6509
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 78.2137
                       Mean reward: 374.49
               Mean episode length: 142.83
    Episode_Reward/reaching_object: 1.2642
     Episode_Reward/lifting_object: 73.3903
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0958
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.5833
--------------------------------------------------------------------------------
                   Total timesteps: 143032320
                    Iteration time: 2.27s
                      Time elapsed: 00:56:54
                               ETA: 00:40:54

################################################################################
                     [1m Learning iteration 1455/2500 [0m                     

                       Computation: 42379 steps/s (collection: 2.216s, learning 0.104s)
             Mean action noise std: 3.11
          Mean value_function loss: 383.5701
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 78.2208
                       Mean reward: 359.55
               Mean episode length: 137.09
    Episode_Reward/reaching_object: 1.2547
     Episode_Reward/lifting_object: 73.2121
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 26.1250
--------------------------------------------------------------------------------
                   Total timesteps: 143130624
                    Iteration time: 2.32s
                      Time elapsed: 00:56:57
                               ETA: 00:40:52

################################################################################
                     [1m Learning iteration 1456/2500 [0m                     

                       Computation: 41848 steps/s (collection: 2.255s, learning 0.094s)
             Mean action noise std: 3.11
          Mean value_function loss: 389.2402
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 78.2299
                       Mean reward: 359.06
               Mean episode length: 138.27
    Episode_Reward/reaching_object: 1.2603
     Episode_Reward/lifting_object: 73.6120
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0530
          Episode_Reward/joint_vel: -0.0935
      Episode_Termination/time_out: 1.6667
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.8333
--------------------------------------------------------------------------------
                   Total timesteps: 143228928
                    Iteration time: 2.35s
                      Time elapsed: 00:56:59
                               ETA: 00:40:50

################################################################################
                     [1m Learning iteration 1457/2500 [0m                     

                       Computation: 41217 steps/s (collection: 2.283s, learning 0.102s)
             Mean action noise std: 3.11
          Mean value_function loss: 378.9333
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 78.2402
                       Mean reward: 357.52
               Mean episode length: 136.06
    Episode_Reward/reaching_object: 1.2534
     Episode_Reward/lifting_object: 73.6577
      Episode_Reward/object_height: 0.0048
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 1.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.2083
--------------------------------------------------------------------------------
                   Total timesteps: 143327232
                    Iteration time: 2.39s
                      Time elapsed: 00:57:01
                               ETA: 00:40:47

################################################################################
                     [1m Learning iteration 1458/2500 [0m                     

                       Computation: 41599 steps/s (collection: 2.237s, learning 0.126s)
             Mean action noise std: 3.12
          Mean value_function loss: 395.7201
               Mean surrogate loss: -0.0045
                 Mean entropy loss: 78.2575
                       Mean reward: 416.65
               Mean episode length: 154.12
    Episode_Reward/reaching_object: 1.2915
     Episode_Reward/lifting_object: 76.3200
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0543
          Episode_Reward/joint_vel: -0.0960
      Episode_Termination/time_out: 2.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.9583
--------------------------------------------------------------------------------
                   Total timesteps: 143425536
                    Iteration time: 2.36s
                      Time elapsed: 00:57:04
                               ETA: 00:40:45

################################################################################
                     [1m Learning iteration 1459/2500 [0m                     

                       Computation: 43079 steps/s (collection: 2.191s, learning 0.091s)
             Mean action noise std: 3.12
          Mean value_function loss: 380.7592
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 78.2775
                       Mean reward: 390.48
               Mean episode length: 149.13
    Episode_Reward/reaching_object: 1.2687
     Episode_Reward/lifting_object: 74.9832
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0941
      Episode_Termination/time_out: 2.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.0833
--------------------------------------------------------------------------------
                   Total timesteps: 143523840
                    Iteration time: 2.28s
                      Time elapsed: 00:57:06
                               ETA: 00:40:43

################################################################################
                     [1m Learning iteration 1460/2500 [0m                     

                       Computation: 42060 steps/s (collection: 2.242s, learning 0.096s)
             Mean action noise std: 3.12
          Mean value_function loss: 393.3905
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 78.2919
                       Mean reward: 392.68
               Mean episode length: 147.75
    Episode_Reward/reaching_object: 1.2637
     Episode_Reward/lifting_object: 74.8912
      Episode_Reward/object_height: 0.0050
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0934
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.0833
--------------------------------------------------------------------------------
                   Total timesteps: 143622144
                    Iteration time: 2.34s
                      Time elapsed: 00:57:08
                               ETA: 00:40:40

################################################################################
                     [1m Learning iteration 1461/2500 [0m                     

                       Computation: 41788 steps/s (collection: 2.212s, learning 0.140s)
             Mean action noise std: 3.12
          Mean value_function loss: 381.8390
               Mean surrogate loss: -0.0050
                 Mean entropy loss: 78.3062
                       Mean reward: 412.49
               Mean episode length: 153.92
    Episode_Reward/reaching_object: 1.2833
     Episode_Reward/lifting_object: 76.7189
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0544
          Episode_Reward/joint_vel: -0.0944
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 24.6250
--------------------------------------------------------------------------------
                   Total timesteps: 143720448
                    Iteration time: 2.35s
                      Time elapsed: 00:57:11
                               ETA: 00:40:38

################################################################################
                     [1m Learning iteration 1462/2500 [0m                     

                       Computation: 41550 steps/s (collection: 2.273s, learning 0.093s)
             Mean action noise std: 3.12
          Mean value_function loss: 403.1912
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 78.3283
                       Mean reward: 390.01
               Mean episode length: 143.78
    Episode_Reward/reaching_object: 1.2767
     Episode_Reward/lifting_object: 76.5030
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0542
          Episode_Reward/joint_vel: -0.0940
      Episode_Termination/time_out: 2.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 143818752
                    Iteration time: 2.37s
                      Time elapsed: 00:57:13
                               ETA: 00:40:36

################################################################################
                     [1m Learning iteration 1463/2500 [0m                     

                       Computation: 43014 steps/s (collection: 2.190s, learning 0.095s)
             Mean action noise std: 3.12
          Mean value_function loss: 388.6578
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 78.3381
                       Mean reward: 374.74
               Mean episode length: 142.02
    Episode_Reward/reaching_object: 1.3005
     Episode_Reward/lifting_object: 78.0827
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0551
          Episode_Reward/joint_vel: -0.0961
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 25.6250
--------------------------------------------------------------------------------
                   Total timesteps: 143917056
                    Iteration time: 2.29s
                      Time elapsed: 00:57:15
                               ETA: 00:40:33

################################################################################
                     [1m Learning iteration 1464/2500 [0m                     

                       Computation: 42381 steps/s (collection: 2.211s, learning 0.108s)
             Mean action noise std: 3.13
          Mean value_function loss: 381.7966
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 78.3455
                       Mean reward: 394.02
               Mean episode length: 146.96
    Episode_Reward/reaching_object: 1.2550
     Episode_Reward/lifting_object: 75.4094
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0535
          Episode_Reward/joint_vel: -0.0919
      Episode_Termination/time_out: 1.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.7083
--------------------------------------------------------------------------------
                   Total timesteps: 144015360
                    Iteration time: 2.32s
                      Time elapsed: 00:57:18
                               ETA: 00:40:31

################################################################################
                     [1m Learning iteration 1465/2500 [0m                     

                       Computation: 41491 steps/s (collection: 2.243s, learning 0.127s)
             Mean action noise std: 3.13
          Mean value_function loss: 388.2542
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 78.3565
                       Mean reward: 392.41
               Mean episode length: 147.99
    Episode_Reward/reaching_object: 1.3068
     Episode_Reward/lifting_object: 78.4844
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0556
          Episode_Reward/joint_vel: -0.0961
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.2083
--------------------------------------------------------------------------------
                   Total timesteps: 144113664
                    Iteration time: 2.37s
                      Time elapsed: 00:57:20
                               ETA: 00:40:29

################################################################################
                     [1m Learning iteration 1466/2500 [0m                     

                       Computation: 41516 steps/s (collection: 2.258s, learning 0.110s)
             Mean action noise std: 3.13
          Mean value_function loss: 383.3818
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 78.3730
                       Mean reward: 437.13
               Mean episode length: 159.59
    Episode_Reward/reaching_object: 1.3054
     Episode_Reward/lifting_object: 78.9578
      Episode_Reward/object_height: 0.0054
        Episode_Reward/action_rate: -0.0554
          Episode_Reward/joint_vel: -0.0943
      Episode_Termination/time_out: 2.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 25.8333
--------------------------------------------------------------------------------
                   Total timesteps: 144211968
                    Iteration time: 2.37s
                      Time elapsed: 00:57:23
                               ETA: 00:40:26

################################################################################
                     [1m Learning iteration 1467/2500 [0m                     

                       Computation: 41691 steps/s (collection: 2.236s, learning 0.122s)
             Mean action noise std: 3.13
          Mean value_function loss: 413.3701
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 78.3817
                       Mean reward: 405.51
               Mean episode length: 150.31
    Episode_Reward/reaching_object: 1.2677
     Episode_Reward/lifting_object: 76.6492
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0923
      Episode_Termination/time_out: 2.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 27.4167
--------------------------------------------------------------------------------
                   Total timesteps: 144310272
                    Iteration time: 2.36s
                      Time elapsed: 00:57:25
                               ETA: 00:40:24

################################################################################
                     [1m Learning iteration 1468/2500 [0m                     

                       Computation: 42063 steps/s (collection: 2.243s, learning 0.094s)
             Mean action noise std: 3.13
          Mean value_function loss: 400.8935
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 78.3889
                       Mean reward: 407.19
               Mean episode length: 151.16
    Episode_Reward/reaching_object: 1.2481
     Episode_Reward/lifting_object: 76.1334
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0897
      Episode_Termination/time_out: 1.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 27.2083
--------------------------------------------------------------------------------
                   Total timesteps: 144408576
                    Iteration time: 2.34s
                      Time elapsed: 00:57:27
                               ETA: 00:40:22

################################################################################
                     [1m Learning iteration 1469/2500 [0m                     

                       Computation: 41788 steps/s (collection: 2.237s, learning 0.115s)
             Mean action noise std: 3.13
          Mean value_function loss: 409.4771
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 78.4025
                       Mean reward: 412.84
               Mean episode length: 152.19
    Episode_Reward/reaching_object: 1.2523
     Episode_Reward/lifting_object: 76.6595
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 2.2083
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 27.5833
--------------------------------------------------------------------------------
                   Total timesteps: 144506880
                    Iteration time: 2.35s
                      Time elapsed: 00:57:30
                               ETA: 00:40:19

################################################################################
                     [1m Learning iteration 1470/2500 [0m                     

                       Computation: 42446 steps/s (collection: 2.198s, learning 0.118s)
             Mean action noise std: 3.14
          Mean value_function loss: 438.6732
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 78.4273
                       Mean reward: 380.03
               Mean episode length: 139.60
    Episode_Reward/reaching_object: 1.2572
     Episode_Reward/lifting_object: 77.4953
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0895
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.1667
     Episode_Termination/robot_out: 29.3333
--------------------------------------------------------------------------------
                   Total timesteps: 144605184
                    Iteration time: 2.32s
                      Time elapsed: 00:57:32
                               ETA: 00:40:17

################################################################################
                     [1m Learning iteration 1471/2500 [0m                     

                       Computation: 43188 steps/s (collection: 2.188s, learning 0.089s)
             Mean action noise std: 3.14
          Mean value_function loss: 425.1955
               Mean surrogate loss: 0.0038
                 Mean entropy loss: 78.4446
                       Mean reward: 371.34
               Mean episode length: 136.18
    Episode_Reward/reaching_object: 1.2297
     Episode_Reward/lifting_object: 75.4618
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0885
      Episode_Termination/time_out: 1.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.2083
--------------------------------------------------------------------------------
                   Total timesteps: 144703488
                    Iteration time: 2.28s
                      Time elapsed: 00:57:34
                               ETA: 00:40:14

################################################################################
                     [1m Learning iteration 1472/2500 [0m                     

                       Computation: 42251 steps/s (collection: 2.223s, learning 0.103s)
             Mean action noise std: 3.14
          Mean value_function loss: 414.8371
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 78.4517
                       Mean reward: 393.27
               Mean episode length: 143.72
    Episode_Reward/reaching_object: 1.2425
     Episode_Reward/lifting_object: 77.0879
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0873
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 26.4583
--------------------------------------------------------------------------------
                   Total timesteps: 144801792
                    Iteration time: 2.33s
                      Time elapsed: 00:57:36
                               ETA: 00:40:12

################################################################################
                     [1m Learning iteration 1473/2500 [0m                     

                       Computation: 40364 steps/s (collection: 2.283s, learning 0.153s)
             Mean action noise std: 3.14
          Mean value_function loss: 417.6169
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 78.4614
                       Mean reward: 384.30
               Mean episode length: 138.47
    Episode_Reward/reaching_object: 1.2041
     Episode_Reward/lifting_object: 74.7498
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0842
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.7917
--------------------------------------------------------------------------------
                   Total timesteps: 144900096
                    Iteration time: 2.44s
                      Time elapsed: 00:57:39
                               ETA: 00:40:10

################################################################################
                     [1m Learning iteration 1474/2500 [0m                     

                       Computation: 41437 steps/s (collection: 2.261s, learning 0.112s)
             Mean action noise std: 3.14
          Mean value_function loss: 443.9325
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 78.4720
                       Mean reward: 380.99
               Mean episode length: 137.54
    Episode_Reward/reaching_object: 1.2183
     Episode_Reward/lifting_object: 76.1162
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0517
          Episode_Reward/joint_vel: -0.0841
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 28.5000
--------------------------------------------------------------------------------
                   Total timesteps: 144998400
                    Iteration time: 2.37s
                      Time elapsed: 00:57:41
                               ETA: 00:40:08

################################################################################
                     [1m Learning iteration 1475/2500 [0m                     

                       Computation: 42472 steps/s (collection: 2.207s, learning 0.108s)
             Mean action noise std: 3.14
          Mean value_function loss: 433.0245
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 78.4838
                       Mean reward: 388.79
               Mean episode length: 141.27
    Episode_Reward/reaching_object: 1.2234
     Episode_Reward/lifting_object: 76.2140
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0518
          Episode_Reward/joint_vel: -0.0834
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 29.5000
--------------------------------------------------------------------------------
                   Total timesteps: 145096704
                    Iteration time: 2.31s
                      Time elapsed: 00:57:44
                               ETA: 00:40:05

################################################################################
                     [1m Learning iteration 1476/2500 [0m                     

                       Computation: 42311 steps/s (collection: 2.202s, learning 0.121s)
             Mean action noise std: 3.14
          Mean value_function loss: 440.3959
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 78.4903
                       Mean reward: 342.75
               Mean episode length: 127.02
    Episode_Reward/reaching_object: 1.1839
     Episode_Reward/lifting_object: 73.8730
      Episode_Reward/object_height: 0.0052
        Episode_Reward/action_rate: -0.0502
          Episode_Reward/joint_vel: -0.0807
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.2500
--------------------------------------------------------------------------------
                   Total timesteps: 145195008
                    Iteration time: 2.32s
                      Time elapsed: 00:57:46
                               ETA: 00:40:03

################################################################################
                     [1m Learning iteration 1477/2500 [0m                     

                       Computation: 42794 steps/s (collection: 2.207s, learning 0.090s)
             Mean action noise std: 3.14
          Mean value_function loss: 469.6048
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 78.4999
                       Mean reward: 341.65
               Mean episode length: 124.96
    Episode_Reward/reaching_object: 1.1499
     Episode_Reward/lifting_object: 72.7356
      Episode_Reward/object_height: 0.0051
        Episode_Reward/action_rate: -0.0488
          Episode_Reward/joint_vel: -0.0767
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 29.1667
--------------------------------------------------------------------------------
                   Total timesteps: 145293312
                    Iteration time: 2.30s
                      Time elapsed: 00:57:48
                               ETA: 00:40:00

################################################################################
                     [1m Learning iteration 1478/2500 [0m                     

                       Computation: 42905 steps/s (collection: 2.204s, learning 0.088s)
             Mean action noise std: 3.14
          Mean value_function loss: 458.5414
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 78.5103
                       Mean reward: 387.61
               Mean episode length: 138.90
    Episode_Reward/reaching_object: 1.1798
     Episode_Reward/lifting_object: 74.2546
      Episode_Reward/object_height: 0.0053
        Episode_Reward/action_rate: -0.0504
          Episode_Reward/joint_vel: -0.0812
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.6250
--------------------------------------------------------------------------------
                   Total timesteps: 145391616
                    Iteration time: 2.29s
                      Time elapsed: 00:57:51
                               ETA: 00:39:58

################################################################################
                     [1m Learning iteration 1479/2500 [0m                     

                       Computation: 41811 steps/s (collection: 2.230s, learning 0.122s)
             Mean action noise std: 3.15
          Mean value_function loss: 463.3879
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 78.5203
                       Mean reward: 399.34
               Mean episode length: 141.53
    Episode_Reward/reaching_object: 1.2227
     Episode_Reward/lifting_object: 77.3259
      Episode_Reward/object_height: 0.0056
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0815
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.0417
--------------------------------------------------------------------------------
                   Total timesteps: 145489920
                    Iteration time: 2.35s
                      Time elapsed: 00:57:53
                               ETA: 00:39:56

################################################################################
                     [1m Learning iteration 1480/2500 [0m                     

                       Computation: 42255 steps/s (collection: 2.217s, learning 0.109s)
             Mean action noise std: 3.15
          Mean value_function loss: 454.8353
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 78.5351
                       Mean reward: 397.65
               Mean episode length: 140.80
    Episode_Reward/reaching_object: 1.1784
     Episode_Reward/lifting_object: 74.8055
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0503
          Episode_Reward/joint_vel: -0.0785
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.2917
--------------------------------------------------------------------------------
                   Total timesteps: 145588224
                    Iteration time: 2.33s
                      Time elapsed: 00:57:55
                               ETA: 00:39:53

################################################################################
                     [1m Learning iteration 1481/2500 [0m                     

                       Computation: 42162 steps/s (collection: 2.219s, learning 0.112s)
             Mean action noise std: 3.15
          Mean value_function loss: 444.1937
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 78.5502
                       Mean reward: 381.82
               Mean episode length: 136.04
    Episode_Reward/reaching_object: 1.1658
     Episode_Reward/lifting_object: 75.0175
      Episode_Reward/object_height: 0.0055
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0768
      Episode_Termination/time_out: 0.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.5417
--------------------------------------------------------------------------------
                   Total timesteps: 145686528
                    Iteration time: 2.33s
                      Time elapsed: 00:57:58
                               ETA: 00:39:51

################################################################################
                     [1m Learning iteration 1482/2500 [0m                     

                       Computation: 42745 steps/s (collection: 2.203s, learning 0.097s)
             Mean action noise std: 3.15
          Mean value_function loss: 435.3907
               Mean surrogate loss: -0.0036
                 Mean entropy loss: 78.5586
                       Mean reward: 395.53
               Mean episode length: 140.52
    Episode_Reward/reaching_object: 1.2035
     Episode_Reward/lifting_object: 77.5510
      Episode_Reward/object_height: 0.0058
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0795
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.0417
--------------------------------------------------------------------------------
                   Total timesteps: 145784832
                    Iteration time: 2.30s
                      Time elapsed: 00:58:00
                               ETA: 00:39:49

################################################################################
                     [1m Learning iteration 1483/2500 [0m                     

                       Computation: 41810 steps/s (collection: 2.245s, learning 0.107s)
             Mean action noise std: 3.15
          Mean value_function loss: 407.4385
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 78.5717
                       Mean reward: 429.00
               Mean episode length: 149.31
    Episode_Reward/reaching_object: 1.2149
     Episode_Reward/lifting_object: 78.6855
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0797
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 24.9167
--------------------------------------------------------------------------------
                   Total timesteps: 145883136
                    Iteration time: 2.35s
                      Time elapsed: 00:58:02
                               ETA: 00:39:46

################################################################################
                     [1m Learning iteration 1484/2500 [0m                     

                       Computation: 42845 steps/s (collection: 2.187s, learning 0.108s)
             Mean action noise std: 3.15
          Mean value_function loss: 477.2909
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 78.5869
                       Mean reward: 390.00
               Mean episode length: 137.49
    Episode_Reward/reaching_object: 1.1990
     Episode_Reward/lifting_object: 77.7623
      Episode_Reward/object_height: 0.0059
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0784
      Episode_Termination/time_out: 1.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 27.4583
--------------------------------------------------------------------------------
                   Total timesteps: 145981440
                    Iteration time: 2.29s
                      Time elapsed: 00:58:04
                               ETA: 00:39:44

################################################################################
                     [1m Learning iteration 1485/2500 [0m                     

                       Computation: 42930 steps/s (collection: 2.202s, learning 0.088s)
             Mean action noise std: 3.15
          Mean value_function loss: 421.4845
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 78.5961
                       Mean reward: 389.90
               Mean episode length: 137.68
    Episode_Reward/reaching_object: 1.2232
     Episode_Reward/lifting_object: 80.2237
      Episode_Reward/object_height: 0.0062
        Episode_Reward/action_rate: -0.0524
          Episode_Reward/joint_vel: -0.0789
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.2500
     Episode_Termination/robot_out: 26.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146079744
                    Iteration time: 2.29s
                      Time elapsed: 00:58:07
                               ETA: 00:39:41

################################################################################
                     [1m Learning iteration 1486/2500 [0m                     

                       Computation: 42390 steps/s (collection: 2.187s, learning 0.132s)
             Mean action noise std: 3.16
          Mean value_function loss: 457.7882
               Mean surrogate loss: -0.0041
                 Mean entropy loss: 78.6064
                       Mean reward: 417.01
               Mean episode length: 144.26
    Episode_Reward/reaching_object: 1.2423
     Episode_Reward/lifting_object: 82.6072
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0527
          Episode_Reward/joint_vel: -0.0783
      Episode_Termination/time_out: 1.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 29.7083
--------------------------------------------------------------------------------
                   Total timesteps: 146178048
                    Iteration time: 2.32s
                      Time elapsed: 00:58:09
                               ETA: 00:39:39

################################################################################
                     [1m Learning iteration 1487/2500 [0m                     

                       Computation: 42804 steps/s (collection: 2.182s, learning 0.114s)
             Mean action noise std: 3.16
          Mean value_function loss: 443.5884
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 78.6231
                       Mean reward: 435.80
               Mean episode length: 149.02
    Episode_Reward/reaching_object: 1.2416
     Episode_Reward/lifting_object: 82.1407
      Episode_Reward/object_height: 0.0064
        Episode_Reward/action_rate: -0.0529
          Episode_Reward/joint_vel: -0.0780
      Episode_Termination/time_out: 0.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.0417
--------------------------------------------------------------------------------
                   Total timesteps: 146276352
                    Iteration time: 2.30s
                      Time elapsed: 00:58:11
                               ETA: 00:39:37

################################################################################
                     [1m Learning iteration 1488/2500 [0m                     

                       Computation: 42698 steps/s (collection: 2.204s, learning 0.099s)
             Mean action noise std: 3.16
          Mean value_function loss: 443.6835
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 78.6402
                       Mean reward: 416.00
               Mean episode length: 143.69
    Episode_Reward/reaching_object: 1.2649
     Episode_Reward/lifting_object: 83.8404
      Episode_Reward/object_height: 0.0066
        Episode_Reward/action_rate: -0.0541
          Episode_Reward/joint_vel: -0.0799
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 28.9167
--------------------------------------------------------------------------------
                   Total timesteps: 146374656
                    Iteration time: 2.30s
                      Time elapsed: 00:58:14
                               ETA: 00:39:34

################################################################################
                     [1m Learning iteration 1489/2500 [0m                     

                       Computation: 41567 steps/s (collection: 2.235s, learning 0.130s)
             Mean action noise std: 3.16
          Mean value_function loss: 444.9781
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 78.6456
                       Mean reward: 404.93
               Mean episode length: 139.58
    Episode_Reward/reaching_object: 1.1994
     Episode_Reward/lifting_object: 80.1779
      Episode_Reward/object_height: 0.0065
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0748
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.4583
--------------------------------------------------------------------------------
                   Total timesteps: 146472960
                    Iteration time: 2.36s
                      Time elapsed: 00:58:16
                               ETA: 00:39:32

################################################################################
                     [1m Learning iteration 1490/2500 [0m                     

                       Computation: 43101 steps/s (collection: 2.185s, learning 0.096s)
             Mean action noise std: 3.16
          Mean value_function loss: 474.6930
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 78.6556
                       Mean reward: 417.30
               Mean episode length: 144.61
    Episode_Reward/reaching_object: 1.2178
     Episode_Reward/lifting_object: 82.0565
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0735
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.2083
     Episode_Termination/robot_out: 28.1667
--------------------------------------------------------------------------------
                   Total timesteps: 146571264
                    Iteration time: 2.28s
                      Time elapsed: 00:58:18
                               ETA: 00:39:30

################################################################################
                     [1m Learning iteration 1491/2500 [0m                     

                       Computation: 42802 steps/s (collection: 2.200s, learning 0.097s)
             Mean action noise std: 3.16
          Mean value_function loss: 472.6488
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 78.6705
                       Mean reward: 413.43
               Mean episode length: 142.96
    Episode_Reward/reaching_object: 1.2008
     Episode_Reward/lifting_object: 80.9421
      Episode_Reward/object_height: 0.0067
        Episode_Reward/action_rate: -0.0515
          Episode_Reward/joint_vel: -0.0726
      Episode_Termination/time_out: 1.4583
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 28.2083
--------------------------------------------------------------------------------
                   Total timesteps: 146669568
                    Iteration time: 2.30s
                      Time elapsed: 00:58:21
                               ETA: 00:39:27

################################################################################
                     [1m Learning iteration 1492/2500 [0m                     

                       Computation: 43145 steps/s (collection: 2.166s, learning 0.113s)
             Mean action noise std: 3.16
          Mean value_function loss: 489.0235
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 78.6779
                       Mean reward: 396.53
               Mean episode length: 136.00
    Episode_Reward/reaching_object: 1.2177
     Episode_Reward/lifting_object: 83.1565
      Episode_Reward/object_height: 0.0070
        Episode_Reward/action_rate: -0.0520
          Episode_Reward/joint_vel: -0.0733
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.2500
--------------------------------------------------------------------------------
                   Total timesteps: 146767872
                    Iteration time: 2.28s
                      Time elapsed: 00:58:23
                               ETA: 00:39:25

################################################################################
                     [1m Learning iteration 1493/2500 [0m                     

                       Computation: 42020 steps/s (collection: 2.203s, learning 0.137s)
             Mean action noise std: 3.17
          Mean value_function loss: 476.5119
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 78.6847
                       Mean reward: 415.50
               Mean episode length: 140.03
    Episode_Reward/reaching_object: 1.1984
     Episode_Reward/lifting_object: 82.7133
      Episode_Reward/object_height: 0.0072
        Episode_Reward/action_rate: -0.0513
          Episode_Reward/joint_vel: -0.0701
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 28.4583
--------------------------------------------------------------------------------
                   Total timesteps: 146866176
                    Iteration time: 2.34s
                      Time elapsed: 00:58:25
                               ETA: 00:39:22

################################################################################
                     [1m Learning iteration 1494/2500 [0m                     

                       Computation: 43007 steps/s (collection: 2.189s, learning 0.097s)
             Mean action noise std: 3.17
          Mean value_function loss: 486.0911
               Mean surrogate loss: -0.0044
                 Mean entropy loss: 78.6956
                       Mean reward: 423.39
               Mean episode length: 138.88
    Episode_Reward/reaching_object: 1.1617
     Episode_Reward/lifting_object: 81.1530
      Episode_Reward/object_height: 0.0071
        Episode_Reward/action_rate: -0.0496
          Episode_Reward/joint_vel: -0.0669
      Episode_Termination/time_out: 0.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 28.5417
--------------------------------------------------------------------------------
                   Total timesteps: 146964480
                    Iteration time: 2.29s
                      Time elapsed: 00:58:28
                               ETA: 00:39:20

################################################################################
                     [1m Learning iteration 1495/2500 [0m                     

                       Computation: 43378 steps/s (collection: 2.176s, learning 0.091s)
             Mean action noise std: 3.17
          Mean value_function loss: 461.6173
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 78.7140
                       Mean reward: 423.35
               Mean episode length: 138.72
    Episode_Reward/reaching_object: 1.2056
     Episode_Reward/lifting_object: 84.7364
      Episode_Reward/object_height: 0.0077
        Episode_Reward/action_rate: -0.0514
          Episode_Reward/joint_vel: -0.0676
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 25.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147062784
                    Iteration time: 2.27s
                      Time elapsed: 00:58:30
                               ETA: 00:39:18

################################################################################
                     [1m Learning iteration 1496/2500 [0m                     

                       Computation: 41778 steps/s (collection: 2.202s, learning 0.151s)
             Mean action noise std: 3.17
          Mean value_function loss: 474.3487
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 78.7355
                       Mean reward: 407.82
               Mean episode length: 135.54
    Episode_Reward/reaching_object: 1.1990
     Episode_Reward/lifting_object: 84.5459
      Episode_Reward/object_height: 0.0078
        Episode_Reward/action_rate: -0.0511
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 0.7500
Episode_Termination/object_dropping: 0.1250
     Episode_Termination/robot_out: 26.7917
--------------------------------------------------------------------------------
                   Total timesteps: 147161088
                    Iteration time: 2.35s
                      Time elapsed: 00:58:32
                               ETA: 00:39:15

################################################################################
                     [1m Learning iteration 1497/2500 [0m                     

                       Computation: 43330 steps/s (collection: 2.181s, learning 0.088s)
             Mean action noise std: 3.17
          Mean value_function loss: 512.3332
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 78.7457
                       Mean reward: 434.70
               Mean episode length: 143.80
    Episode_Reward/reaching_object: 1.1970
     Episode_Reward/lifting_object: 85.0688
      Episode_Reward/object_height: 0.0081
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0656
      Episode_Termination/time_out: 1.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 29.5000
--------------------------------------------------------------------------------
                   Total timesteps: 147259392
                    Iteration time: 2.27s
                      Time elapsed: 00:58:34
                               ETA: 00:39:13

################################################################################
                     [1m Learning iteration 1498/2500 [0m                     

                       Computation: 43001 steps/s (collection: 2.188s, learning 0.098s)
             Mean action noise std: 3.17
          Mean value_function loss: 489.2134
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 78.7542
                       Mean reward: 415.93
               Mean episode length: 135.31
    Episode_Reward/reaching_object: 1.1949
     Episode_Reward/lifting_object: 85.5584
      Episode_Reward/object_height: 0.0084
        Episode_Reward/action_rate: -0.0512
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.1667
--------------------------------------------------------------------------------
                   Total timesteps: 147357696
                    Iteration time: 2.29s
                      Time elapsed: 00:58:37
                               ETA: 00:39:11

################################################################################
                     [1m Learning iteration 1499/2500 [0m                     

                       Computation: 42181 steps/s (collection: 2.240s, learning 0.091s)
             Mean action noise std: 3.17
          Mean value_function loss: 444.4134
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 78.7610
                       Mean reward: 464.65
               Mean episode length: 149.41
    Episode_Reward/reaching_object: 1.2220
     Episode_Reward/lifting_object: 87.7859
      Episode_Reward/object_height: 0.0089
        Episode_Reward/action_rate: -0.0522
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 1.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 25.2083
--------------------------------------------------------------------------------
                   Total timesteps: 147456000
                    Iteration time: 2.33s
                      Time elapsed: 00:58:39
                               ETA: 00:39:08

################################################################################
                     [1m Learning iteration 1500/2500 [0m                     

                       Computation: 42406 steps/s (collection: 2.182s, learning 0.136s)
             Mean action noise std: 3.17
          Mean value_function loss: 449.3917
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 78.7667
                       Mean reward: 475.73
               Mean episode length: 153.75
    Episode_Reward/reaching_object: 1.2823
     Episode_Reward/lifting_object: 92.7679
      Episode_Reward/object_height: 0.0096
        Episode_Reward/action_rate: -0.0545
          Episode_Reward/joint_vel: -0.0666
      Episode_Termination/time_out: 1.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 24.7917
--------------------------------------------------------------------------------
                   Total timesteps: 147554304
                    Iteration time: 2.32s
                      Time elapsed: 00:58:41
                               ETA: 00:39:06

################################################################################
                     [1m Learning iteration 1501/2500 [0m                     

                       Computation: 42766 steps/s (collection: 2.191s, learning 0.108s)
             Mean action noise std: 3.18
          Mean value_function loss: 476.6657
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 78.7752
                       Mean reward: 463.37
               Mean episode length: 151.37
    Episode_Reward/reaching_object: 1.2535
     Episode_Reward/lifting_object: 91.6407
      Episode_Reward/object_height: 0.0098
        Episode_Reward/action_rate: -0.0531
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 1.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.9167
--------------------------------------------------------------------------------
                   Total timesteps: 147652608
                    Iteration time: 2.30s
                      Time elapsed: 00:58:44
                               ETA: 00:39:03

################################################################################
                     [1m Learning iteration 1502/2500 [0m                     

                       Computation: 43520 steps/s (collection: 2.165s, learning 0.093s)
             Mean action noise std: 3.18
          Mean value_function loss: 493.4585
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 78.7822
                       Mean reward: 479.10
               Mean episode length: 155.16
    Episode_Reward/reaching_object: 1.2207
     Episode_Reward/lifting_object: 88.6820
      Episode_Reward/object_height: 0.0095
        Episode_Reward/action_rate: -0.0519
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 0.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 26.7500
--------------------------------------------------------------------------------
                   Total timesteps: 147750912
                    Iteration time: 2.26s
                      Time elapsed: 00:58:46
                               ETA: 00:39:01

################################################################################
                     [1m Learning iteration 1503/2500 [0m                     

                       Computation: 42227 steps/s (collection: 2.193s, learning 0.135s)
             Mean action noise std: 3.18
          Mean value_function loss: 520.2592
               Mean surrogate loss: -0.0038
                 Mean entropy loss: 78.7881
                       Mean reward: 475.36
               Mean episode length: 148.58
    Episode_Reward/reaching_object: 1.2700
     Episode_Reward/lifting_object: 94.2675
      Episode_Reward/object_height: 0.0107
        Episode_Reward/action_rate: -0.0534
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 1.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 27.2083
--------------------------------------------------------------------------------
                   Total timesteps: 147849216
                    Iteration time: 2.33s
                      Time elapsed: 00:58:48
                               ETA: 00:38:59

################################################################################
                     [1m Learning iteration 1504/2500 [0m                     

                       Computation: 42891 steps/s (collection: 2.189s, learning 0.103s)
             Mean action noise std: 3.18
          Mean value_function loss: 512.8962
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 78.7979
                       Mean reward: 475.37
               Mean episode length: 150.53
    Episode_Reward/reaching_object: 1.2515
     Episode_Reward/lifting_object: 92.7372
      Episode_Reward/object_height: 0.0108
        Episode_Reward/action_rate: -0.0525
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 1.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 26.2500
--------------------------------------------------------------------------------
                   Total timesteps: 147947520
                    Iteration time: 2.29s
                      Time elapsed: 00:58:51
                               ETA: 00:38:56

################################################################################
                     [1m Learning iteration 1505/2500 [0m                     

                       Computation: 42769 steps/s (collection: 2.187s, learning 0.112s)
             Mean action noise std: 3.18
          Mean value_function loss: 515.4157
               Mean surrogate loss: 0.0020
                 Mean entropy loss: 78.8038
                       Mean reward: 469.54
               Mean episode length: 145.03
    Episode_Reward/reaching_object: 1.2729
     Episode_Reward/lifting_object: 96.3776
      Episode_Reward/object_height: 0.0116
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 25.6667
--------------------------------------------------------------------------------
                   Total timesteps: 148045824
                    Iteration time: 2.30s
                      Time elapsed: 00:58:53
                               ETA: 00:38:54

################################################################################
                     [1m Learning iteration 1506/2500 [0m                     

                       Computation: 43600 steps/s (collection: 2.145s, learning 0.109s)
             Mean action noise std: 3.18
          Mean value_function loss: 498.4081
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 78.8082
                       Mean reward: 464.81
               Mean episode length: 144.41
    Episode_Reward/reaching_object: 1.2543
     Episode_Reward/lifting_object: 95.3004
      Episode_Reward/object_height: 0.0117
        Episode_Reward/action_rate: -0.0526
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 1.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.8333
--------------------------------------------------------------------------------
                   Total timesteps: 148144128
                    Iteration time: 2.25s
                      Time elapsed: 00:58:55
                               ETA: 00:38:52

################################################################################
                     [1m Learning iteration 1507/2500 [0m                     

                       Computation: 43835 steps/s (collection: 2.151s, learning 0.092s)
             Mean action noise std: 3.18
          Mean value_function loss: 492.8464
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 78.8160
                       Mean reward: 466.74
               Mean episode length: 145.28
    Episode_Reward/reaching_object: 1.2681
     Episode_Reward/lifting_object: 97.1640
      Episode_Reward/object_height: 0.0125
        Episode_Reward/action_rate: -0.0532
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 1.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 24.7083
--------------------------------------------------------------------------------
                   Total timesteps: 148242432
                    Iteration time: 2.24s
                      Time elapsed: 00:58:57
                               ETA: 00:38:49

################################################################################
                     [1m Learning iteration 1508/2500 [0m                     

                       Computation: 43557 steps/s (collection: 2.156s, learning 0.101s)
             Mean action noise std: 3.18
          Mean value_function loss: 464.4125
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 78.8231
                       Mean reward: 474.50
               Mean episode length: 144.26
    Episode_Reward/reaching_object: 1.2471
     Episode_Reward/lifting_object: 96.7314
      Episode_Reward/object_height: 0.0131
        Episode_Reward/action_rate: -0.0523
          Episode_Reward/joint_vel: -0.0558
      Episode_Termination/time_out: 1.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 22.2083
--------------------------------------------------------------------------------
                   Total timesteps: 148340736
                    Iteration time: 2.26s
                      Time elapsed: 00:59:00
                               ETA: 00:38:47

################################################################################
                     [1m Learning iteration 1509/2500 [0m                     

                       Computation: 43145 steps/s (collection: 2.162s, learning 0.116s)
             Mean action noise std: 3.18
          Mean value_function loss: 481.1788
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 78.8298
                       Mean reward: 506.08
               Mean episode length: 155.27
    Episode_Reward/reaching_object: 1.3026
     Episode_Reward/lifting_object: 101.2093
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0546
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 1.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 23.2083
--------------------------------------------------------------------------------
                   Total timesteps: 148439040
                    Iteration time: 2.28s
                      Time elapsed: 00:59:02
                               ETA: 00:38:44

################################################################################
                     [1m Learning iteration 1510/2500 [0m                     

                       Computation: 42750 steps/s (collection: 2.161s, learning 0.138s)
             Mean action noise std: 3.18
          Mean value_function loss: 465.2675
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 78.8360
                       Mean reward: 497.78
               Mean episode length: 153.69
    Episode_Reward/reaching_object: 1.3536
     Episode_Reward/lifting_object: 106.4389
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0560
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 1.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 148537344
                    Iteration time: 2.30s
                      Time elapsed: 00:59:04
                               ETA: 00:38:42

################################################################################
                     [1m Learning iteration 1511/2500 [0m                     

                       Computation: 42950 steps/s (collection: 2.187s, learning 0.102s)
             Mean action noise std: 3.18
          Mean value_function loss: 448.9416
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 78.8420
                       Mean reward: 554.95
               Mean episode length: 167.73
    Episode_Reward/reaching_object: 1.3650
     Episode_Reward/lifting_object: 106.7078
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0570
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 1.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 21.2083
--------------------------------------------------------------------------------
                   Total timesteps: 148635648
                    Iteration time: 2.29s
                      Time elapsed: 00:59:06
                               ETA: 00:38:40

################################################################################
                     [1m Learning iteration 1512/2500 [0m                     

                       Computation: 43446 steps/s (collection: 2.171s, learning 0.092s)
             Mean action noise std: 3.19
          Mean value_function loss: 479.9887
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 78.8560
                       Mean reward: 560.48
               Mean episode length: 171.20
    Episode_Reward/reaching_object: 1.3775
     Episode_Reward/lifting_object: 107.2229
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0576
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 2.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 22.4167
--------------------------------------------------------------------------------
                   Total timesteps: 148733952
                    Iteration time: 2.26s
                      Time elapsed: 00:59:09
                               ETA: 00:38:37

################################################################################
                     [1m Learning iteration 1513/2500 [0m                     

                       Computation: 43193 steps/s (collection: 2.146s, learning 0.130s)
             Mean action noise std: 3.19
          Mean value_function loss: 462.4476
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 78.8657
                       Mean reward: 548.92
               Mean episode length: 168.29
    Episode_Reward/reaching_object: 1.3984
     Episode_Reward/lifting_object: 109.2932
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 2.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 20.2500
--------------------------------------------------------------------------------
                   Total timesteps: 148832256
                    Iteration time: 2.28s
                      Time elapsed: 00:59:11
                               ETA: 00:38:35

################################################################################
                     [1m Learning iteration 1514/2500 [0m                     

                       Computation: 43425 steps/s (collection: 2.149s, learning 0.115s)
             Mean action noise std: 3.19
          Mean value_function loss: 493.3735
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 78.8753
                       Mean reward: 544.74
               Mean episode length: 164.67
    Episode_Reward/reaching_object: 1.3973
     Episode_Reward/lifting_object: 109.1833
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0584
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 2.8750
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 22.6667
--------------------------------------------------------------------------------
                   Total timesteps: 148930560
                    Iteration time: 2.26s
                      Time elapsed: 00:59:13
                               ETA: 00:38:32

################################################################################
                     [1m Learning iteration 1515/2500 [0m                     

                       Computation: 44195 steps/s (collection: 2.135s, learning 0.089s)
             Mean action noise std: 3.19
          Mean value_function loss: 451.1131
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 78.8830
                       Mean reward: 572.91
               Mean episode length: 173.47
    Episode_Reward/reaching_object: 1.4467
     Episode_Reward/lifting_object: 113.6190
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 3.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 19.0417
--------------------------------------------------------------------------------
                   Total timesteps: 149028864
                    Iteration time: 2.22s
                      Time elapsed: 00:59:15
                               ETA: 00:38:30

################################################################################
                     [1m Learning iteration 1516/2500 [0m                     

                       Computation: 43460 steps/s (collection: 2.157s, learning 0.105s)
             Mean action noise std: 3.19
          Mean value_function loss: 419.0180
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 78.8875
                       Mean reward: 581.31
               Mean episode length: 174.44
    Episode_Reward/reaching_object: 1.4658
     Episode_Reward/lifting_object: 115.9123
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0607
          Episode_Reward/joint_vel: -0.0633
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 18.1667
--------------------------------------------------------------------------------
                   Total timesteps: 149127168
                    Iteration time: 2.26s
                      Time elapsed: 00:59:18
                               ETA: 00:38:28

################################################################################
                     [1m Learning iteration 1517/2500 [0m                     

                       Computation: 43797 steps/s (collection: 2.155s, learning 0.089s)
             Mean action noise std: 3.19
          Mean value_function loss: 441.6272
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 78.8914
                       Mean reward: 608.69
               Mean episode length: 179.93
    Episode_Reward/reaching_object: 1.4871
     Episode_Reward/lifting_object: 118.1350
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0611
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 3.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 18.3750
--------------------------------------------------------------------------------
                   Total timesteps: 149225472
                    Iteration time: 2.24s
                      Time elapsed: 00:59:20
                               ETA: 00:38:25

################################################################################
                     [1m Learning iteration 1518/2500 [0m                     

                       Computation: 43858 steps/s (collection: 2.143s, learning 0.098s)
             Mean action noise std: 3.19
          Mean value_function loss: 463.8668
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 78.9036
                       Mean reward: 586.72
               Mean episode length: 174.89
    Episode_Reward/reaching_object: 1.4625
     Episode_Reward/lifting_object: 116.1739
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0599
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 3.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 19.0000
--------------------------------------------------------------------------------
                   Total timesteps: 149323776
                    Iteration time: 2.24s
                      Time elapsed: 00:59:22
                               ETA: 00:38:23

################################################################################
                     [1m Learning iteration 1519/2500 [0m                     

                       Computation: 43417 steps/s (collection: 2.146s, learning 0.119s)
             Mean action noise std: 3.19
          Mean value_function loss: 445.7358
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 78.9157
                       Mean reward: 641.24
               Mean episode length: 187.96
    Episode_Reward/reaching_object: 1.5034
     Episode_Reward/lifting_object: 119.8835
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 4.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.6250
--------------------------------------------------------------------------------
                   Total timesteps: 149422080
                    Iteration time: 2.26s
                      Time elapsed: 00:59:24
                               ETA: 00:38:20

################################################################################
                     [1m Learning iteration 1520/2500 [0m                     

                       Computation: 43842 steps/s (collection: 2.142s, learning 0.100s)
             Mean action noise std: 3.19
          Mean value_function loss: 416.6751
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 78.9215
                       Mean reward: 623.93
               Mean episode length: 183.74
    Episode_Reward/reaching_object: 1.5251
     Episode_Reward/lifting_object: 122.1537
      Episode_Reward/object_height: 0.0198
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0641
      Episode_Termination/time_out: 4.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.3333
--------------------------------------------------------------------------------
                   Total timesteps: 149520384
                    Iteration time: 2.24s
                      Time elapsed: 00:59:27
                               ETA: 00:38:18

################################################################################
                     [1m Learning iteration 1521/2500 [0m                     

                       Computation: 42801 steps/s (collection: 2.186s, learning 0.110s)
             Mean action noise std: 3.20
          Mean value_function loss: 417.6246
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 78.9338
                       Mean reward: 583.07
               Mean episode length: 171.01
    Episode_Reward/reaching_object: 1.5044
     Episode_Reward/lifting_object: 121.1397
      Episode_Reward/object_height: 0.0196
        Episode_Reward/action_rate: -0.0616
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 4.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.9167
--------------------------------------------------------------------------------
                   Total timesteps: 149618688
                    Iteration time: 2.30s
                      Time elapsed: 00:59:29
                               ETA: 00:38:16

################################################################################
                     [1m Learning iteration 1522/2500 [0m                     

                       Computation: 43933 steps/s (collection: 2.150s, learning 0.088s)
             Mean action noise std: 3.20
          Mean value_function loss: 444.9960
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 78.9526
                       Mean reward: 624.83
               Mean episode length: 182.36
    Episode_Reward/reaching_object: 1.5321
     Episode_Reward/lifting_object: 123.4455
      Episode_Reward/object_height: 0.0200
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 4.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 17.3750
--------------------------------------------------------------------------------
                   Total timesteps: 149716992
                    Iteration time: 2.24s
                      Time elapsed: 00:59:31
                               ETA: 00:38:13

################################################################################
                     [1m Learning iteration 1523/2500 [0m                     

                       Computation: 44062 steps/s (collection: 2.137s, learning 0.094s)
             Mean action noise std: 3.20
          Mean value_function loss: 444.3397
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 78.9621
                       Mean reward: 617.44
               Mean episode length: 180.37
    Episode_Reward/reaching_object: 1.5463
     Episode_Reward/lifting_object: 125.7022
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0628
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 5.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.3333
--------------------------------------------------------------------------------
                   Total timesteps: 149815296
                    Iteration time: 2.23s
                      Time elapsed: 00:59:34
                               ETA: 00:38:11

################################################################################
                     [1m Learning iteration 1524/2500 [0m                     

                       Computation: 43727 steps/s (collection: 2.149s, learning 0.099s)
             Mean action noise std: 3.20
          Mean value_function loss: 441.2830
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 78.9680
                       Mean reward: 585.64
               Mean episode length: 177.00
    Episode_Reward/reaching_object: 1.5547
     Episode_Reward/lifting_object: 126.2930
      Episode_Reward/object_height: 0.0206
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 5.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.5833
--------------------------------------------------------------------------------
                   Total timesteps: 149913600
                    Iteration time: 2.25s
                      Time elapsed: 00:59:36
                               ETA: 00:38:08

################################################################################
                     [1m Learning iteration 1525/2500 [0m                     

                       Computation: 44407 steps/s (collection: 2.101s, learning 0.113s)
             Mean action noise std: 3.20
          Mean value_function loss: 458.0360
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 78.9811
                       Mean reward: 614.57
               Mean episode length: 180.95
    Episode_Reward/reaching_object: 1.5353
     Episode_Reward/lifting_object: 125.7929
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0623
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 6.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 17.3333
--------------------------------------------------------------------------------
                   Total timesteps: 150011904
                    Iteration time: 2.21s
                      Time elapsed: 00:59:38
                               ETA: 00:38:06

################################################################################
                     [1m Learning iteration 1526/2500 [0m                     

                       Computation: 44012 steps/s (collection: 2.129s, learning 0.105s)
             Mean action noise std: 3.20
          Mean value_function loss: 444.0619
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 78.9977
                       Mean reward: 694.28
               Mean episode length: 194.04
    Episode_Reward/reaching_object: 1.5813
     Episode_Reward/lifting_object: 131.2963
      Episode_Reward/object_height: 0.0223
        Episode_Reward/action_rate: -0.0638
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 6.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.4583
--------------------------------------------------------------------------------
                   Total timesteps: 150110208
                    Iteration time: 2.23s
                      Time elapsed: 00:59:40
                               ETA: 00:38:03

################################################################################
                     [1m Learning iteration 1527/2500 [0m                     

                       Computation: 43921 steps/s (collection: 2.119s, learning 0.119s)
             Mean action noise std: 3.20
          Mean value_function loss: 414.6855
               Mean surrogate loss: 0.0026
                 Mean entropy loss: 79.0072
                       Mean reward: 682.85
               Mean episode length: 195.12
    Episode_Reward/reaching_object: 1.5610
     Episode_Reward/lifting_object: 128.5556
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0632
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 5.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.2083
--------------------------------------------------------------------------------
                   Total timesteps: 150208512
                    Iteration time: 2.24s
                      Time elapsed: 00:59:42
                               ETA: 00:38:01

################################################################################
                     [1m Learning iteration 1528/2500 [0m                     

                       Computation: 44558 steps/s (collection: 2.115s, learning 0.091s)
             Mean action noise std: 3.21
          Mean value_function loss: 443.4574
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 79.0129
                       Mean reward: 674.16
               Mean episode length: 189.04
    Episode_Reward/reaching_object: 1.5486
     Episode_Reward/lifting_object: 129.4817
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0622
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 6.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 15.6667
--------------------------------------------------------------------------------
                   Total timesteps: 150306816
                    Iteration time: 2.21s
                      Time elapsed: 00:59:45
                               ETA: 00:37:59

################################################################################
                     [1m Learning iteration 1529/2500 [0m                     

                       Computation: 43992 steps/s (collection: 2.146s, learning 0.089s)
             Mean action noise std: 3.21
          Mean value_function loss: 423.4101
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 79.0228
                       Mean reward: 659.93
               Mean episode length: 186.92
    Episode_Reward/reaching_object: 1.5729
     Episode_Reward/lifting_object: 132.2034
      Episode_Reward/object_height: 0.0226
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0630
      Episode_Termination/time_out: 6.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.0833
--------------------------------------------------------------------------------
                   Total timesteps: 150405120
                    Iteration time: 2.23s
                      Time elapsed: 00:59:47
                               ETA: 00:37:56

################################################################################
                     [1m Learning iteration 1530/2500 [0m                     

                       Computation: 43926 steps/s (collection: 2.133s, learning 0.105s)
             Mean action noise std: 3.21
          Mean value_function loss: 442.7074
               Mean surrogate loss: 0.0083
                 Mean entropy loss: 79.0393
                       Mean reward: 682.14
               Mean episode length: 192.29
    Episode_Reward/reaching_object: 1.6039
     Episode_Reward/lifting_object: 135.4518
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0642
          Episode_Reward/joint_vel: -0.0639
      Episode_Termination/time_out: 7.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 150503424
                    Iteration time: 2.24s
                      Time elapsed: 00:59:49
                               ETA: 00:37:54

################################################################################
                     [1m Learning iteration 1531/2500 [0m                     

                       Computation: 43264 steps/s (collection: 2.158s, learning 0.115s)
             Mean action noise std: 3.21
          Mean value_function loss: 458.1854
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 79.0472
                       Mean reward: 699.56
               Mean episode length: 192.03
    Episode_Reward/reaching_object: 1.5339
     Episode_Reward/lifting_object: 129.7787
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0615
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 6.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 16.0000
--------------------------------------------------------------------------------
                   Total timesteps: 150601728
                    Iteration time: 2.27s
                      Time elapsed: 00:59:51
                               ETA: 00:37:51

################################################################################
                     [1m Learning iteration 1532/2500 [0m                     

                       Computation: 44329 steps/s (collection: 2.118s, learning 0.100s)
             Mean action noise std: 3.21
          Mean value_function loss: 450.1031
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 79.0531
                       Mean reward: 652.34
               Mean episode length: 183.22
    Episode_Reward/reaching_object: 1.5706
     Episode_Reward/lifting_object: 133.6457
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0630
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 14.8750
--------------------------------------------------------------------------------
                   Total timesteps: 150700032
                    Iteration time: 2.22s
                      Time elapsed: 00:59:54
                               ETA: 00:37:49

################################################################################
                     [1m Learning iteration 1533/2500 [0m                     

                       Computation: 44493 steps/s (collection: 2.117s, learning 0.093s)
             Mean action noise std: 3.21
          Mean value_function loss: 422.5710
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 79.0690
                       Mean reward: 699.84
               Mean episode length: 195.49
    Episode_Reward/reaching_object: 1.5864
     Episode_Reward/lifting_object: 135.4355
      Episode_Reward/object_height: 0.0239
        Episode_Reward/action_rate: -0.0635
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 6.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 13.8750
--------------------------------------------------------------------------------
                   Total timesteps: 150798336
                    Iteration time: 2.21s
                      Time elapsed: 00:59:56
                               ETA: 00:37:47

################################################################################
                     [1m Learning iteration 1534/2500 [0m                     

                       Computation: 44312 steps/s (collection: 2.114s, learning 0.104s)
             Mean action noise std: 3.21
          Mean value_function loss: 417.8337
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 79.0835
                       Mean reward: 690.23
               Mean episode length: 191.34
    Episode_Reward/reaching_object: 1.5703
     Episode_Reward/lifting_object: 133.2610
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0633
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 6.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.2083
--------------------------------------------------------------------------------
                   Total timesteps: 150896640
                    Iteration time: 2.22s
                      Time elapsed: 00:59:58
                               ETA: 00:37:44

################################################################################
                     [1m Learning iteration 1535/2500 [0m                     

                       Computation: 43738 steps/s (collection: 2.154s, learning 0.094s)
             Mean action noise std: 3.22
          Mean value_function loss: 417.5799
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 79.1058
                       Mean reward: 720.47
               Mean episode length: 198.36
    Episode_Reward/reaching_object: 1.5958
     Episode_Reward/lifting_object: 137.1449
      Episode_Reward/object_height: 0.0251
        Episode_Reward/action_rate: -0.0636
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 13.7917
--------------------------------------------------------------------------------
                   Total timesteps: 150994944
                    Iteration time: 2.25s
                      Time elapsed: 01:00:00
                               ETA: 00:37:42

################################################################################
                     [1m Learning iteration 1536/2500 [0m                     

                       Computation: 43416 steps/s (collection: 2.110s, learning 0.155s)
             Mean action noise std: 3.22
          Mean value_function loss: 413.8654
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 79.1252
                       Mean reward: 704.87
               Mean episode length: 196.09
    Episode_Reward/reaching_object: 1.6078
     Episode_Reward/lifting_object: 137.1410
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 8.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 13.2917
--------------------------------------------------------------------------------
                   Total timesteps: 151093248
                    Iteration time: 2.26s
                      Time elapsed: 01:00:03
                               ETA: 00:37:39

################################################################################
                     [1m Learning iteration 1537/2500 [0m                     

                       Computation: 44435 steps/s (collection: 2.123s, learning 0.090s)
             Mean action noise std: 3.22
          Mean value_function loss: 384.6708
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 79.1383
                       Mean reward: 662.66
               Mean episode length: 187.70
    Episode_Reward/reaching_object: 1.5972
     Episode_Reward/lifting_object: 135.2888
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0644
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 7.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.9167
--------------------------------------------------------------------------------
                   Total timesteps: 151191552
                    Iteration time: 2.21s
                      Time elapsed: 01:00:05
                               ETA: 00:37:37

################################################################################
                     [1m Learning iteration 1538/2500 [0m                     

                       Computation: 44469 steps/s (collection: 2.112s, learning 0.099s)
             Mean action noise std: 3.22
          Mean value_function loss: 366.7146
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 79.1466
                       Mean reward: 668.61
               Mean episode length: 189.31
    Episode_Reward/reaching_object: 1.6373
     Episode_Reward/lifting_object: 139.9238
      Episode_Reward/object_height: 0.0249
        Episode_Reward/action_rate: -0.0653
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 8.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.1667
--------------------------------------------------------------------------------
                   Total timesteps: 151289856
                    Iteration time: 2.21s
                      Time elapsed: 01:00:07
                               ETA: 00:37:34

################################################################################
                     [1m Learning iteration 1539/2500 [0m                     

                       Computation: 44091 steps/s (collection: 2.138s, learning 0.092s)
             Mean action noise std: 3.22
          Mean value_function loss: 380.1532
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 79.1535
                       Mean reward: 663.65
               Mean episode length: 186.89
    Episode_Reward/reaching_object: 1.6545
     Episode_Reward/lifting_object: 141.0500
      Episode_Reward/object_height: 0.0244
        Episode_Reward/action_rate: -0.0661
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 8.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.6667
--------------------------------------------------------------------------------
                   Total timesteps: 151388160
                    Iteration time: 2.23s
                      Time elapsed: 01:00:09
                               ETA: 00:37:32

################################################################################
                     [1m Learning iteration 1540/2500 [0m                     

                       Computation: 42564 steps/s (collection: 2.128s, learning 0.181s)
             Mean action noise std: 3.22
          Mean value_function loss: 353.7855
               Mean surrogate loss: 0.0045
                 Mean entropy loss: 79.1562
                       Mean reward: 726.73
               Mean episode length: 199.97
    Episode_Reward/reaching_object: 1.6608
     Episode_Reward/lifting_object: 141.4297
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 9.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1250
--------------------------------------------------------------------------------
                   Total timesteps: 151486464
                    Iteration time: 2.31s
                      Time elapsed: 01:00:12
                               ETA: 00:37:30

################################################################################
                     [1m Learning iteration 1541/2500 [0m                     

                       Computation: 44000 steps/s (collection: 2.146s, learning 0.089s)
             Mean action noise std: 3.22
          Mean value_function loss: 356.2014
               Mean surrogate loss: 0.0054
                 Mean entropy loss: 79.1573
                       Mean reward: 737.57
               Mean episode length: 203.75
    Episode_Reward/reaching_object: 1.6505
     Episode_Reward/lifting_object: 141.2293
      Episode_Reward/object_height: 0.0237
        Episode_Reward/action_rate: -0.0656
          Episode_Reward/joint_vel: -0.0646
      Episode_Termination/time_out: 10.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.0833
--------------------------------------------------------------------------------
                   Total timesteps: 151584768
                    Iteration time: 2.23s
                      Time elapsed: 01:00:14
                               ETA: 00:37:27

################################################################################
                     [1m Learning iteration 1542/2500 [0m                     

                       Computation: 44264 steps/s (collection: 2.111s, learning 0.110s)
             Mean action noise std: 3.22
          Mean value_function loss: 361.2183
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 79.1585
                       Mean reward: 706.88
               Mean episode length: 196.81
    Episode_Reward/reaching_object: 1.6817
     Episode_Reward/lifting_object: 144.9104
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0666
          Episode_Reward/joint_vel: -0.0653
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.1250
--------------------------------------------------------------------------------
                   Total timesteps: 151683072
                    Iteration time: 2.22s
                      Time elapsed: 01:00:16
                               ETA: 00:37:25

################################################################################
                     [1m Learning iteration 1543/2500 [0m                     

                       Computation: 44420 steps/s (collection: 2.114s, learning 0.100s)
             Mean action noise std: 3.22
          Mean value_function loss: 337.4660
               Mean surrogate loss: 0.0055
                 Mean entropy loss: 79.1608
                       Mean reward: 717.51
               Mean episode length: 199.03
    Episode_Reward/reaching_object: 1.6879
     Episode_Reward/lifting_object: 144.2364
      Episode_Reward/object_height: 0.0236
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.5833
--------------------------------------------------------------------------------
                   Total timesteps: 151781376
                    Iteration time: 2.21s
                      Time elapsed: 01:00:18
                               ETA: 00:37:22

################################################################################
                     [1m Learning iteration 1544/2500 [0m                     

                       Computation: 43478 steps/s (collection: 2.152s, learning 0.109s)
             Mean action noise std: 3.22
          Mean value_function loss: 377.2546
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 79.1623
                       Mean reward: 708.13
               Mean episode length: 199.78
    Episode_Reward/reaching_object: 1.6435
     Episode_Reward/lifting_object: 140.4318
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 9.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.8750
--------------------------------------------------------------------------------
                   Total timesteps: 151879680
                    Iteration time: 2.26s
                      Time elapsed: 01:00:20
                               ETA: 00:37:20

################################################################################
                     [1m Learning iteration 1545/2500 [0m                     

                       Computation: 43522 steps/s (collection: 2.136s, learning 0.123s)
             Mean action noise std: 3.22
          Mean value_function loss: 352.1514
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 79.1633
                       Mean reward: 690.49
               Mean episode length: 192.87
    Episode_Reward/reaching_object: 1.7014
     Episode_Reward/lifting_object: 145.7565
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0675
      Episode_Termination/time_out: 10.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.2917
--------------------------------------------------------------------------------
                   Total timesteps: 151977984
                    Iteration time: 2.26s
                      Time elapsed: 01:00:23
                               ETA: 00:37:18

################################################################################
                     [1m Learning iteration 1546/2500 [0m                     

                       Computation: 43598 steps/s (collection: 2.135s, learning 0.120s)
             Mean action noise std: 3.22
          Mean value_function loss: 365.9637
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 79.1645
                       Mean reward: 734.43
               Mean episode length: 201.72
    Episode_Reward/reaching_object: 1.6848
     Episode_Reward/lifting_object: 144.7956
      Episode_Reward/object_height: 0.0230
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.4583
--------------------------------------------------------------------------------
                   Total timesteps: 152076288
                    Iteration time: 2.25s
                      Time elapsed: 01:00:25
                               ETA: 00:37:15

################################################################################
                     [1m Learning iteration 1547/2500 [0m                     

                       Computation: 44089 steps/s (collection: 2.136s, learning 0.094s)
             Mean action noise std: 3.22
          Mean value_function loss: 364.1773
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 79.1675
                       Mean reward: 760.29
               Mean episode length: 208.59
    Episode_Reward/reaching_object: 1.6601
     Episode_Reward/lifting_object: 143.2995
      Episode_Reward/object_height: 0.0227
        Episode_Reward/action_rate: -0.0654
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 9.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.2083
--------------------------------------------------------------------------------
                   Total timesteps: 152174592
                    Iteration time: 2.23s
                      Time elapsed: 01:00:27
                               ETA: 00:37:13

################################################################################
                     [1m Learning iteration 1548/2500 [0m                     

                       Computation: 43647 steps/s (collection: 2.129s, learning 0.123s)
             Mean action noise std: 3.22
          Mean value_function loss: 317.8520
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 79.1697
                       Mean reward: 743.94
               Mean episode length: 206.01
    Episode_Reward/reaching_object: 1.7031
     Episode_Reward/lifting_object: 146.1999
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 10.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2083
--------------------------------------------------------------------------------
                   Total timesteps: 152272896
                    Iteration time: 2.25s
                      Time elapsed: 01:00:29
                               ETA: 00:37:10

################################################################################
                     [1m Learning iteration 1549/2500 [0m                     

                       Computation: 44022 steps/s (collection: 2.116s, learning 0.117s)
             Mean action noise std: 3.22
          Mean value_function loss: 321.7032
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 79.1727
                       Mean reward: 738.41
               Mean episode length: 202.11
    Episode_Reward/reaching_object: 1.6732
     Episode_Reward/lifting_object: 144.0422
      Episode_Reward/object_height: 0.0224
        Episode_Reward/action_rate: -0.0660
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 9.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.2500
--------------------------------------------------------------------------------
                   Total timesteps: 152371200
                    Iteration time: 2.23s
                      Time elapsed: 01:00:32
                               ETA: 00:37:08

################################################################################
                     [1m Learning iteration 1550/2500 [0m                     

                       Computation: 44713 steps/s (collection: 2.106s, learning 0.093s)
             Mean action noise std: 3.22
          Mean value_function loss: 343.8036
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 79.1811
                       Mean reward: 740.15
               Mean episode length: 205.36
    Episode_Reward/reaching_object: 1.7151
     Episode_Reward/lifting_object: 148.0773
      Episode_Reward/object_height: 0.0231
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 9.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.3333
--------------------------------------------------------------------------------
                   Total timesteps: 152469504
                    Iteration time: 2.20s
                      Time elapsed: 01:00:34
                               ETA: 00:37:06

################################################################################
                     [1m Learning iteration 1551/2500 [0m                     

                       Computation: 44027 steps/s (collection: 2.119s, learning 0.114s)
             Mean action noise std: 3.23
          Mean value_function loss: 386.8871
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 79.1897
                       Mean reward: 736.17
               Mean episode length: 202.17
    Episode_Reward/reaching_object: 1.6741
     Episode_Reward/lifting_object: 143.7517
      Episode_Reward/object_height: 0.0219
        Episode_Reward/action_rate: -0.0658
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 9.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 152567808
                    Iteration time: 2.23s
                      Time elapsed: 01:00:36
                               ETA: 00:37:03

################################################################################
                     [1m Learning iteration 1552/2500 [0m                     

                       Computation: 44688 steps/s (collection: 2.112s, learning 0.088s)
             Mean action noise std: 3.23
          Mean value_function loss: 351.0583
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 79.1963
                       Mean reward: 724.47
               Mean episode length: 201.43
    Episode_Reward/reaching_object: 1.6937
     Episode_Reward/lifting_object: 145.6613
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.0648
      Episode_Termination/time_out: 10.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.8750
--------------------------------------------------------------------------------
                   Total timesteps: 152666112
                    Iteration time: 2.20s
                      Time elapsed: 01:00:38
                               ETA: 00:37:01

################################################################################
                     [1m Learning iteration 1553/2500 [0m                     

                       Computation: 43473 steps/s (collection: 2.130s, learning 0.131s)
             Mean action noise std: 3.23
          Mean value_function loss: 372.6398
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 79.2021
                       Mean reward: 743.79
               Mean episode length: 205.56
    Episode_Reward/reaching_object: 1.6901
     Episode_Reward/lifting_object: 144.8332
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 10.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.7500
--------------------------------------------------------------------------------
                   Total timesteps: 152764416
                    Iteration time: 2.26s
                      Time elapsed: 01:00:41
                               ETA: 00:36:58

################################################################################
                     [1m Learning iteration 1554/2500 [0m                     

                       Computation: 42673 steps/s (collection: 2.181s, learning 0.123s)
             Mean action noise std: 3.23
          Mean value_function loss: 405.8382
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 79.2089
                       Mean reward: 729.28
               Mean episode length: 201.98
    Episode_Reward/reaching_object: 1.6794
     Episode_Reward/lifting_object: 143.4415
      Episode_Reward/object_height: 0.0212
        Episode_Reward/action_rate: -0.0662
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.9583
--------------------------------------------------------------------------------
                   Total timesteps: 152862720
                    Iteration time: 2.30s
                      Time elapsed: 01:00:43
                               ETA: 00:36:56

################################################################################
                     [1m Learning iteration 1555/2500 [0m                     

                       Computation: 43920 steps/s (collection: 2.141s, learning 0.098s)
             Mean action noise std: 3.23
          Mean value_function loss: 376.0168
               Mean surrogate loss: 0.0068
                 Mean entropy loss: 79.2147
                       Mean reward: 777.07
               Mean episode length: 211.33
    Episode_Reward/reaching_object: 1.6702
     Episode_Reward/lifting_object: 143.8822
      Episode_Reward/object_height: 0.0215
        Episode_Reward/action_rate: -0.0652
          Episode_Reward/joint_vel: -0.0640
      Episode_Termination/time_out: 10.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 11.2917
--------------------------------------------------------------------------------
                   Total timesteps: 152961024
                    Iteration time: 2.24s
                      Time elapsed: 01:00:45
                               ETA: 00:36:54

################################################################################
                     [1m Learning iteration 1556/2500 [0m                     

                       Computation: 43896 steps/s (collection: 2.136s, learning 0.104s)
             Mean action noise std: 3.23
          Mean value_function loss: 379.1281
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 79.2176
                       Mean reward: 731.44
               Mean episode length: 203.87
    Episode_Reward/reaching_object: 1.7153
     Episode_Reward/lifting_object: 147.5967
      Episode_Reward/object_height: 0.0222
        Episode_Reward/action_rate: -0.0668
          Episode_Reward/joint_vel: -0.0651
      Episode_Termination/time_out: 10.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 10.3750
--------------------------------------------------------------------------------
                   Total timesteps: 153059328
                    Iteration time: 2.24s
                      Time elapsed: 01:00:47
                               ETA: 00:36:51

################################################################################
                     [1m Learning iteration 1557/2500 [0m                     

                       Computation: 43156 steps/s (collection: 2.157s, learning 0.121s)
             Mean action noise std: 3.23
          Mean value_function loss: 335.6159
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 79.2245
                       Mean reward: 749.41
               Mean episode length: 206.61
    Episode_Reward/reaching_object: 1.7227
     Episode_Reward/lifting_object: 147.7256
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0671
          Episode_Reward/joint_vel: -0.0661
      Episode_Termination/time_out: 11.0417
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 153157632
                    Iteration time: 2.28s
                      Time elapsed: 01:00:50
                               ETA: 00:36:49

################################################################################
                     [1m Learning iteration 1558/2500 [0m                     

                       Computation: 42202 steps/s (collection: 2.199s, learning 0.130s)
             Mean action noise std: 3.23
          Mean value_function loss: 321.1567
               Mean surrogate loss: 0.0037
                 Mean entropy loss: 79.2270
                       Mean reward: 725.42
               Mean episode length: 201.57
    Episode_Reward/reaching_object: 1.6975
     Episode_Reward/lifting_object: 145.3860
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0654
      Episode_Termination/time_out: 9.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.1250
--------------------------------------------------------------------------------
                   Total timesteps: 153255936
                    Iteration time: 2.33s
                      Time elapsed: 01:00:52
                               ETA: 00:36:46

################################################################################
                     [1m Learning iteration 1559/2500 [0m                     

                       Computation: 43827 steps/s (collection: 2.150s, learning 0.093s)
             Mean action noise std: 3.23
          Mean value_function loss: 323.1800
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 79.2292
                       Mean reward: 724.55
               Mean episode length: 197.97
    Episode_Reward/reaching_object: 1.7049
     Episode_Reward/lifting_object: 146.9882
      Episode_Reward/object_height: 0.0216
        Episode_Reward/action_rate: -0.0663
          Episode_Reward/joint_vel: -0.0647
      Episode_Termination/time_out: 9.2083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 9.2917
--------------------------------------------------------------------------------
                   Total timesteps: 153354240
                    Iteration time: 2.24s
                      Time elapsed: 01:00:54
                               ETA: 00:36:44

################################################################################
                     [1m Learning iteration 1560/2500 [0m                     

                       Computation: 44251 steps/s (collection: 2.132s, learning 0.090s)
             Mean action noise std: 3.23
          Mean value_function loss: 363.7143
               Mean surrogate loss: 0.0058
                 Mean entropy loss: 79.2372
                       Mean reward: 721.49
               Mean episode length: 196.71
    Episode_Reward/reaching_object: 1.6789
     Episode_Reward/lifting_object: 144.3560
      Episode_Reward/object_height: 0.0209
        Episode_Reward/action_rate: -0.0655
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 9.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 10.1667
--------------------------------------------------------------------------------
                   Total timesteps: 153452544
                    Iteration time: 2.22s
                      Time elapsed: 01:00:56
                               ETA: 00:36:42

################################################################################
                     [1m Learning iteration 1561/2500 [0m                     

                       Computation: 43545 steps/s (collection: 2.165s, learning 0.092s)
             Mean action noise std: 3.23
          Mean value_function loss: 341.9807
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 79.2403
                       Mean reward: 764.45
               Mean episode length: 210.44
    Episode_Reward/reaching_object: 1.7272
     Episode_Reward/lifting_object: 148.3844
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0673
          Episode_Reward/joint_vel: -0.0658
      Episode_Termination/time_out: 10.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153550848
                    Iteration time: 2.26s
                      Time elapsed: 01:00:59
                               ETA: 00:36:39

################################################################################
                     [1m Learning iteration 1562/2500 [0m                     

                       Computation: 43017 steps/s (collection: 2.162s, learning 0.124s)
             Mean action noise std: 3.23
          Mean value_function loss: 348.7585
               Mean surrogate loss: 0.0047
                 Mean entropy loss: 79.2414
                       Mean reward: 725.46
               Mean episode length: 197.84
    Episode_Reward/reaching_object: 1.7053
     Episode_Reward/lifting_object: 147.1240
      Episode_Reward/object_height: 0.0217
        Episode_Reward/action_rate: -0.0665
          Episode_Reward/joint_vel: -0.0649
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 9.7083
--------------------------------------------------------------------------------
                   Total timesteps: 153649152
                    Iteration time: 2.29s
                      Time elapsed: 01:01:01
                               ETA: 00:36:37

################################################################################
                     [1m Learning iteration 1563/2500 [0m                     

                       Computation: 43371 steps/s (collection: 2.159s, learning 0.108s)
             Mean action noise std: 3.23
          Mean value_function loss: 296.9072
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 79.2426
                       Mean reward: 798.74
               Mean episode length: 214.95
    Episode_Reward/reaching_object: 1.7296
     Episode_Reward/lifting_object: 150.2657
      Episode_Reward/object_height: 0.0228
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0634
      Episode_Termination/time_out: 10.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.9167
--------------------------------------------------------------------------------
                   Total timesteps: 153747456
                    Iteration time: 2.27s
                      Time elapsed: 01:01:03
                               ETA: 00:36:34

################################################################################
                     [1m Learning iteration 1564/2500 [0m                     

                       Computation: 43647 steps/s (collection: 2.146s, learning 0.107s)
             Mean action noise std: 3.23
          Mean value_function loss: 323.8023
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 79.2468
                       Mean reward: 802.87
               Mean episode length: 215.31
    Episode_Reward/reaching_object: 1.7383
     Episode_Reward/lifting_object: 152.3689
      Episode_Reward/object_height: 0.0229
        Episode_Reward/action_rate: -0.0670
          Episode_Reward/joint_vel: -0.0629
      Episode_Termination/time_out: 11.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 9.5000
--------------------------------------------------------------------------------
                   Total timesteps: 153845760
                    Iteration time: 2.25s
                      Time elapsed: 01:01:05
                               ETA: 00:36:32

################################################################################
                     [1m Learning iteration 1565/2500 [0m                     

                       Computation: 43429 steps/s (collection: 2.140s, learning 0.124s)
             Mean action noise std: 3.23
          Mean value_function loss: 278.7999
               Mean surrogate loss: 0.0088
                 Mean entropy loss: 79.2512
                       Mean reward: 792.51
               Mean episode length: 212.31
    Episode_Reward/reaching_object: 1.8134
     Episode_Reward/lifting_object: 159.4034
      Episode_Reward/object_height: 0.0241
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.0644
      Episode_Termination/time_out: 11.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.0417
--------------------------------------------------------------------------------
                   Total timesteps: 153944064
                    Iteration time: 2.26s
                      Time elapsed: 01:01:08
                               ETA: 00:36:30

################################################################################
                     [1m Learning iteration 1566/2500 [0m                     

                       Computation: 43415 steps/s (collection: 2.131s, learning 0.133s)
             Mean action noise std: 3.23
          Mean value_function loss: 269.6044
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 79.2523
                       Mean reward: 823.40
               Mean episode length: 222.80
    Episode_Reward/reaching_object: 1.8182
     Episode_Reward/lifting_object: 160.2709
      Episode_Reward/object_height: 0.0243
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 12.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.7917
--------------------------------------------------------------------------------
                   Total timesteps: 154042368
                    Iteration time: 2.26s
                      Time elapsed: 01:01:10
                               ETA: 00:36:27

################################################################################
                     [1m Learning iteration 1567/2500 [0m                     

                       Computation: 43098 steps/s (collection: 2.162s, learning 0.119s)
             Mean action noise std: 3.23
          Mean value_function loss: 261.9771
               Mean surrogate loss: 0.0040
                 Mean entropy loss: 79.2532
                       Mean reward: 754.75
               Mean episode length: 204.99
    Episode_Reward/reaching_object: 1.7899
     Episode_Reward/lifting_object: 157.0766
      Episode_Reward/object_height: 0.0234
        Episode_Reward/action_rate: -0.0688
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 11.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.6250
--------------------------------------------------------------------------------
                   Total timesteps: 154140672
                    Iteration time: 2.28s
                      Time elapsed: 01:01:12
                               ETA: 00:36:25

################################################################################
                     [1m Learning iteration 1568/2500 [0m                     

                       Computation: 42620 steps/s (collection: 2.210s, learning 0.097s)
             Mean action noise std: 3.23
          Mean value_function loss: 253.6119
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 79.2542
                       Mean reward: 814.00
               Mean episode length: 217.99
    Episode_Reward/reaching_object: 1.7914
     Episode_Reward/lifting_object: 157.5813
      Episode_Reward/object_height: 0.0232
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 6.3333
--------------------------------------------------------------------------------
                   Total timesteps: 154238976
                    Iteration time: 2.31s
                      Time elapsed: 01:01:15
                               ETA: 00:36:23

################################################################################
                     [1m Learning iteration 1569/2500 [0m                     

                       Computation: 41888 steps/s (collection: 2.210s, learning 0.137s)
             Mean action noise std: 3.23
          Mean value_function loss: 291.9685
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 79.2560
                       Mean reward: 780.92
               Mean episode length: 208.79
    Episode_Reward/reaching_object: 1.7528
     Episode_Reward/lifting_object: 154.8323
      Episode_Reward/object_height: 0.0221
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 11.3333
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 8.0417
--------------------------------------------------------------------------------
                   Total timesteps: 154337280
                    Iteration time: 2.35s
                      Time elapsed: 01:01:17
                               ETA: 00:36:20

################################################################################
                     [1m Learning iteration 1570/2500 [0m                     

                       Computation: 41549 steps/s (collection: 2.246s, learning 0.120s)
             Mean action noise std: 3.23
          Mean value_function loss: 324.5462
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 79.2569
                       Mean reward: 783.76
               Mean episode length: 214.06
    Episode_Reward/reaching_object: 1.7811
     Episode_Reward/lifting_object: 157.4230
      Episode_Reward/object_height: 0.0220
        Episode_Reward/action_rate: -0.0692
          Episode_Reward/joint_vel: -0.0637
      Episode_Termination/time_out: 11.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.5417
--------------------------------------------------------------------------------
                   Total timesteps: 154435584
                    Iteration time: 2.37s
                      Time elapsed: 01:01:19
                               ETA: 00:36:18

################################################################################
                     [1m Learning iteration 1571/2500 [0m                     

                       Computation: 43064 steps/s (collection: 2.184s, learning 0.099s)
             Mean action noise std: 3.23
          Mean value_function loss: 257.8149
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 79.2579
                       Mean reward: 807.46
               Mean episode length: 219.99
    Episode_Reward/reaching_object: 1.7872
     Episode_Reward/lifting_object: 158.4147
      Episode_Reward/object_height: 0.0218
        Episode_Reward/action_rate: -0.0694
          Episode_Reward/joint_vel: -0.0643
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.3750
--------------------------------------------------------------------------------
                   Total timesteps: 154533888
                    Iteration time: 2.28s
                      Time elapsed: 01:01:22
                               ETA: 00:36:15

################################################################################
                     [1m Learning iteration 1572/2500 [0m                     

                       Computation: 43629 steps/s (collection: 2.154s, learning 0.099s)
             Mean action noise std: 3.23
          Mean value_function loss: 258.7938
               Mean surrogate loss: 0.0094
                 Mean entropy loss: 79.2596
                       Mean reward: 794.97
               Mean episode length: 215.98
    Episode_Reward/reaching_object: 1.7721
     Episode_Reward/lifting_object: 157.2794
      Episode_Reward/object_height: 0.0211
        Episode_Reward/action_rate: -0.0689
          Episode_Reward/joint_vel: -0.0636
      Episode_Termination/time_out: 12.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.0000
--------------------------------------------------------------------------------
                   Total timesteps: 154632192
                    Iteration time: 2.25s
                      Time elapsed: 01:01:24
                               ETA: 00:36:13

################################################################################
                     [1m Learning iteration 1573/2500 [0m                     

                       Computation: 41757 steps/s (collection: 2.238s, learning 0.116s)
             Mean action noise std: 3.23
          Mean value_function loss: 274.1380
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 79.2607
                       Mean reward: 789.27
               Mean episode length: 214.42
    Episode_Reward/reaching_object: 1.7537
     Episode_Reward/lifting_object: 156.2771
      Episode_Reward/object_height: 0.0204
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 154730496
                    Iteration time: 2.35s
                      Time elapsed: 01:01:26
                               ETA: 00:36:11

################################################################################
                     [1m Learning iteration 1574/2500 [0m                     

                       Computation: 41514 steps/s (collection: 2.254s, learning 0.114s)
             Mean action noise std: 3.23
          Mean value_function loss: 307.2054
               Mean surrogate loss: 0.0042
                 Mean entropy loss: 79.2625
                       Mean reward: 797.14
               Mean episode length: 215.24
    Episode_Reward/reaching_object: 1.7537
     Episode_Reward/lifting_object: 157.0257
      Episode_Reward/object_height: 0.0199
        Episode_Reward/action_rate: -0.0685
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.8333
--------------------------------------------------------------------------------
                   Total timesteps: 154828800
                    Iteration time: 2.37s
                      Time elapsed: 01:01:29
                               ETA: 00:36:08

################################################################################
                     [1m Learning iteration 1575/2500 [0m                     

                       Computation: 42132 steps/s (collection: 2.218s, learning 0.116s)
             Mean action noise std: 3.23
          Mean value_function loss: 287.8383
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 79.2640
                       Mean reward: 749.05
               Mean episode length: 203.64
    Episode_Reward/reaching_object: 1.7217
     Episode_Reward/lifting_object: 154.4467
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 12.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 7.1250
--------------------------------------------------------------------------------
                   Total timesteps: 154927104
                    Iteration time: 2.33s
                      Time elapsed: 01:01:31
                               ETA: 00:36:06

################################################################################
                     [1m Learning iteration 1576/2500 [0m                     

                       Computation: 41937 steps/s (collection: 2.232s, learning 0.113s)
             Mean action noise std: 3.23
          Mean value_function loss: 265.1491
               Mean surrogate loss: 0.0027
                 Mean entropy loss: 79.2645
                       Mean reward: 805.90
               Mean episode length: 216.78
    Episode_Reward/reaching_object: 1.7276
     Episode_Reward/lifting_object: 156.7567
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0678
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 11.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 6.9583
--------------------------------------------------------------------------------
                   Total timesteps: 155025408
                    Iteration time: 2.34s
                      Time elapsed: 01:01:33
                               ETA: 00:36:04

################################################################################
                     [1m Learning iteration 1577/2500 [0m                     

                       Computation: 42444 steps/s (collection: 2.211s, learning 0.105s)
             Mean action noise std: 3.23
          Mean value_function loss: 279.6106
               Mean surrogate loss: 0.0011
                 Mean entropy loss: 79.2656
                       Mean reward: 798.07
               Mean episode length: 215.37
    Episode_Reward/reaching_object: 1.7517
     Episode_Reward/lifting_object: 159.4708
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0691
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2083
--------------------------------------------------------------------------------
                   Total timesteps: 155123712
                    Iteration time: 2.32s
                      Time elapsed: 01:01:36
                               ETA: 00:36:01

################################################################################
                     [1m Learning iteration 1578/2500 [0m                     

                       Computation: 42338 steps/s (collection: 2.208s, learning 0.114s)
             Mean action noise std: 3.24
          Mean value_function loss: 274.0388
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 79.2734
                       Mean reward: 725.33
               Mean episode length: 197.90
    Episode_Reward/reaching_object: 1.6757
     Episode_Reward/lifting_object: 152.5518
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0664
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 11.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.2500
--------------------------------------------------------------------------------
                   Total timesteps: 155222016
                    Iteration time: 2.32s
                      Time elapsed: 01:01:38
                               ETA: 00:35:59

################################################################################
                     [1m Learning iteration 1579/2500 [0m                     

                       Computation: 41783 steps/s (collection: 2.229s, learning 0.124s)
             Mean action noise std: 3.24
          Mean value_function loss: 277.0779
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 79.2861
                       Mean reward: 798.99
               Mean episode length: 215.65
    Episode_Reward/reaching_object: 1.6882
     Episode_Reward/lifting_object: 153.4554
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0669
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 12.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 7.6667
--------------------------------------------------------------------------------
                   Total timesteps: 155320320
                    Iteration time: 2.35s
                      Time elapsed: 01:01:40
                               ETA: 00:35:57

################################################################################
                     [1m Learning iteration 1580/2500 [0m                     

                       Computation: 42388 steps/s (collection: 2.222s, learning 0.097s)
             Mean action noise std: 3.24
          Mean value_function loss: 239.5698
               Mean surrogate loss: 0.0039
                 Mean entropy loss: 79.2981
                       Mean reward: 791.71
               Mean episode length: 214.60
    Episode_Reward/reaching_object: 1.7038
     Episode_Reward/lifting_object: 155.0853
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0674
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 11.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 155418624
                    Iteration time: 2.32s
                      Time elapsed: 01:01:43
                               ETA: 00:35:54

################################################################################
                     [1m Learning iteration 1581/2500 [0m                     

                       Computation: 42478 steps/s (collection: 2.207s, learning 0.107s)
             Mean action noise std: 3.24
          Mean value_function loss: 223.4763
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 79.3027
                       Mean reward: 769.85
               Mean episode length: 210.49
    Episode_Reward/reaching_object: 1.7625
     Episode_Reward/lifting_object: 160.1832
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0697
          Episode_Reward/joint_vel: -0.0622
      Episode_Termination/time_out: 12.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2917
--------------------------------------------------------------------------------
                   Total timesteps: 155516928
                    Iteration time: 2.31s
                      Time elapsed: 01:01:45
                               ETA: 00:35:52

################################################################################
                     [1m Learning iteration 1582/2500 [0m                     

                       Computation: 42856 steps/s (collection: 2.175s, learning 0.119s)
             Mean action noise std: 3.24
          Mean value_function loss: 239.8357
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 79.3132
                       Mean reward: 795.93
               Mean episode length: 213.58
    Episode_Reward/reaching_object: 1.7418
     Episode_Reward/lifting_object: 158.4286
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0686
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.9583
--------------------------------------------------------------------------------
                   Total timesteps: 155615232
                    Iteration time: 2.29s
                      Time elapsed: 01:01:47
                               ETA: 00:35:50

################################################################################
                     [1m Learning iteration 1583/2500 [0m                     

                       Computation: 42768 steps/s (collection: 2.192s, learning 0.106s)
             Mean action noise std: 3.24
          Mean value_function loss: 225.1574
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 79.3281
                       Mean reward: 850.32
               Mean episode length: 227.75
    Episode_Reward/reaching_object: 1.8159
     Episode_Reward/lifting_object: 165.9078
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0709
          Episode_Reward/joint_vel: -0.0623
      Episode_Termination/time_out: 12.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.3750
--------------------------------------------------------------------------------
                   Total timesteps: 155713536
                    Iteration time: 2.30s
                      Time elapsed: 01:01:49
                               ETA: 00:35:47

################################################################################
                     [1m Learning iteration 1584/2500 [0m                     

                       Computation: 42531 steps/s (collection: 2.197s, learning 0.114s)
             Mean action noise std: 3.24
          Mean value_function loss: 214.3662
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 79.3412
                       Mean reward: 859.48
               Mean episode length: 228.35
    Episode_Reward/reaching_object: 1.8230
     Episode_Reward/lifting_object: 165.8984
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0712
          Episode_Reward/joint_vel: -0.0625
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 155811840
                    Iteration time: 2.31s
                      Time elapsed: 01:01:52
                               ETA: 00:35:45

################################################################################
                     [1m Learning iteration 1585/2500 [0m                     

                       Computation: 42376 steps/s (collection: 2.196s, learning 0.124s)
             Mean action noise std: 3.25
          Mean value_function loss: 216.2414
               Mean surrogate loss: 0.0161
                 Mean entropy loss: 79.3507
                       Mean reward: 831.73
               Mean episode length: 223.16
    Episode_Reward/reaching_object: 1.8419
     Episode_Reward/lifting_object: 167.2335
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0719
          Episode_Reward/joint_vel: -0.0628
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.7500
--------------------------------------------------------------------------------
                   Total timesteps: 155910144
                    Iteration time: 2.32s
                      Time elapsed: 01:01:54
                               ETA: 00:35:43

################################################################################
                     [1m Learning iteration 1586/2500 [0m                     

                       Computation: 42139 steps/s (collection: 2.217s, learning 0.116s)
             Mean action noise std: 3.25
          Mean value_function loss: 204.1772
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 79.3554
                       Mean reward: 833.52
               Mean episode length: 223.44
    Episode_Reward/reaching_object: 1.8522
     Episode_Reward/lifting_object: 167.9804
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0718
          Episode_Reward/joint_vel: -0.0624
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.3333
--------------------------------------------------------------------------------
                   Total timesteps: 156008448
                    Iteration time: 2.33s
                      Time elapsed: 01:01:56
                               ETA: 00:35:40

################################################################################
                     [1m Learning iteration 1587/2500 [0m                     

                       Computation: 43894 steps/s (collection: 2.139s, learning 0.100s)
             Mean action noise std: 3.25
          Mean value_function loss: 215.8172
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 79.3599
                       Mean reward: 830.36
               Mean episode length: 223.43
    Episode_Reward/reaching_object: 1.8069
     Episode_Reward/lifting_object: 163.4525
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0705
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 156106752
                    Iteration time: 2.24s
                      Time elapsed: 01:01:59
                               ETA: 00:35:38

################################################################################
                     [1m Learning iteration 1588/2500 [0m                     

                       Computation: 42730 steps/s (collection: 2.175s, learning 0.126s)
             Mean action noise std: 3.25
          Mean value_function loss: 196.9719
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 79.3620
                       Mean reward: 879.31
               Mean episode length: 232.59
    Episode_Reward/reaching_object: 1.8692
     Episode_Reward/lifting_object: 169.8860
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0722
          Episode_Reward/joint_vel: -0.0619
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 156205056
                    Iteration time: 2.30s
                      Time elapsed: 01:02:01
                               ETA: 00:35:35

################################################################################
                     [1m Learning iteration 1589/2500 [0m                     

                       Computation: 42074 steps/s (collection: 2.206s, learning 0.130s)
             Mean action noise std: 3.25
          Mean value_function loss: 219.6678
               Mean surrogate loss: 0.0031
                 Mean entropy loss: 79.3626
                       Mean reward: 878.63
               Mean episode length: 233.64
    Episode_Reward/reaching_object: 1.8337
     Episode_Reward/lifting_object: 166.3824
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0713
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 14.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8333
--------------------------------------------------------------------------------
                   Total timesteps: 156303360
                    Iteration time: 2.34s
                      Time elapsed: 01:02:03
                               ETA: 00:35:33

################################################################################
                     [1m Learning iteration 1590/2500 [0m                     

                       Computation: 42553 steps/s (collection: 2.191s, learning 0.119s)
             Mean action noise std: 3.25
          Mean value_function loss: 238.7353
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 79.3634
                       Mean reward: 774.69
               Mean episode length: 210.50
    Episode_Reward/reaching_object: 1.7922
     Episode_Reward/lifting_object: 163.1369
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0699
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 12.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.4167
--------------------------------------------------------------------------------
                   Total timesteps: 156401664
                    Iteration time: 2.31s
                      Time elapsed: 01:02:06
                               ETA: 00:35:31

################################################################################
                     [1m Learning iteration 1591/2500 [0m                     

                       Computation: 42871 steps/s (collection: 2.168s, learning 0.125s)
             Mean action noise std: 3.25
          Mean value_function loss: 259.4173
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 79.3674
                       Mean reward: 778.69
               Mean episode length: 210.25
    Episode_Reward/reaching_object: 1.7363
     Episode_Reward/lifting_object: 157.7046
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0684
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 6.7083
--------------------------------------------------------------------------------
                   Total timesteps: 156499968
                    Iteration time: 2.29s
                      Time elapsed: 01:02:08
                               ETA: 00:35:28

################################################################################
                     [1m Learning iteration 1592/2500 [0m                     

                       Computation: 43235 steps/s (collection: 2.179s, learning 0.095s)
             Mean action noise std: 3.25
          Mean value_function loss: 247.3743
               Mean surrogate loss: 0.0052
                 Mean entropy loss: 79.3786
                       Mean reward: 773.96
               Mean episode length: 210.99
    Episode_Reward/reaching_object: 1.7411
     Episode_Reward/lifting_object: 158.2423
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0683
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.8750
--------------------------------------------------------------------------------
                   Total timesteps: 156598272
                    Iteration time: 2.27s
                      Time elapsed: 01:02:10
                               ETA: 00:35:26

################################################################################
                     [1m Learning iteration 1593/2500 [0m                     

                       Computation: 43002 steps/s (collection: 2.155s, learning 0.131s)
             Mean action noise std: 3.25
          Mean value_function loss: 200.4898
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 79.3838
                       Mean reward: 814.16
               Mean episode length: 219.00
    Episode_Reward/reaching_object: 1.8374
     Episode_Reward/lifting_object: 167.5639
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0719
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 12.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0833
--------------------------------------------------------------------------------
                   Total timesteps: 156696576
                    Iteration time: 2.29s
                      Time elapsed: 01:02:12
                               ETA: 00:35:24

################################################################################
                     [1m Learning iteration 1594/2500 [0m                     

                       Computation: 43083 steps/s (collection: 2.160s, learning 0.121s)
             Mean action noise std: 3.25
          Mean value_function loss: 188.9648
               Mean surrogate loss: 0.0043
                 Mean entropy loss: 79.3921
                       Mean reward: 856.71
               Mean episode length: 228.43
    Episode_Reward/reaching_object: 1.8166
     Episode_Reward/lifting_object: 165.9421
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0709
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 12.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 156794880
                    Iteration time: 2.28s
                      Time elapsed: 01:02:15
                               ETA: 00:35:21

################################################################################
                     [1m Learning iteration 1595/2500 [0m                     

                       Computation: 42784 steps/s (collection: 2.209s, learning 0.089s)
             Mean action noise std: 3.25
          Mean value_function loss: 190.9345
               Mean surrogate loss: 0.0028
                 Mean entropy loss: 79.3961
                       Mean reward: 854.80
               Mean episode length: 229.42
    Episode_Reward/reaching_object: 1.8652
     Episode_Reward/lifting_object: 170.0313
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 156893184
                    Iteration time: 2.30s
                      Time elapsed: 01:02:17
                               ETA: 00:35:19

################################################################################
                     [1m Learning iteration 1596/2500 [0m                     

                       Computation: 42940 steps/s (collection: 2.160s, learning 0.130s)
             Mean action noise std: 3.25
          Mean value_function loss: 188.7750
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 79.3973
                       Mean reward: 879.59
               Mean episode length: 235.90
    Episode_Reward/reaching_object: 1.8692
     Episode_Reward/lifting_object: 170.5594
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0621
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 156991488
                    Iteration time: 2.29s
                      Time elapsed: 01:02:19
                               ETA: 00:35:16

################################################################################
                     [1m Learning iteration 1597/2500 [0m                     

                       Computation: 42669 steps/s (collection: 2.193s, learning 0.111s)
             Mean action noise std: 3.25
          Mean value_function loss: 174.8254
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 79.3989
                       Mean reward: 866.49
               Mean episode length: 234.76
    Episode_Reward/reaching_object: 1.8561
     Episode_Reward/lifting_object: 169.7105
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0618
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 157089792
                    Iteration time: 2.30s
                      Time elapsed: 01:02:22
                               ETA: 00:35:14

################################################################################
                     [1m Learning iteration 1598/2500 [0m                     

                       Computation: 43522 steps/s (collection: 2.141s, learning 0.118s)
             Mean action noise std: 3.25
          Mean value_function loss: 223.7391
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 79.4020
                       Mean reward: 871.91
               Mean episode length: 233.31
    Episode_Reward/reaching_object: 1.8145
     Episode_Reward/lifting_object: 165.7340
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0715
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 157188096
                    Iteration time: 2.26s
                      Time elapsed: 01:02:24
                               ETA: 00:35:12

################################################################################
                     [1m Learning iteration 1599/2500 [0m                     

                       Computation: 42410 steps/s (collection: 2.198s, learning 0.120s)
             Mean action noise std: 3.25
          Mean value_function loss: 180.7236
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 79.4098
                       Mean reward: 879.17
               Mean episode length: 235.37
    Episode_Reward/reaching_object: 1.8350
     Episode_Reward/lifting_object: 167.6959
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0720
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9167
--------------------------------------------------------------------------------
                   Total timesteps: 157286400
                    Iteration time: 2.32s
                      Time elapsed: 01:02:26
                               ETA: 00:35:09

################################################################################
                     [1m Learning iteration 1600/2500 [0m                     

                       Computation: 43133 steps/s (collection: 2.148s, learning 0.132s)
             Mean action noise std: 3.25
          Mean value_function loss: 176.5001
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 79.4145
                       Mean reward: 822.96
               Mean episode length: 221.72
    Episode_Reward/reaching_object: 1.8138
     Episode_Reward/lifting_object: 164.9515
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0713
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.8750
--------------------------------------------------------------------------------
                   Total timesteps: 157384704
                    Iteration time: 2.28s
                      Time elapsed: 01:02:28
                               ETA: 00:35:07

################################################################################
                     [1m Learning iteration 1601/2500 [0m                     

                       Computation: 42503 steps/s (collection: 2.192s, learning 0.120s)
             Mean action noise std: 3.26
          Mean value_function loss: 164.1348
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 79.4242
                       Mean reward: 881.28
               Mean episode length: 235.78
    Episode_Reward/reaching_object: 1.8663
     Episode_Reward/lifting_object: 169.6271
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0729
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 3.5833
--------------------------------------------------------------------------------
                   Total timesteps: 157483008
                    Iteration time: 2.31s
                      Time elapsed: 01:02:31
                               ETA: 00:35:05

################################################################################
                     [1m Learning iteration 1602/2500 [0m                     

                       Computation: 43535 steps/s (collection: 2.138s, learning 0.121s)
             Mean action noise std: 3.26
          Mean value_function loss: 156.8122
               Mean surrogate loss: 0.0212
                 Mean entropy loss: 79.4424
                       Mean reward: 893.65
               Mean episode length: 235.72
    Episode_Reward/reaching_object: 1.8940
     Episode_Reward/lifting_object: 173.1634
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 157581312
                    Iteration time: 2.26s
                      Time elapsed: 01:02:33
                               ETA: 00:35:02

################################################################################
                     [1m Learning iteration 1603/2500 [0m                     

                       Computation: 43862 steps/s (collection: 2.144s, learning 0.098s)
             Mean action noise std: 3.26
          Mean value_function loss: 146.7788
               Mean surrogate loss: 0.0090
                 Mean entropy loss: 79.4460
                       Mean reward: 863.21
               Mean episode length: 228.61
    Episode_Reward/reaching_object: 1.8829
     Episode_Reward/lifting_object: 171.4878
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 157679616
                    Iteration time: 2.24s
                      Time elapsed: 01:02:35
                               ETA: 00:35:00

################################################################################
                     [1m Learning iteration 1604/2500 [0m                     

                       Computation: 43990 steps/s (collection: 2.132s, learning 0.103s)
             Mean action noise std: 3.26
          Mean value_function loss: 177.1861
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 79.4481
                       Mean reward: 860.45
               Mean episode length: 230.45
    Episode_Reward/reaching_object: 1.8720
     Episode_Reward/lifting_object: 170.0133
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 157777920
                    Iteration time: 2.23s
                      Time elapsed: 01:02:38
                               ETA: 00:34:57

################################################################################
                     [1m Learning iteration 1605/2500 [0m                     

                       Computation: 43563 steps/s (collection: 2.141s, learning 0.116s)
             Mean action noise std: 3.26
          Mean value_function loss: 215.1172
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 79.4523
                       Mean reward: 839.86
               Mean episode length: 227.93
    Episode_Reward/reaching_object: 1.8346
     Episode_Reward/lifting_object: 166.7738
      Episode_Reward/object_height: 0.0147
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4167
--------------------------------------------------------------------------------
                   Total timesteps: 157876224
                    Iteration time: 2.26s
                      Time elapsed: 01:02:40
                               ETA: 00:34:55

################################################################################
                     [1m Learning iteration 1606/2500 [0m                     

                       Computation: 43872 steps/s (collection: 2.124s, learning 0.117s)
             Mean action noise std: 3.26
          Mean value_function loss: 243.2253
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 79.4575
                       Mean reward: 783.41
               Mean episode length: 210.89
    Episode_Reward/reaching_object: 1.7675
     Episode_Reward/lifting_object: 161.2879
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0698
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 5.7083
--------------------------------------------------------------------------------
                   Total timesteps: 157974528
                    Iteration time: 2.24s
                      Time elapsed: 01:02:42
                               ETA: 00:34:53

################################################################################
                     [1m Learning iteration 1607/2500 [0m                     

                       Computation: 42718 steps/s (collection: 2.179s, learning 0.122s)
             Mean action noise std: 3.26
          Mean value_function loss: 220.4803
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 79.4677
                       Mean reward: 802.50
               Mean episode length: 214.57
    Episode_Reward/reaching_object: 1.7669
     Episode_Reward/lifting_object: 161.4183
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0695
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.0833
--------------------------------------------------------------------------------
                   Total timesteps: 158072832
                    Iteration time: 2.30s
                      Time elapsed: 01:02:44
                               ETA: 00:34:50

################################################################################
                     [1m Learning iteration 1608/2500 [0m                     

                       Computation: 43771 steps/s (collection: 2.138s, learning 0.108s)
             Mean action noise std: 3.26
          Mean value_function loss: 221.3404
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 79.4751
                       Mean reward: 770.93
               Mean episode length: 210.85
    Episode_Reward/reaching_object: 1.7704
     Episode_Reward/lifting_object: 161.8993
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0702
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 5.2083
--------------------------------------------------------------------------------
                   Total timesteps: 158171136
                    Iteration time: 2.25s
                      Time elapsed: 01:02:47
                               ETA: 00:34:48

################################################################################
                     [1m Learning iteration 1609/2500 [0m                     

                       Computation: 43695 steps/s (collection: 2.145s, learning 0.105s)
             Mean action noise std: 3.26
          Mean value_function loss: 215.0753
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 79.4768
                       Mean reward: 833.74
               Mean episode length: 223.26
    Episode_Reward/reaching_object: 1.7889
     Episode_Reward/lifting_object: 164.5684
      Episode_Reward/object_height: 0.0141
        Episode_Reward/action_rate: -0.0708
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.8750
--------------------------------------------------------------------------------
                   Total timesteps: 158269440
                    Iteration time: 2.25s
                      Time elapsed: 01:02:49
                               ETA: 00:34:46

################################################################################
                     [1m Learning iteration 1610/2500 [0m                     

                       Computation: 43067 steps/s (collection: 2.164s, learning 0.119s)
             Mean action noise std: 3.26
          Mean value_function loss: 209.7832
               Mean surrogate loss: 0.0046
                 Mean entropy loss: 79.4780
                       Mean reward: 857.61
               Mean episode length: 228.75
    Episode_Reward/reaching_object: 1.7857
     Episode_Reward/lifting_object: 164.8161
      Episode_Reward/object_height: 0.0142
        Episode_Reward/action_rate: -0.0710
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 158367744
                    Iteration time: 2.28s
                      Time elapsed: 01:02:51
                               ETA: 00:34:43

################################################################################
                     [1m Learning iteration 1611/2500 [0m                     

                       Computation: 42797 steps/s (collection: 2.162s, learning 0.135s)
             Mean action noise std: 3.26
          Mean value_function loss: 191.1823
               Mean surrogate loss: 0.0029
                 Mean entropy loss: 79.4799
                       Mean reward: 812.87
               Mean episode length: 217.80
    Episode_Reward/reaching_object: 1.7756
     Episode_Reward/lifting_object: 163.9104
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0710
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.2917
--------------------------------------------------------------------------------
                   Total timesteps: 158466048
                    Iteration time: 2.30s
                      Time elapsed: 01:02:53
                               ETA: 00:34:41

################################################################################
                     [1m Learning iteration 1612/2500 [0m                     

                       Computation: 42478 steps/s (collection: 2.170s, learning 0.145s)
             Mean action noise std: 3.26
          Mean value_function loss: 165.8887
               Mean surrogate loss: 0.0060
                 Mean entropy loss: 79.4816
                       Mean reward: 862.06
               Mean episode length: 230.54
    Episode_Reward/reaching_object: 1.8152
     Episode_Reward/lifting_object: 167.7023
      Episode_Reward/object_height: 0.0139
        Episode_Reward/action_rate: -0.0722
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 158564352
                    Iteration time: 2.31s
                      Time elapsed: 01:02:56
                               ETA: 00:34:38

################################################################################
                     [1m Learning iteration 1613/2500 [0m                     

                       Computation: 42730 steps/s (collection: 2.189s, learning 0.112s)
             Mean action noise std: 3.26
          Mean value_function loss: 173.0781
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 79.4827
                       Mean reward: 869.94
               Mean episode length: 232.39
    Episode_Reward/reaching_object: 1.8405
     Episode_Reward/lifting_object: 169.9017
      Episode_Reward/object_height: 0.0138
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4583
--------------------------------------------------------------------------------
                   Total timesteps: 158662656
                    Iteration time: 2.30s
                      Time elapsed: 01:02:58
                               ETA: 00:34:36

################################################################################
                     [1m Learning iteration 1614/2500 [0m                     

                       Computation: 43636 steps/s (collection: 2.123s, learning 0.130s)
             Mean action noise std: 3.26
          Mean value_function loss: 168.0674
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 79.4852
                       Mean reward: 842.01
               Mean episode length: 224.80
    Episode_Reward/reaching_object: 1.8243
     Episode_Reward/lifting_object: 168.1846
      Episode_Reward/object_height: 0.0133
        Episode_Reward/action_rate: -0.0720
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 12.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 158760960
                    Iteration time: 2.25s
                      Time elapsed: 01:03:00
                               ETA: 00:34:34

################################################################################
                     [1m Learning iteration 1615/2500 [0m                     

                       Computation: 43427 steps/s (collection: 2.133s, learning 0.131s)
             Mean action noise std: 3.26
          Mean value_function loss: 157.6297
               Mean surrogate loss: 0.0064
                 Mean entropy loss: 79.4951
                       Mean reward: 846.21
               Mean episode length: 226.32
    Episode_Reward/reaching_object: 1.8501
     Episode_Reward/lifting_object: 169.2312
      Episode_Reward/object_height: 0.0136
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.9583
--------------------------------------------------------------------------------
                   Total timesteps: 158859264
                    Iteration time: 2.26s
                      Time elapsed: 01:03:03
                               ETA: 00:34:31

################################################################################
                     [1m Learning iteration 1616/2500 [0m                     

                       Computation: 43764 steps/s (collection: 2.123s, learning 0.123s)
             Mean action noise std: 3.27
          Mean value_function loss: 145.7289
               Mean surrogate loss: 0.0087
                 Mean entropy loss: 79.4989
                       Mean reward: 855.91
               Mean episode length: 231.29
    Episode_Reward/reaching_object: 1.9065
     Episode_Reward/lifting_object: 173.2074
      Episode_Reward/object_height: 0.0140
        Episode_Reward/action_rate: -0.0744
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 158957568
                    Iteration time: 2.25s
                      Time elapsed: 01:03:05
                               ETA: 00:34:29

################################################################################
                     [1m Learning iteration 1617/2500 [0m                     

                       Computation: 43052 steps/s (collection: 2.158s, learning 0.125s)
             Mean action noise std: 3.27
          Mean value_function loss: 150.2334
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 79.5021
                       Mean reward: 884.02
               Mean episode length: 234.39
    Episode_Reward/reaching_object: 1.9153
     Episode_Reward/lifting_object: 173.6785
      Episode_Reward/object_height: 0.0143
        Episode_Reward/action_rate: -0.0743
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.7917
--------------------------------------------------------------------------------
                   Total timesteps: 159055872
                    Iteration time: 2.28s
                      Time elapsed: 01:03:07
                               ETA: 00:34:27

################################################################################
                     [1m Learning iteration 1618/2500 [0m                     

                       Computation: 43382 steps/s (collection: 2.144s, learning 0.122s)
             Mean action noise std: 3.27
          Mean value_function loss: 171.5939
               Mean surrogate loss: 0.0035
                 Mean entropy loss: 79.5106
                       Mean reward: 874.93
               Mean episode length: 233.55
    Episode_Reward/reaching_object: 1.9344
     Episode_Reward/lifting_object: 175.3305
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0749
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 159154176
                    Iteration time: 2.27s
                      Time elapsed: 01:03:09
                               ETA: 00:34:24

################################################################################
                     [1m Learning iteration 1619/2500 [0m                     

                       Computation: 43691 steps/s (collection: 2.144s, learning 0.106s)
             Mean action noise std: 3.27
          Mean value_function loss: 183.6484
               Mean surrogate loss: 0.0051
                 Mean entropy loss: 79.5136
                       Mean reward: 878.67
               Mean episode length: 231.76
    Episode_Reward/reaching_object: 1.8853
     Episode_Reward/lifting_object: 170.5066
      Episode_Reward/object_height: 0.0148
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.0000
--------------------------------------------------------------------------------
                   Total timesteps: 159252480
                    Iteration time: 2.25s
                      Time elapsed: 01:03:12
                               ETA: 00:34:22

################################################################################
                     [1m Learning iteration 1620/2500 [0m                     

                       Computation: 43569 steps/s (collection: 2.134s, learning 0.122s)
             Mean action noise std: 3.27
          Mean value_function loss: 138.8991
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 79.5168
                       Mean reward: 896.97
               Mean episode length: 236.78
    Episode_Reward/reaching_object: 1.9569
     Episode_Reward/lifting_object: 176.7802
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 159350784
                    Iteration time: 2.26s
                      Time elapsed: 01:03:14
                               ETA: 00:34:19

################################################################################
                     [1m Learning iteration 1621/2500 [0m                     

                       Computation: 43709 steps/s (collection: 2.136s, learning 0.113s)
             Mean action noise std: 3.27
          Mean value_function loss: 188.7943
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 79.5251
                       Mean reward: 840.16
               Mean episode length: 224.57
    Episode_Reward/reaching_object: 1.8964
     Episode_Reward/lifting_object: 170.8456
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0728
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 159449088
                    Iteration time: 2.25s
                      Time elapsed: 01:03:16
                               ETA: 00:34:17

################################################################################
                     [1m Learning iteration 1622/2500 [0m                     

                       Computation: 43071 steps/s (collection: 2.173s, learning 0.110s)
             Mean action noise std: 3.27
          Mean value_function loss: 164.6140
               Mean surrogate loss: 0.0019
                 Mean entropy loss: 79.5414
                       Mean reward: 882.43
               Mean episode length: 233.90
    Episode_Reward/reaching_object: 1.9332
     Episode_Reward/lifting_object: 174.4682
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0742
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 159547392
                    Iteration time: 2.28s
                      Time elapsed: 01:03:18
                               ETA: 00:34:15

################################################################################
                     [1m Learning iteration 1623/2500 [0m                     

                       Computation: 43845 steps/s (collection: 2.132s, learning 0.110s)
             Mean action noise std: 3.27
          Mean value_function loss: 201.0973
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 79.5476
                       Mean reward: 748.48
               Mean episode length: 204.72
    Episode_Reward/reaching_object: 1.8317
     Episode_Reward/lifting_object: 163.6088
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0712
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 13.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.1667
--------------------------------------------------------------------------------
                   Total timesteps: 159645696
                    Iteration time: 2.24s
                      Time elapsed: 01:03:21
                               ETA: 00:34:12

################################################################################
                     [1m Learning iteration 1624/2500 [0m                     

                       Computation: 43694 steps/s (collection: 2.128s, learning 0.122s)
             Mean action noise std: 3.27
          Mean value_function loss: 188.6089
               Mean surrogate loss: 0.0005
                 Mean entropy loss: 79.5569
                       Mean reward: 859.48
               Mean episode length: 227.75
    Episode_Reward/reaching_object: 1.9119
     Episode_Reward/lifting_object: 171.8764
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0733
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 12.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 159744000
                    Iteration time: 2.25s
                      Time elapsed: 01:03:23
                               ETA: 00:34:10

################################################################################
                     [1m Learning iteration 1625/2500 [0m                     

                       Computation: 42941 steps/s (collection: 2.177s, learning 0.113s)
             Mean action noise std: 3.27
          Mean value_function loss: 131.0684
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 79.5680
                       Mean reward: 876.29
               Mean episode length: 232.89
    Episode_Reward/reaching_object: 1.9623
     Episode_Reward/lifting_object: 176.6443
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 159842304
                    Iteration time: 2.29s
                      Time elapsed: 01:03:25
                               ETA: 00:34:07

################################################################################
                     [1m Learning iteration 1626/2500 [0m                     

                       Computation: 43213 steps/s (collection: 2.172s, learning 0.103s)
             Mean action noise std: 3.27
          Mean value_function loss: 134.8338
               Mean surrogate loss: 0.0032
                 Mean entropy loss: 79.5709
                       Mean reward: 921.88
               Mean episode length: 243.52
    Episode_Reward/reaching_object: 1.9396
     Episode_Reward/lifting_object: 174.6032
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0743
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 159940608
                    Iteration time: 2.27s
                      Time elapsed: 01:03:27
                               ETA: 00:34:05

################################################################################
                     [1m Learning iteration 1627/2500 [0m                     

                       Computation: 44118 steps/s (collection: 2.126s, learning 0.102s)
             Mean action noise std: 3.28
          Mean value_function loss: 191.6243
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 79.5771
                       Mean reward: 846.46
               Mean episode length: 225.77
    Episode_Reward/reaching_object: 1.9257
     Episode_Reward/lifting_object: 172.5241
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0738
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 160038912
                    Iteration time: 2.23s
                      Time elapsed: 01:03:30
                               ETA: 00:34:03

################################################################################
                     [1m Learning iteration 1628/2500 [0m                     

                       Computation: 44111 steps/s (collection: 2.100s, learning 0.128s)
             Mean action noise std: 3.28
          Mean value_function loss: 172.3335
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 79.5972
                       Mean reward: 867.05
               Mean episode length: 229.38
    Episode_Reward/reaching_object: 1.9058
     Episode_Reward/lifting_object: 169.5471
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0726
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 160137216
                    Iteration time: 2.23s
                      Time elapsed: 01:03:32
                               ETA: 00:34:00

################################################################################
                     [1m Learning iteration 1629/2500 [0m                     

                       Computation: 43981 steps/s (collection: 2.119s, learning 0.116s)
             Mean action noise std: 3.28
          Mean value_function loss: 187.9345
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 79.6127
                       Mean reward: 884.44
               Mean episode length: 234.31
    Episode_Reward/reaching_object: 1.9361
     Episode_Reward/lifting_object: 172.0508
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0735
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.9583
--------------------------------------------------------------------------------
                   Total timesteps: 160235520
                    Iteration time: 2.24s
                      Time elapsed: 01:03:34
                               ETA: 00:33:58

################################################################################
                     [1m Learning iteration 1630/2500 [0m                     

                       Computation: 44010 steps/s (collection: 2.125s, learning 0.109s)
             Mean action noise std: 3.28
          Mean value_function loss: 186.8462
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 79.6298
                       Mean reward: 884.41
               Mean episode length: 232.15
    Episode_Reward/reaching_object: 1.9507
     Episode_Reward/lifting_object: 172.8618
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0739
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 160333824
                    Iteration time: 2.23s
                      Time elapsed: 01:03:36
                               ETA: 00:33:55

################################################################################
                     [1m Learning iteration 1631/2500 [0m                     

                       Computation: 43962 steps/s (collection: 2.124s, learning 0.113s)
             Mean action noise std: 3.28
          Mean value_function loss: 180.8100
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 79.6490
                       Mean reward: 854.22
               Mean episode length: 227.25
    Episode_Reward/reaching_object: 1.9317
     Episode_Reward/lifting_object: 170.0919
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 160432128
                    Iteration time: 2.24s
                      Time elapsed: 01:03:39
                               ETA: 00:33:53

################################################################################
                     [1m Learning iteration 1632/2500 [0m                     

                       Computation: 43853 steps/s (collection: 2.137s, learning 0.105s)
             Mean action noise std: 3.29
          Mean value_function loss: 139.5065
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 79.6725
                       Mean reward: 883.67
               Mean episode length: 234.84
    Episode_Reward/reaching_object: 1.9817
     Episode_Reward/lifting_object: 175.2402
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0748
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.7083
--------------------------------------------------------------------------------
                   Total timesteps: 160530432
                    Iteration time: 2.24s
                      Time elapsed: 01:03:41
                               ETA: 00:33:51

################################################################################
                     [1m Learning iteration 1633/2500 [0m                     

                       Computation: 44056 steps/s (collection: 2.128s, learning 0.103s)
             Mean action noise std: 3.29
          Mean value_function loss: 155.1476
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 79.6821
                       Mean reward: 894.34
               Mean episode length: 236.01
    Episode_Reward/reaching_object: 1.9566
     Episode_Reward/lifting_object: 172.7804
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0739
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 160628736
                    Iteration time: 2.23s
                      Time elapsed: 01:03:43
                               ETA: 00:33:48

################################################################################
                     [1m Learning iteration 1634/2500 [0m                     

                       Computation: 44268 steps/s (collection: 2.104s, learning 0.116s)
             Mean action noise std: 3.29
          Mean value_function loss: 167.2621
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 79.6914
                       Mean reward: 837.49
               Mean episode length: 223.42
    Episode_Reward/reaching_object: 1.9236
     Episode_Reward/lifting_object: 169.9969
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0731
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 160727040
                    Iteration time: 2.22s
                      Time elapsed: 01:03:45
                               ETA: 00:33:46

################################################################################
                     [1m Learning iteration 1635/2500 [0m                     

                       Computation: 44492 steps/s (collection: 2.089s, learning 0.120s)
             Mean action noise std: 3.29
          Mean value_function loss: 193.1888
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 79.7027
                       Mean reward: 861.12
               Mean episode length: 227.36
    Episode_Reward/reaching_object: 1.9128
     Episode_Reward/lifting_object: 169.1697
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0724
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 11.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.8333
--------------------------------------------------------------------------------
                   Total timesteps: 160825344
                    Iteration time: 2.21s
                      Time elapsed: 01:03:47
                               ETA: 00:33:43

################################################################################
                     [1m Learning iteration 1636/2500 [0m                     

                       Computation: 44300 steps/s (collection: 2.101s, learning 0.118s)
             Mean action noise std: 3.29
          Mean value_function loss: 205.8633
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 79.7298
                       Mean reward: 795.10
               Mean episode length: 214.26
    Episode_Reward/reaching_object: 1.9260
     Episode_Reward/lifting_object: 169.6422
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0732
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 13.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.2500
--------------------------------------------------------------------------------
                   Total timesteps: 160923648
                    Iteration time: 2.22s
                      Time elapsed: 01:03:50
                               ETA: 00:33:41

################################################################################
                     [1m Learning iteration 1637/2500 [0m                     

                       Computation: 43657 steps/s (collection: 2.127s, learning 0.125s)
             Mean action noise std: 3.29
          Mean value_function loss: 195.7024
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 79.7418
                       Mean reward: 869.13
               Mean episode length: 230.84
    Episode_Reward/reaching_object: 1.9362
     Episode_Reward/lifting_object: 171.3879
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0735
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 4.0417
--------------------------------------------------------------------------------
                   Total timesteps: 161021952
                    Iteration time: 2.25s
                      Time elapsed: 01:03:52
                               ETA: 00:33:39

################################################################################
                     [1m Learning iteration 1638/2500 [0m                     

                       Computation: 44187 steps/s (collection: 2.122s, learning 0.103s)
             Mean action noise std: 3.30
          Mean value_function loss: 164.1666
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 79.7589
                       Mean reward: 890.47
               Mean episode length: 236.82
    Episode_Reward/reaching_object: 1.9630
     Episode_Reward/lifting_object: 174.3833
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 161120256
                    Iteration time: 2.22s
                      Time elapsed: 01:03:54
                               ETA: 00:33:36

################################################################################
                     [1m Learning iteration 1639/2500 [0m                     

                       Computation: 44046 steps/s (collection: 2.131s, learning 0.101s)
             Mean action noise std: 3.30
          Mean value_function loss: 166.2113
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 79.7726
                       Mean reward: 850.27
               Mean episode length: 226.69
    Episode_Reward/reaching_object: 1.9697
     Episode_Reward/lifting_object: 175.4914
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 161218560
                    Iteration time: 2.23s
                      Time elapsed: 01:03:56
                               ETA: 00:33:34

################################################################################
                     [1m Learning iteration 1640/2500 [0m                     

                       Computation: 43861 steps/s (collection: 2.122s, learning 0.119s)
             Mean action noise std: 3.30
          Mean value_function loss: 169.8448
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 79.7880
                       Mean reward: 931.82
               Mean episode length: 243.99
    Episode_Reward/reaching_object: 1.9795
     Episode_Reward/lifting_object: 177.2635
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0755
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 161316864
                    Iteration time: 2.24s
                      Time elapsed: 01:03:59
                               ETA: 00:33:31

################################################################################
                     [1m Learning iteration 1641/2500 [0m                     

                       Computation: 43961 steps/s (collection: 2.113s, learning 0.124s)
             Mean action noise std: 3.30
          Mean value_function loss: 161.4516
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 79.8110
                       Mean reward: 924.33
               Mean episode length: 242.00
    Episode_Reward/reaching_object: 1.9531
     Episode_Reward/lifting_object: 174.9497
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0747
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.1667
--------------------------------------------------------------------------------
                   Total timesteps: 161415168
                    Iteration time: 2.24s
                      Time elapsed: 01:04:01
                               ETA: 00:33:29

################################################################################
                     [1m Learning iteration 1642/2500 [0m                     

                       Computation: 43514 steps/s (collection: 2.136s, learning 0.124s)
             Mean action noise std: 3.30
          Mean value_function loss: 157.6458
               Mean surrogate loss: 0.0136
                 Mean entropy loss: 79.8248
                       Mean reward: 867.51
               Mean episode length: 230.42
    Episode_Reward/reaching_object: 1.9575
     Episode_Reward/lifting_object: 175.3223
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 161513472
                    Iteration time: 2.26s
                      Time elapsed: 01:04:03
                               ETA: 00:33:27

################################################################################
                     [1m Learning iteration 1643/2500 [0m                     

                       Computation: 43361 steps/s (collection: 2.148s, learning 0.119s)
             Mean action noise std: 3.30
          Mean value_function loss: 142.3589
               Mean surrogate loss: 0.0048
                 Mean entropy loss: 79.8279
                       Mean reward: 873.25
               Mean episode length: 230.82
    Episode_Reward/reaching_object: 1.9494
     Episode_Reward/lifting_object: 174.8595
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0751
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 12.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 161611776
                    Iteration time: 2.27s
                      Time elapsed: 01:04:05
                               ETA: 00:33:24

################################################################################
                     [1m Learning iteration 1644/2500 [0m                     

                       Computation: 43964 steps/s (collection: 2.127s, learning 0.109s)
             Mean action noise std: 3.30
          Mean value_function loss: 162.4134
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 79.8293
                       Mean reward: 852.47
               Mean episode length: 224.55
    Episode_Reward/reaching_object: 1.9325
     Episode_Reward/lifting_object: 173.4953
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0749
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2917
--------------------------------------------------------------------------------
                   Total timesteps: 161710080
                    Iteration time: 2.24s
                      Time elapsed: 01:04:08
                               ETA: 00:33:22

################################################################################
                     [1m Learning iteration 1645/2500 [0m                     

                       Computation: 43821 steps/s (collection: 2.127s, learning 0.116s)
             Mean action noise std: 3.30
          Mean value_function loss: 168.3512
               Mean surrogate loss: 0.0021
                 Mean entropy loss: 79.8305
                       Mean reward: 894.96
               Mean episode length: 235.34
    Episode_Reward/reaching_object: 1.9231
     Episode_Reward/lifting_object: 173.0802
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0746
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 13.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.4167
--------------------------------------------------------------------------------
                   Total timesteps: 161808384
                    Iteration time: 2.24s
                      Time elapsed: 01:04:10
                               ETA: 00:33:20

################################################################################
                     [1m Learning iteration 1646/2500 [0m                     

                       Computation: 43733 steps/s (collection: 2.126s, learning 0.122s)
             Mean action noise std: 3.30
          Mean value_function loss: 226.2879
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 79.8330
                       Mean reward: 831.02
               Mean episode length: 222.23
    Episode_Reward/reaching_object: 1.8650
     Episode_Reward/lifting_object: 168.0283
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0730
          Episode_Reward/joint_vel: -0.0570
      Episode_Termination/time_out: 13.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.5417
--------------------------------------------------------------------------------
                   Total timesteps: 161906688
                    Iteration time: 2.25s
                      Time elapsed: 01:04:12
                               ETA: 00:33:17

################################################################################
                     [1m Learning iteration 1647/2500 [0m                     

                       Computation: 43742 steps/s (collection: 2.131s, learning 0.117s)
             Mean action noise std: 3.30
          Mean value_function loss: 200.9297
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 79.8389
                       Mean reward: 821.20
               Mean episode length: 219.96
    Episode_Reward/reaching_object: 1.8364
     Episode_Reward/lifting_object: 165.7002
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0722
          Episode_Reward/joint_vel: -0.0568
      Episode_Termination/time_out: 14.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 4.4583
--------------------------------------------------------------------------------
                   Total timesteps: 162004992
                    Iteration time: 2.25s
                      Time elapsed: 01:04:14
                               ETA: 00:33:15

################################################################################
                     [1m Learning iteration 1648/2500 [0m                     

                       Computation: 43127 steps/s (collection: 2.162s, learning 0.118s)
             Mean action noise std: 3.30
          Mean value_function loss: 186.8622
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 79.8440
                       Mean reward: 788.13
               Mean episode length: 211.90
    Episode_Reward/reaching_object: 1.8029
     Episode_Reward/lifting_object: 162.9105
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0714
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 4.1250
--------------------------------------------------------------------------------
                   Total timesteps: 162103296
                    Iteration time: 2.28s
                      Time elapsed: 01:04:17
                               ETA: 00:33:12

################################################################################
                     [1m Learning iteration 1649/2500 [0m                     

                       Computation: 44069 steps/s (collection: 2.137s, learning 0.094s)
             Mean action noise std: 3.31
          Mean value_function loss: 178.4554
               Mean surrogate loss: 0.0025
                 Mean entropy loss: 79.8516
                       Mean reward: 869.60
               Mean episode length: 229.49
    Episode_Reward/reaching_object: 1.9026
     Episode_Reward/lifting_object: 172.6645
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0749
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.3333
--------------------------------------------------------------------------------
                   Total timesteps: 162201600
                    Iteration time: 2.23s
                      Time elapsed: 01:04:19
                               ETA: 00:33:10

################################################################################
                     [1m Learning iteration 1650/2500 [0m                     

                       Computation: 43617 steps/s (collection: 2.124s, learning 0.130s)
             Mean action noise std: 3.31
          Mean value_function loss: 148.4234
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 79.8585
                       Mean reward: 897.50
               Mean episode length: 238.03
    Episode_Reward/reaching_object: 1.9159
     Episode_Reward/lifting_object: 173.6860
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0752
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 16.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 162299904
                    Iteration time: 2.25s
                      Time elapsed: 01:04:21
                               ETA: 00:33:08

################################################################################
                     [1m Learning iteration 1651/2500 [0m                     

                       Computation: 43484 steps/s (collection: 2.136s, learning 0.125s)
             Mean action noise std: 3.31
          Mean value_function loss: 148.6411
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 79.8765
                       Mean reward: 861.78
               Mean episode length: 229.27
    Episode_Reward/reaching_object: 1.9164
     Episode_Reward/lifting_object: 173.4197
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0754
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.9167
--------------------------------------------------------------------------------
                   Total timesteps: 162398208
                    Iteration time: 2.26s
                      Time elapsed: 01:04:23
                               ETA: 00:33:05

################################################################################
                     [1m Learning iteration 1652/2500 [0m                     

                       Computation: 43132 steps/s (collection: 2.167s, learning 0.113s)
             Mean action noise std: 3.31
          Mean value_function loss: 131.6800
               Mean surrogate loss: 0.0033
                 Mean entropy loss: 79.8970
                       Mean reward: 888.23
               Mean episode length: 236.29
    Episode_Reward/reaching_object: 1.9647
     Episode_Reward/lifting_object: 178.0923
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 162496512
                    Iteration time: 2.28s
                      Time elapsed: 01:04:26
                               ETA: 00:33:03

################################################################################
                     [1m Learning iteration 1653/2500 [0m                     

                       Computation: 43297 steps/s (collection: 2.146s, learning 0.125s)
             Mean action noise std: 3.31
          Mean value_function loss: 155.7015
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 79.9022
                       Mean reward: 861.40
               Mean episode length: 227.97
    Episode_Reward/reaching_object: 1.8894
     Episode_Reward/lifting_object: 170.8104
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0744
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 12.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162594816
                    Iteration time: 2.27s
                      Time elapsed: 01:04:28
                               ETA: 00:33:01

################################################################################
                     [1m Learning iteration 1654/2500 [0m                     

                       Computation: 44542 steps/s (collection: 2.109s, learning 0.098s)
             Mean action noise std: 3.31
          Mean value_function loss: 131.1513
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 79.9151
                       Mean reward: 907.07
               Mean episode length: 238.57
    Episode_Reward/reaching_object: 1.9436
     Episode_Reward/lifting_object: 175.2833
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0764
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 14.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 162693120
                    Iteration time: 2.21s
                      Time elapsed: 01:04:30
                               ETA: 00:32:58

################################################################################
                     [1m Learning iteration 1655/2500 [0m                     

                       Computation: 43824 steps/s (collection: 2.116s, learning 0.128s)
             Mean action noise std: 3.32
          Mean value_function loss: 115.7442
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 79.9292
                       Mean reward: 905.40
               Mean episode length: 239.47
    Episode_Reward/reaching_object: 1.9692
     Episode_Reward/lifting_object: 177.5296
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0771
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 162791424
                    Iteration time: 2.24s
                      Time elapsed: 01:04:32
                               ETA: 00:32:56

################################################################################
                     [1m Learning iteration 1656/2500 [0m                     

                       Computation: 43903 steps/s (collection: 2.119s, learning 0.120s)
             Mean action noise std: 3.32
          Mean value_function loss: 121.5944
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 79.9437
                       Mean reward: 885.60
               Mean episode length: 236.16
    Episode_Reward/reaching_object: 1.9418
     Episode_Reward/lifting_object: 174.2017
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0761
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 13.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 162889728
                    Iteration time: 2.24s
                      Time elapsed: 01:04:35
                               ETA: 00:32:53

################################################################################
                     [1m Learning iteration 1657/2500 [0m                     

                       Computation: 44421 steps/s (collection: 2.103s, learning 0.110s)
             Mean action noise std: 3.32
          Mean value_function loss: 176.0535
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 79.9565
                       Mean reward: 857.27
               Mean episode length: 227.43
    Episode_Reward/reaching_object: 1.9565
     Episode_Reward/lifting_object: 175.1472
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0764
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2083
--------------------------------------------------------------------------------
                   Total timesteps: 162988032
                    Iteration time: 2.21s
                      Time elapsed: 01:04:37
                               ETA: 00:32:51

################################################################################
                     [1m Learning iteration 1658/2500 [0m                     

                       Computation: 43751 steps/s (collection: 2.129s, learning 0.118s)
             Mean action noise std: 3.32
          Mean value_function loss: 161.3406
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 79.9729
                       Mean reward: 871.31
               Mean episode length: 231.41
    Episode_Reward/reaching_object: 1.9699
     Episode_Reward/lifting_object: 176.1572
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0766
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.0417
--------------------------------------------------------------------------------
                   Total timesteps: 163086336
                    Iteration time: 2.25s
                      Time elapsed: 01:04:39
                               ETA: 00:32:49

################################################################################
                     [1m Learning iteration 1659/2500 [0m                     

                       Computation: 43900 steps/s (collection: 2.111s, learning 0.129s)
             Mean action noise std: 3.32
          Mean value_function loss: 132.9790
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 79.9898
                       Mean reward: 865.53
               Mean episode length: 228.31
    Episode_Reward/reaching_object: 1.9822
     Episode_Reward/lifting_object: 177.4640
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0769
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 163184640
                    Iteration time: 2.24s
                      Time elapsed: 01:04:41
                               ETA: 00:32:46

################################################################################
                     [1m Learning iteration 1660/2500 [0m                     

                       Computation: 43487 steps/s (collection: 2.137s, learning 0.123s)
             Mean action noise std: 3.33
          Mean value_function loss: 125.9128
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 80.0039
                       Mean reward: 901.47
               Mean episode length: 239.03
    Episode_Reward/reaching_object: 2.0021
     Episode_Reward/lifting_object: 178.4997
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0780
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 16.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 163282944
                    Iteration time: 2.26s
                      Time elapsed: 01:04:44
                               ETA: 00:32:44

################################################################################
                     [1m Learning iteration 1661/2500 [0m                     

                       Computation: 43470 steps/s (collection: 2.147s, learning 0.115s)
             Mean action noise std: 3.33
          Mean value_function loss: 122.2774
               Mean surrogate loss: 0.0023
                 Mean entropy loss: 80.0253
                       Mean reward: 900.29
               Mean episode length: 237.03
    Episode_Reward/reaching_object: 1.9989
     Episode_Reward/lifting_object: 178.8741
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0781
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 163381248
                    Iteration time: 2.26s
                      Time elapsed: 01:04:46
                               ETA: 00:32:41

################################################################################
                     [1m Learning iteration 1662/2500 [0m                     

                       Computation: 44042 steps/s (collection: 2.115s, learning 0.117s)
             Mean action noise std: 3.33
          Mean value_function loss: 117.9295
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 80.0304
                       Mean reward: 899.70
               Mean episode length: 237.83
    Episode_Reward/reaching_object: 1.9869
     Episode_Reward/lifting_object: 177.5278
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 163479552
                    Iteration time: 2.23s
                      Time elapsed: 01:04:48
                               ETA: 00:32:39

################################################################################
                     [1m Learning iteration 1663/2500 [0m                     

                       Computation: 43694 steps/s (collection: 2.128s, learning 0.122s)
             Mean action noise std: 3.33
          Mean value_function loss: 122.2345
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 80.0362
                       Mean reward: 867.41
               Mean episode length: 229.97
    Episode_Reward/reaching_object: 1.9449
     Episode_Reward/lifting_object: 173.9925
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0763
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 163577856
                    Iteration time: 2.25s
                      Time elapsed: 01:04:50
                               ETA: 00:32:37

################################################################################
                     [1m Learning iteration 1664/2500 [0m                     

                       Computation: 43733 steps/s (collection: 2.121s, learning 0.127s)
             Mean action noise std: 3.33
          Mean value_function loss: 109.4763
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 80.0470
                       Mean reward: 895.59
               Mean episode length: 237.73
    Episode_Reward/reaching_object: 1.9486
     Episode_Reward/lifting_object: 174.1882
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0767
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 13.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 163676160
                    Iteration time: 2.25s
                      Time elapsed: 01:04:53
                               ETA: 00:32:34

################################################################################
                     [1m Learning iteration 1665/2500 [0m                     

                       Computation: 43645 steps/s (collection: 2.147s, learning 0.106s)
             Mean action noise std: 3.33
          Mean value_function loss: 110.6550
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 80.0553
                       Mean reward: 926.47
               Mean episode length: 243.93
    Episode_Reward/reaching_object: 1.9746
     Episode_Reward/lifting_object: 176.7898
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0777
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 163774464
                    Iteration time: 2.25s
                      Time elapsed: 01:04:55
                               ETA: 00:32:32

################################################################################
                     [1m Learning iteration 1666/2500 [0m                     

                       Computation: 26288 steps/s (collection: 3.622s, learning 0.118s)
             Mean action noise std: 3.33
          Mean value_function loss: 99.5527
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 80.0623
                       Mean reward: 915.65
               Mean episode length: 241.71
    Episode_Reward/reaching_object: 1.9886
     Episode_Reward/lifting_object: 178.5634
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0782
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 163872768
                    Iteration time: 3.74s
                      Time elapsed: 01:04:59
                               ETA: 00:32:30

################################################################################
                     [1m Learning iteration 1667/2500 [0m                     

                       Computation: 14568 steps/s (collection: 6.622s, learning 0.125s)
             Mean action noise std: 3.33
          Mean value_function loss: 111.3570
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 80.0746
                       Mean reward: 894.02
               Mean episode length: 235.98
    Episode_Reward/reaching_object: 1.9775
     Episode_Reward/lifting_object: 177.1092
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0780
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 163971072
                    Iteration time: 6.75s
                      Time elapsed: 01:05:05
                               ETA: 00:32:30

################################################################################
                     [1m Learning iteration 1668/2500 [0m                     

                       Computation: 14238 steps/s (collection: 6.776s, learning 0.129s)
             Mean action noise std: 3.34
          Mean value_function loss: 133.3017
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 80.0912
                       Mean reward: 925.67
               Mean episode length: 245.46
    Episode_Reward/reaching_object: 1.9871
     Episode_Reward/lifting_object: 178.1002
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0785
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 164069376
                    Iteration time: 6.90s
                      Time elapsed: 01:05:12
                               ETA: 00:32:30

################################################################################
                     [1m Learning iteration 1669/2500 [0m                     

                       Computation: 14444 steps/s (collection: 6.688s, learning 0.118s)
             Mean action noise std: 3.34
          Mean value_function loss: 113.9554
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 80.1072
                       Mean reward: 881.18
               Mean episode length: 233.98
    Episode_Reward/reaching_object: 1.9798
     Episode_Reward/lifting_object: 176.9965
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0783
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 164167680
                    Iteration time: 6.81s
                      Time elapsed: 01:05:19
                               ETA: 00:32:30

################################################################################
                     [1m Learning iteration 1670/2500 [0m                     

                       Computation: 14174 steps/s (collection: 6.825s, learning 0.111s)
             Mean action noise std: 3.34
          Mean value_function loss: 91.8487
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 80.1137
                       Mean reward: 891.20
               Mean episode length: 236.38
    Episode_Reward/reaching_object: 1.9870
     Episode_Reward/lifting_object: 178.0103
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0786
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 164265984
                    Iteration time: 6.94s
                      Time elapsed: 01:05:26
                               ETA: 00:32:30

################################################################################
                     [1m Learning iteration 1671/2500 [0m                     

                       Computation: 14555 steps/s (collection: 6.633s, learning 0.121s)
             Mean action noise std: 3.34
          Mean value_function loss: 110.8388
               Mean surrogate loss: 0.0050
                 Mean entropy loss: 80.1269
                       Mean reward: 896.28
               Mean episode length: 237.36
    Episode_Reward/reaching_object: 2.0013
     Episode_Reward/lifting_object: 179.6589
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0792
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 164364288
                    Iteration time: 6.75s
                      Time elapsed: 01:05:33
                               ETA: 00:32:30

################################################################################
                     [1m Learning iteration 1672/2500 [0m                     

                       Computation: 14393 steps/s (collection: 6.698s, learning 0.132s)
             Mean action noise std: 3.34
          Mean value_function loss: 108.3604
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 80.1351
                       Mean reward: 918.23
               Mean episode length: 242.26
    Episode_Reward/reaching_object: 1.9813
     Episode_Reward/lifting_object: 177.9492
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0784
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 164462592
                    Iteration time: 6.83s
                      Time elapsed: 01:05:40
                               ETA: 00:32:30

################################################################################
                     [1m Learning iteration 1673/2500 [0m                     

                       Computation: 14287 steps/s (collection: 6.754s, learning 0.126s)
             Mean action noise std: 3.34
          Mean value_function loss: 130.3502
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 80.1400
                       Mean reward: 911.85
               Mean episode length: 239.87
    Episode_Reward/reaching_object: 1.9631
     Episode_Reward/lifting_object: 176.1068
      Episode_Reward/object_height: 0.0151
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 164560896
                    Iteration time: 6.88s
                      Time elapsed: 01:05:46
                               ETA: 00:32:29

################################################################################
                     [1m Learning iteration 1674/2500 [0m                     

                       Computation: 14386 steps/s (collection: 6.712s, learning 0.121s)
             Mean action noise std: 3.34
          Mean value_function loss: 148.6750
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 80.1540
                       Mean reward: 870.52
               Mean episode length: 230.65
    Episode_Reward/reaching_object: 1.9496
     Episode_Reward/lifting_object: 174.6170
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0773
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6667
--------------------------------------------------------------------------------
                   Total timesteps: 164659200
                    Iteration time: 6.83s
                      Time elapsed: 01:05:53
                               ETA: 00:32:29

################################################################################
                     [1m Learning iteration 1675/2500 [0m                     

                       Computation: 24365 steps/s (collection: 3.924s, learning 0.111s)
             Mean action noise std: 3.34
          Mean value_function loss: 143.0501
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 80.1609
                       Mean reward: 844.54
               Mean episode length: 224.75
    Episode_Reward/reaching_object: 1.9464
     Episode_Reward/lifting_object: 174.7092
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0772
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.5833
--------------------------------------------------------------------------------
                   Total timesteps: 164757504
                    Iteration time: 4.03s
                      Time elapsed: 01:05:57
                               ETA: 00:32:28

################################################################################
                     [1m Learning iteration 1676/2500 [0m                     

                       Computation: 44582 steps/s (collection: 2.095s, learning 0.110s)
             Mean action noise std: 3.34
          Mean value_function loss: 137.8770
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 80.1696
                       Mean reward: 876.82
               Mean episode length: 233.37
    Episode_Reward/reaching_object: 1.9548
     Episode_Reward/lifting_object: 175.1148
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 164855808
                    Iteration time: 2.20s
                      Time elapsed: 01:06:00
                               ETA: 00:32:25

################################################################################
                     [1m Learning iteration 1677/2500 [0m                     

                       Computation: 45443 steps/s (collection: 2.044s, learning 0.120s)
             Mean action noise std: 3.35
          Mean value_function loss: 190.2047
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 80.1871
                       Mean reward: 896.26
               Mean episode length: 236.05
    Episode_Reward/reaching_object: 1.9245
     Episode_Reward/lifting_object: 173.4898
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0766
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5417
--------------------------------------------------------------------------------
                   Total timesteps: 164954112
                    Iteration time: 2.16s
                      Time elapsed: 01:06:02
                               ETA: 00:32:23

################################################################################
                     [1m Learning iteration 1678/2500 [0m                     

                       Computation: 43924 steps/s (collection: 2.106s, learning 0.133s)
             Mean action noise std: 3.35
          Mean value_function loss: 162.1323
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 80.1999
                       Mean reward: 917.23
               Mean episode length: 240.76
    Episode_Reward/reaching_object: 1.9647
     Episode_Reward/lifting_object: 177.6439
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0778
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.2500
--------------------------------------------------------------------------------
                   Total timesteps: 165052416
                    Iteration time: 2.24s
                      Time elapsed: 01:06:04
                               ETA: 00:32:20

################################################################################
                     [1m Learning iteration 1679/2500 [0m                     

                       Computation: 45033 steps/s (collection: 2.077s, learning 0.106s)
             Mean action noise std: 3.35
          Mean value_function loss: 165.4987
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 80.2081
                       Mean reward: 898.91
               Mean episode length: 236.68
    Episode_Reward/reaching_object: 1.9209
     Episode_Reward/lifting_object: 173.4280
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0766
          Episode_Reward/joint_vel: -0.0572
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 3.5000
--------------------------------------------------------------------------------
                   Total timesteps: 165150720
                    Iteration time: 2.18s
                      Time elapsed: 01:06:06
                               ETA: 00:32:18

################################################################################
                     [1m Learning iteration 1680/2500 [0m                     

                       Computation: 44211 steps/s (collection: 2.123s, learning 0.100s)
             Mean action noise std: 3.35
          Mean value_function loss: 145.8816
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 80.2218
                       Mean reward: 916.25
               Mean episode length: 241.30
    Episode_Reward/reaching_object: 1.9554
     Episode_Reward/lifting_object: 176.5823
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.8333
--------------------------------------------------------------------------------
                   Total timesteps: 165249024
                    Iteration time: 2.22s
                      Time elapsed: 01:06:08
                               ETA: 00:32:16

################################################################################
                     [1m Learning iteration 1681/2500 [0m                     

                       Computation: 44689 steps/s (collection: 2.089s, learning 0.111s)
             Mean action noise std: 3.35
          Mean value_function loss: 141.9056
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 80.2314
                       Mean reward: 889.63
               Mean episode length: 234.58
    Episode_Reward/reaching_object: 1.9810
     Episode_Reward/lifting_object: 179.1864
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0787
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 165347328
                    Iteration time: 2.20s
                      Time elapsed: 01:06:11
                               ETA: 00:32:13

################################################################################
                     [1m Learning iteration 1682/2500 [0m                     

                       Computation: 45095 steps/s (collection: 2.059s, learning 0.121s)
             Mean action noise std: 3.35
          Mean value_function loss: 131.0551
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 80.2368
                       Mean reward: 894.78
               Mean episode length: 235.68
    Episode_Reward/reaching_object: 1.9529
     Episode_Reward/lifting_object: 176.6379
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0780
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 165445632
                    Iteration time: 2.18s
                      Time elapsed: 01:06:13
                               ETA: 00:32:11

################################################################################
                     [1m Learning iteration 1683/2500 [0m                     

                       Computation: 41020 steps/s (collection: 2.303s, learning 0.094s)
             Mean action noise std: 3.35
          Mean value_function loss: 144.0108
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 80.2398
                       Mean reward: 884.28
               Mean episode length: 232.46
    Episode_Reward/reaching_object: 1.9300
     Episode_Reward/lifting_object: 174.6979
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0771
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 165543936
                    Iteration time: 2.40s
                      Time elapsed: 01:06:15
                               ETA: 00:32:08

################################################################################
                     [1m Learning iteration 1684/2500 [0m                     

                       Computation: 42092 steps/s (collection: 2.178s, learning 0.157s)
             Mean action noise std: 3.35
          Mean value_function loss: 124.7113
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 80.2430
                       Mean reward: 882.21
               Mean episode length: 233.34
    Episode_Reward/reaching_object: 1.9474
     Episode_Reward/lifting_object: 175.3906
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5000
--------------------------------------------------------------------------------
                   Total timesteps: 165642240
                    Iteration time: 2.34s
                      Time elapsed: 01:06:17
                               ETA: 00:32:06

################################################################################
                     [1m Learning iteration 1685/2500 [0m                     

                       Computation: 45425 steps/s (collection: 2.070s, learning 0.094s)
             Mean action noise std: 3.36
          Mean value_function loss: 125.7619
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 80.2567
                       Mean reward: 880.77
               Mean episode length: 231.33
    Episode_Reward/reaching_object: 1.9572
     Episode_Reward/lifting_object: 176.6714
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 165740544
                    Iteration time: 2.16s
                      Time elapsed: 01:06:20
                               ETA: 00:32:03

################################################################################
                     [1m Learning iteration 1686/2500 [0m                     

                       Computation: 44670 steps/s (collection: 2.101s, learning 0.100s)
             Mean action noise std: 3.36
          Mean value_function loss: 134.4695
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 80.2816
                       Mean reward: 917.68
               Mean episode length: 240.57
    Episode_Reward/reaching_object: 1.9716
     Episode_Reward/lifting_object: 177.6913
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0785
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 165838848
                    Iteration time: 2.20s
                      Time elapsed: 01:06:22
                               ETA: 00:32:01

################################################################################
                     [1m Learning iteration 1687/2500 [0m                     

                       Computation: 42709 steps/s (collection: 2.135s, learning 0.167s)
             Mean action noise std: 3.36
          Mean value_function loss: 122.0079
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 80.2949
                       Mean reward: 901.04
               Mean episode length: 237.61
    Episode_Reward/reaching_object: 2.0123
     Episode_Reward/lifting_object: 180.9389
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0803
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 165937152
                    Iteration time: 2.30s
                      Time elapsed: 01:06:24
                               ETA: 00:31:59

################################################################################
                     [1m Learning iteration 1688/2500 [0m                     

                       Computation: 45147 steps/s (collection: 2.070s, learning 0.107s)
             Mean action noise std: 3.36
          Mean value_function loss: 107.2877
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 80.3023
                       Mean reward: 904.23
               Mean episode length: 238.30
    Episode_Reward/reaching_object: 2.0119
     Episode_Reward/lifting_object: 181.0516
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0801
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 166035456
                    Iteration time: 2.18s
                      Time elapsed: 01:06:26
                               ETA: 00:31:56

################################################################################
                     [1m Learning iteration 1689/2500 [0m                     

                       Computation: 45331 steps/s (collection: 2.064s, learning 0.105s)
             Mean action noise std: 3.36
          Mean value_function loss: 129.8952
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 80.3071
                       Mean reward: 899.01
               Mean episode length: 239.16
    Episode_Reward/reaching_object: 1.9967
     Episode_Reward/lifting_object: 178.6475
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 166133760
                    Iteration time: 2.17s
                      Time elapsed: 01:06:28
                               ETA: 00:31:54

################################################################################
                     [1m Learning iteration 1690/2500 [0m                     

                       Computation: 45523 steps/s (collection: 2.039s, learning 0.121s)
             Mean action noise std: 3.36
          Mean value_function loss: 125.1356
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 80.3107
                       Mean reward: 894.76
               Mean episode length: 236.79
    Episode_Reward/reaching_object: 1.9852
     Episode_Reward/lifting_object: 177.8410
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0790
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 166232064
                    Iteration time: 2.16s
                      Time elapsed: 01:06:31
                               ETA: 00:31:51

################################################################################
                     [1m Learning iteration 1691/2500 [0m                     

                       Computation: 43604 steps/s (collection: 2.121s, learning 0.133s)
             Mean action noise std: 3.36
          Mean value_function loss: 122.9472
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 80.3154
                       Mean reward: 910.63
               Mean episode length: 239.41
    Episode_Reward/reaching_object: 1.9943
     Episode_Reward/lifting_object: 178.5829
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0794
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 166330368
                    Iteration time: 2.25s
                      Time elapsed: 01:06:33
                               ETA: 00:31:49

################################################################################
                     [1m Learning iteration 1692/2500 [0m                     

                       Computation: 45030 steps/s (collection: 2.060s, learning 0.123s)
             Mean action noise std: 3.36
          Mean value_function loss: 122.3729
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 80.3338
                       Mean reward: 846.08
               Mean episode length: 226.33
    Episode_Reward/reaching_object: 1.9870
     Episode_Reward/lifting_object: 177.5421
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0795
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 166428672
                    Iteration time: 2.18s
                      Time elapsed: 01:06:35
                               ETA: 00:31:46

################################################################################
                     [1m Learning iteration 1693/2500 [0m                     

                       Computation: 43160 steps/s (collection: 2.153s, learning 0.125s)
             Mean action noise std: 3.37
          Mean value_function loss: 115.4132
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 80.3582
                       Mean reward: 871.29
               Mean episode length: 230.61
    Episode_Reward/reaching_object: 1.9761
     Episode_Reward/lifting_object: 176.9862
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0789
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 166526976
                    Iteration time: 2.28s
                      Time elapsed: 01:06:37
                               ETA: 00:31:44

################################################################################
                     [1m Learning iteration 1694/2500 [0m                     

                       Computation: 42937 steps/s (collection: 2.180s, learning 0.109s)
             Mean action noise std: 3.37
          Mean value_function loss: 108.3313
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 80.3660
                       Mean reward: 902.88
               Mean episode length: 238.63
    Episode_Reward/reaching_object: 1.9832
     Episode_Reward/lifting_object: 177.9366
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0793
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 166625280
                    Iteration time: 2.29s
                      Time elapsed: 01:06:40
                               ETA: 00:31:42

################################################################################
                     [1m Learning iteration 1695/2500 [0m                     

                       Computation: 45303 steps/s (collection: 2.074s, learning 0.096s)
             Mean action noise std: 3.37
          Mean value_function loss: 123.8768
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 80.3708
                       Mean reward: 907.86
               Mean episode length: 238.30
    Episode_Reward/reaching_object: 1.9912
     Episode_Reward/lifting_object: 179.0873
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0795
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 166723584
                    Iteration time: 2.17s
                      Time elapsed: 01:06:42
                               ETA: 00:31:39

################################################################################
                     [1m Learning iteration 1696/2500 [0m                     

                       Computation: 45333 steps/s (collection: 2.078s, learning 0.091s)
             Mean action noise std: 3.37
          Mean value_function loss: 130.2892
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 80.3849
                       Mean reward: 897.42
               Mean episode length: 236.51
    Episode_Reward/reaching_object: 1.9653
     Episode_Reward/lifting_object: 176.1080
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0787
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 166821888
                    Iteration time: 2.17s
                      Time elapsed: 01:06:44
                               ETA: 00:31:37

################################################################################
                     [1m Learning iteration 1697/2500 [0m                     

                       Computation: 43534 steps/s (collection: 2.139s, learning 0.119s)
             Mean action noise std: 3.37
          Mean value_function loss: 140.9769
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 80.4017
                       Mean reward: 882.09
               Mean episode length: 234.61
    Episode_Reward/reaching_object: 1.9446
     Episode_Reward/lifting_object: 174.2050
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0779
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 166920192
                    Iteration time: 2.26s
                      Time elapsed: 01:06:46
                               ETA: 00:31:34

################################################################################
                     [1m Learning iteration 1698/2500 [0m                     

                       Computation: 44555 steps/s (collection: 2.105s, learning 0.102s)
             Mean action noise std: 3.37
          Mean value_function loss: 84.0026
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 80.4117
                       Mean reward: 911.28
               Mean episode length: 239.64
    Episode_Reward/reaching_object: 2.0077
     Episode_Reward/lifting_object: 180.2242
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0801
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 167018496
                    Iteration time: 2.21s
                      Time elapsed: 01:06:48
                               ETA: 00:31:32

################################################################################
                     [1m Learning iteration 1699/2500 [0m                     

                       Computation: 44968 steps/s (collection: 2.075s, learning 0.111s)
             Mean action noise std: 3.37
          Mean value_function loss: 105.6029
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 80.4240
                       Mean reward: 903.99
               Mean episode length: 238.71
    Episode_Reward/reaching_object: 1.9942
     Episode_Reward/lifting_object: 178.4057
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0795
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 167116800
                    Iteration time: 2.19s
                      Time elapsed: 01:06:51
                               ETA: 00:31:29

################################################################################
                     [1m Learning iteration 1700/2500 [0m                     

                       Computation: 44428 steps/s (collection: 2.098s, learning 0.115s)
             Mean action noise std: 3.38
          Mean value_function loss: 96.6802
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 80.4356
                       Mean reward: 919.75
               Mean episode length: 241.49
    Episode_Reward/reaching_object: 2.0056
     Episode_Reward/lifting_object: 179.2020
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0798
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 167215104
                    Iteration time: 2.21s
                      Time elapsed: 01:06:53
                               ETA: 00:31:27

################################################################################
                     [1m Learning iteration 1701/2500 [0m                     

                       Computation: 42771 steps/s (collection: 2.146s, learning 0.153s)
             Mean action noise std: 3.38
          Mean value_function loss: 92.7902
               Mean surrogate loss: -0.0039
                 Mean entropy loss: 80.4473
                       Mean reward: 901.05
               Mean episode length: 238.23
    Episode_Reward/reaching_object: 2.0236
     Episode_Reward/lifting_object: 180.7601
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0806
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 167313408
                    Iteration time: 2.30s
                      Time elapsed: 01:06:55
                               ETA: 00:31:25

################################################################################
                     [1m Learning iteration 1702/2500 [0m                     

                       Computation: 44636 steps/s (collection: 2.107s, learning 0.095s)
             Mean action noise std: 3.38
          Mean value_function loss: 84.6534
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 80.4688
                       Mean reward: 924.58
               Mean episode length: 243.42
    Episode_Reward/reaching_object: 2.0531
     Episode_Reward/lifting_object: 183.8478
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0817
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 167411712
                    Iteration time: 2.20s
                      Time elapsed: 01:06:57
                               ETA: 00:31:22

################################################################################
                     [1m Learning iteration 1703/2500 [0m                     

                       Computation: 44801 steps/s (collection: 2.107s, learning 0.087s)
             Mean action noise std: 3.38
          Mean value_function loss: 97.1289
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 80.4794
                       Mean reward: 932.22
               Mean episode length: 244.24
    Episode_Reward/reaching_object: 2.0195
     Episode_Reward/lifting_object: 180.6763
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0805
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 167510016
                    Iteration time: 2.19s
                      Time elapsed: 01:07:00
                               ETA: 00:31:20

################################################################################
                     [1m Learning iteration 1704/2500 [0m                     

                       Computation: 45191 steps/s (collection: 2.074s, learning 0.101s)
             Mean action noise std: 3.38
          Mean value_function loss: 123.8613
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 80.4885
                       Mean reward: 892.92
               Mean episode length: 234.04
    Episode_Reward/reaching_object: 1.9952
     Episode_Reward/lifting_object: 178.4641
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0796
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 167608320
                    Iteration time: 2.18s
                      Time elapsed: 01:07:02
                               ETA: 00:31:17

################################################################################
                     [1m Learning iteration 1705/2500 [0m                     

                       Computation: 41309 steps/s (collection: 2.198s, learning 0.182s)
             Mean action noise std: 3.38
          Mean value_function loss: 117.6613
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 80.5039
                       Mean reward: 884.52
               Mean episode length: 233.05
    Episode_Reward/reaching_object: 2.0318
     Episode_Reward/lifting_object: 181.6070
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0812
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 167706624
                    Iteration time: 2.38s
                      Time elapsed: 01:07:04
                               ETA: 00:31:15

################################################################################
                     [1m Learning iteration 1706/2500 [0m                     

                       Computation: 45290 steps/s (collection: 2.076s, learning 0.094s)
             Mean action noise std: 3.38
          Mean value_function loss: 92.5369
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 80.5082
                       Mean reward: 924.73
               Mean episode length: 242.13
    Episode_Reward/reaching_object: 2.0325
     Episode_Reward/lifting_object: 182.0140
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0811
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 167804928
                    Iteration time: 2.17s
                      Time elapsed: 01:07:06
                               ETA: 00:31:13

################################################################################
                     [1m Learning iteration 1707/2500 [0m                     

                       Computation: 44937 steps/s (collection: 2.066s, learning 0.121s)
             Mean action noise std: 3.39
          Mean value_function loss: 122.0570
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 80.5132
                       Mean reward: 904.47
               Mean episode length: 238.86
    Episode_Reward/reaching_object: 1.9930
     Episode_Reward/lifting_object: 178.5612
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0801
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 167903232
                    Iteration time: 2.19s
                      Time elapsed: 01:07:08
                               ETA: 00:31:10

################################################################################
                     [1m Learning iteration 1708/2500 [0m                     

                       Computation: 44751 steps/s (collection: 2.083s, learning 0.114s)
             Mean action noise std: 3.39
          Mean value_function loss: 109.1987
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 80.5268
                       Mean reward: 909.68
               Mean episode length: 239.12
    Episode_Reward/reaching_object: 1.9611
     Episode_Reward/lifting_object: 175.8866
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0794
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 168001536
                    Iteration time: 2.20s
                      Time elapsed: 01:07:11
                               ETA: 00:31:08

################################################################################
                     [1m Learning iteration 1709/2500 [0m                     

                       Computation: 44627 steps/s (collection: 2.066s, learning 0.137s)
             Mean action noise std: 3.39
          Mean value_function loss: 123.2232
               Mean surrogate loss: 0.0012
                 Mean entropy loss: 80.5315
                       Mean reward: 865.19
               Mean episode length: 229.68
    Episode_Reward/reaching_object: 1.9639
     Episode_Reward/lifting_object: 176.7455
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0795
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 168099840
                    Iteration time: 2.20s
                      Time elapsed: 01:07:13
                               ETA: 00:31:05

################################################################################
                     [1m Learning iteration 1710/2500 [0m                     

                       Computation: 44936 steps/s (collection: 2.085s, learning 0.103s)
             Mean action noise std: 3.39
          Mean value_function loss: 108.7989
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 80.5334
                       Mean reward: 927.55
               Mean episode length: 243.55
    Episode_Reward/reaching_object: 1.9951
     Episode_Reward/lifting_object: 180.2604
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0812
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 168198144
                    Iteration time: 2.19s
                      Time elapsed: 01:07:15
                               ETA: 00:31:03

################################################################################
                     [1m Learning iteration 1711/2500 [0m                     

                       Computation: 43550 steps/s (collection: 2.165s, learning 0.092s)
             Mean action noise std: 3.39
          Mean value_function loss: 96.0533
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 80.5360
                       Mean reward: 912.68
               Mean episode length: 239.82
    Episode_Reward/reaching_object: 1.9949
     Episode_Reward/lifting_object: 180.8846
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0811
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 16.3750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 168296448
                    Iteration time: 2.26s
                      Time elapsed: 01:07:17
                               ETA: 00:31:00

################################################################################
                     [1m Learning iteration 1712/2500 [0m                     

                       Computation: 44520 steps/s (collection: 2.116s, learning 0.093s)
             Mean action noise std: 3.39
          Mean value_function loss: 119.6047
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 80.5398
                       Mean reward: 892.22
               Mean episode length: 234.92
    Episode_Reward/reaching_object: 1.9745
     Episode_Reward/lifting_object: 179.0863
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0804
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 168394752
                    Iteration time: 2.21s
                      Time elapsed: 01:07:19
                               ETA: 00:30:58

################################################################################
                     [1m Learning iteration 1713/2500 [0m                     

                       Computation: 42364 steps/s (collection: 2.221s, learning 0.100s)
             Mean action noise std: 3.39
          Mean value_function loss: 110.0846
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 80.5448
                       Mean reward: 913.74
               Mean episode length: 240.26
    Episode_Reward/reaching_object: 1.9831
     Episode_Reward/lifting_object: 180.3059
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0809
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 168493056
                    Iteration time: 2.32s
                      Time elapsed: 01:07:22
                               ETA: 00:30:56

################################################################################
                     [1m Learning iteration 1714/2500 [0m                     

                       Computation: 44152 steps/s (collection: 2.132s, learning 0.094s)
             Mean action noise std: 3.39
          Mean value_function loss: 115.6572
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 80.5578
                       Mean reward: 877.79
               Mean episode length: 232.01
    Episode_Reward/reaching_object: 1.9411
     Episode_Reward/lifting_object: 175.6736
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0797
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 168591360
                    Iteration time: 2.23s
                      Time elapsed: 01:07:24
                               ETA: 00:30:53

################################################################################
                     [1m Learning iteration 1715/2500 [0m                     

                       Computation: 44379 steps/s (collection: 2.109s, learning 0.107s)
             Mean action noise std: 3.39
          Mean value_function loss: 110.7548
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 80.5659
                       Mean reward: 910.40
               Mean episode length: 240.71
    Episode_Reward/reaching_object: 1.9903
     Episode_Reward/lifting_object: 180.4399
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0816
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 168689664
                    Iteration time: 2.22s
                      Time elapsed: 01:07:26
                               ETA: 00:30:51

################################################################################
                     [1m Learning iteration 1716/2500 [0m                     

                       Computation: 43643 steps/s (collection: 2.140s, learning 0.113s)
             Mean action noise std: 3.39
          Mean value_function loss: 99.4493
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 80.5701
                       Mean reward: 896.98
               Mean episode length: 235.80
    Episode_Reward/reaching_object: 1.9623
     Episode_Reward/lifting_object: 178.1340
      Episode_Reward/object_height: 0.0152
        Episode_Reward/action_rate: -0.0802
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 13.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 168787968
                    Iteration time: 2.25s
                      Time elapsed: 01:07:28
                               ETA: 00:30:48

################################################################################
                     [1m Learning iteration 1717/2500 [0m                     

                       Computation: 44929 steps/s (collection: 2.095s, learning 0.093s)
             Mean action noise std: 3.39
          Mean value_function loss: 93.4542
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 80.5796
                       Mean reward: 939.81
               Mean episode length: 246.80
    Episode_Reward/reaching_object: 1.9911
     Episode_Reward/lifting_object: 180.0200
      Episode_Reward/object_height: 0.0153
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 168886272
                    Iteration time: 2.19s
                      Time elapsed: 01:07:31
                               ETA: 00:30:46

################################################################################
                     [1m Learning iteration 1718/2500 [0m                     

                       Computation: 43634 steps/s (collection: 2.156s, learning 0.097s)
             Mean action noise std: 3.39
          Mean value_function loss: 105.7053
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 80.5914
                       Mean reward: 879.33
               Mean episode length: 232.92
    Episode_Reward/reaching_object: 1.9916
     Episode_Reward/lifting_object: 180.1880
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0812
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 168984576
                    Iteration time: 2.25s
                      Time elapsed: 01:07:33
                               ETA: 00:30:43

################################################################################
                     [1m Learning iteration 1719/2500 [0m                     

                       Computation: 40335 steps/s (collection: 2.315s, learning 0.122s)
             Mean action noise std: 3.40
          Mean value_function loss: 111.4599
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 80.5984
                       Mean reward: 918.55
               Mean episode length: 240.84
    Episode_Reward/reaching_object: 2.0020
     Episode_Reward/lifting_object: 181.4138
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0815
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 169082880
                    Iteration time: 2.44s
                      Time elapsed: 01:07:35
                               ETA: 00:30:41

################################################################################
                     [1m Learning iteration 1720/2500 [0m                     

                       Computation: 43492 steps/s (collection: 2.160s, learning 0.101s)
             Mean action noise std: 3.40
          Mean value_function loss: 104.4709
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 80.6080
                       Mean reward: 926.69
               Mean episode length: 243.29
    Episode_Reward/reaching_object: 1.9908
     Episode_Reward/lifting_object: 180.1844
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 169181184
                    Iteration time: 2.26s
                      Time elapsed: 01:07:38
                               ETA: 00:30:39

################################################################################
                     [1m Learning iteration 1721/2500 [0m                     

                       Computation: 44780 steps/s (collection: 2.103s, learning 0.092s)
             Mean action noise std: 3.40
          Mean value_function loss: 98.8945
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 80.6132
                       Mean reward: 919.21
               Mean episode length: 240.58
    Episode_Reward/reaching_object: 1.9875
     Episode_Reward/lifting_object: 180.0274
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0812
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 169279488
                    Iteration time: 2.20s
                      Time elapsed: 01:07:40
                               ETA: 00:30:36

################################################################################
                     [1m Learning iteration 1722/2500 [0m                     

                       Computation: 45407 steps/s (collection: 2.072s, learning 0.093s)
             Mean action noise std: 3.40
          Mean value_function loss: 107.9920
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 80.6195
                       Mean reward: 903.15
               Mean episode length: 236.80
    Episode_Reward/reaching_object: 1.9861
     Episode_Reward/lifting_object: 179.6276
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0809
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 169377792
                    Iteration time: 2.16s
                      Time elapsed: 01:07:42
                               ETA: 00:30:34

################################################################################
                     [1m Learning iteration 1723/2500 [0m                     

                       Computation: 45257 steps/s (collection: 2.075s, learning 0.097s)
             Mean action noise std: 3.40
          Mean value_function loss: 122.7349
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 80.6332
                       Mean reward: 884.40
               Mean episode length: 235.85
    Episode_Reward/reaching_object: 1.9940
     Episode_Reward/lifting_object: 179.3188
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0812
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 169476096
                    Iteration time: 2.17s
                      Time elapsed: 01:07:44
                               ETA: 00:30:31

################################################################################
                     [1m Learning iteration 1724/2500 [0m                     

                       Computation: 45512 steps/s (collection: 2.046s, learning 0.114s)
             Mean action noise std: 3.40
          Mean value_function loss: 91.0287
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 80.6468
                       Mean reward: 923.99
               Mean episode length: 242.08
    Episode_Reward/reaching_object: 2.0104
     Episode_Reward/lifting_object: 181.2786
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0815
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 169574400
                    Iteration time: 2.16s
                      Time elapsed: 01:07:46
                               ETA: 00:30:29

################################################################################
                     [1m Learning iteration 1725/2500 [0m                     

                       Computation: 45443 steps/s (collection: 2.050s, learning 0.113s)
             Mean action noise std: 3.40
          Mean value_function loss: 118.0872
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 80.6564
                       Mean reward: 926.24
               Mean episode length: 242.94
    Episode_Reward/reaching_object: 1.9770
     Episode_Reward/lifting_object: 177.6973
      Episode_Reward/object_height: 0.0155
        Episode_Reward/action_rate: -0.0804
          Episode_Reward/joint_vel: -0.0571
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 169672704
                    Iteration time: 2.16s
                      Time elapsed: 01:07:48
                               ETA: 00:30:27

################################################################################
                     [1m Learning iteration 1726/2500 [0m                     

                       Computation: 45966 steps/s (collection: 2.023s, learning 0.116s)
             Mean action noise std: 3.40
          Mean value_function loss: 120.5990
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 80.6650
                       Mean reward: 889.31
               Mean episode length: 234.78
    Episode_Reward/reaching_object: 1.9893
     Episode_Reward/lifting_object: 178.5993
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0809
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 169771008
                    Iteration time: 2.14s
                      Time elapsed: 01:07:51
                               ETA: 00:30:24

################################################################################
                     [1m Learning iteration 1727/2500 [0m                     

                       Computation: 45220 steps/s (collection: 2.061s, learning 0.112s)
             Mean action noise std: 3.40
          Mean value_function loss: 128.3970
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 80.6709
                       Mean reward: 892.28
               Mean episode length: 234.70
    Episode_Reward/reaching_object: 1.9699
     Episode_Reward/lifting_object: 176.9625
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0799
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 169869312
                    Iteration time: 2.17s
                      Time elapsed: 01:07:53
                               ETA: 00:30:22

################################################################################
                     [1m Learning iteration 1728/2500 [0m                     

                       Computation: 45633 steps/s (collection: 2.040s, learning 0.115s)
             Mean action noise std: 3.40
          Mean value_function loss: 122.1466
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 80.6741
                       Mean reward: 911.74
               Mean episode length: 239.94
    Episode_Reward/reaching_object: 2.0274
     Episode_Reward/lifting_object: 181.8927
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0821
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 169967616
                    Iteration time: 2.15s
                      Time elapsed: 01:07:55
                               ETA: 00:30:19

################################################################################
                     [1m Learning iteration 1729/2500 [0m                     

                       Computation: 45980 steps/s (collection: 2.035s, learning 0.103s)
             Mean action noise std: 3.41
          Mean value_function loss: 99.6752
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 80.6845
                       Mean reward: 909.57
               Mean episode length: 239.38
    Episode_Reward/reaching_object: 1.9974
     Episode_Reward/lifting_object: 179.0403
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0810
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 170065920
                    Iteration time: 2.14s
                      Time elapsed: 01:07:57
                               ETA: 00:30:17

################################################################################
                     [1m Learning iteration 1730/2500 [0m                     

                       Computation: 45139 steps/s (collection: 2.069s, learning 0.109s)
             Mean action noise std: 3.41
          Mean value_function loss: 135.2497
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 80.7051
                       Mean reward: 879.93
               Mean episode length: 233.46
    Episode_Reward/reaching_object: 1.9723
     Episode_Reward/lifting_object: 176.6747
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0801
          Episode_Reward/joint_vel: -0.0565
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 170164224
                    Iteration time: 2.18s
                      Time elapsed: 01:07:59
                               ETA: 00:30:14

################################################################################
                     [1m Learning iteration 1731/2500 [0m                     

                       Computation: 45660 steps/s (collection: 2.039s, learning 0.114s)
             Mean action noise std: 3.41
          Mean value_function loss: 114.5698
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 80.7196
                       Mean reward: 901.86
               Mean episode length: 238.25
    Episode_Reward/reaching_object: 2.0170
     Episode_Reward/lifting_object: 180.6536
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0815
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 170262528
                    Iteration time: 2.15s
                      Time elapsed: 01:08:01
                               ETA: 00:30:12

################################################################################
                     [1m Learning iteration 1732/2500 [0m                     

                       Computation: 45077 steps/s (collection: 2.056s, learning 0.125s)
             Mean action noise std: 3.41
          Mean value_function loss: 109.0154
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 80.7276
                       Mean reward: 892.82
               Mean episode length: 235.96
    Episode_Reward/reaching_object: 2.0227
     Episode_Reward/lifting_object: 181.0544
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0820
          Episode_Reward/joint_vel: -0.0577
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 170360832
                    Iteration time: 2.18s
                      Time elapsed: 01:08:04
                               ETA: 00:30:09

################################################################################
                     [1m Learning iteration 1733/2500 [0m                     

                       Computation: 45280 steps/s (collection: 2.058s, learning 0.113s)
             Mean action noise std: 3.41
          Mean value_function loss: 111.2189
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 80.7378
                       Mean reward: 913.19
               Mean episode length: 240.52
    Episode_Reward/reaching_object: 2.0023
     Episode_Reward/lifting_object: 179.4156
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 170459136
                    Iteration time: 2.17s
                      Time elapsed: 01:08:06
                               ETA: 00:30:07

################################################################################
                     [1m Learning iteration 1734/2500 [0m                     

                       Computation: 45568 steps/s (collection: 2.069s, learning 0.088s)
             Mean action noise std: 3.41
          Mean value_function loss: 84.8403
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 80.7499
                       Mean reward: 881.87
               Mean episode length: 233.25
    Episode_Reward/reaching_object: 2.0195
     Episode_Reward/lifting_object: 181.1812
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0821
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 170557440
                    Iteration time: 2.16s
                      Time elapsed: 01:08:08
                               ETA: 00:30:05

################################################################################
                     [1m Learning iteration 1735/2500 [0m                     

                       Computation: 45602 steps/s (collection: 2.047s, learning 0.109s)
             Mean action noise std: 3.41
          Mean value_function loss: 95.9892
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 80.7576
                       Mean reward: 877.78
               Mean episode length: 231.48
    Episode_Reward/reaching_object: 2.0091
     Episode_Reward/lifting_object: 180.3371
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0818
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 170655744
                    Iteration time: 2.16s
                      Time elapsed: 01:08:10
                               ETA: 00:30:02

################################################################################
                     [1m Learning iteration 1736/2500 [0m                     

                       Computation: 45476 steps/s (collection: 2.046s, learning 0.116s)
             Mean action noise std: 3.41
          Mean value_function loss: 73.2581
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 80.7650
                       Mean reward: 935.63
               Mean episode length: 245.01
    Episode_Reward/reaching_object: 2.0277
     Episode_Reward/lifting_object: 182.0343
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 170754048
                    Iteration time: 2.16s
                      Time elapsed: 01:08:12
                               ETA: 00:30:00

################################################################################
                     [1m Learning iteration 1737/2500 [0m                     

                       Computation: 45451 steps/s (collection: 2.058s, learning 0.105s)
             Mean action noise std: 3.42
          Mean value_function loss: 88.6811
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 80.7807
                       Mean reward: 918.79
               Mean episode length: 242.38
    Episode_Reward/reaching_object: 2.0100
     Episode_Reward/lifting_object: 180.0046
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0824
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 170852352
                    Iteration time: 2.16s
                      Time elapsed: 01:08:14
                               ETA: 00:29:57

################################################################################
                     [1m Learning iteration 1738/2500 [0m                     

                       Computation: 45069 steps/s (collection: 2.054s, learning 0.127s)
             Mean action noise std: 3.42
          Mean value_function loss: 87.8885
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 80.7981
                       Mean reward: 924.76
               Mean episode length: 243.26
    Episode_Reward/reaching_object: 1.9887
     Episode_Reward/lifting_object: 178.6257
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0817
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 170950656
                    Iteration time: 2.18s
                      Time elapsed: 01:08:17
                               ETA: 00:29:55

################################################################################
                     [1m Learning iteration 1739/2500 [0m                     

                       Computation: 44128 steps/s (collection: 2.116s, learning 0.112s)
             Mean action noise std: 3.42
          Mean value_function loss: 130.9790
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 80.8132
                       Mean reward: 900.35
               Mean episode length: 236.24
    Episode_Reward/reaching_object: 1.9885
     Episode_Reward/lifting_object: 179.3548
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0814
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 171048960
                    Iteration time: 2.23s
                      Time elapsed: 01:08:19
                               ETA: 00:29:52

################################################################################
                     [1m Learning iteration 1740/2500 [0m                     

                       Computation: 44925 steps/s (collection: 2.074s, learning 0.114s)
             Mean action noise std: 3.42
          Mean value_function loss: 115.9590
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 80.8259
                       Mean reward: 895.67
               Mean episode length: 235.81
    Episode_Reward/reaching_object: 1.9977
     Episode_Reward/lifting_object: 180.2400
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0820
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 171147264
                    Iteration time: 2.19s
                      Time elapsed: 01:08:21
                               ETA: 00:29:50

################################################################################
                     [1m Learning iteration 1741/2500 [0m                     

                       Computation: 44390 steps/s (collection: 2.111s, learning 0.103s)
             Mean action noise std: 3.42
          Mean value_function loss: 113.8760
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 80.8371
                       Mean reward: 884.03
               Mean episode length: 233.07
    Episode_Reward/reaching_object: 1.9619
     Episode_Reward/lifting_object: 176.8330
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0808
          Episode_Reward/joint_vel: -0.0576
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 171245568
                    Iteration time: 2.21s
                      Time elapsed: 01:08:23
                               ETA: 00:29:48

################################################################################
                     [1m Learning iteration 1742/2500 [0m                     

                       Computation: 45283 steps/s (collection: 2.070s, learning 0.101s)
             Mean action noise std: 3.42
          Mean value_function loss: 85.6813
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 80.8445
                       Mean reward: 917.45
               Mean episode length: 240.04
    Episode_Reward/reaching_object: 1.9896
     Episode_Reward/lifting_object: 179.4347
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0821
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 171343872
                    Iteration time: 2.17s
                      Time elapsed: 01:08:25
                               ETA: 00:29:45

################################################################################
                     [1m Learning iteration 1743/2500 [0m                     

                       Computation: 45090 steps/s (collection: 2.063s, learning 0.117s)
             Mean action noise std: 3.42
          Mean value_function loss: 103.5132
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 80.8532
                       Mean reward: 900.03
               Mean episode length: 236.80
    Episode_Reward/reaching_object: 2.0089
     Episode_Reward/lifting_object: 181.7995
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0830
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 171442176
                    Iteration time: 2.18s
                      Time elapsed: 01:08:28
                               ETA: 00:29:43

################################################################################
                     [1m Learning iteration 1744/2500 [0m                     

                       Computation: 45292 steps/s (collection: 2.051s, learning 0.120s)
             Mean action noise std: 3.42
          Mean value_function loss: 91.3084
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 80.8582
                       Mean reward: 917.11
               Mean episode length: 241.71
    Episode_Reward/reaching_object: 2.0062
     Episode_Reward/lifting_object: 181.7898
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0832
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 171540480
                    Iteration time: 2.17s
                      Time elapsed: 01:08:30
                               ETA: 00:29:40

################################################################################
                     [1m Learning iteration 1745/2500 [0m                     

                       Computation: 45593 steps/s (collection: 2.049s, learning 0.107s)
             Mean action noise std: 3.43
          Mean value_function loss: 92.0318
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 80.8670
                       Mean reward: 907.69
               Mean episode length: 238.46
    Episode_Reward/reaching_object: 1.9633
     Episode_Reward/lifting_object: 177.6261
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0817
          Episode_Reward/joint_vel: -0.0575
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 171638784
                    Iteration time: 2.16s
                      Time elapsed: 01:08:32
                               ETA: 00:29:38

################################################################################
                     [1m Learning iteration 1746/2500 [0m                     

                       Computation: 45447 steps/s (collection: 2.050s, learning 0.113s)
             Mean action noise std: 3.43
          Mean value_function loss: 117.4448
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 80.8763
                       Mean reward: 890.31
               Mean episode length: 233.63
    Episode_Reward/reaching_object: 1.9561
     Episode_Reward/lifting_object: 177.8710
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0817
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 171737088
                    Iteration time: 2.16s
                      Time elapsed: 01:08:34
                               ETA: 00:29:35

################################################################################
                     [1m Learning iteration 1747/2500 [0m                     

                       Computation: 45319 steps/s (collection: 2.055s, learning 0.114s)
             Mean action noise std: 3.43
          Mean value_function loss: 137.3174
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 80.8847
                       Mean reward: 888.81
               Mean episode length: 233.44
    Episode_Reward/reaching_object: 1.9156
     Episode_Reward/lifting_object: 174.2696
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0802
          Episode_Reward/joint_vel: -0.0566
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 171835392
                    Iteration time: 2.17s
                      Time elapsed: 01:08:36
                               ETA: 00:29:33

################################################################################
                     [1m Learning iteration 1748/2500 [0m                     

                       Computation: 45362 steps/s (collection: 2.054s, learning 0.113s)
             Mean action noise std: 3.43
          Mean value_function loss: 94.3002
               Mean surrogate loss: 0.0010
                 Mean entropy loss: 80.8876
                       Mean reward: 909.22
               Mean episode length: 238.82
    Episode_Reward/reaching_object: 1.9819
     Episode_Reward/lifting_object: 180.3918
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0828
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 171933696
                    Iteration time: 2.17s
                      Time elapsed: 01:08:38
                               ETA: 00:29:30

################################################################################
                     [1m Learning iteration 1749/2500 [0m                     

                       Computation: 45033 steps/s (collection: 2.056s, learning 0.127s)
             Mean action noise std: 3.43
          Mean value_function loss: 101.3623
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 80.8914
                       Mean reward: 919.22
               Mean episode length: 241.35
    Episode_Reward/reaching_object: 1.9847
     Episode_Reward/lifting_object: 180.6808
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0833
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 172032000
                    Iteration time: 2.18s
                      Time elapsed: 01:08:41
                               ETA: 00:29:28

################################################################################
                     [1m Learning iteration 1750/2500 [0m                     

                       Computation: 45347 steps/s (collection: 2.070s, learning 0.098s)
             Mean action noise std: 3.43
          Mean value_function loss: 118.5546
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 80.8953
                       Mean reward: 903.05
               Mean episode length: 237.08
    Episode_Reward/reaching_object: 1.9530
     Episode_Reward/lifting_object: 177.8136
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0824
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 172130304
                    Iteration time: 2.17s
                      Time elapsed: 01:08:43
                               ETA: 00:29:26

################################################################################
                     [1m Learning iteration 1751/2500 [0m                     

                       Computation: 44683 steps/s (collection: 2.101s, learning 0.099s)
             Mean action noise std: 3.43
          Mean value_function loss: 93.8744
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 80.8992
                       Mean reward: 922.22
               Mean episode length: 242.55
    Episode_Reward/reaching_object: 1.9589
     Episode_Reward/lifting_object: 178.3846
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 172228608
                    Iteration time: 2.20s
                      Time elapsed: 01:08:45
                               ETA: 00:29:23

################################################################################
                     [1m Learning iteration 1752/2500 [0m                     

                       Computation: 45117 steps/s (collection: 2.057s, learning 0.122s)
             Mean action noise std: 3.43
          Mean value_function loss: 93.2307
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 80.9054
                       Mean reward: 923.94
               Mean episode length: 242.36
    Episode_Reward/reaching_object: 1.9693
     Episode_Reward/lifting_object: 179.5813
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0829
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 172326912
                    Iteration time: 2.18s
                      Time elapsed: 01:08:47
                               ETA: 00:29:21

################################################################################
                     [1m Learning iteration 1753/2500 [0m                     

                       Computation: 45034 steps/s (collection: 2.067s, learning 0.116s)
             Mean action noise std: 3.43
          Mean value_function loss: 108.8739
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 80.9140
                       Mean reward: 907.19
               Mean episode length: 239.20
    Episode_Reward/reaching_object: 1.9647
     Episode_Reward/lifting_object: 178.7822
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0829
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 172425216
                    Iteration time: 2.18s
                      Time elapsed: 01:08:49
                               ETA: 00:29:18

################################################################################
                     [1m Learning iteration 1754/2500 [0m                     

                       Computation: 45324 steps/s (collection: 2.050s, learning 0.119s)
             Mean action noise std: 3.43
          Mean value_function loss: 95.9977
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 80.9270
                       Mean reward: 911.39
               Mean episode length: 239.28
    Episode_Reward/reaching_object: 1.9586
     Episode_Reward/lifting_object: 178.5511
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0579
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 172523520
                    Iteration time: 2.17s
                      Time elapsed: 01:08:51
                               ETA: 00:29:16

################################################################################
                     [1m Learning iteration 1755/2500 [0m                     

                       Computation: 44356 steps/s (collection: 2.108s, learning 0.109s)
             Mean action noise std: 3.43
          Mean value_function loss: 95.8080
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 80.9385
                       Mean reward: 922.28
               Mean episode length: 242.34
    Episode_Reward/reaching_object: 1.9711
     Episode_Reward/lifting_object: 179.6608
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0833
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 172621824
                    Iteration time: 2.22s
                      Time elapsed: 01:08:54
                               ETA: 00:29:13

################################################################################
                     [1m Learning iteration 1756/2500 [0m                     

                       Computation: 43156 steps/s (collection: 2.155s, learning 0.123s)
             Mean action noise std: 3.44
          Mean value_function loss: 135.9078
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 80.9476
                       Mean reward: 885.61
               Mean episode length: 233.58
    Episode_Reward/reaching_object: 1.9554
     Episode_Reward/lifting_object: 177.9788
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0826
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 172720128
                    Iteration time: 2.28s
                      Time elapsed: 01:08:56
                               ETA: 00:29:11

################################################################################
                     [1m Learning iteration 1757/2500 [0m                     

                       Computation: 45840 steps/s (collection: 2.036s, learning 0.108s)
             Mean action noise std: 3.44
          Mean value_function loss: 111.9474
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 80.9587
                       Mean reward: 923.72
               Mean episode length: 241.32
    Episode_Reward/reaching_object: 1.9905
     Episode_Reward/lifting_object: 181.5708
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0837
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 172818432
                    Iteration time: 2.14s
                      Time elapsed: 01:08:58
                               ETA: 00:29:09

################################################################################
                     [1m Learning iteration 1758/2500 [0m                     

                       Computation: 44735 steps/s (collection: 2.100s, learning 0.098s)
             Mean action noise std: 3.44
          Mean value_function loss: 124.4180
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 80.9727
                       Mean reward: 889.66
               Mean episode length: 234.58
    Episode_Reward/reaching_object: 1.9653
     Episode_Reward/lifting_object: 179.3320
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0830
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 13.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 172916736
                    Iteration time: 2.20s
                      Time elapsed: 01:09:00
                               ETA: 00:29:06

################################################################################
                     [1m Learning iteration 1759/2500 [0m                     

                       Computation: 45917 steps/s (collection: 2.049s, learning 0.092s)
             Mean action noise std: 3.44
          Mean value_function loss: 104.1791
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 80.9884
                       Mean reward: 910.41
               Mean episode length: 238.85
    Episode_Reward/reaching_object: 1.9766
     Episode_Reward/lifting_object: 179.9828
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0835
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 173015040
                    Iteration time: 2.14s
                      Time elapsed: 01:09:02
                               ETA: 00:29:04

################################################################################
                     [1m Learning iteration 1760/2500 [0m                     

                       Computation: 45274 steps/s (collection: 2.068s, learning 0.103s)
             Mean action noise std: 3.44
          Mean value_function loss: 110.7645
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 81.0050
                       Mean reward: 921.09
               Mean episode length: 243.46
    Episode_Reward/reaching_object: 1.9543
     Episode_Reward/lifting_object: 177.4376
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0828
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173113344
                    Iteration time: 2.17s
                      Time elapsed: 01:09:05
                               ETA: 00:29:01

################################################################################
                     [1m Learning iteration 1761/2500 [0m                     

                       Computation: 45114 steps/s (collection: 2.056s, learning 0.123s)
             Mean action noise std: 3.44
          Mean value_function loss: 101.8219
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 81.0164
                       Mean reward: 922.36
               Mean episode length: 242.77
    Episode_Reward/reaching_object: 1.9932
     Episode_Reward/lifting_object: 181.6160
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0841
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 17.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 173211648
                    Iteration time: 2.18s
                      Time elapsed: 01:09:07
                               ETA: 00:28:59

################################################################################
                     [1m Learning iteration 1762/2500 [0m                     

                       Computation: 45361 steps/s (collection: 2.072s, learning 0.095s)
             Mean action noise std: 3.44
          Mean value_function loss: 75.9041
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 81.0229
                       Mean reward: 900.71
               Mean episode length: 238.21
    Episode_Reward/reaching_object: 1.9925
     Episode_Reward/lifting_object: 181.3451
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0840
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 173309952
                    Iteration time: 2.17s
                      Time elapsed: 01:09:09
                               ETA: 00:28:56

################################################################################
                     [1m Learning iteration 1763/2500 [0m                     

                       Computation: 44453 steps/s (collection: 2.097s, learning 0.115s)
             Mean action noise std: 3.45
          Mean value_function loss: 102.6169
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 81.0316
                       Mean reward: 886.80
               Mean episode length: 234.32
    Episode_Reward/reaching_object: 1.9878
     Episode_Reward/lifting_object: 180.6084
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0838
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 173408256
                    Iteration time: 2.21s
                      Time elapsed: 01:09:11
                               ETA: 00:28:54

################################################################################
                     [1m Learning iteration 1764/2500 [0m                     

                       Computation: 44523 steps/s (collection: 2.093s, learning 0.115s)
             Mean action noise std: 3.45
          Mean value_function loss: 73.4973
               Mean surrogate loss: 0.0013
                 Mean entropy loss: 81.0435
                       Mean reward: 914.44
               Mean episode length: 240.65
    Episode_Reward/reaching_object: 2.0197
     Episode_Reward/lifting_object: 183.1124
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0849
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 173506560
                    Iteration time: 2.21s
                      Time elapsed: 01:09:13
                               ETA: 00:28:52

################################################################################
                     [1m Learning iteration 1765/2500 [0m                     

                       Computation: 44337 steps/s (collection: 2.098s, learning 0.119s)
             Mean action noise std: 3.45
          Mean value_function loss: 67.6466
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 81.0481
                       Mean reward: 947.17
               Mean episode length: 248.30
    Episode_Reward/reaching_object: 2.0266
     Episode_Reward/lifting_object: 183.8015
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 173604864
                    Iteration time: 2.22s
                      Time elapsed: 01:09:16
                               ETA: 00:28:49

################################################################################
                     [1m Learning iteration 1766/2500 [0m                     

                       Computation: 45502 steps/s (collection: 2.065s, learning 0.096s)
             Mean action noise std: 3.45
          Mean value_function loss: 84.6337
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 81.0559
                       Mean reward: 904.33
               Mean episode length: 238.20
    Episode_Reward/reaching_object: 1.9990
     Episode_Reward/lifting_object: 180.9960
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0841
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 173703168
                    Iteration time: 2.16s
                      Time elapsed: 01:09:18
                               ETA: 00:28:47

################################################################################
                     [1m Learning iteration 1767/2500 [0m                     

                       Computation: 45554 steps/s (collection: 2.060s, learning 0.098s)
             Mean action noise std: 3.45
          Mean value_function loss: 90.6295
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 81.0642
                       Mean reward: 888.40
               Mean episode length: 234.66
    Episode_Reward/reaching_object: 2.0103
     Episode_Reward/lifting_object: 181.4410
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0843
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 173801472
                    Iteration time: 2.16s
                      Time elapsed: 01:09:20
                               ETA: 00:28:44

################################################################################
                     [1m Learning iteration 1768/2500 [0m                     

                       Computation: 44646 steps/s (collection: 2.090s, learning 0.112s)
             Mean action noise std: 3.45
          Mean value_function loss: 116.9083
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 81.0677
                       Mean reward: 898.40
               Mean episode length: 237.20
    Episode_Reward/reaching_object: 2.0051
     Episode_Reward/lifting_object: 181.0203
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0844
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 173899776
                    Iteration time: 2.20s
                      Time elapsed: 01:09:22
                               ETA: 00:28:42

################################################################################
                     [1m Learning iteration 1769/2500 [0m                     

                       Computation: 45241 steps/s (collection: 2.060s, learning 0.113s)
             Mean action noise std: 3.45
          Mean value_function loss: 104.3099
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 81.0718
                       Mean reward: 877.06
               Mean episode length: 231.46
    Episode_Reward/reaching_object: 1.9941
     Episode_Reward/lifting_object: 179.4620
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0839
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 173998080
                    Iteration time: 2.17s
                      Time elapsed: 01:09:24
                               ETA: 00:28:40

################################################################################
                     [1m Learning iteration 1770/2500 [0m                     

                       Computation: 45461 steps/s (collection: 2.047s, learning 0.116s)
             Mean action noise std: 3.45
          Mean value_function loss: 71.8037
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 81.0776
                       Mean reward: 912.93
               Mean episode length: 240.75
    Episode_Reward/reaching_object: 2.0243
     Episode_Reward/lifting_object: 181.6262
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0848
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 174096384
                    Iteration time: 2.16s
                      Time elapsed: 01:09:26
                               ETA: 00:28:37

################################################################################
                     [1m Learning iteration 1771/2500 [0m                     

                       Computation: 45435 steps/s (collection: 2.055s, learning 0.109s)
             Mean action noise std: 3.45
          Mean value_function loss: 89.3217
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 81.0816
                       Mean reward: 929.23
               Mean episode length: 244.89
    Episode_Reward/reaching_object: 2.0260
     Episode_Reward/lifting_object: 181.6691
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0849
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 174194688
                    Iteration time: 2.16s
                      Time elapsed: 01:09:29
                               ETA: 00:28:35

################################################################################
                     [1m Learning iteration 1772/2500 [0m                     

                       Computation: 45262 steps/s (collection: 2.058s, learning 0.114s)
             Mean action noise std: 3.45
          Mean value_function loss: 74.1188
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 81.0875
                       Mean reward: 931.55
               Mean episode length: 243.93
    Episode_Reward/reaching_object: 2.0388
     Episode_Reward/lifting_object: 182.7283
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0852
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 174292992
                    Iteration time: 2.17s
                      Time elapsed: 01:09:31
                               ETA: 00:28:32

################################################################################
                     [1m Learning iteration 1773/2500 [0m                     

                       Computation: 44516 steps/s (collection: 2.076s, learning 0.132s)
             Mean action noise std: 3.45
          Mean value_function loss: 81.8060
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 81.0895
                       Mean reward: 931.92
               Mean episode length: 244.46
    Episode_Reward/reaching_object: 2.0213
     Episode_Reward/lifting_object: 180.6353
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0848
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 174391296
                    Iteration time: 2.21s
                      Time elapsed: 01:09:33
                               ETA: 00:28:30

################################################################################
                     [1m Learning iteration 1774/2500 [0m                     

                       Computation: 45222 steps/s (collection: 2.062s, learning 0.112s)
             Mean action noise std: 3.45
          Mean value_function loss: 84.8047
               Mean surrogate loss: 0.0016
                 Mean entropy loss: 81.0915
                       Mean reward: 937.27
               Mean episode length: 245.47
    Episode_Reward/reaching_object: 2.0118
     Episode_Reward/lifting_object: 179.5326
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0843
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 174489600
                    Iteration time: 2.17s
                      Time elapsed: 01:09:35
                               ETA: 00:28:27

################################################################################
                     [1m Learning iteration 1775/2500 [0m                     

                       Computation: 44845 steps/s (collection: 2.081s, learning 0.111s)
             Mean action noise std: 3.45
          Mean value_function loss: 80.7125
               Mean surrogate loss: -0.0006
                 Mean entropy loss: 81.0934
                       Mean reward: 895.75
               Mean episode length: 235.98
    Episode_Reward/reaching_object: 2.0335
     Episode_Reward/lifting_object: 181.8287
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0847
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 174587904
                    Iteration time: 2.19s
                      Time elapsed: 01:09:37
                               ETA: 00:28:25

################################################################################
                     [1m Learning iteration 1776/2500 [0m                     

                       Computation: 45027 steps/s (collection: 2.069s, learning 0.115s)
             Mean action noise std: 3.45
          Mean value_function loss: 70.1161
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 81.0987
                       Mean reward: 920.16
               Mean episode length: 241.83
    Episode_Reward/reaching_object: 2.0358
     Episode_Reward/lifting_object: 181.7640
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0852
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 174686208
                    Iteration time: 2.18s
                      Time elapsed: 01:09:40
                               ETA: 00:28:23

################################################################################
                     [1m Learning iteration 1777/2500 [0m                     

                       Computation: 45338 steps/s (collection: 2.055s, learning 0.114s)
             Mean action noise std: 3.45
          Mean value_function loss: 90.5962
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 81.1015
                       Mean reward: 910.12
               Mean episode length: 239.95
    Episode_Reward/reaching_object: 2.0238
     Episode_Reward/lifting_object: 180.6824
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0845
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 174784512
                    Iteration time: 2.17s
                      Time elapsed: 01:09:42
                               ETA: 00:28:20

################################################################################
                     [1m Learning iteration 1778/2500 [0m                     

                       Computation: 45277 steps/s (collection: 2.060s, learning 0.112s)
             Mean action noise std: 3.45
          Mean value_function loss: 100.5954
               Mean surrogate loss: 0.0004
                 Mean entropy loss: 81.1035
                       Mean reward: 919.47
               Mean episode length: 241.94
    Episode_Reward/reaching_object: 2.0436
     Episode_Reward/lifting_object: 181.8836
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 174882816
                    Iteration time: 2.17s
                      Time elapsed: 01:09:44
                               ETA: 00:28:18

################################################################################
                     [1m Learning iteration 1779/2500 [0m                     

                       Computation: 45203 steps/s (collection: 2.050s, learning 0.125s)
             Mean action noise std: 3.45
          Mean value_function loss: 71.7279
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 81.1059
                       Mean reward: 926.58
               Mean episode length: 242.74
    Episode_Reward/reaching_object: 2.0512
     Episode_Reward/lifting_object: 182.8486
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0855
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 174981120
                    Iteration time: 2.17s
                      Time elapsed: 01:09:46
                               ETA: 00:28:15

################################################################################
                     [1m Learning iteration 1780/2500 [0m                     

                       Computation: 44890 steps/s (collection: 2.058s, learning 0.132s)
             Mean action noise std: 3.45
          Mean value_function loss: 91.6243
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 81.1112
                       Mean reward: 911.19
               Mean episode length: 238.50
    Episode_Reward/reaching_object: 2.0247
     Episode_Reward/lifting_object: 180.3215
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0844
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 175079424
                    Iteration time: 2.19s
                      Time elapsed: 01:09:48
                               ETA: 00:28:13

################################################################################
                     [1m Learning iteration 1781/2500 [0m                     

                       Computation: 44931 steps/s (collection: 2.074s, learning 0.114s)
             Mean action noise std: 3.45
          Mean value_function loss: 78.5395
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 81.1157
                       Mean reward: 927.75
               Mean episode length: 242.85
    Episode_Reward/reaching_object: 2.0339
     Episode_Reward/lifting_object: 181.8003
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0848
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 175177728
                    Iteration time: 2.19s
                      Time elapsed: 01:09:50
                               ETA: 00:28:10

################################################################################
                     [1m Learning iteration 1782/2500 [0m                     

                       Computation: 45457 steps/s (collection: 2.050s, learning 0.113s)
             Mean action noise std: 3.46
          Mean value_function loss: 73.3135
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.1230
                       Mean reward: 909.33
               Mean episode length: 238.16
    Episode_Reward/reaching_object: 2.0287
     Episode_Reward/lifting_object: 180.8359
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0848
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 175276032
                    Iteration time: 2.16s
                      Time elapsed: 01:09:53
                               ETA: 00:28:08

################################################################################
                     [1m Learning iteration 1783/2500 [0m                     

                       Computation: 45296 steps/s (collection: 2.056s, learning 0.114s)
             Mean action noise std: 3.46
          Mean value_function loss: 88.2068
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 81.1387
                       Mean reward: 917.35
               Mean episode length: 241.76
    Episode_Reward/reaching_object: 2.0278
     Episode_Reward/lifting_object: 180.8615
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0848
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 175374336
                    Iteration time: 2.17s
                      Time elapsed: 01:09:55
                               ETA: 00:28:06

################################################################################
                     [1m Learning iteration 1784/2500 [0m                     

                       Computation: 43780 steps/s (collection: 2.126s, learning 0.119s)
             Mean action noise std: 3.46
          Mean value_function loss: 72.1655
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 81.1517
                       Mean reward: 929.37
               Mean episode length: 244.03
    Episode_Reward/reaching_object: 2.0526
     Episode_Reward/lifting_object: 183.1763
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0858
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 175472640
                    Iteration time: 2.25s
                      Time elapsed: 01:09:57
                               ETA: 00:28:03

################################################################################
                     [1m Learning iteration 1785/2500 [0m                     

                       Computation: 45405 steps/s (collection: 2.076s, learning 0.089s)
             Mean action noise std: 3.46
          Mean value_function loss: 93.1767
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 81.1576
                       Mean reward: 939.72
               Mean episode length: 246.22
    Episode_Reward/reaching_object: 2.0404
     Episode_Reward/lifting_object: 182.0824
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0853
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 175570944
                    Iteration time: 2.17s
                      Time elapsed: 01:09:59
                               ETA: 00:28:01

################################################################################
                     [1m Learning iteration 1786/2500 [0m                     

                       Computation: 45702 steps/s (collection: 2.041s, learning 0.110s)
             Mean action noise std: 3.46
          Mean value_function loss: 100.4269
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 81.1653
                       Mean reward: 891.49
               Mean episode length: 233.78
    Episode_Reward/reaching_object: 1.9919
     Episode_Reward/lifting_object: 177.9258
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0835
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 175669248
                    Iteration time: 2.15s
                      Time elapsed: 01:10:01
                               ETA: 00:27:58

################################################################################
                     [1m Learning iteration 1787/2500 [0m                     

                       Computation: 45566 steps/s (collection: 2.043s, learning 0.115s)
             Mean action noise std: 3.46
          Mean value_function loss: 84.0600
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 81.1750
                       Mean reward: 873.62
               Mean episode length: 230.51
    Episode_Reward/reaching_object: 2.0229
     Episode_Reward/lifting_object: 180.6256
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0848
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 175767552
                    Iteration time: 2.16s
                      Time elapsed: 01:10:04
                               ETA: 00:27:56

################################################################################
                     [1m Learning iteration 1788/2500 [0m                     

                       Computation: 44395 steps/s (collection: 2.091s, learning 0.123s)
             Mean action noise std: 3.46
          Mean value_function loss: 79.1393
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 81.1813
                       Mean reward: 899.86
               Mean episode length: 237.00
    Episode_Reward/reaching_object: 2.0216
     Episode_Reward/lifting_object: 180.5688
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0849
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 175865856
                    Iteration time: 2.21s
                      Time elapsed: 01:10:06
                               ETA: 00:27:54

################################################################################
                     [1m Learning iteration 1789/2500 [0m                     

                       Computation: 45199 steps/s (collection: 2.055s, learning 0.120s)
             Mean action noise std: 3.46
          Mean value_function loss: 81.9024
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 81.1897
                       Mean reward: 896.11
               Mean episode length: 235.66
    Episode_Reward/reaching_object: 2.0129
     Episode_Reward/lifting_object: 180.6260
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0848
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 175964160
                    Iteration time: 2.17s
                      Time elapsed: 01:10:08
                               ETA: 00:27:51

################################################################################
                     [1m Learning iteration 1790/2500 [0m                     

                       Computation: 45599 steps/s (collection: 2.052s, learning 0.104s)
             Mean action noise std: 3.46
          Mean value_function loss: 67.5925
               Mean surrogate loss: -0.0014
                 Mean entropy loss: 81.1983
                       Mean reward: 912.56
               Mean episode length: 239.75
    Episode_Reward/reaching_object: 2.0386
     Episode_Reward/lifting_object: 183.1805
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0859
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 176062464
                    Iteration time: 2.16s
                      Time elapsed: 01:10:10
                               ETA: 00:27:49

################################################################################
                     [1m Learning iteration 1791/2500 [0m                     

                       Computation: 45314 steps/s (collection: 2.065s, learning 0.104s)
             Mean action noise std: 3.46
          Mean value_function loss: 91.3159
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 81.2081
                       Mean reward: 918.44
               Mean episode length: 242.38
    Episode_Reward/reaching_object: 2.0065
     Episode_Reward/lifting_object: 180.0643
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0848
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 176160768
                    Iteration time: 2.17s
                      Time elapsed: 01:10:12
                               ETA: 00:27:46

################################################################################
                     [1m Learning iteration 1792/2500 [0m                     

                       Computation: 45662 steps/s (collection: 2.040s, learning 0.113s)
             Mean action noise std: 3.47
          Mean value_function loss: 78.1240
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 81.2130
                       Mean reward: 915.46
               Mean episode length: 239.80
    Episode_Reward/reaching_object: 2.0195
     Episode_Reward/lifting_object: 181.7552
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 176259072
                    Iteration time: 2.15s
                      Time elapsed: 01:10:14
                               ETA: 00:27:44

################################################################################
                     [1m Learning iteration 1793/2500 [0m                     

                       Computation: 45406 steps/s (collection: 2.053s, learning 0.112s)
             Mean action noise std: 3.47
          Mean value_function loss: 85.3172
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 81.2213
                       Mean reward: 918.22
               Mean episode length: 240.35
    Episode_Reward/reaching_object: 1.9932
     Episode_Reward/lifting_object: 179.1527
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0847
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 176357376
                    Iteration time: 2.16s
                      Time elapsed: 01:10:17
                               ETA: 00:27:41

################################################################################
                     [1m Learning iteration 1794/2500 [0m                     

                       Computation: 44853 steps/s (collection: 2.072s, learning 0.120s)
             Mean action noise std: 3.47
          Mean value_function loss: 111.7235
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 81.2274
                       Mean reward: 887.89
               Mean episode length: 234.58
    Episode_Reward/reaching_object: 1.9857
     Episode_Reward/lifting_object: 178.1067
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0847
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 176455680
                    Iteration time: 2.19s
                      Time elapsed: 01:10:19
                               ETA: 00:27:39

################################################################################
                     [1m Learning iteration 1795/2500 [0m                     

                       Computation: 45150 steps/s (collection: 2.060s, learning 0.117s)
             Mean action noise std: 3.47
          Mean value_function loss: 99.7986
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 81.2384
                       Mean reward: 899.70
               Mean episode length: 236.83
    Episode_Reward/reaching_object: 1.9811
     Episode_Reward/lifting_object: 177.9878
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0843
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 176553984
                    Iteration time: 2.18s
                      Time elapsed: 01:10:21
                               ETA: 00:27:37

################################################################################
                     [1m Learning iteration 1796/2500 [0m                     

                       Computation: 45196 steps/s (collection: 2.058s, learning 0.117s)
             Mean action noise std: 3.47
          Mean value_function loss: 95.3237
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 81.2490
                       Mean reward: 928.66
               Mean episode length: 243.26
    Episode_Reward/reaching_object: 1.9895
     Episode_Reward/lifting_object: 179.1900
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0846
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 176652288
                    Iteration time: 2.18s
                      Time elapsed: 01:10:23
                               ETA: 00:27:34

################################################################################
                     [1m Learning iteration 1797/2500 [0m                     

                       Computation: 45738 steps/s (collection: 2.032s, learning 0.117s)
             Mean action noise std: 3.47
          Mean value_function loss: 69.0514
               Mean surrogate loss: -0.0000
                 Mean entropy loss: 81.2589
                       Mean reward: 941.97
               Mean episode length: 246.98
    Episode_Reward/reaching_object: 2.0278
     Episode_Reward/lifting_object: 182.8331
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0857
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 13.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 176750592
                    Iteration time: 2.15s
                      Time elapsed: 01:10:25
                               ETA: 00:27:32

################################################################################
                     [1m Learning iteration 1798/2500 [0m                     

                       Computation: 45231 steps/s (collection: 2.064s, learning 0.109s)
             Mean action noise std: 3.47
          Mean value_function loss: 88.4716
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 81.2625
                       Mean reward: 930.51
               Mean episode length: 244.88
    Episode_Reward/reaching_object: 2.0322
     Episode_Reward/lifting_object: 183.1526
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0864
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 176848896
                    Iteration time: 2.17s
                      Time elapsed: 01:10:27
                               ETA: 00:27:29

################################################################################
                     [1m Learning iteration 1799/2500 [0m                     

                       Computation: 45575 steps/s (collection: 2.052s, learning 0.105s)
             Mean action noise std: 3.47
          Mean value_function loss: 80.2103
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 81.2699
                       Mean reward: 920.42
               Mean episode length: 243.03
    Episode_Reward/reaching_object: 2.0167
     Episode_Reward/lifting_object: 181.1354
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0860
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 176947200
                    Iteration time: 2.16s
                      Time elapsed: 01:10:30
                               ETA: 00:27:27

################################################################################
                     [1m Learning iteration 1800/2500 [0m                     

                       Computation: 45372 steps/s (collection: 2.052s, learning 0.114s)
             Mean action noise std: 3.47
          Mean value_function loss: 70.8356
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 81.2765
                       Mean reward: 944.94
               Mean episode length: 248.24
    Episode_Reward/reaching_object: 2.0154
     Episode_Reward/lifting_object: 181.1641
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0856
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 177045504
                    Iteration time: 2.17s
                      Time elapsed: 01:10:32
                               ETA: 00:27:24

################################################################################
                     [1m Learning iteration 1801/2500 [0m                     

                       Computation: 44405 steps/s (collection: 2.100s, learning 0.114s)
             Mean action noise std: 3.47
          Mean value_function loss: 76.2130
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 81.2862
                       Mean reward: 918.92
               Mean episode length: 241.42
    Episode_Reward/reaching_object: 2.0259
     Episode_Reward/lifting_object: 182.0952
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0864
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 177143808
                    Iteration time: 2.21s
                      Time elapsed: 01:10:34
                               ETA: 00:27:22

################################################################################
                     [1m Learning iteration 1802/2500 [0m                     

                       Computation: 44207 steps/s (collection: 2.100s, learning 0.124s)
             Mean action noise std: 3.48
          Mean value_function loss: 78.2515
               Mean surrogate loss: -0.0003
                 Mean entropy loss: 81.2970
                       Mean reward: 935.67
               Mean episode length: 244.52
    Episode_Reward/reaching_object: 2.0335
     Episode_Reward/lifting_object: 182.7554
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0864
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 177242112
                    Iteration time: 2.22s
                      Time elapsed: 01:10:36
                               ETA: 00:27:20

################################################################################
                     [1m Learning iteration 1803/2500 [0m                     

                       Computation: 45551 steps/s (collection: 2.069s, learning 0.089s)
             Mean action noise std: 3.48
          Mean value_function loss: 89.7901
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.3017
                       Mean reward: 894.01
               Mean episode length: 235.09
    Episode_Reward/reaching_object: 1.9932
     Episode_Reward/lifting_object: 178.8877
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0849
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 16.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 177340416
                    Iteration time: 2.16s
                      Time elapsed: 01:10:38
                               ETA: 00:27:17

################################################################################
                     [1m Learning iteration 1804/2500 [0m                     

                       Computation: 45183 steps/s (collection: 2.060s, learning 0.116s)
             Mean action noise std: 3.48
          Mean value_function loss: 67.7926
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 81.3102
                       Mean reward: 891.88
               Mean episode length: 235.78
    Episode_Reward/reaching_object: 2.0475
     Episode_Reward/lifting_object: 183.6087
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0867
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 177438720
                    Iteration time: 2.18s
                      Time elapsed: 01:10:41
                               ETA: 00:27:15

################################################################################
                     [1m Learning iteration 1805/2500 [0m                     

                       Computation: 44944 steps/s (collection: 2.075s, learning 0.112s)
             Mean action noise std: 3.48
          Mean value_function loss: 82.7203
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 81.3136
                       Mean reward: 932.04
               Mean episode length: 243.60
    Episode_Reward/reaching_object: 2.0002
     Episode_Reward/lifting_object: 179.1046
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 177537024
                    Iteration time: 2.19s
                      Time elapsed: 01:10:43
                               ETA: 00:27:12

################################################################################
                     [1m Learning iteration 1806/2500 [0m                     

                       Computation: 46036 steps/s (collection: 2.045s, learning 0.090s)
             Mean action noise std: 3.48
          Mean value_function loss: 88.4596
               Mean surrogate loss: 0.0009
                 Mean entropy loss: 81.3157
                       Mean reward: 900.22
               Mean episode length: 238.55
    Episode_Reward/reaching_object: 2.0230
     Episode_Reward/lifting_object: 181.4064
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0861
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 177635328
                    Iteration time: 2.14s
                      Time elapsed: 01:10:45
                               ETA: 00:27:10

################################################################################
                     [1m Learning iteration 1807/2500 [0m                     

                       Computation: 45182 steps/s (collection: 2.062s, learning 0.114s)
             Mean action noise std: 3.48
          Mean value_function loss: 92.2496
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 81.3181
                       Mean reward: 879.67
               Mean episode length: 233.13
    Episode_Reward/reaching_object: 2.0140
     Episode_Reward/lifting_object: 180.3932
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 177733632
                    Iteration time: 2.18s
                      Time elapsed: 01:10:47
                               ETA: 00:27:08

################################################################################
                     [1m Learning iteration 1808/2500 [0m                     

                       Computation: 45282 steps/s (collection: 2.050s, learning 0.121s)
             Mean action noise std: 3.48
          Mean value_function loss: 132.7704
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 81.3261
                       Mean reward: 897.66
               Mean episode length: 236.26
    Episode_Reward/reaching_object: 1.9805
     Episode_Reward/lifting_object: 177.6086
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0849
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 177831936
                    Iteration time: 2.17s
                      Time elapsed: 01:10:49
                               ETA: 00:27:05

################################################################################
                     [1m Learning iteration 1809/2500 [0m                     

                       Computation: 44991 steps/s (collection: 2.067s, learning 0.118s)
             Mean action noise std: 3.48
          Mean value_function loss: 76.3233
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 81.3345
                       Mean reward: 930.92
               Mean episode length: 244.93
    Episode_Reward/reaching_object: 2.0149
     Episode_Reward/lifting_object: 181.2774
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0860
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 177930240
                    Iteration time: 2.18s
                      Time elapsed: 01:10:51
                               ETA: 00:27:03

################################################################################
                     [1m Learning iteration 1810/2500 [0m                     

                       Computation: 45531 steps/s (collection: 2.053s, learning 0.106s)
             Mean action noise std: 3.48
          Mean value_function loss: 70.0221
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 81.3441
                       Mean reward: 904.54
               Mean episode length: 238.36
    Episode_Reward/reaching_object: 2.0148
     Episode_Reward/lifting_object: 181.0622
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 178028544
                    Iteration time: 2.16s
                      Time elapsed: 01:10:54
                               ETA: 00:27:00

################################################################################
                     [1m Learning iteration 1811/2500 [0m                     

                       Computation: 45357 steps/s (collection: 2.053s, learning 0.115s)
             Mean action noise std: 3.48
          Mean value_function loss: 70.7254
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 81.3545
                       Mean reward: 897.94
               Mean episode length: 236.47
    Episode_Reward/reaching_object: 2.0024
     Episode_Reward/lifting_object: 180.0025
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0858
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 178126848
                    Iteration time: 2.17s
                      Time elapsed: 01:10:56
                               ETA: 00:26:58

################################################################################
                     [1m Learning iteration 1812/2500 [0m                     

                       Computation: 45138 steps/s (collection: 2.065s, learning 0.113s)
             Mean action noise std: 3.48
          Mean value_function loss: 126.6816
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 81.3724
                       Mean reward: 909.46
               Mean episode length: 239.79
    Episode_Reward/reaching_object: 2.0124
     Episode_Reward/lifting_object: 181.2654
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 178225152
                    Iteration time: 2.18s
                      Time elapsed: 01:10:58
                               ETA: 00:26:55

################################################################################
                     [1m Learning iteration 1813/2500 [0m                     

                       Computation: 45322 steps/s (collection: 2.055s, learning 0.114s)
             Mean action noise std: 3.49
          Mean value_function loss: 104.6957
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 81.3898
                       Mean reward: 920.28
               Mean episode length: 242.01
    Episode_Reward/reaching_object: 2.0236
     Episode_Reward/lifting_object: 182.1503
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0868
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 178323456
                    Iteration time: 2.17s
                      Time elapsed: 01:11:00
                               ETA: 00:26:53

################################################################################
                     [1m Learning iteration 1814/2500 [0m                     

                       Computation: 45772 steps/s (collection: 2.050s, learning 0.098s)
             Mean action noise std: 3.49
          Mean value_function loss: 77.6495
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 81.4070
                       Mean reward: 906.24
               Mean episode length: 238.78
    Episode_Reward/reaching_object: 2.0162
     Episode_Reward/lifting_object: 181.1486
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0865
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 178421760
                    Iteration time: 2.15s
                      Time elapsed: 01:11:02
                               ETA: 00:26:51

################################################################################
                     [1m Learning iteration 1815/2500 [0m                     

                       Computation: 45665 steps/s (collection: 2.048s, learning 0.105s)
             Mean action noise std: 3.49
          Mean value_function loss: 94.0530
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 81.4179
                       Mean reward: 936.12
               Mean episode length: 245.59
    Episode_Reward/reaching_object: 2.0358
     Episode_Reward/lifting_object: 182.9952
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0869
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 178520064
                    Iteration time: 2.15s
                      Time elapsed: 01:11:04
                               ETA: 00:26:48

################################################################################
                     [1m Learning iteration 1816/2500 [0m                     

                       Computation: 45925 steps/s (collection: 2.035s, learning 0.106s)
             Mean action noise std: 3.49
          Mean value_function loss: 81.2974
               Mean surrogate loss: -0.0001
                 Mean entropy loss: 81.4200
                       Mean reward: 905.26
               Mean episode length: 237.31
    Episode_Reward/reaching_object: 2.0250
     Episode_Reward/lifting_object: 181.5810
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0865
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 178618368
                    Iteration time: 2.14s
                      Time elapsed: 01:11:06
                               ETA: 00:26:46

################################################################################
                     [1m Learning iteration 1817/2500 [0m                     

                       Computation: 45609 steps/s (collection: 2.048s, learning 0.108s)
             Mean action noise std: 3.49
          Mean value_function loss: 96.5305
               Mean surrogate loss: 0.0034
                 Mean entropy loss: 81.4215
                       Mean reward: 900.32
               Mean episode length: 237.26
    Episode_Reward/reaching_object: 2.0078
     Episode_Reward/lifting_object: 179.5431
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0857
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 178716672
                    Iteration time: 2.16s
                      Time elapsed: 01:11:09
                               ETA: 00:26:43

################################################################################
                     [1m Learning iteration 1818/2500 [0m                     

                       Computation: 45553 steps/s (collection: 2.044s, learning 0.114s)
             Mean action noise std: 3.49
          Mean value_function loss: 67.1095
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.4242
                       Mean reward: 948.38
               Mean episode length: 247.12
    Episode_Reward/reaching_object: 2.0316
     Episode_Reward/lifting_object: 181.7509
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0868
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 178814976
                    Iteration time: 2.16s
                      Time elapsed: 01:11:11
                               ETA: 00:26:41

################################################################################
                     [1m Learning iteration 1819/2500 [0m                     

                       Computation: 45449 steps/s (collection: 2.049s, learning 0.114s)
             Mean action noise std: 3.49
          Mean value_function loss: 95.7144
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 81.4318
                       Mean reward: 917.30
               Mean episode length: 241.44
    Episode_Reward/reaching_object: 2.0238
     Episode_Reward/lifting_object: 181.1708
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0865
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 178913280
                    Iteration time: 2.16s
                      Time elapsed: 01:11:13
                               ETA: 00:26:39

################################################################################
                     [1m Learning iteration 1820/2500 [0m                     

                       Computation: 45395 steps/s (collection: 2.053s, learning 0.112s)
             Mean action noise std: 3.49
          Mean value_function loss: 99.3078
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.4396
                       Mean reward: 898.98
               Mean episode length: 236.37
    Episode_Reward/reaching_object: 2.0257
     Episode_Reward/lifting_object: 181.8166
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0866
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 179011584
                    Iteration time: 2.17s
                      Time elapsed: 01:11:15
                               ETA: 00:26:36

################################################################################
                     [1m Learning iteration 1821/2500 [0m                     

                       Computation: 45631 steps/s (collection: 2.043s, learning 0.112s)
             Mean action noise std: 3.49
          Mean value_function loss: 77.2482
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 81.4492
                       Mean reward: 925.87
               Mean episode length: 242.54
    Episode_Reward/reaching_object: 1.9892
     Episode_Reward/lifting_object: 178.3875
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 179109888
                    Iteration time: 2.15s
                      Time elapsed: 01:11:17
                               ETA: 00:26:34

################################################################################
                     [1m Learning iteration 1822/2500 [0m                     

                       Computation: 44862 steps/s (collection: 2.072s, learning 0.120s)
             Mean action noise std: 3.49
          Mean value_function loss: 91.5753
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 81.4624
                       Mean reward: 917.66
               Mean episode length: 240.41
    Episode_Reward/reaching_object: 1.9855
     Episode_Reward/lifting_object: 177.8938
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0854
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 179208192
                    Iteration time: 2.19s
                      Time elapsed: 01:11:19
                               ETA: 00:26:31

################################################################################
                     [1m Learning iteration 1823/2500 [0m                     

                       Computation: 45118 steps/s (collection: 2.061s, learning 0.118s)
             Mean action noise std: 3.50
          Mean value_function loss: 70.5357
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 81.4810
                       Mean reward: 932.13
               Mean episode length: 243.92
    Episode_Reward/reaching_object: 2.0411
     Episode_Reward/lifting_object: 183.1999
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 179306496
                    Iteration time: 2.18s
                      Time elapsed: 01:11:22
                               ETA: 00:26:29

################################################################################
                     [1m Learning iteration 1824/2500 [0m                     

                       Computation: 44877 steps/s (collection: 2.075s, learning 0.115s)
             Mean action noise std: 3.50
          Mean value_function loss: 102.1888
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 81.4900
                       Mean reward: 911.62
               Mean episode length: 240.69
    Episode_Reward/reaching_object: 2.0049
     Episode_Reward/lifting_object: 180.0599
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 179404800
                    Iteration time: 2.19s
                      Time elapsed: 01:11:24
                               ETA: 00:26:26

################################################################################
                     [1m Learning iteration 1825/2500 [0m                     

                       Computation: 45006 steps/s (collection: 2.073s, learning 0.111s)
             Mean action noise std: 3.50
          Mean value_function loss: 111.2839
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 81.5035
                       Mean reward: 920.48
               Mean episode length: 242.42
    Episode_Reward/reaching_object: 1.9996
     Episode_Reward/lifting_object: 179.7050
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 179503104
                    Iteration time: 2.18s
                      Time elapsed: 01:11:26
                               ETA: 00:26:24

################################################################################
                     [1m Learning iteration 1826/2500 [0m                     

                       Computation: 45179 steps/s (collection: 2.064s, learning 0.112s)
             Mean action noise std: 3.50
          Mean value_function loss: 89.0853
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 81.5125
                       Mean reward: 927.88
               Mean episode length: 243.47
    Episode_Reward/reaching_object: 1.9915
     Episode_Reward/lifting_object: 179.0142
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0862
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 179601408
                    Iteration time: 2.18s
                      Time elapsed: 01:11:28
                               ETA: 00:26:22

################################################################################
                     [1m Learning iteration 1827/2500 [0m                     

                       Computation: 45223 steps/s (collection: 2.056s, learning 0.118s)
             Mean action noise std: 3.50
          Mean value_function loss: 76.4960
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 81.5233
                       Mean reward: 938.46
               Mean episode length: 245.30
    Episode_Reward/reaching_object: 2.0155
     Episode_Reward/lifting_object: 181.9941
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0869
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 179699712
                    Iteration time: 2.17s
                      Time elapsed: 01:11:30
                               ETA: 00:26:19

################################################################################
                     [1m Learning iteration 1828/2500 [0m                     

                       Computation: 42142 steps/s (collection: 2.202s, learning 0.130s)
             Mean action noise std: 3.50
          Mean value_function loss: 98.4413
               Mean surrogate loss: 0.0002
                 Mean entropy loss: 81.5299
                       Mean reward: 896.89
               Mean episode length: 235.09
    Episode_Reward/reaching_object: 2.0097
     Episode_Reward/lifting_object: 181.3555
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0868
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 179798016
                    Iteration time: 2.33s
                      Time elapsed: 01:11:33
                               ETA: 00:26:17

################################################################################
                     [1m Learning iteration 1829/2500 [0m                     

                       Computation: 42535 steps/s (collection: 2.213s, learning 0.098s)
             Mean action noise std: 3.50
          Mean value_function loss: 141.4104
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 81.5346
                       Mean reward: 878.53
               Mean episode length: 231.95
    Episode_Reward/reaching_object: 1.9522
     Episode_Reward/lifting_object: 175.7214
      Episode_Reward/object_height: 0.0156
        Episode_Reward/action_rate: -0.0848
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.5417
--------------------------------------------------------------------------------
                   Total timesteps: 179896320
                    Iteration time: 2.31s
                      Time elapsed: 01:11:35
                               ETA: 00:26:15

################################################################################
                     [1m Learning iteration 1830/2500 [0m                     

                       Computation: 42657 steps/s (collection: 2.188s, learning 0.117s)
             Mean action noise std: 3.50
          Mean value_function loss: 84.8849
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 81.5378
                       Mean reward: 937.41
               Mean episode length: 245.78
    Episode_Reward/reaching_object: 2.0115
     Episode_Reward/lifting_object: 181.6916
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0866
          Episode_Reward/joint_vel: -0.0583
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 179994624
                    Iteration time: 2.30s
                      Time elapsed: 01:11:37
                               ETA: 00:26:12

################################################################################
                     [1m Learning iteration 1831/2500 [0m                     

                       Computation: 39846 steps/s (collection: 2.331s, learning 0.136s)
             Mean action noise std: 3.50
          Mean value_function loss: 81.8437
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 81.5433
                       Mean reward: 921.00
               Mean episode length: 241.28
    Episode_Reward/reaching_object: 1.9789
     Episode_Reward/lifting_object: 178.2666
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0856
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 180092928
                    Iteration time: 2.47s
                      Time elapsed: 01:11:40
                               ETA: 00:26:10

################################################################################
                     [1m Learning iteration 1832/2500 [0m                     

                       Computation: 39776 steps/s (collection: 2.367s, learning 0.104s)
             Mean action noise std: 3.50
          Mean value_function loss: 93.4069
               Mean surrogate loss: -0.0009
                 Mean entropy loss: 81.5514
                       Mean reward: 893.40
               Mean episode length: 236.09
    Episode_Reward/reaching_object: 1.9926
     Episode_Reward/lifting_object: 179.4810
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0866
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 180191232
                    Iteration time: 2.47s
                      Time elapsed: 01:11:42
                               ETA: 00:26:08

################################################################################
                     [1m Learning iteration 1833/2500 [0m                     

                       Computation: 42973 steps/s (collection: 2.172s, learning 0.116s)
             Mean action noise std: 3.50
          Mean value_function loss: 107.5416
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 81.5580
                       Mean reward: 906.72
               Mean episode length: 238.88
    Episode_Reward/reaching_object: 1.9980
     Episode_Reward/lifting_object: 179.9839
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0867
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 16.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180289536
                    Iteration time: 2.29s
                      Time elapsed: 01:11:45
                               ETA: 00:26:05

################################################################################
                     [1m Learning iteration 1834/2500 [0m                     

                       Computation: 43517 steps/s (collection: 2.146s, learning 0.112s)
             Mean action noise std: 3.51
          Mean value_function loss: 117.5939
               Mean surrogate loss: -0.0007
                 Mean entropy loss: 81.5695
                       Mean reward: 887.17
               Mean episode length: 234.81
    Episode_Reward/reaching_object: 1.9883
     Episode_Reward/lifting_object: 178.9768
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0864
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180387840
                    Iteration time: 2.26s
                      Time elapsed: 01:11:47
                               ETA: 00:26:03

################################################################################
                     [1m Learning iteration 1835/2500 [0m                     

                       Computation: 43859 steps/s (collection: 2.116s, learning 0.125s)
             Mean action noise std: 3.51
          Mean value_function loss: 115.3734
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 81.5772
                       Mean reward: 909.29
               Mean episode length: 238.80
    Episode_Reward/reaching_object: 1.9758
     Episode_Reward/lifting_object: 178.0302
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0857
          Episode_Reward/joint_vel: -0.0582
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 180486144
                    Iteration time: 2.24s
                      Time elapsed: 01:11:49
                               ETA: 00:26:00

################################################################################
                     [1m Learning iteration 1836/2500 [0m                     

                       Computation: 43164 steps/s (collection: 2.176s, learning 0.101s)
             Mean action noise std: 3.51
          Mean value_function loss: 115.7307
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 81.5860
                       Mean reward: 906.31
               Mean episode length: 238.23
    Episode_Reward/reaching_object: 1.9937
     Episode_Reward/lifting_object: 179.5365
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0865
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 180584448
                    Iteration time: 2.28s
                      Time elapsed: 01:11:51
                               ETA: 00:25:58

################################################################################
                     [1m Learning iteration 1837/2500 [0m                     

                       Computation: 42146 steps/s (collection: 2.235s, learning 0.098s)
             Mean action noise std: 3.51
          Mean value_function loss: 101.6805
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 81.5941
                       Mean reward: 917.24
               Mean episode length: 242.02
    Episode_Reward/reaching_object: 1.9890
     Episode_Reward/lifting_object: 178.9328
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0865
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 180682752
                    Iteration time: 2.33s
                      Time elapsed: 01:11:54
                               ETA: 00:25:56

################################################################################
                     [1m Learning iteration 1838/2500 [0m                     

                       Computation: 43111 steps/s (collection: 2.152s, learning 0.129s)
             Mean action noise std: 3.51
          Mean value_function loss: 134.9068
               Mean surrogate loss: -0.0005
                 Mean entropy loss: 81.6012
                       Mean reward: 894.60
               Mean episode length: 235.55
    Episode_Reward/reaching_object: 1.9385
     Episode_Reward/lifting_object: 174.3722
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0842
          Episode_Reward/joint_vel: -0.0574
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.6250
--------------------------------------------------------------------------------
                   Total timesteps: 180781056
                    Iteration time: 2.28s
                      Time elapsed: 01:11:56
                               ETA: 00:25:53

################################################################################
                     [1m Learning iteration 1839/2500 [0m                     

                       Computation: 44147 steps/s (collection: 2.128s, learning 0.098s)
             Mean action noise std: 3.51
          Mean value_function loss: 122.2781
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 81.6071
                       Mean reward: 873.00
               Mean episode length: 230.34
    Episode_Reward/reaching_object: 1.9782
     Episode_Reward/lifting_object: 178.1155
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0858
          Episode_Reward/joint_vel: -0.0578
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 180879360
                    Iteration time: 2.23s
                      Time elapsed: 01:11:58
                               ETA: 00:25:51

################################################################################
                     [1m Learning iteration 1840/2500 [0m                     

                       Computation: 44391 steps/s (collection: 2.094s, learning 0.121s)
             Mean action noise std: 3.51
          Mean value_function loss: 110.7710
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 81.6223
                       Mean reward: 908.24
               Mean episode length: 237.85
    Episode_Reward/reaching_object: 2.0093
     Episode_Reward/lifting_object: 181.0686
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0872
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 180977664
                    Iteration time: 2.21s
                      Time elapsed: 01:12:00
                               ETA: 00:25:49

################################################################################
                     [1m Learning iteration 1841/2500 [0m                     

                       Computation: 41761 steps/s (collection: 2.230s, learning 0.124s)
             Mean action noise std: 3.51
          Mean value_function loss: 106.6491
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 81.6325
                       Mean reward: 885.79
               Mean episode length: 233.36
    Episode_Reward/reaching_object: 1.9778
     Episode_Reward/lifting_object: 177.8022
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0857
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 181075968
                    Iteration time: 2.35s
                      Time elapsed: 01:12:03
                               ETA: 00:25:46

################################################################################
                     [1m Learning iteration 1842/2500 [0m                     

                       Computation: 45003 steps/s (collection: 2.092s, learning 0.092s)
             Mean action noise std: 3.51
          Mean value_function loss: 78.4607
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 81.6401
                       Mean reward: 934.17
               Mean episode length: 244.63
    Episode_Reward/reaching_object: 2.0236
     Episode_Reward/lifting_object: 182.0799
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0879
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 181174272
                    Iteration time: 2.18s
                      Time elapsed: 01:12:05
                               ETA: 00:25:44

################################################################################
                     [1m Learning iteration 1843/2500 [0m                     

                       Computation: 44184 steps/s (collection: 2.085s, learning 0.140s)
             Mean action noise std: 3.51
          Mean value_function loss: 94.8308
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 81.6507
                       Mean reward: 934.40
               Mean episode length: 245.48
    Episode_Reward/reaching_object: 2.0256
     Episode_Reward/lifting_object: 181.9225
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0878
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 181272576
                    Iteration time: 2.22s
                      Time elapsed: 01:12:07
                               ETA: 00:25:41

################################################################################
                     [1m Learning iteration 1844/2500 [0m                     

                       Computation: 44925 steps/s (collection: 2.097s, learning 0.092s)
             Mean action noise std: 3.52
          Mean value_function loss: 108.6783
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 81.6622
                       Mean reward: 895.17
               Mean episode length: 235.32
    Episode_Reward/reaching_object: 1.9825
     Episode_Reward/lifting_object: 177.7383
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0861
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 16.0417
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 181370880
                    Iteration time: 2.19s
                      Time elapsed: 01:12:09
                               ETA: 00:25:39

################################################################################
                     [1m Learning iteration 1845/2500 [0m                     

                       Computation: 44958 steps/s (collection: 2.093s, learning 0.094s)
             Mean action noise std: 3.52
          Mean value_function loss: 96.9494
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 81.6743
                       Mean reward: 907.87
               Mean episode length: 238.56
    Episode_Reward/reaching_object: 2.0165
     Episode_Reward/lifting_object: 181.2391
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0874
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 181469184
                    Iteration time: 2.19s
                      Time elapsed: 01:12:12
                               ETA: 00:25:37

################################################################################
                     [1m Learning iteration 1846/2500 [0m                     

                       Computation: 44478 steps/s (collection: 2.097s, learning 0.114s)
             Mean action noise std: 3.52
          Mean value_function loss: 76.3643
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 81.6910
                       Mean reward: 925.58
               Mean episode length: 242.51
    Episode_Reward/reaching_object: 1.9971
     Episode_Reward/lifting_object: 179.3343
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0866
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 181567488
                    Iteration time: 2.21s
                      Time elapsed: 01:12:14
                               ETA: 00:25:34

################################################################################
                     [1m Learning iteration 1847/2500 [0m                     

                       Computation: 44845 steps/s (collection: 2.090s, learning 0.103s)
             Mean action noise std: 3.52
          Mean value_function loss: 89.2196
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.7108
                       Mean reward: 921.76
               Mean episode length: 243.26
    Episode_Reward/reaching_object: 2.0247
     Episode_Reward/lifting_object: 181.5653
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0881
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 181665792
                    Iteration time: 2.19s
                      Time elapsed: 01:12:16
                               ETA: 00:25:32

################################################################################
                     [1m Learning iteration 1848/2500 [0m                     

                       Computation: 43964 steps/s (collection: 2.129s, learning 0.107s)
             Mean action noise std: 3.52
          Mean value_function loss: 101.6313
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 81.7288
                       Mean reward: 897.10
               Mean episode length: 236.34
    Episode_Reward/reaching_object: 2.0177
     Episode_Reward/lifting_object: 181.2480
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 181764096
                    Iteration time: 2.24s
                      Time elapsed: 01:12:18
                               ETA: 00:25:29

################################################################################
                     [1m Learning iteration 1849/2500 [0m                     

                       Computation: 45280 steps/s (collection: 2.084s, learning 0.087s)
             Mean action noise std: 3.52
          Mean value_function loss: 103.7592
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 81.7422
                       Mean reward: 912.90
               Mean episode length: 240.56
    Episode_Reward/reaching_object: 2.0110
     Episode_Reward/lifting_object: 180.4742
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 13.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 181862400
                    Iteration time: 2.17s
                      Time elapsed: 01:12:20
                               ETA: 00:25:27

################################################################################
                     [1m Learning iteration 1850/2500 [0m                     

                       Computation: 40590 steps/s (collection: 2.294s, learning 0.127s)
             Mean action noise std: 3.53
          Mean value_function loss: 90.1873
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 81.7541
                       Mean reward: 880.48
               Mean episode length: 231.84
    Episode_Reward/reaching_object: 2.0079
     Episode_Reward/lifting_object: 180.2047
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0876
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 181960704
                    Iteration time: 2.42s
                      Time elapsed: 01:12:23
                               ETA: 00:25:25

################################################################################
                     [1m Learning iteration 1851/2500 [0m                     

                       Computation: 40518 steps/s (collection: 2.297s, learning 0.130s)
             Mean action noise std: 3.53
          Mean value_function loss: 99.0467
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 81.7608
                       Mean reward: 904.39
               Mean episode length: 239.40
    Episode_Reward/reaching_object: 2.0107
     Episode_Reward/lifting_object: 180.3625
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0879
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 182059008
                    Iteration time: 2.43s
                      Time elapsed: 01:12:25
                               ETA: 00:25:22

################################################################################
                     [1m Learning iteration 1852/2500 [0m                     

                       Computation: 45114 steps/s (collection: 2.078s, learning 0.101s)
             Mean action noise std: 3.53
          Mean value_function loss: 118.1752
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 81.7661
                       Mean reward: 900.04
               Mean episode length: 236.99
    Episode_Reward/reaching_object: 2.0024
     Episode_Reward/lifting_object: 179.6132
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0877
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 182157312
                    Iteration time: 2.18s
                      Time elapsed: 01:12:27
                               ETA: 00:25:20

################################################################################
                     [1m Learning iteration 1853/2500 [0m                     

                       Computation: 44194 steps/s (collection: 2.118s, learning 0.107s)
             Mean action noise std: 3.53
          Mean value_function loss: 108.3484
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 81.7688
                       Mean reward: 907.71
               Mean episode length: 238.41
    Episode_Reward/reaching_object: 1.9899
     Episode_Reward/lifting_object: 178.6705
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0872
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 182255616
                    Iteration time: 2.22s
                      Time elapsed: 01:12:30
                               ETA: 00:25:18

################################################################################
                     [1m Learning iteration 1854/2500 [0m                     

                       Computation: 42984 steps/s (collection: 2.161s, learning 0.126s)
             Mean action noise std: 3.53
          Mean value_function loss: 76.0911
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 81.7715
                       Mean reward: 940.09
               Mean episode length: 246.55
    Episode_Reward/reaching_object: 2.0108
     Episode_Reward/lifting_object: 180.6538
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0878
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 182353920
                    Iteration time: 2.29s
                      Time elapsed: 01:12:32
                               ETA: 00:25:15

################################################################################
                     [1m Learning iteration 1855/2500 [0m                     

                       Computation: 35351 steps/s (collection: 2.613s, learning 0.168s)
             Mean action noise std: 3.53
          Mean value_function loss: 87.8906
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.7806
                       Mean reward: 928.64
               Mean episode length: 243.32
    Episode_Reward/reaching_object: 2.0127
     Episode_Reward/lifting_object: 180.7295
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0881
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 182452224
                    Iteration time: 2.78s
                      Time elapsed: 01:12:35
                               ETA: 00:25:13

################################################################################
                     [1m Learning iteration 1856/2500 [0m                     

                       Computation: 35413 steps/s (collection: 2.660s, learning 0.116s)
             Mean action noise std: 3.53
          Mean value_function loss: 78.8219
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 81.7918
                       Mean reward: 938.25
               Mean episode length: 245.99
    Episode_Reward/reaching_object: 1.9987
     Episode_Reward/lifting_object: 179.1662
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0874
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 182550528
                    Iteration time: 2.78s
                      Time elapsed: 01:12:37
                               ETA: 00:25:11

################################################################################
                     [1m Learning iteration 1857/2500 [0m                     

                       Computation: 38746 steps/s (collection: 2.359s, learning 0.179s)
             Mean action noise std: 3.53
          Mean value_function loss: 57.6434
               Mean surrogate loss: 0.0018
                 Mean entropy loss: 81.8024
                       Mean reward: 926.77
               Mean episode length: 245.19
    Episode_Reward/reaching_object: 2.0505
     Episode_Reward/lifting_object: 183.6702
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 182648832
                    Iteration time: 2.54s
                      Time elapsed: 01:12:40
                               ETA: 00:25:09

################################################################################
                     [1m Learning iteration 1858/2500 [0m                     

                       Computation: 39536 steps/s (collection: 2.383s, learning 0.103s)
             Mean action noise std: 3.53
          Mean value_function loss: 101.2747
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 81.8070
                       Mean reward: 887.91
               Mean episode length: 234.59
    Episode_Reward/reaching_object: 1.9929
     Episode_Reward/lifting_object: 178.3026
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0874
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 182747136
                    Iteration time: 2.49s
                      Time elapsed: 01:12:42
                               ETA: 00:25:06

################################################################################
                     [1m Learning iteration 1859/2500 [0m                     

                       Computation: 42375 steps/s (collection: 2.199s, learning 0.121s)
             Mean action noise std: 3.53
          Mean value_function loss: 102.5830
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 81.8175
                       Mean reward: 934.18
               Mean episode length: 244.09
    Episode_Reward/reaching_object: 2.0172
     Episode_Reward/lifting_object: 181.0684
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0885
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 182845440
                    Iteration time: 2.32s
                      Time elapsed: 01:12:45
                               ETA: 00:25:04

################################################################################
                     [1m Learning iteration 1860/2500 [0m                     

                       Computation: 42533 steps/s (collection: 2.199s, learning 0.112s)
             Mean action noise std: 3.53
          Mean value_function loss: 88.8144
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 81.8241
                       Mean reward: 917.92
               Mean episode length: 240.27
    Episode_Reward/reaching_object: 1.9996
     Episode_Reward/lifting_object: 179.3795
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0880
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 15.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 182943744
                    Iteration time: 2.31s
                      Time elapsed: 01:12:47
                               ETA: 00:25:02

################################################################################
                     [1m Learning iteration 1861/2500 [0m                     

                       Computation: 40640 steps/s (collection: 2.294s, learning 0.125s)
             Mean action noise std: 3.54
          Mean value_function loss: 81.7593
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 81.8338
                       Mean reward: 932.45
               Mean episode length: 244.50
    Episode_Reward/reaching_object: 2.0073
     Episode_Reward/lifting_object: 180.0746
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0887
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 183042048
                    Iteration time: 2.42s
                      Time elapsed: 01:12:49
                               ETA: 00:24:59

################################################################################
                     [1m Learning iteration 1862/2500 [0m                     

                       Computation: 41441 steps/s (collection: 2.253s, learning 0.119s)
             Mean action noise std: 3.54
          Mean value_function loss: 87.3218
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 81.8478
                       Mean reward: 920.77
               Mean episode length: 241.57
    Episode_Reward/reaching_object: 1.9684
     Episode_Reward/lifting_object: 176.7492
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0871
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 183140352
                    Iteration time: 2.37s
                      Time elapsed: 01:12:52
                               ETA: 00:24:57

################################################################################
                     [1m Learning iteration 1863/2500 [0m                     

                       Computation: 39980 steps/s (collection: 2.309s, learning 0.150s)
             Mean action noise std: 3.54
          Mean value_function loss: 106.2200
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 81.8590
                       Mean reward: 913.91
               Mean episode length: 239.18
    Episode_Reward/reaching_object: 1.9944
     Episode_Reward/lifting_object: 179.9403
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0878
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 183238656
                    Iteration time: 2.46s
                      Time elapsed: 01:12:54
                               ETA: 00:24:55

################################################################################
                     [1m Learning iteration 1864/2500 [0m                     

                       Computation: 41572 steps/s (collection: 2.184s, learning 0.181s)
             Mean action noise std: 3.54
          Mean value_function loss: 123.7740
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 81.8681
                       Mean reward: 883.44
               Mean episode length: 233.03
    Episode_Reward/reaching_object: 1.9713
     Episode_Reward/lifting_object: 177.8373
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0870
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 183336960
                    Iteration time: 2.36s
                      Time elapsed: 01:12:57
                               ETA: 00:24:52

################################################################################
                     [1m Learning iteration 1865/2500 [0m                     

                       Computation: 40732 steps/s (collection: 2.294s, learning 0.119s)
             Mean action noise std: 3.54
          Mean value_function loss: 129.6418
               Mean surrogate loss: 0.0007
                 Mean entropy loss: 81.8745
                       Mean reward: 901.76
               Mean episode length: 238.23
    Episode_Reward/reaching_object: 1.9763
     Episode_Reward/lifting_object: 177.7982
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0875
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 183435264
                    Iteration time: 2.41s
                      Time elapsed: 01:12:59
                               ETA: 00:24:50

################################################################################
                     [1m Learning iteration 1866/2500 [0m                     

                       Computation: 42170 steps/s (collection: 2.228s, learning 0.104s)
             Mean action noise std: 3.54
          Mean value_function loss: 91.8134
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 81.8785
                       Mean reward: 906.09
               Mean episode length: 240.30
    Episode_Reward/reaching_object: 1.9896
     Episode_Reward/lifting_object: 179.1127
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0882
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 183533568
                    Iteration time: 2.33s
                      Time elapsed: 01:13:01
                               ETA: 00:24:48

################################################################################
                     [1m Learning iteration 1867/2500 [0m                     

                       Computation: 43261 steps/s (collection: 2.160s, learning 0.113s)
             Mean action noise std: 3.54
          Mean value_function loss: 95.6709
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 81.8883
                       Mean reward: 919.89
               Mean episode length: 240.81
    Episode_Reward/reaching_object: 1.9930
     Episode_Reward/lifting_object: 179.6193
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0881
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 183631872
                    Iteration time: 2.27s
                      Time elapsed: 01:13:04
                               ETA: 00:24:45

################################################################################
                     [1m Learning iteration 1868/2500 [0m                     

                       Computation: 43824 steps/s (collection: 2.129s, learning 0.115s)
             Mean action noise std: 3.54
          Mean value_function loss: 114.9169
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 81.9046
                       Mean reward: 884.84
               Mean episode length: 233.80
    Episode_Reward/reaching_object: 1.9710
     Episode_Reward/lifting_object: 177.7054
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0877
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9583
--------------------------------------------------------------------------------
                   Total timesteps: 183730176
                    Iteration time: 2.24s
                      Time elapsed: 01:13:06
                               ETA: 00:24:43

################################################################################
                     [1m Learning iteration 1869/2500 [0m                     

                       Computation: 40597 steps/s (collection: 2.289s, learning 0.133s)
             Mean action noise std: 3.54
          Mean value_function loss: 110.0837
               Mean surrogate loss: 0.0024
                 Mean entropy loss: 81.9175
                       Mean reward: 912.42
               Mean episode length: 238.77
    Episode_Reward/reaching_object: 1.9855
     Episode_Reward/lifting_object: 179.2556
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0878
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 16.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 183828480
                    Iteration time: 2.42s
                      Time elapsed: 01:13:08
                               ETA: 00:24:40

################################################################################
                     [1m Learning iteration 1870/2500 [0m                     

                       Computation: 38177 steps/s (collection: 2.413s, learning 0.162s)
             Mean action noise std: 3.55
          Mean value_function loss: 60.4359
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 81.9196
                       Mean reward: 927.54
               Mean episode length: 243.12
    Episode_Reward/reaching_object: 2.0188
     Episode_Reward/lifting_object: 182.7175
      Episode_Reward/object_height: 0.0180
        Episode_Reward/action_rate: -0.0890
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 13.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.9167
--------------------------------------------------------------------------------
                   Total timesteps: 183926784
                    Iteration time: 2.57s
                      Time elapsed: 01:13:11
                               ETA: 00:24:38

################################################################################
                     [1m Learning iteration 1871/2500 [0m                     

                       Computation: 38206 steps/s (collection: 2.402s, learning 0.171s)
             Mean action noise std: 3.55
          Mean value_function loss: 129.8541
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 81.9272
                       Mean reward: 890.91
               Mean episode length: 236.62
    Episode_Reward/reaching_object: 1.9959
     Episode_Reward/lifting_object: 179.4266
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0885
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 184025088
                    Iteration time: 2.57s
                      Time elapsed: 01:13:14
                               ETA: 00:24:36

################################################################################
                     [1m Learning iteration 1872/2500 [0m                     

                       Computation: 39066 steps/s (collection: 2.373s, learning 0.143s)
             Mean action noise std: 3.55
          Mean value_function loss: 79.7151
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 81.9440
                       Mean reward: 925.54
               Mean episode length: 243.21
    Episode_Reward/reaching_object: 2.0161
     Episode_Reward/lifting_object: 181.5045
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0889
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 184123392
                    Iteration time: 2.52s
                      Time elapsed: 01:13:16
                               ETA: 00:24:34

################################################################################
                     [1m Learning iteration 1873/2500 [0m                     

                       Computation: 36230 steps/s (collection: 2.518s, learning 0.196s)
             Mean action noise std: 3.55
          Mean value_function loss: 87.1809
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 81.9598
                       Mean reward: 925.94
               Mean episode length: 241.91
    Episode_Reward/reaching_object: 2.0160
     Episode_Reward/lifting_object: 181.5086
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0890
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 184221696
                    Iteration time: 2.71s
                      Time elapsed: 01:13:19
                               ETA: 00:24:31

################################################################################
                     [1m Learning iteration 1874/2500 [0m                     

                       Computation: 37750 steps/s (collection: 2.478s, learning 0.126s)
             Mean action noise std: 3.55
          Mean value_function loss: 87.6225
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 81.9694
                       Mean reward: 910.48
               Mean episode length: 239.29
    Episode_Reward/reaching_object: 2.0258
     Episode_Reward/lifting_object: 182.6103
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 184320000
                    Iteration time: 2.60s
                      Time elapsed: 01:13:21
                               ETA: 00:24:29

################################################################################
                     [1m Learning iteration 1875/2500 [0m                     

                       Computation: 35965 steps/s (collection: 2.567s, learning 0.167s)
             Mean action noise std: 3.55
          Mean value_function loss: 85.1903
               Mean surrogate loss: 0.0003
                 Mean entropy loss: 81.9738
                       Mean reward: 911.25
               Mean episode length: 238.91
    Episode_Reward/reaching_object: 2.0232
     Episode_Reward/lifting_object: 182.1707
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0895
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 184418304
                    Iteration time: 2.73s
                      Time elapsed: 01:13:24
                               ETA: 00:24:27

################################################################################
                     [1m Learning iteration 1876/2500 [0m                     

                       Computation: 40316 steps/s (collection: 2.260s, learning 0.179s)
             Mean action noise std: 3.55
          Mean value_function loss: 79.5170
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 81.9754
                       Mean reward: 926.37
               Mean episode length: 243.70
    Episode_Reward/reaching_object: 2.0215
     Episode_Reward/lifting_object: 181.5181
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 184516608
                    Iteration time: 2.44s
                      Time elapsed: 01:13:27
                               ETA: 00:24:25

################################################################################
                     [1m Learning iteration 1877/2500 [0m                     

                       Computation: 39916 steps/s (collection: 2.312s, learning 0.151s)
             Mean action noise std: 3.55
          Mean value_function loss: 111.1421
               Mean surrogate loss: 0.0000
                 Mean entropy loss: 81.9770
                       Mean reward: 942.45
               Mean episode length: 246.65
    Episode_Reward/reaching_object: 1.9923
     Episode_Reward/lifting_object: 178.5739
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0886
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.9167
--------------------------------------------------------------------------------
                   Total timesteps: 184614912
                    Iteration time: 2.46s
                      Time elapsed: 01:13:29
                               ETA: 00:24:22

################################################################################
                     [1m Learning iteration 1878/2500 [0m                     

                       Computation: 41131 steps/s (collection: 2.239s, learning 0.151s)
             Mean action noise std: 3.55
          Mean value_function loss: 79.9972
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 81.9799
                       Mean reward: 930.03
               Mean episode length: 243.98
    Episode_Reward/reaching_object: 2.0217
     Episode_Reward/lifting_object: 181.3322
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 184713216
                    Iteration time: 2.39s
                      Time elapsed: 01:13:31
                               ETA: 00:24:20

################################################################################
                     [1m Learning iteration 1879/2500 [0m                     

                       Computation: 41633 steps/s (collection: 2.209s, learning 0.152s)
             Mean action noise std: 3.55
          Mean value_function loss: 83.4788
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 81.9886
                       Mean reward: 923.10
               Mean episode length: 242.77
    Episode_Reward/reaching_object: 2.0413
     Episode_Reward/lifting_object: 182.6381
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0903
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 16.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 184811520
                    Iteration time: 2.36s
                      Time elapsed: 01:13:34
                               ETA: 00:24:18

################################################################################
                     [1m Learning iteration 1880/2500 [0m                     

                       Computation: 40006 steps/s (collection: 2.332s, learning 0.125s)
             Mean action noise std: 3.56
          Mean value_function loss: 82.8872
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 82.0062
                       Mean reward: 881.14
               Mean episode length: 233.00
    Episode_Reward/reaching_object: 2.0156
     Episode_Reward/lifting_object: 179.9997
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0896
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 184909824
                    Iteration time: 2.46s
                      Time elapsed: 01:13:36
                               ETA: 00:24:15

################################################################################
                     [1m Learning iteration 1881/2500 [0m                     

                       Computation: 41063 steps/s (collection: 2.284s, learning 0.110s)
             Mean action noise std: 3.56
          Mean value_function loss: 58.2200
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 82.0131
                       Mean reward: 926.66
               Mean episode length: 242.90
    Episode_Reward/reaching_object: 2.0563
     Episode_Reward/lifting_object: 183.5439
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0912
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 185008128
                    Iteration time: 2.39s
                      Time elapsed: 01:13:39
                               ETA: 00:24:13

################################################################################
                     [1m Learning iteration 1882/2500 [0m                     

                       Computation: 40311 steps/s (collection: 2.344s, learning 0.095s)
             Mean action noise std: 3.56
          Mean value_function loss: 75.0263
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 82.0256
                       Mean reward: 884.90
               Mean episode length: 234.59
    Episode_Reward/reaching_object: 2.0197
     Episode_Reward/lifting_object: 179.9652
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0895
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 185106432
                    Iteration time: 2.44s
                      Time elapsed: 01:13:41
                               ETA: 00:24:11

################################################################################
                     [1m Learning iteration 1883/2500 [0m                     

                       Computation: 40818 steps/s (collection: 2.212s, learning 0.196s)
             Mean action noise std: 3.56
          Mean value_function loss: 79.2326
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 82.0353
                       Mean reward: 893.44
               Mean episode length: 236.32
    Episode_Reward/reaching_object: 2.0350
     Episode_Reward/lifting_object: 181.0760
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 14.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 185204736
                    Iteration time: 2.41s
                      Time elapsed: 01:13:43
                               ETA: 00:24:08

################################################################################
                     [1m Learning iteration 1884/2500 [0m                     

                       Computation: 41153 steps/s (collection: 2.276s, learning 0.113s)
             Mean action noise std: 3.56
          Mean value_function loss: 78.3034
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 82.0447
                       Mean reward: 928.28
               Mean episode length: 243.83
    Episode_Reward/reaching_object: 2.0291
     Episode_Reward/lifting_object: 180.2238
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 185303040
                    Iteration time: 2.39s
                      Time elapsed: 01:13:46
                               ETA: 00:24:06

################################################################################
                     [1m Learning iteration 1885/2500 [0m                     

                       Computation: 43133 steps/s (collection: 2.155s, learning 0.124s)
             Mean action noise std: 3.56
          Mean value_function loss: 80.7401
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 82.0602
                       Mean reward: 914.64
               Mean episode length: 239.96
    Episode_Reward/reaching_object: 2.0454
     Episode_Reward/lifting_object: 181.7010
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0901
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 185401344
                    Iteration time: 2.28s
                      Time elapsed: 01:13:48
                               ETA: 00:24:04

################################################################################
                     [1m Learning iteration 1886/2500 [0m                     

                       Computation: 39442 steps/s (collection: 2.368s, learning 0.125s)
             Mean action noise std: 3.56
          Mean value_function loss: 57.5966
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 82.0698
                       Mean reward: 918.27
               Mean episode length: 241.87
    Episode_Reward/reaching_object: 2.0430
     Episode_Reward/lifting_object: 181.0688
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185499648
                    Iteration time: 2.49s
                      Time elapsed: 01:13:51
                               ETA: 00:24:01

################################################################################
                     [1m Learning iteration 1887/2500 [0m                     

                       Computation: 42488 steps/s (collection: 2.205s, learning 0.109s)
             Mean action noise std: 3.56
          Mean value_function loss: 62.2189
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 82.0737
                       Mean reward: 943.06
               Mean episode length: 246.61
    Episode_Reward/reaching_object: 2.0655
     Episode_Reward/lifting_object: 183.1750
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 16.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 185597952
                    Iteration time: 2.31s
                      Time elapsed: 01:13:53
                               ETA: 00:23:59

################################################################################
                     [1m Learning iteration 1888/2500 [0m                     

                       Computation: 40500 steps/s (collection: 2.288s, learning 0.139s)
             Mean action noise std: 3.56
          Mean value_function loss: 72.2474
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 82.0804
                       Mean reward: 944.23
               Mean episode length: 248.97
    Episode_Reward/reaching_object: 2.0803
     Episode_Reward/lifting_object: 184.4585
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0916
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8750
--------------------------------------------------------------------------------
                   Total timesteps: 185696256
                    Iteration time: 2.43s
                      Time elapsed: 01:13:55
                               ETA: 00:23:57

################################################################################
                     [1m Learning iteration 1889/2500 [0m                     

                       Computation: 43088 steps/s (collection: 2.179s, learning 0.103s)
             Mean action noise std: 3.57
          Mean value_function loss: 78.5420
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 82.0949
                       Mean reward: 920.82
               Mean episode length: 242.42
    Episode_Reward/reaching_object: 2.0479
     Episode_Reward/lifting_object: 181.1787
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0903
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 16.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 185794560
                    Iteration time: 2.28s
                      Time elapsed: 01:13:58
                               ETA: 00:23:54

################################################################################
                     [1m Learning iteration 1890/2500 [0m                     

                       Computation: 43186 steps/s (collection: 2.134s, learning 0.143s)
             Mean action noise std: 3.57
          Mean value_function loss: 69.3333
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 82.1112
                       Mean reward: 919.95
               Mean episode length: 242.49
    Episode_Reward/reaching_object: 2.0636
     Episode_Reward/lifting_object: 182.2518
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0912
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 0.8333
--------------------------------------------------------------------------------
                   Total timesteps: 185892864
                    Iteration time: 2.28s
                      Time elapsed: 01:14:00
                               ETA: 00:23:52

################################################################################
                     [1m Learning iteration 1891/2500 [0m                     

                       Computation: 43098 steps/s (collection: 2.174s, learning 0.107s)
             Mean action noise std: 3.57
          Mean value_function loss: 60.0285
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 82.1224
                       Mean reward: 917.59
               Mean episode length: 240.83
    Episode_Reward/reaching_object: 2.0459
     Episode_Reward/lifting_object: 181.0116
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0902
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 14.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 185991168
                    Iteration time: 2.28s
                      Time elapsed: 01:14:02
                               ETA: 00:23:50

################################################################################
                     [1m Learning iteration 1892/2500 [0m                     

                       Computation: 42648 steps/s (collection: 2.166s, learning 0.139s)
             Mean action noise std: 3.57
          Mean value_function loss: 64.8003
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 82.1284
                       Mean reward: 898.74
               Mean episode length: 237.55
    Episode_Reward/reaching_object: 2.0498
     Episode_Reward/lifting_object: 181.4021
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0905
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186089472
                    Iteration time: 2.30s
                      Time elapsed: 01:14:04
                               ETA: 00:23:47

################################################################################
                     [1m Learning iteration 1893/2500 [0m                     

                       Computation: 43381 steps/s (collection: 2.161s, learning 0.106s)
             Mean action noise std: 3.57
          Mean value_function loss: 82.4189
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 82.1349
                       Mean reward: 888.83
               Mean episode length: 235.10
    Episode_Reward/reaching_object: 2.0315
     Episode_Reward/lifting_object: 179.9426
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 186187776
                    Iteration time: 2.27s
                      Time elapsed: 01:14:07
                               ETA: 00:23:45

################################################################################
                     [1m Learning iteration 1894/2500 [0m                     

                       Computation: 43452 steps/s (collection: 2.147s, learning 0.116s)
             Mean action noise std: 3.57
          Mean value_function loss: 68.9933
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 82.1488
                       Mean reward: 926.94
               Mean episode length: 243.25
    Episode_Reward/reaching_object: 2.0770
     Episode_Reward/lifting_object: 184.2906
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0919
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186286080
                    Iteration time: 2.26s
                      Time elapsed: 01:14:09
                               ETA: 00:23:42

################################################################################
                     [1m Learning iteration 1895/2500 [0m                     

                       Computation: 43739 steps/s (collection: 2.130s, learning 0.117s)
             Mean action noise std: 3.57
          Mean value_function loss: 60.9377
               Mean surrogate loss: -0.0040
                 Mean entropy loss: 82.1676
                       Mean reward: 916.39
               Mean episode length: 240.63
    Episode_Reward/reaching_object: 2.0624
     Episode_Reward/lifting_object: 183.3488
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0914
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0833
--------------------------------------------------------------------------------
                   Total timesteps: 186384384
                    Iteration time: 2.25s
                      Time elapsed: 01:14:11
                               ETA: 00:23:40

################################################################################
                     [1m Learning iteration 1896/2500 [0m                     

                       Computation: 42196 steps/s (collection: 2.173s, learning 0.157s)
             Mean action noise std: 3.58
          Mean value_function loss: 92.1085
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 82.1862
                       Mean reward: 911.15
               Mean episode length: 239.72
    Episode_Reward/reaching_object: 2.0402
     Episode_Reward/lifting_object: 181.0001
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0906
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 186482688
                    Iteration time: 2.33s
                      Time elapsed: 01:14:14
                               ETA: 00:23:38

################################################################################
                     [1m Learning iteration 1897/2500 [0m                     

                       Computation: 43178 steps/s (collection: 2.152s, learning 0.125s)
             Mean action noise std: 3.58
          Mean value_function loss: 109.6386
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 82.1963
                       Mean reward: 908.00
               Mean episode length: 238.69
    Episode_Reward/reaching_object: 2.0411
     Episode_Reward/lifting_object: 181.0563
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 17.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 186580992
                    Iteration time: 2.28s
                      Time elapsed: 01:14:16
                               ETA: 00:23:35

################################################################################
                     [1m Learning iteration 1898/2500 [0m                     

                       Computation: 42823 steps/s (collection: 2.200s, learning 0.096s)
             Mean action noise std: 3.58
          Mean value_function loss: 67.8732
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 82.2011
                       Mean reward: 945.91
               Mean episode length: 247.21
    Episode_Reward/reaching_object: 2.0752
     Episode_Reward/lifting_object: 184.7084
      Episode_Reward/object_height: 0.0194
        Episode_Reward/action_rate: -0.0922
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 186679296
                    Iteration time: 2.30s
                      Time elapsed: 01:14:18
                               ETA: 00:23:33

################################################################################
                     [1m Learning iteration 1899/2500 [0m                     

                       Computation: 41702 steps/s (collection: 2.195s, learning 0.163s)
             Mean action noise std: 3.58
          Mean value_function loss: 91.9087
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 82.2050
                       Mean reward: 912.80
               Mean episode length: 240.31
    Episode_Reward/reaching_object: 2.0231
     Episode_Reward/lifting_object: 179.6248
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 186777600
                    Iteration time: 2.36s
                      Time elapsed: 01:14:21
                               ETA: 00:23:31

################################################################################
                     [1m Learning iteration 1900/2500 [0m                     

                       Computation: 44437 steps/s (collection: 2.121s, learning 0.092s)
             Mean action noise std: 3.58
          Mean value_function loss: 81.0967
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 82.2097
                       Mean reward: 903.53
               Mean episode length: 239.13
    Episode_Reward/reaching_object: 2.0441
     Episode_Reward/lifting_object: 181.4150
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0908
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 186875904
                    Iteration time: 2.21s
                      Time elapsed: 01:14:23
                               ETA: 00:23:28

################################################################################
                     [1m Learning iteration 1901/2500 [0m                     

                       Computation: 42652 steps/s (collection: 2.213s, learning 0.092s)
             Mean action noise std: 3.58
          Mean value_function loss: 96.5408
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 82.2211
                       Mean reward: 921.48
               Mean episode length: 242.21
    Episode_Reward/reaching_object: 2.0193
     Episode_Reward/lifting_object: 179.4745
      Episode_Reward/object_height: 0.0191
        Episode_Reward/action_rate: -0.0899
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 14.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 186974208
                    Iteration time: 2.30s
                      Time elapsed: 01:14:25
                               ETA: 00:23:26

################################################################################
                     [1m Learning iteration 1902/2500 [0m                     

                       Computation: 43590 steps/s (collection: 2.142s, learning 0.113s)
             Mean action noise std: 3.58
          Mean value_function loss: 98.0756
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 82.2312
                       Mean reward: 859.29
               Mean episode length: 227.90
    Episode_Reward/reaching_object: 2.0302
     Episode_Reward/lifting_object: 180.5806
      Episode_Reward/object_height: 0.0193
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 187072512
                    Iteration time: 2.26s
                      Time elapsed: 01:14:27
                               ETA: 00:23:23

################################################################################
                     [1m Learning iteration 1903/2500 [0m                     

                       Computation: 43146 steps/s (collection: 2.150s, learning 0.128s)
             Mean action noise std: 3.58
          Mean value_function loss: 86.9911
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 82.2422
                       Mean reward: 904.27
               Mean episode length: 238.07
    Episode_Reward/reaching_object: 2.0126
     Episode_Reward/lifting_object: 178.9077
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0581
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 187170816
                    Iteration time: 2.28s
                      Time elapsed: 01:14:30
                               ETA: 00:23:21

################################################################################
                     [1m Learning iteration 1904/2500 [0m                     

                       Computation: 43775 steps/s (collection: 2.142s, learning 0.104s)
             Mean action noise std: 3.58
          Mean value_function loss: 106.3614
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 82.2456
                       Mean reward: 911.73
               Mean episode length: 239.50
    Episode_Reward/reaching_object: 2.0230
     Episode_Reward/lifting_object: 180.2096
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0898
          Episode_Reward/joint_vel: -0.0580
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 187269120
                    Iteration time: 2.25s
                      Time elapsed: 01:14:32
                               ETA: 00:23:19

################################################################################
                     [1m Learning iteration 1905/2500 [0m                     

                       Computation: 43236 steps/s (collection: 2.166s, learning 0.108s)
             Mean action noise std: 3.58
          Mean value_function loss: 91.8053
               Mean surrogate loss: -0.0010
                 Mean entropy loss: 82.2489
                       Mean reward: 924.24
               Mean episode length: 242.66
    Episode_Reward/reaching_object: 2.0127
     Episode_Reward/lifting_object: 178.7095
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0897
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 187367424
                    Iteration time: 2.27s
                      Time elapsed: 01:14:34
                               ETA: 00:23:16

################################################################################
                     [1m Learning iteration 1906/2500 [0m                     

                       Computation: 39160 steps/s (collection: 2.367s, learning 0.144s)
             Mean action noise std: 3.58
          Mean value_function loss: 84.5993
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 82.2549
                       Mean reward: 929.77
               Mean episode length: 244.53
    Episode_Reward/reaching_object: 2.0095
     Episode_Reward/lifting_object: 178.4261
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0895
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 187465728
                    Iteration time: 2.51s
                      Time elapsed: 01:14:37
                               ETA: 00:23:14

################################################################################
                     [1m Learning iteration 1907/2500 [0m                     

                       Computation: 43428 steps/s (collection: 2.125s, learning 0.139s)
             Mean action noise std: 3.59
          Mean value_function loss: 82.4375
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 82.2676
                       Mean reward: 912.00
               Mean episode length: 240.11
    Episode_Reward/reaching_object: 2.0252
     Episode_Reward/lifting_object: 179.8623
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0902
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 187564032
                    Iteration time: 2.26s
                      Time elapsed: 01:14:39
                               ETA: 00:23:12

################################################################################
                     [1m Learning iteration 1908/2500 [0m                     

                       Computation: 42741 steps/s (collection: 2.170s, learning 0.130s)
             Mean action noise std: 3.59
          Mean value_function loss: 96.3600
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 82.2844
                       Mean reward: 930.41
               Mean episode length: 243.60
    Episode_Reward/reaching_object: 2.0435
     Episode_Reward/lifting_object: 181.7828
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 16.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 187662336
                    Iteration time: 2.30s
                      Time elapsed: 01:14:41
                               ETA: 00:23:09

################################################################################
                     [1m Learning iteration 1909/2500 [0m                     

                       Computation: 44319 steps/s (collection: 2.117s, learning 0.102s)
             Mean action noise std: 3.59
          Mean value_function loss: 71.8036
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 82.3006
                       Mean reward: 914.53
               Mean episode length: 240.74
    Episode_Reward/reaching_object: 2.0576
     Episode_Reward/lifting_object: 182.8962
      Episode_Reward/object_height: 0.0185
        Episode_Reward/action_rate: -0.0914
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 187760640
                    Iteration time: 2.22s
                      Time elapsed: 01:14:43
                               ETA: 00:23:07

################################################################################
                     [1m Learning iteration 1910/2500 [0m                     

                       Computation: 40228 steps/s (collection: 2.353s, learning 0.091s)
             Mean action noise std: 3.59
          Mean value_function loss: 73.4541
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 82.3155
                       Mean reward: 918.51
               Mean episode length: 241.03
    Episode_Reward/reaching_object: 2.0432
     Episode_Reward/lifting_object: 181.1533
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0911
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 187858944
                    Iteration time: 2.44s
                      Time elapsed: 01:14:46
                               ETA: 00:23:05

################################################################################
                     [1m Learning iteration 1911/2500 [0m                     

                       Computation: 43116 steps/s (collection: 2.168s, learning 0.112s)
             Mean action noise std: 3.59
          Mean value_function loss: 100.7847
               Mean surrogate loss: -0.0037
                 Mean entropy loss: 82.3285
                       Mean reward: 892.55
               Mean episode length: 234.58
    Episode_Reward/reaching_object: 2.0130
     Episode_Reward/lifting_object: 178.7069
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0899
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 187957248
                    Iteration time: 2.28s
                      Time elapsed: 01:14:48
                               ETA: 00:23:02

################################################################################
                     [1m Learning iteration 1912/2500 [0m                     

                       Computation: 41840 steps/s (collection: 2.236s, learning 0.114s)
             Mean action noise std: 3.59
          Mean value_function loss: 88.7746
               Mean surrogate loss: -0.0029
                 Mean entropy loss: 82.3382
                       Mean reward: 896.49
               Mean episode length: 235.75
    Episode_Reward/reaching_object: 2.0418
     Episode_Reward/lifting_object: 181.1426
      Episode_Reward/object_height: 0.0187
        Episode_Reward/action_rate: -0.0908
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 188055552
                    Iteration time: 2.35s
                      Time elapsed: 01:14:50
                               ETA: 00:23:00

################################################################################
                     [1m Learning iteration 1913/2500 [0m                     

                       Computation: 42680 steps/s (collection: 2.201s, learning 0.102s)
             Mean action noise std: 3.59
          Mean value_function loss: 80.2787
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 82.3472
                       Mean reward: 903.38
               Mean episode length: 237.09
    Episode_Reward/reaching_object: 2.0452
     Episode_Reward/lifting_object: 181.5985
      Episode_Reward/object_height: 0.0188
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.2917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 188153856
                    Iteration time: 2.30s
                      Time elapsed: 01:14:53
                               ETA: 00:22:58

################################################################################
                     [1m Learning iteration 1914/2500 [0m                     

                       Computation: 42747 steps/s (collection: 2.194s, learning 0.106s)
             Mean action noise std: 3.59
          Mean value_function loss: 100.8294
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 82.3521
                       Mean reward: 872.24
               Mean episode length: 230.11
    Episode_Reward/reaching_object: 2.0194
     Episode_Reward/lifting_object: 178.9944
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0901
          Episode_Reward/joint_vel: -0.0585
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 188252160
                    Iteration time: 2.30s
                      Time elapsed: 01:14:55
                               ETA: 00:22:55

################################################################################
                     [1m Learning iteration 1915/2500 [0m                     

                       Computation: 41057 steps/s (collection: 2.270s, learning 0.125s)
             Mean action noise std: 3.59
          Mean value_function loss: 64.5365
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 82.3561
                       Mean reward: 917.11
               Mean episode length: 240.52
    Episode_Reward/reaching_object: 2.0633
     Episode_Reward/lifting_object: 183.4911
      Episode_Reward/object_height: 0.0192
        Episode_Reward/action_rate: -0.0920
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 188350464
                    Iteration time: 2.39s
                      Time elapsed: 01:14:57
                               ETA: 00:22:53

################################################################################
                     [1m Learning iteration 1916/2500 [0m                     

                       Computation: 40621 steps/s (collection: 2.307s, learning 0.113s)
             Mean action noise std: 3.60
          Mean value_function loss: 97.5898
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 82.3638
                       Mean reward: 900.08
               Mean episode length: 235.93
    Episode_Reward/reaching_object: 2.0584
     Episode_Reward/lifting_object: 183.1057
      Episode_Reward/object_height: 0.0190
        Episode_Reward/action_rate: -0.0922
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 188448768
                    Iteration time: 2.42s
                      Time elapsed: 01:15:00
                               ETA: 00:22:51

################################################################################
                     [1m Learning iteration 1917/2500 [0m                     

                       Computation: 43338 steps/s (collection: 2.160s, learning 0.108s)
             Mean action noise std: 3.60
          Mean value_function loss: 79.8911
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 82.3725
                       Mean reward: 911.27
               Mean episode length: 240.25
    Episode_Reward/reaching_object: 2.0332
     Episode_Reward/lifting_object: 181.2101
      Episode_Reward/object_height: 0.0189
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 188547072
                    Iteration time: 2.27s
                      Time elapsed: 01:15:02
                               ETA: 00:22:48

################################################################################
                     [1m Learning iteration 1918/2500 [0m                     

                       Computation: 42855 steps/s (collection: 2.158s, learning 0.136s)
             Mean action noise std: 3.60
          Mean value_function loss: 92.6125
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 82.3812
                       Mean reward: 911.54
               Mean episode length: 238.53
    Episode_Reward/reaching_object: 2.0257
     Episode_Reward/lifting_object: 180.6620
      Episode_Reward/object_height: 0.0184
        Episode_Reward/action_rate: -0.0909
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 17.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 188645376
                    Iteration time: 2.29s
                      Time elapsed: 01:15:04
                               ETA: 00:22:46

################################################################################
                     [1m Learning iteration 1919/2500 [0m                     

                       Computation: 41882 steps/s (collection: 2.240s, learning 0.107s)
             Mean action noise std: 3.60
          Mean value_function loss: 87.7621
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 82.3873
                       Mean reward: 893.74
               Mean episode length: 234.70
    Episode_Reward/reaching_object: 2.0079
     Episode_Reward/lifting_object: 179.2361
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0903
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 13.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 188743680
                    Iteration time: 2.35s
                      Time elapsed: 01:15:07
                               ETA: 00:22:43

################################################################################
                     [1m Learning iteration 1920/2500 [0m                     

                       Computation: 42591 steps/s (collection: 2.182s, learning 0.126s)
             Mean action noise std: 3.60
          Mean value_function loss: 92.7704
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 82.3931
                       Mean reward: 929.27
               Mean episode length: 244.72
    Episode_Reward/reaching_object: 2.0325
     Episode_Reward/lifting_object: 181.4244
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0917
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 188841984
                    Iteration time: 2.31s
                      Time elapsed: 01:15:09
                               ETA: 00:22:41

################################################################################
                     [1m Learning iteration 1921/2500 [0m                     

                       Computation: 42872 steps/s (collection: 2.184s, learning 0.109s)
             Mean action noise std: 3.60
          Mean value_function loss: 88.6986
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 82.3975
                       Mean reward: 883.63
               Mean episode length: 233.19
    Episode_Reward/reaching_object: 2.0209
     Episode_Reward/lifting_object: 180.7598
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0912
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 188940288
                    Iteration time: 2.29s
                      Time elapsed: 01:15:11
                               ETA: 00:22:39

################################################################################
                     [1m Learning iteration 1922/2500 [0m                     

                       Computation: 41950 steps/s (collection: 2.167s, learning 0.177s)
             Mean action noise std: 3.60
          Mean value_function loss: 98.9759
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 82.4052
                       Mean reward: 888.01
               Mean episode length: 233.22
    Episode_Reward/reaching_object: 2.0081
     Episode_Reward/lifting_object: 180.1249
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 189038592
                    Iteration time: 2.34s
                      Time elapsed: 01:15:14
                               ETA: 00:22:36

################################################################################
                     [1m Learning iteration 1923/2500 [0m                     

                       Computation: 39686 steps/s (collection: 2.372s, learning 0.105s)
             Mean action noise std: 3.60
          Mean value_function loss: 91.6160
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 82.4160
                       Mean reward: 893.98
               Mean episode length: 235.60
    Episode_Reward/reaching_object: 2.0328
     Episode_Reward/lifting_object: 182.5343
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 16.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 189136896
                    Iteration time: 2.48s
                      Time elapsed: 01:15:16
                               ETA: 00:22:34

################################################################################
                     [1m Learning iteration 1924/2500 [0m                     

                       Computation: 43744 steps/s (collection: 2.152s, learning 0.095s)
             Mean action noise std: 3.60
          Mean value_function loss: 90.3819
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 82.4258
                       Mean reward: 917.36
               Mean episode length: 241.27
    Episode_Reward/reaching_object: 2.0290
     Episode_Reward/lifting_object: 182.3143
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0923
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 189235200
                    Iteration time: 2.25s
                      Time elapsed: 01:15:18
                               ETA: 00:22:32

################################################################################
                     [1m Learning iteration 1925/2500 [0m                     

                       Computation: 41482 steps/s (collection: 2.211s, learning 0.159s)
             Mean action noise std: 3.60
          Mean value_function loss: 97.2955
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 82.4342
                       Mean reward: 888.47
               Mean episode length: 233.25
    Episode_Reward/reaching_object: 1.9840
     Episode_Reward/lifting_object: 178.4627
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 189333504
                    Iteration time: 2.37s
                      Time elapsed: 01:15:21
                               ETA: 00:22:29

################################################################################
                     [1m Learning iteration 1926/2500 [0m                     

                       Computation: 42673 steps/s (collection: 2.208s, learning 0.096s)
             Mean action noise std: 3.60
          Mean value_function loss: 122.2223
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 82.4383
                       Mean reward: 845.51
               Mean episode length: 224.94
    Episode_Reward/reaching_object: 1.9681
     Episode_Reward/lifting_object: 176.8271
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 189431808
                    Iteration time: 2.30s
                      Time elapsed: 01:15:23
                               ETA: 00:22:27

################################################################################
                     [1m Learning iteration 1927/2500 [0m                     

                       Computation: 43121 steps/s (collection: 2.165s, learning 0.115s)
             Mean action noise std: 3.60
          Mean value_function loss: 127.6600
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 82.4415
                       Mean reward: 883.22
               Mean episode length: 233.27
    Episode_Reward/reaching_object: 1.9613
     Episode_Reward/lifting_object: 176.2007
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0899
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 189530112
                    Iteration time: 2.28s
                      Time elapsed: 01:15:25
                               ETA: 00:22:25

################################################################################
                     [1m Learning iteration 1928/2500 [0m                     

                       Computation: 42021 steps/s (collection: 2.186s, learning 0.154s)
             Mean action noise std: 3.61
          Mean value_function loss: 104.9690
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 82.4457
                       Mean reward: 906.34
               Mean episode length: 239.31
    Episode_Reward/reaching_object: 2.0009
     Episode_Reward/lifting_object: 180.3770
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0916
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 17.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8750
--------------------------------------------------------------------------------
                   Total timesteps: 189628416
                    Iteration time: 2.34s
                      Time elapsed: 01:15:28
                               ETA: 00:22:22

################################################################################
                     [1m Learning iteration 1929/2500 [0m                     

                       Computation: 42873 steps/s (collection: 2.184s, learning 0.109s)
             Mean action noise std: 3.61
          Mean value_function loss: 99.9217
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 82.4508
                       Mean reward: 887.67
               Mean episode length: 233.69
    Episode_Reward/reaching_object: 1.9955
     Episode_Reward/lifting_object: 179.8959
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0917
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 16.3333
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 189726720
                    Iteration time: 2.29s
                      Time elapsed: 01:15:30
                               ETA: 00:22:20

################################################################################
                     [1m Learning iteration 1930/2500 [0m                     

                       Computation: 40666 steps/s (collection: 2.273s, learning 0.145s)
             Mean action noise std: 3.61
          Mean value_function loss: 115.9281
               Mean surrogate loss: -0.0008
                 Mean entropy loss: 82.4554
                       Mean reward: 917.28
               Mean episode length: 239.75
    Episode_Reward/reaching_object: 1.9574
     Episode_Reward/lifting_object: 177.2786
      Episode_Reward/object_height: 0.0170
        Episode_Reward/action_rate: -0.0902
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 14.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 189825024
                    Iteration time: 2.42s
                      Time elapsed: 01:15:32
                               ETA: 00:22:18

################################################################################
                     [1m Learning iteration 1931/2500 [0m                     

                       Computation: 39579 steps/s (collection: 2.337s, learning 0.147s)
             Mean action noise std: 3.61
          Mean value_function loss: 129.8599
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 82.4608
                       Mean reward: 871.25
               Mean episode length: 229.93
    Episode_Reward/reaching_object: 1.9618
     Episode_Reward/lifting_object: 177.7153
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2083
--------------------------------------------------------------------------------
                   Total timesteps: 189923328
                    Iteration time: 2.48s
                      Time elapsed: 01:15:35
                               ETA: 00:22:15

################################################################################
                     [1m Learning iteration 1932/2500 [0m                     

                       Computation: 42653 steps/s (collection: 2.215s, learning 0.090s)
             Mean action noise std: 3.61
          Mean value_function loss: 71.5502
               Mean surrogate loss: -0.0015
                 Mean entropy loss: 82.4659
                       Mean reward: 930.15
               Mean episode length: 244.90
    Episode_Reward/reaching_object: 2.0157
     Episode_Reward/lifting_object: 182.9224
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0926
          Episode_Reward/joint_vel: -0.0608
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 190021632
                    Iteration time: 2.30s
                      Time elapsed: 01:15:37
                               ETA: 00:22:13

################################################################################
                     [1m Learning iteration 1933/2500 [0m                     

                       Computation: 42063 steps/s (collection: 2.237s, learning 0.100s)
             Mean action noise std: 3.61
          Mean value_function loss: 106.2802
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 82.4710
                       Mean reward: 904.23
               Mean episode length: 238.35
    Episode_Reward/reaching_object: 1.9801
     Episode_Reward/lifting_object: 179.8802
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0913
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 14.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.8333
--------------------------------------------------------------------------------
                   Total timesteps: 190119936
                    Iteration time: 2.34s
                      Time elapsed: 01:15:40
                               ETA: 00:22:11

################################################################################
                     [1m Learning iteration 1934/2500 [0m                     

                       Computation: 42584 steps/s (collection: 2.214s, learning 0.095s)
             Mean action noise std: 3.61
          Mean value_function loss: 98.8210
               Mean surrogate loss: -0.0023
                 Mean entropy loss: 82.4782
                       Mean reward: 933.59
               Mean episode length: 243.82
    Episode_Reward/reaching_object: 1.9969
     Episode_Reward/lifting_object: 181.8912
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0924
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 15.0417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 190218240
                    Iteration time: 2.31s
                      Time elapsed: 01:15:42
                               ETA: 00:22:08

################################################################################
                     [1m Learning iteration 1935/2500 [0m                     

                       Computation: 41084 steps/s (collection: 2.239s, learning 0.153s)
             Mean action noise std: 3.61
          Mean value_function loss: 138.1364
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 82.4855
                       Mean reward: 919.54
               Mean episode length: 240.75
    Episode_Reward/reaching_object: 1.9535
     Episode_Reward/lifting_object: 178.2652
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0905
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 14.5833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 190316544
                    Iteration time: 2.39s
                      Time elapsed: 01:15:44
                               ETA: 00:22:06

################################################################################
                     [1m Learning iteration 1936/2500 [0m                     

                       Computation: 41416 steps/s (collection: 2.247s, learning 0.127s)
             Mean action noise std: 3.61
          Mean value_function loss: 122.4254
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 82.4985
                       Mean reward: 933.88
               Mean episode length: 244.85
    Episode_Reward/reaching_object: 1.9589
     Episode_Reward/lifting_object: 179.2055
      Episode_Reward/object_height: 0.0174
        Episode_Reward/action_rate: -0.0911
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 14.7083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 190414848
                    Iteration time: 2.37s
                      Time elapsed: 01:15:47
                               ETA: 00:22:03

################################################################################
                     [1m Learning iteration 1937/2500 [0m                     

                       Computation: 40232 steps/s (collection: 2.267s, learning 0.176s)
             Mean action noise std: 3.61
          Mean value_function loss: 128.5149
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 82.5087
                       Mean reward: 882.91
               Mean episode length: 232.90
    Episode_Reward/reaching_object: 1.9431
     Episode_Reward/lifting_object: 177.9645
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0907
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.5833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2917
--------------------------------------------------------------------------------
                   Total timesteps: 190513152
                    Iteration time: 2.44s
                      Time elapsed: 01:15:49
                               ETA: 00:22:01

################################################################################
                     [1m Learning iteration 1938/2500 [0m                     

                       Computation: 40798 steps/s (collection: 2.286s, learning 0.124s)
             Mean action noise std: 3.61
          Mean value_function loss: 132.0073
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 82.5104
                       Mean reward: 894.20
               Mean episode length: 234.38
    Episode_Reward/reaching_object: 1.9373
     Episode_Reward/lifting_object: 177.4808
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0907
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 190611456
                    Iteration time: 2.41s
                      Time elapsed: 01:15:51
                               ETA: 00:21:59

################################################################################
                     [1m Learning iteration 1939/2500 [0m                     

                       Computation: 42795 steps/s (collection: 2.199s, learning 0.099s)
             Mean action noise std: 3.61
          Mean value_function loss: 128.4815
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 82.5147
                       Mean reward: 929.81
               Mean episode length: 244.03
    Episode_Reward/reaching_object: 1.9454
     Episode_Reward/lifting_object: 178.6585
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0912
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 190709760
                    Iteration time: 2.30s
                      Time elapsed: 01:15:54
                               ETA: 00:21:56

################################################################################
                     [1m Learning iteration 1940/2500 [0m                     

                       Computation: 40130 steps/s (collection: 2.267s, learning 0.183s)
             Mean action noise std: 3.61
          Mean value_function loss: 129.5997
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 82.5200
                       Mean reward: 892.64
               Mean episode length: 234.91
    Episode_Reward/reaching_object: 1.9289
     Episode_Reward/lifting_object: 177.2278
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0904
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 13.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 190808064
                    Iteration time: 2.45s
                      Time elapsed: 01:15:56
                               ETA: 00:21:54

################################################################################
                     [1m Learning iteration 1941/2500 [0m                     

                       Computation: 40236 steps/s (collection: 2.336s, learning 0.107s)
             Mean action noise std: 3.62
          Mean value_function loss: 115.3464
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 82.5291
                       Mean reward: 921.66
               Mean episode length: 241.43
    Episode_Reward/reaching_object: 1.9260
     Episode_Reward/lifting_object: 176.8375
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0907
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 14.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1250
--------------------------------------------------------------------------------
                   Total timesteps: 190906368
                    Iteration time: 2.44s
                      Time elapsed: 01:15:59
                               ETA: 00:21:52

################################################################################
                     [1m Learning iteration 1942/2500 [0m                     

                       Computation: 42377 steps/s (collection: 2.202s, learning 0.118s)
             Mean action noise std: 3.62
          Mean value_function loss: 128.5184
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 82.5377
                       Mean reward: 918.63
               Mean episode length: 241.54
    Episode_Reward/reaching_object: 1.9383
     Episode_Reward/lifting_object: 178.7115
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0911
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 191004672
                    Iteration time: 2.32s
                      Time elapsed: 01:16:01
                               ETA: 00:21:49

################################################################################
                     [1m Learning iteration 1943/2500 [0m                     

                       Computation: 42390 steps/s (collection: 2.219s, learning 0.100s)
             Mean action noise std: 3.62
          Mean value_function loss: 139.2655
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 82.5439
                       Mean reward: 894.38
               Mean episode length: 235.27
    Episode_Reward/reaching_object: 1.9137
     Episode_Reward/lifting_object: 176.4814
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0902
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 13.4583
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 191102976
                    Iteration time: 2.32s
                      Time elapsed: 01:16:03
                               ETA: 00:21:47

################################################################################
                     [1m Learning iteration 1944/2500 [0m                     

                       Computation: 43112 steps/s (collection: 2.165s, learning 0.115s)
             Mean action noise std: 3.62
          Mean value_function loss: 111.1962
               Mean surrogate loss: 0.0006
                 Mean entropy loss: 82.5523
                       Mean reward: 907.52
               Mean episode length: 238.53
    Episode_Reward/reaching_object: 1.9355
     Episode_Reward/lifting_object: 177.9969
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0913
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 15.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.1667
--------------------------------------------------------------------------------
                   Total timesteps: 191201280
                    Iteration time: 2.28s
                      Time elapsed: 01:16:06
                               ETA: 00:21:45

################################################################################
                     [1m Learning iteration 1945/2500 [0m                     

                       Computation: 43536 steps/s (collection: 2.166s, learning 0.092s)
             Mean action noise std: 3.62
          Mean value_function loss: 91.9343
               Mean surrogate loss: 0.0001
                 Mean entropy loss: 82.5540
                       Mean reward: 909.63
               Mean episode length: 238.73
    Episode_Reward/reaching_object: 1.9571
     Episode_Reward/lifting_object: 180.1603
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0919
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 14.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 191299584
                    Iteration time: 2.26s
                      Time elapsed: 01:16:08
                               ETA: 00:21:42

################################################################################
                     [1m Learning iteration 1946/2500 [0m                     

                       Computation: 41929 steps/s (collection: 2.212s, learning 0.132s)
             Mean action noise std: 3.62
          Mean value_function loss: 118.0476
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 82.5559
                       Mean reward: 904.68
               Mean episode length: 238.30
    Episode_Reward/reaching_object: 1.9433
     Episode_Reward/lifting_object: 178.7265
      Episode_Reward/object_height: 0.0169
        Episode_Reward/action_rate: -0.0913
          Episode_Reward/joint_vel: -0.0603
      Episode_Termination/time_out: 15.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 191397888
                    Iteration time: 2.34s
                      Time elapsed: 01:16:10
                               ETA: 00:21:40

################################################################################
                     [1m Learning iteration 1947/2500 [0m                     

                       Computation: 43207 steps/s (collection: 2.159s, learning 0.117s)
             Mean action noise std: 3.62
          Mean value_function loss: 84.4017
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 82.5612
                       Mean reward: 922.68
               Mean episode length: 241.94
    Episode_Reward/reaching_object: 1.9833
     Episode_Reward/lifting_object: 182.3150
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0931
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 191496192
                    Iteration time: 2.28s
                      Time elapsed: 01:16:12
                               ETA: 00:21:38

################################################################################
                     [1m Learning iteration 1948/2500 [0m                     

                       Computation: 43200 steps/s (collection: 2.176s, learning 0.099s)
             Mean action noise std: 3.62
          Mean value_function loss: 86.1877
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 82.5660
                       Mean reward: 929.59
               Mean episode length: 244.76
    Episode_Reward/reaching_object: 1.9709
     Episode_Reward/lifting_object: 180.7441
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0922
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 15.7083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 191594496
                    Iteration time: 2.28s
                      Time elapsed: 01:16:15
                               ETA: 00:21:35

################################################################################
                     [1m Learning iteration 1949/2500 [0m                     

                       Computation: 43107 steps/s (collection: 2.154s, learning 0.127s)
             Mean action noise std: 3.62
          Mean value_function loss: 101.3564
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 82.5692
                       Mean reward: 930.99
               Mean episode length: 244.75
    Episode_Reward/reaching_object: 1.9863
     Episode_Reward/lifting_object: 181.6022
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.0611
      Episode_Termination/time_out: 17.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 191692800
                    Iteration time: 2.28s
                      Time elapsed: 01:16:17
                               ETA: 00:21:33

################################################################################
                     [1m Learning iteration 1950/2500 [0m                     

                       Computation: 41800 steps/s (collection: 2.205s, learning 0.147s)
             Mean action noise std: 3.62
          Mean value_function loss: 66.1480
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 82.5730
                       Mean reward: 936.34
               Mean episode length: 245.58
    Episode_Reward/reaching_object: 1.9977
     Episode_Reward/lifting_object: 183.0107
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0935
          Episode_Reward/joint_vel: -0.0609
      Episode_Termination/time_out: 15.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 191791104
                    Iteration time: 2.35s
                      Time elapsed: 01:16:19
                               ETA: 00:21:31

################################################################################
                     [1m Learning iteration 1951/2500 [0m                     

                       Computation: 42180 steps/s (collection: 2.227s, learning 0.104s)
             Mean action noise std: 3.62
          Mean value_function loss: 108.6634
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 82.5783
                       Mean reward: 902.73
               Mean episode length: 237.54
    Episode_Reward/reaching_object: 1.9543
     Episode_Reward/lifting_object: 178.2629
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0918
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 14.3333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 191889408
                    Iteration time: 2.33s
                      Time elapsed: 01:16:22
                               ETA: 00:21:28

################################################################################
                     [1m Learning iteration 1952/2500 [0m                     

                       Computation: 43766 steps/s (collection: 2.156s, learning 0.091s)
             Mean action noise std: 3.62
          Mean value_function loss: 119.3068
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 82.5851
                       Mean reward: 900.87
               Mean episode length: 236.49
    Episode_Reward/reaching_object: 1.9162
     Episode_Reward/lifting_object: 174.6625
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0900
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 16.1250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 191987712
                    Iteration time: 2.25s
                      Time elapsed: 01:16:24
                               ETA: 00:21:26

################################################################################
                     [1m Learning iteration 1953/2500 [0m                     

                       Computation: 41679 steps/s (collection: 2.198s, learning 0.161s)
             Mean action noise std: 3.62
          Mean value_function loss: 101.2653
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 82.5906
                       Mean reward: 918.19
               Mean episode length: 240.36
    Episode_Reward/reaching_object: 1.9684
     Episode_Reward/lifting_object: 179.4310
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0918
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 14.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7083
--------------------------------------------------------------------------------
                   Total timesteps: 192086016
                    Iteration time: 2.36s
                      Time elapsed: 01:16:26
                               ETA: 00:21:24

################################################################################
                     [1m Learning iteration 1954/2500 [0m                     

                       Computation: 35248 steps/s (collection: 2.494s, learning 0.295s)
             Mean action noise std: 3.62
          Mean value_function loss: 117.1470
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 82.5996
                       Mean reward: 905.21
               Mean episode length: 237.67
    Episode_Reward/reaching_object: 1.9496
     Episode_Reward/lifting_object: 178.1404
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0911
          Episode_Reward/joint_vel: -0.0590
      Episode_Termination/time_out: 14.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 192184320
                    Iteration time: 2.79s
                      Time elapsed: 01:16:29
                               ETA: 00:21:21

################################################################################
                     [1m Learning iteration 1955/2500 [0m                     

                       Computation: 36233 steps/s (collection: 2.557s, learning 0.156s)
             Mean action noise std: 3.63
          Mean value_function loss: 94.4534
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 82.6107
                       Mean reward: 924.23
               Mean episode length: 244.47
    Episode_Reward/reaching_object: 1.9812
     Episode_Reward/lifting_object: 180.4559
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0927
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 15.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6250
--------------------------------------------------------------------------------
                   Total timesteps: 192282624
                    Iteration time: 2.71s
                      Time elapsed: 01:16:32
                               ETA: 00:21:19

################################################################################
                     [1m Learning iteration 1956/2500 [0m                     

                       Computation: 38955 steps/s (collection: 2.401s, learning 0.123s)
             Mean action noise std: 3.63
          Mean value_function loss: 102.5150
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 82.6149
                       Mean reward: 903.38
               Mean episode length: 237.63
    Episode_Reward/reaching_object: 1.9820
     Episode_Reward/lifting_object: 180.9853
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0928
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 192380928
                    Iteration time: 2.52s
                      Time elapsed: 01:16:34
                               ETA: 00:21:17

################################################################################
                     [1m Learning iteration 1957/2500 [0m                     

                       Computation: 43640 steps/s (collection: 2.128s, learning 0.125s)
             Mean action noise std: 3.63
          Mean value_function loss: 97.9127
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 82.6187
                       Mean reward: 899.79
               Mean episode length: 237.77
    Episode_Reward/reaching_object: 1.9795
     Episode_Reward/lifting_object: 180.4955
      Episode_Reward/object_height: 0.0168
        Episode_Reward/action_rate: -0.0925
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 192479232
                    Iteration time: 2.25s
                      Time elapsed: 01:16:37
                               ETA: 00:21:14

################################################################################
                     [1m Learning iteration 1958/2500 [0m                     

                       Computation: 42866 steps/s (collection: 2.170s, learning 0.124s)
             Mean action noise std: 3.63
          Mean value_function loss: 89.5044
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 82.6290
                       Mean reward: 890.69
               Mean episode length: 233.54
    Episode_Reward/reaching_object: 1.9685
     Episode_Reward/lifting_object: 180.0984
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 16.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 192577536
                    Iteration time: 2.29s
                      Time elapsed: 01:16:39
                               ETA: 00:21:12

################################################################################
                     [1m Learning iteration 1959/2500 [0m                     

                       Computation: 37730 steps/s (collection: 2.469s, learning 0.137s)
             Mean action noise std: 3.63
          Mean value_function loss: 108.5061
               Mean surrogate loss: 0.0041
                 Mean entropy loss: 82.6373
                       Mean reward: 924.24
               Mean episode length: 242.88
    Episode_Reward/reaching_object: 1.9535
     Episode_Reward/lifting_object: 178.8312
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0919
          Episode_Reward/joint_vel: -0.0599
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 192675840
                    Iteration time: 2.61s
                      Time elapsed: 01:16:41
                               ETA: 00:21:10

################################################################################
                     [1m Learning iteration 1960/2500 [0m                     

                       Computation: 41034 steps/s (collection: 2.271s, learning 0.125s)
             Mean action noise std: 3.63
          Mean value_function loss: 140.4032
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 82.6396
                       Mean reward: 938.35
               Mean episode length: 245.15
    Episode_Reward/reaching_object: 1.9575
     Episode_Reward/lifting_object: 179.6232
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0920
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 16.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0000
--------------------------------------------------------------------------------
                   Total timesteps: 192774144
                    Iteration time: 2.40s
                      Time elapsed: 01:16:44
                               ETA: 00:21:07

################################################################################
                     [1m Learning iteration 1961/2500 [0m                     

                       Computation: 37184 steps/s (collection: 2.497s, learning 0.147s)
             Mean action noise std: 3.63
          Mean value_function loss: 158.3344
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 82.6459
                       Mean reward: 890.23
               Mean episode length: 234.89
    Episode_Reward/reaching_object: 1.9111
     Episode_Reward/lifting_object: 175.0823
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0903
          Episode_Reward/joint_vel: -0.0589
      Episode_Termination/time_out: 13.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 3.0000
--------------------------------------------------------------------------------
                   Total timesteps: 192872448
                    Iteration time: 2.64s
                      Time elapsed: 01:16:47
                               ETA: 00:21:05

################################################################################
                     [1m Learning iteration 1962/2500 [0m                     

                       Computation: 39519 steps/s (collection: 2.387s, learning 0.101s)
             Mean action noise std: 3.63
          Mean value_function loss: 139.0455
               Mean surrogate loss: -0.0013
                 Mean entropy loss: 82.6579
                       Mean reward: 872.61
               Mean episode length: 230.04
    Episode_Reward/reaching_object: 1.9432
     Episode_Reward/lifting_object: 178.5871
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0916
          Episode_Reward/joint_vel: -0.0592
      Episode_Termination/time_out: 15.6250
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3750
--------------------------------------------------------------------------------
                   Total timesteps: 192970752
                    Iteration time: 2.49s
                      Time elapsed: 01:16:49
                               ETA: 00:21:03

################################################################################
                     [1m Learning iteration 1963/2500 [0m                     

                       Computation: 40431 steps/s (collection: 2.330s, learning 0.101s)
             Mean action noise std: 3.63
          Mean value_function loss: 125.6968
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 82.6740
                       Mean reward: 878.98
               Mean episode length: 231.18
    Episode_Reward/reaching_object: 1.9459
     Episode_Reward/lifting_object: 178.8536
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 15.2500
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 193069056
                    Iteration time: 2.43s
                      Time elapsed: 01:16:51
                               ETA: 00:21:01

################################################################################
                     [1m Learning iteration 1964/2500 [0m                     

                       Computation: 30594 steps/s (collection: 3.023s, learning 0.190s)
             Mean action noise std: 3.64
          Mean value_function loss: 125.9247
               Mean surrogate loss: -0.0019
                 Mean entropy loss: 82.6916
                       Mean reward: 921.09
               Mean episode length: 241.83
    Episode_Reward/reaching_object: 1.9140
     Episode_Reward/lifting_object: 175.7901
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0911
          Episode_Reward/joint_vel: -0.0594
      Episode_Termination/time_out: 14.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.3333
--------------------------------------------------------------------------------
                   Total timesteps: 193167360
                    Iteration time: 3.21s
                      Time elapsed: 01:16:55
                               ETA: 00:20:58

################################################################################
                     [1m Learning iteration 1965/2500 [0m                     

                       Computation: 31446 steps/s (collection: 2.953s, learning 0.174s)
             Mean action noise std: 3.64
          Mean value_function loss: 139.1621
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 82.6987
                       Mean reward: 885.84
               Mean episode length: 232.09
    Episode_Reward/reaching_object: 1.9490
     Episode_Reward/lifting_object: 179.5969
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0923
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.9583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4583
--------------------------------------------------------------------------------
                   Total timesteps: 193265664
                    Iteration time: 3.13s
                      Time elapsed: 01:16:58
                               ETA: 00:20:56

################################################################################
                     [1m Learning iteration 1966/2500 [0m                     

                       Computation: 27544 steps/s (collection: 3.340s, learning 0.229s)
             Mean action noise std: 3.64
          Mean value_function loss: 160.2946
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 82.7099
                       Mean reward: 843.73
               Mean episode length: 223.53
    Episode_Reward/reaching_object: 1.8832
     Episode_Reward/lifting_object: 173.5683
      Episode_Reward/object_height: 0.0154
        Episode_Reward/action_rate: -0.0894
          Episode_Reward/joint_vel: -0.0573
      Episode_Termination/time_out: 13.6667
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 2.8750
--------------------------------------------------------------------------------
                   Total timesteps: 193363968
                    Iteration time: 3.57s
                      Time elapsed: 01:17:01
                               ETA: 00:20:54

################################################################################
                     [1m Learning iteration 1967/2500 [0m                     

                       Computation: 35325 steps/s (collection: 2.678s, learning 0.104s)
             Mean action noise std: 3.64
          Mean value_function loss: 120.6056
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 82.7220
                       Mean reward: 857.73
               Mean episode length: 225.42
    Episode_Reward/reaching_object: 1.9214
     Episode_Reward/lifting_object: 177.1484
      Episode_Reward/object_height: 0.0157
        Episode_Reward/action_rate: -0.0910
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 14.1667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0833
--------------------------------------------------------------------------------
                   Total timesteps: 193462272
                    Iteration time: 2.78s
                      Time elapsed: 01:17:04
                               ETA: 00:20:52

################################################################################
                     [1m Learning iteration 1968/2500 [0m                     

                       Computation: 40219 steps/s (collection: 2.309s, learning 0.136s)
             Mean action noise std: 3.64
          Mean value_function loss: 131.6367
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 82.7308
                       Mean reward: 929.27
               Mean episode length: 243.26
    Episode_Reward/reaching_object: 1.9380
     Episode_Reward/lifting_object: 178.0725
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0916
          Episode_Reward/joint_vel: -0.0588
      Episode_Termination/time_out: 15.6667
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.4167
--------------------------------------------------------------------------------
                   Total timesteps: 193560576
                    Iteration time: 2.44s
                      Time elapsed: 01:17:07
                               ETA: 00:20:50

################################################################################
                     [1m Learning iteration 1969/2500 [0m                     

                       Computation: 41996 steps/s (collection: 2.213s, learning 0.128s)
             Mean action noise std: 3.64
          Mean value_function loss: 106.9090
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 82.7458
                       Mean reward: 889.12
               Mean episode length: 233.68
    Episode_Reward/reaching_object: 1.9408
     Episode_Reward/lifting_object: 177.9644
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0920
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 193658880
                    Iteration time: 2.34s
                      Time elapsed: 01:17:09
                               ETA: 00:20:47

################################################################################
                     [1m Learning iteration 1970/2500 [0m                     

                       Computation: 41592 steps/s (collection: 2.238s, learning 0.125s)
             Mean action noise std: 3.64
          Mean value_function loss: 122.4472
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 82.7566
                       Mean reward: 927.41
               Mean episode length: 243.09
    Episode_Reward/reaching_object: 1.9716
     Episode_Reward/lifting_object: 181.1572
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 17.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.0417
--------------------------------------------------------------------------------
                   Total timesteps: 193757184
                    Iteration time: 2.36s
                      Time elapsed: 01:17:11
                               ETA: 00:20:45

################################################################################
                     [1m Learning iteration 1971/2500 [0m                     

                       Computation: 39490 steps/s (collection: 2.340s, learning 0.149s)
             Mean action noise std: 3.64
          Mean value_function loss: 124.7217
               Mean surrogate loss: -0.0012
                 Mean entropy loss: 82.7646
                       Mean reward: 863.26
               Mean episode length: 227.53
    Episode_Reward/reaching_object: 1.9442
     Episode_Reward/lifting_object: 177.7802
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0919
          Episode_Reward/joint_vel: -0.0587
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 2.2500
--------------------------------------------------------------------------------
                   Total timesteps: 193855488
                    Iteration time: 2.49s
                      Time elapsed: 01:17:14
                               ETA: 00:20:43

################################################################################
                     [1m Learning iteration 1972/2500 [0m                     

                       Computation: 42191 steps/s (collection: 2.236s, learning 0.094s)
             Mean action noise std: 3.65
          Mean value_function loss: 76.6790
               Mean surrogate loss: -0.0035
                 Mean entropy loss: 82.7758
                       Mean reward: 945.43
               Mean episode length: 248.17
    Episode_Reward/reaching_object: 1.9754
     Episode_Reward/lifting_object: 180.5864
      Episode_Reward/object_height: 0.0160
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.0593
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 193953792
                    Iteration time: 2.33s
                      Time elapsed: 01:17:16
                               ETA: 00:20:40

################################################################################
                     [1m Learning iteration 1973/2500 [0m                     

                       Computation: 43197 steps/s (collection: 2.172s, learning 0.104s)
             Mean action noise std: 3.65
          Mean value_function loss: 105.2127
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 82.7879
                       Mean reward: 875.23
               Mean episode length: 231.06
    Episode_Reward/reaching_object: 1.9552
     Episode_Reward/lifting_object: 177.8925
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0921
          Episode_Reward/joint_vel: -0.0598
      Episode_Termination/time_out: 15.8750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 194052096
                    Iteration time: 2.28s
                      Time elapsed: 01:17:18
                               ETA: 00:20:38

################################################################################
                     [1m Learning iteration 1974/2500 [0m                     

                       Computation: 42517 steps/s (collection: 2.198s, learning 0.114s)
             Mean action noise std: 3.65
          Mean value_function loss: 89.1540
               Mean surrogate loss: -0.0028
                 Mean entropy loss: 82.7996
                       Mean reward: 914.30
               Mean episode length: 240.42
    Episode_Reward/reaching_object: 1.9940
     Episode_Reward/lifting_object: 181.3190
      Episode_Reward/object_height: 0.0159
        Episode_Reward/action_rate: -0.0939
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 14.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 194150400
                    Iteration time: 2.31s
                      Time elapsed: 01:17:21
                               ETA: 00:20:36

################################################################################
                     [1m Learning iteration 1975/2500 [0m                     

                       Computation: 45030 steps/s (collection: 2.091s, learning 0.093s)
             Mean action noise std: 3.65
          Mean value_function loss: 115.5070
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 82.8135
                       Mean reward: 907.84
               Mean episode length: 238.78
    Episode_Reward/reaching_object: 1.9748
     Episode_Reward/lifting_object: 179.1833
      Episode_Reward/object_height: 0.0158
        Episode_Reward/action_rate: -0.0930
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 15.3750
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 194248704
                    Iteration time: 2.18s
                      Time elapsed: 01:17:23
                               ETA: 00:20:33

################################################################################
                     [1m Learning iteration 1976/2500 [0m                     

                       Computation: 43779 steps/s (collection: 2.124s, learning 0.122s)
             Mean action noise std: 3.65
          Mean value_function loss: 68.4507
               Mean surrogate loss: -0.0018
                 Mean entropy loss: 82.8280
                       Mean reward: 902.01
               Mean episode length: 236.51
    Episode_Reward/reaching_object: 1.9855
     Episode_Reward/lifting_object: 179.9396
      Episode_Reward/object_height: 0.0162
        Episode_Reward/action_rate: -0.0932
          Episode_Reward/joint_vel: -0.0596
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 194347008
                    Iteration time: 2.25s
                      Time elapsed: 01:17:25
                               ETA: 00:20:31

################################################################################
                     [1m Learning iteration 1977/2500 [0m                     

                       Computation: 42496 steps/s (collection: 2.181s, learning 0.133s)
             Mean action noise std: 3.65
          Mean value_function loss: 93.1519
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 82.8364
                       Mean reward: 876.95
               Mean episode length: 231.44
    Episode_Reward/reaching_object: 1.9759
     Episode_Reward/lifting_object: 178.5213
      Episode_Reward/object_height: 0.0163
        Episode_Reward/action_rate: -0.0928
          Episode_Reward/joint_vel: -0.0584
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 194445312
                    Iteration time: 2.31s
                      Time elapsed: 01:17:27
                               ETA: 00:20:28

################################################################################
                     [1m Learning iteration 1978/2500 [0m                     

                       Computation: 44745 steps/s (collection: 2.099s, learning 0.098s)
             Mean action noise std: 3.65
          Mean value_function loss: 76.8067
               Mean surrogate loss: -0.0021
                 Mean entropy loss: 82.8520
                       Mean reward: 918.76
               Mean episode length: 240.57
    Episode_Reward/reaching_object: 1.9902
     Episode_Reward/lifting_object: 179.5472
      Episode_Reward/object_height: 0.0164
        Episode_Reward/action_rate: -0.0933
          Episode_Reward/joint_vel: -0.0591
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 194543616
                    Iteration time: 2.20s
                      Time elapsed: 01:17:30
                               ETA: 00:20:26

################################################################################
                     [1m Learning iteration 1979/2500 [0m                     

                       Computation: 44873 steps/s (collection: 2.084s, learning 0.107s)
             Mean action noise std: 3.66
          Mean value_function loss: 85.7899
               Mean surrogate loss: -0.0027
                 Mean entropy loss: 82.8611
                       Mean reward: 917.10
               Mean episode length: 240.25
    Episode_Reward/reaching_object: 2.0141
     Episode_Reward/lifting_object: 181.3999
      Episode_Reward/object_height: 0.0165
        Episode_Reward/action_rate: -0.0941
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 16.0000
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.5000
--------------------------------------------------------------------------------
                   Total timesteps: 194641920
                    Iteration time: 2.19s
                      Time elapsed: 01:17:32
                               ETA: 00:20:24

################################################################################
                     [1m Learning iteration 1980/2500 [0m                     

                       Computation: 40262 steps/s (collection: 2.304s, learning 0.138s)
             Mean action noise std: 3.66
          Mean value_function loss: 99.1753
               Mean surrogate loss: -0.0002
                 Mean entropy loss: 82.8714
                       Mean reward: 910.06
               Mean episode length: 239.17
    Episode_Reward/reaching_object: 1.9822
     Episode_Reward/lifting_object: 178.0338
      Episode_Reward/object_height: 0.0161
        Episode_Reward/action_rate: -0.0925
          Episode_Reward/joint_vel: -0.0586
      Episode_Termination/time_out: 16.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7917
--------------------------------------------------------------------------------
                   Total timesteps: 194740224
                    Iteration time: 2.44s
                      Time elapsed: 01:17:34
                               ETA: 00:20:21

################################################################################
                     [1m Learning iteration 1981/2500 [0m                     

                       Computation: 42097 steps/s (collection: 2.213s, learning 0.123s)
             Mean action noise std: 3.66
          Mean value_function loss: 83.2609
               Mean surrogate loss: -0.0016
                 Mean entropy loss: 82.8761
                       Mean reward: 909.10
               Mean episode length: 238.67
    Episode_Reward/reaching_object: 2.0285
     Episode_Reward/lifting_object: 181.9228
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0944
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 17.1250
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.4167
--------------------------------------------------------------------------------
                   Total timesteps: 194838528
                    Iteration time: 2.34s
                      Time elapsed: 01:17:37
                               ETA: 00:20:19

################################################################################
                     [1m Learning iteration 1982/2500 [0m                     

                       Computation: 45033 steps/s (collection: 2.063s, learning 0.120s)
             Mean action noise std: 3.66
          Mean value_function loss: 76.1134
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 82.8824
                       Mean reward: 881.34
               Mean episode length: 232.33
    Episode_Reward/reaching_object: 2.0223
     Episode_Reward/lifting_object: 180.8471
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0943
          Episode_Reward/joint_vel: -0.0601
      Episode_Termination/time_out: 15.0000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1667
--------------------------------------------------------------------------------
                   Total timesteps: 194936832
                    Iteration time: 2.18s
                      Time elapsed: 01:17:39
                               ETA: 00:20:17

################################################################################
                     [1m Learning iteration 1983/2500 [0m                     

                       Computation: 45851 steps/s (collection: 2.034s, learning 0.110s)
             Mean action noise std: 3.66
          Mean value_function loss: 114.2383
               Mean surrogate loss: -0.0017
                 Mean entropy loss: 82.8908
                       Mean reward: 917.20
               Mean episode length: 240.58
    Episode_Reward/reaching_object: 2.0211
     Episode_Reward/lifting_object: 181.1088
      Episode_Reward/object_height: 0.0167
        Episode_Reward/action_rate: -0.0939
          Episode_Reward/joint_vel: -0.0597
      Episode_Termination/time_out: 16.2083
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5417
--------------------------------------------------------------------------------
                   Total timesteps: 195035136
                    Iteration time: 2.14s
                      Time elapsed: 01:17:41
                               ETA: 00:20:14

################################################################################
                     [1m Learning iteration 1984/2500 [0m                     

                       Computation: 45490 steps/s (collection: 2.054s, learning 0.107s)
             Mean action noise std: 3.66
          Mean value_function loss: 92.6322
               Mean surrogate loss: -0.0026
                 Mean entropy loss: 82.8986
                       Mean reward: 891.78
               Mean episode length: 234.69
    Episode_Reward/reaching_object: 2.0063
     Episode_Reward/lifting_object: 178.8983
      Episode_Reward/object_height: 0.0166
        Episode_Reward/action_rate: -0.0934
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.0833
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.6667
--------------------------------------------------------------------------------
                   Total timesteps: 195133440
                    Iteration time: 2.16s
                      Time elapsed: 01:17:43
                               ETA: 00:20:12

################################################################################
                     [1m Learning iteration 1985/2500 [0m                     

                       Computation: 44656 steps/s (collection: 2.087s, learning 0.115s)
             Mean action noise std: 3.66
          Mean value_function loss: 70.1588
               Mean surrogate loss: -0.0020
                 Mean entropy loss: 82.9112
                       Mean reward: 910.92
               Mean episode length: 239.13
    Episode_Reward/reaching_object: 2.0438
     Episode_Reward/lifting_object: 182.8322
      Episode_Reward/object_height: 0.0172
        Episode_Reward/action_rate: -0.0949
          Episode_Reward/joint_vel: -0.0604
      Episode_Termination/time_out: 14.7500
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 195231744
                    Iteration time: 2.20s
                      Time elapsed: 01:17:45
                               ETA: 00:20:09

################################################################################
                     [1m Learning iteration 1986/2500 [0m                     

                       Computation: 45419 steps/s (collection: 2.048s, learning 0.117s)
             Mean action noise std: 3.66
          Mean value_function loss: 76.0287
               Mean surrogate loss: 0.0008
                 Mean entropy loss: 82.9265
                       Mean reward: 889.74
               Mean episode length: 234.10
    Episode_Reward/reaching_object: 2.0397
     Episode_Reward/lifting_object: 182.4957
      Episode_Reward/object_height: 0.0173
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.0607
      Episode_Termination/time_out: 15.2083
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2083
--------------------------------------------------------------------------------
                   Total timesteps: 195330048
                    Iteration time: 2.16s
                      Time elapsed: 01:17:47
                               ETA: 00:20:07

################################################################################
                     [1m Learning iteration 1987/2500 [0m                     

                       Computation: 45702 steps/s (collection: 2.038s, learning 0.113s)
             Mean action noise std: 3.66
          Mean value_function loss: 80.5986
               Mean surrogate loss: -0.0033
                 Mean entropy loss: 82.9311
                       Mean reward: 913.97
               Mean episode length: 239.54
    Episode_Reward/reaching_object: 2.0181
     Episode_Reward/lifting_object: 180.2818
      Episode_Reward/object_height: 0.0171
        Episode_Reward/action_rate: -0.0942
          Episode_Reward/joint_vel: -0.0600
      Episode_Termination/time_out: 15.4583
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 195428352
                    Iteration time: 2.15s
                      Time elapsed: 01:17:50
                               ETA: 00:20:05

################################################################################
                     [1m Learning iteration 1988/2500 [0m                     

                       Computation: 45342 steps/s (collection: 2.048s, learning 0.120s)
             Mean action noise std: 3.67
          Mean value_function loss: 83.2443
               Mean surrogate loss: -0.0031
                 Mean entropy loss: 82.9394
                       Mean reward: 905.10
               Mean episode length: 237.59
    Episode_Reward/reaching_object: 2.0296
     Episode_Reward/lifting_object: 181.3672
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0944
          Episode_Reward/joint_vel: -0.0605
      Episode_Termination/time_out: 14.6667
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.2500
--------------------------------------------------------------------------------
                   Total timesteps: 195526656
                    Iteration time: 2.17s
                      Time elapsed: 01:17:52
                               ETA: 00:20:02

################################################################################
                     [1m Learning iteration 1989/2500 [0m                     

                       Computation: 45362 steps/s (collection: 2.048s, learning 0.120s)
             Mean action noise std: 3.67
          Mean value_function loss: 75.8066
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 82.9493
                       Mean reward: 898.64
               Mean episode length: 235.77
    Episode_Reward/reaching_object: 2.0175
     Episode_Reward/lifting_object: 180.4650
      Episode_Reward/object_height: 0.0175
        Episode_Reward/action_rate: -0.0942
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 16.6250
Episode_Termination/object_dropping: 0.0833
     Episode_Termination/robot_out: 1.3333
--------------------------------------------------------------------------------
                   Total timesteps: 195624960
                    Iteration time: 2.17s
                      Time elapsed: 01:17:54
                               ETA: 00:20:00

################################################################################
                     [1m Learning iteration 1990/2500 [0m                     

                       Computation: 44055 steps/s (collection: 2.104s, learning 0.128s)
             Mean action noise std: 3.67
          Mean value_function loss: 92.3663
               Mean surrogate loss: -0.0025
                 Mean entropy loss: 82.9582
                       Mean reward: 926.28
               Mean episode length: 242.34
    Episode_Reward/reaching_object: 2.0075
     Episode_Reward/lifting_object: 179.0380
      Episode_Reward/object_height: 0.0176
        Episode_Reward/action_rate: -0.0938
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 15.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.7500
--------------------------------------------------------------------------------
                   Total timesteps: 195723264
                    Iteration time: 2.23s
                      Time elapsed: 01:17:56
                               ETA: 00:19:57

################################################################################
                     [1m Learning iteration 1991/2500 [0m                     

                       Computation: 44863 steps/s (collection: 2.092s, learning 0.099s)
             Mean action noise std: 3.67
          Mean value_function loss: 79.3526
               Mean surrogate loss: -0.0034
                 Mean entropy loss: 82.9664
                       Mean reward: 913.88
               Mean episode length: 239.90
    Episode_Reward/reaching_object: 2.0241
     Episode_Reward/lifting_object: 180.2852
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.0606
      Episode_Termination/time_out: 17.5000
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.4583
--------------------------------------------------------------------------------
                   Total timesteps: 195821568
                    Iteration time: 2.19s
                      Time elapsed: 01:17:58
                               ETA: 00:19:55

################################################################################
                     [1m Learning iteration 1992/2500 [0m                     

                       Computation: 44139 steps/s (collection: 2.112s, learning 0.115s)
             Mean action noise std: 3.67
          Mean value_function loss: 63.4262
               Mean surrogate loss: -0.0004
                 Mean entropy loss: 82.9748
                       Mean reward: 928.44
               Mean episode length: 243.13
    Episode_Reward/reaching_object: 2.0476
     Episode_Reward/lifting_object: 182.4229
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0955
          Episode_Reward/joint_vel: -0.0614
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.0000
--------------------------------------------------------------------------------
                   Total timesteps: 195919872
                    Iteration time: 2.23s
                      Time elapsed: 01:18:01
                               ETA: 00:19:53

################################################################################
                     [1m Learning iteration 1993/2500 [0m                     

                       Computation: 44485 steps/s (collection: 2.094s, learning 0.116s)
             Mean action noise std: 3.67
          Mean value_function loss: 80.1631
               Mean surrogate loss: -0.0022
                 Mean entropy loss: 82.9787
                       Mean reward: 912.90
               Mean episode length: 239.23
    Episode_Reward/reaching_object: 2.0288
     Episode_Reward/lifting_object: 180.3862
      Episode_Reward/object_height: 0.0181
        Episode_Reward/action_rate: -0.0946
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 14.7917
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.3750
--------------------------------------------------------------------------------
                   Total timesteps: 196018176
                    Iteration time: 2.21s
                      Time elapsed: 01:18:03
                               ETA: 00:19:50

################################################################################
                     [1m Learning iteration 1994/2500 [0m                     

                       Computation: 41677 steps/s (collection: 2.203s, learning 0.156s)
             Mean action noise std: 3.67
          Mean value_function loss: 62.5249
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 82.9884
                       Mean reward: 908.62
               Mean episode length: 238.60
    Episode_Reward/reaching_object: 2.0307
     Episode_Reward/lifting_object: 180.4181
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0949
          Episode_Reward/joint_vel: -0.0615
      Episode_Termination/time_out: 16.0833
Episode_Termination/object_dropping: 0.0417
     Episode_Termination/robot_out: 1.1250
--------------------------------------------------------------------------------
                   Total timesteps: 196116480
                    Iteration time: 2.36s
                      Time elapsed: 01:18:05
                               ETA: 00:19:48

################################################################################
                     [1m Learning iteration 1995/2500 [0m                     

                       Computation: 41610 steps/s (collection: 2.228s, learning 0.135s)
             Mean action noise std: 3.67
          Mean value_function loss: 69.8079
               Mean surrogate loss: -0.0024
                 Mean entropy loss: 83.0004
                       Mean reward: 921.90
               Mean episode length: 243.42
    Episode_Reward/reaching_object: 2.0423
     Episode_Reward/lifting_object: 181.5888
      Episode_Reward/object_height: 0.0183
        Episode_Reward/action_rate: -0.0953
          Episode_Reward/joint_vel: -0.0610
      Episode_Termination/time_out: 14.8333
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 196214784
                    Iteration time: 2.36s
                      Time elapsed: 01:18:08
                               ETA: 00:19:46

################################################################################
                     [1m Learning iteration 1996/2500 [0m                     

                       Computation: 44028 steps/s (collection: 2.141s, learning 0.092s)
             Mean action noise std: 3.67
          Mean value_function loss: 58.5192
               Mean surrogate loss: -0.0032
                 Mean entropy loss: 83.0127
                       Mean reward: 928.83
               Mean episode length: 243.09
    Episode_Reward/reaching_object: 2.0432
     Episode_Reward/lifting_object: 181.8165
      Episode_Reward/object_height: 0.0182
        Episode_Reward/action_rate: -0.0953
          Episode_Reward/joint_vel: -0.0612
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.0417
--------------------------------------------------------------------------------
                   Total timesteps: 196313088
                    Iteration time: 2.23s
                      Time elapsed: 01:18:10
                               ETA: 00:19:43

################################################################################
                     [1m Learning iteration 1997/2500 [0m                     

                       Computation: 45323 steps/s (collection: 2.070s, learning 0.099s)
             Mean action noise std: 3.67
          Mean value_function loss: 109.5585
               Mean surrogate loss: -0.0011
                 Mean entropy loss: 83.0193
                       Mean reward: 905.93
               Mean episode length: 237.99
    Episode_Reward/reaching_object: 2.0243
     Episode_Reward/lifting_object: 179.8262
      Episode_Reward/object_height: 0.0177
        Episode_Reward/action_rate: -0.0943
          Episode_Reward/joint_vel: -0.0602
      Episode_Termination/time_out: 14.5417
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.5833
--------------------------------------------------------------------------------
                   Total timesteps: 196411392
                    Iteration time: 2.17s
                      Time elapsed: 01:18:12
                               ETA: 00:19:41

################################################################################
                     [1m Learning iteration 1998/2500 [0m                     

                       Computation: 45361 steps/s (collection: 2.057s, learning 0.110s)
             Mean action noise std: 3.68
          Mean value_function loss: 78.2592
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 83.0264
                       Mean reward: 913.28
               Mean episode length: 238.80
    Episode_Reward/reaching_object: 2.0381
     Episode_Reward/lifting_object: 181.0687
      Episode_Reward/object_height: 0.0178
        Episode_Reward/action_rate: -0.0947
          Episode_Reward/joint_vel: -0.0595
      Episode_Termination/time_out: 15.4167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 196509696
                    Iteration time: 2.17s
                      Time elapsed: 01:18:14
                               ETA: 00:19:38

################################################################################
                     [1m Learning iteration 1999/2500 [0m                     

                       Computation: 45694 steps/s (collection: 2.039s, learning 0.112s)
             Mean action noise std: 3.68
          Mean value_function loss: 66.7107
               Mean surrogate loss: -0.0030
                 Mean entropy loss: 83.0425
                       Mean reward: 911.25
               Mean episode length: 238.83
    Episode_Reward/reaching_object: 2.0562
     Episode_Reward/lifting_object: 182.9393
      Episode_Reward/object_height: 0.0179
        Episode_Reward/action_rate: -0.0957
          Episode_Reward/joint_vel: -0.0620
      Episode_Termination/time_out: 15.9167
Episode_Termination/object_dropping: 0.0000
     Episode_Termination/robot_out: 1.2917
--------------------------------------------------------------------------------
                   Total timesteps: 196608000
                    Iteration time: 2.15s
                      Time elapsed: 01:18:16
                               ETA: 00:19:36

